{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c53dd7",
   "metadata": {},
   "source": [
    "## Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af044ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name = \"Paul Iusztin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0985b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_engineering.application import utils\n",
    "from llm_engineering.domain.documents import UserDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196497ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Paul', 'Iusztin')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name, last_name = utils.split_user_full_name(full_name)\n",
    "first_name, last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cff455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserDocument(id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), first_name='Paul', last_name='Iusztin')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db1247c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b5fa1f08-75f0-402d-8e88-d1357e346d9e'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = str(user.id)\n",
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc0ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from llm_engineering.domain.base.nosql import NoSQLBaseDocument\n",
    "from llm_engineering.domain.documents import ArticleDocument, Document, PostDocument, RepositoryDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a7a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __fetch_articles(user_id) -> list[NoSQLBaseDocument]:\n",
    "    return ArticleDocument.bulk_find(author_id=user_id)\n",
    "\n",
    "\n",
    "def __fetch_posts(user_id) -> list[NoSQLBaseDocument]:\n",
    "    return PostDocument.bulk_find(author_id=user_id)\n",
    "\n",
    "\n",
    "def __fetch_repositories(user_id) -> list[NoSQLBaseDocument]:\n",
    "    return RepositoryDocument.bulk_find(author_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85aa880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'posts'\n",
      "Query: 'repositories'\n",
      "Query: 'articles'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'posts': [],\n",
       " 'repositories': [],\n",
       " 'articles': [ArticleDocument(id=UUID('34978aea-e179-44b5-975c-7deb64456380'), content={'Title': 'An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin', 'Subtitle': 'From data gathering to productionizing LLMs using LLMOps good practices.', 'Content': \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.1K13ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLet‚Äôs start with Lesson 1 ‚Üì‚Üì‚ÜìLesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component‚Äôs code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask‚Ä¶When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ‚Üì‚Üì‚ÜìFirst, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: ‚ÄúWrite me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].‚ÄùExcited? Let‚Äôs get started üî•2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios‚Ä¶‚Ä¶the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution‚Üí The 3-pipeline architectureLet‚Äôs understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline‚Ä¶also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that‚Ä¶‚Ä¶by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.‚Ü≥ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet‚Äôs understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesn‚Äôt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let‚Äôs say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let‚Äôs zoom in on each component to understand how they work individually and interact with each other. ‚Üì‚Üì‚Üì3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.üîó 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people‚Äôs data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.üîó 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn‚Äôt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes ‚Äî store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren‚Äôt scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust‚Äôs impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won‚Äôt focus too much on any specific LLM.All the experiments will be logged into Comet ML‚Äôs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet‚Äôs experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet‚Äôs prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet‚Äôs prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èHave you enjoyed this article? Then‚Ä¶‚Üì‚Üì‚ÜìJoin 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferences[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps: Metadata storeIntroductionJul 17ListsAI Regulation6 stories¬∑593 savesNatural Language Processing1766 stories¬∑1367 savesPredictive Modeling w/ Python20 stories¬∑1607 savesPractical Guides to Machine Learning10 stories¬∑1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f'),\n",
       "  ArticleDocument(id=UUID('d331f23e-88c6-4606-b397-52842c9a6295'), content={'Title': 'A Real-time Retrieval System for RAG on Social Media Data', 'Subtitle': 'Use a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.', 'Content': 'Real-time Retrieval for RAG on Social Media Data | Decoding MLOpen in appSign upSign inWriteSign upSign inA Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑12 min read¬∑Mar 30, 2024358ListenShareImage by DALL-EIn this article, you will learn how to build a real-time retrieval system for social media data. In our example, we will use only my LinkedIn posts, but our implementation can easily be extended to other platforms supporting written content, such as X, Instagram, or Medium.In this article, you will learn how to:build a streaming pipeline that ingests LinkedIn posts into a vector DB in real-timeclean, chunk, and embed LinkedIn postsbuild a retrieval client to query LinkedIn postsuse a rerank pattern to improve retrieval accuracyvisualize content retrieved for a given query in a 2D plot using UMAPOur implementation focuses on just the retrieval part of an RAG system. But you can quickly hook the retrieved LinkedIn posts to an LLM for post analysis or personalized content generation.Table of Contents:System DesignDataStreaming ingestion pipelineRetrieval clientConclusion1. System DesignThe retrieval system is based on 2 detached components:the streaming ingestion pipelinethe retrieval clientThe architecture of the retrieval system [Image by the Author ‚Äî in collaboration with VectorHub].The streaming ingestion pipeline runs 24/7 to keep the vector DB synced up with current raw LinkedIn posts data source, while the retrieval client is used in RAG applications to query the vector DB. These 2 components communicate with each other only through the vector DB.1.1. The streaming ingestion pipelineThe streaming ingestion pipeline implements the Change Data Capture (CDC) pattern between a data source containing the raw LinkedIn posts and the vector DB used for retrieval.In a real-world scenario, the streaming pipeline listens to a queue populated by all the changes made to the source database. But because we are focusing primarily on the retrieval system, we simulate the data within the queue with a couple of JSON files.The streaming pipeline is built in Python using Bytewax, and cleans, chunks, and embeds the LinkedIn posts before loading them into a Qdrant vector DB.Why do we need a stream engine?Because LinkedIn posts (or any other social media data) evolve frequently, your vector DB can quickly get out of sync. To handle this, you can build a batch pipeline that runs every minute. But to really minimize data lag, to make sure your vector DB stays current with new social media posts, you need to use a streaming pipeline that immediately takes every new item the moment it‚Äôs posted, preprocesses it, and loads it into the vector DB.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines the impressive speed and reliability of Rust with the ease of use and ecosystem of Python.1.2. The retrieval clientOur retrieval client is a standard Python module that preprocesses user queries and searches the vector DB for most similar results. Qdrant vector DB lets us decouple the retrieval client from the streaming ingestion pipeline.Using a semantic-based retrieval system lets us query our LinkedIn post collection very flexibly. For example, we can retrieve similar posts using a variety of query types ‚Äî e.g., posts, questions, sentences.Also, to improve the retrieval system‚Äôs accuracy, we use a rerank pattern.Lastly, to better understand and explain the retrieval process for particular queries, we visualize our results on a 2D plot using UMAP.2. DataWe will ingest 215 LinkedIn posts from my Linked profile ‚Äî Paul Iusztin. Though we simulate the post ingestion step using JSON files, the posts themselves are authentic.Before diving into the code, let‚Äôs take a look at an example LinkedIn post to familiarize ourselves with the challenges it will introduce ‚Üì[    {        \"text\": \"ùó™ùóµùóÆùòÅ do you need to ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ an open-source ùóüùóüùó† to create your own ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùó±ùòÉùó∂ùòÄùóºùóø?\\\\nThis is the ùóüùóüùó† ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùó∏ùó∂ùòÅ you must know ‚Üì\\\\nùóóùóÆùòÅùóÆùòÄùó≤ùòÅ\\\\nThe key component of any successful ML project is the data.\\\\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\\\\nThe best approach is to hire a bunch of experts to create it manually.\\\\nBut, for a PoC, that might get expensive & slow.\\\\nThe good news is that a method called \\\\\"ùòçùò™ùòØùò¶ùòµùò∂ùòØùò™ùòØùò® ùò∏ùò™ùòµùò© ùò•ùò™ùò¥ùòµùò™ùò≠ùò≠ùò¢ùòµùò™ùò∞ùòØ\\\\\" exists.\\\\n ...Along with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\\\\n‚Ü≥ Beam: üîó\\\\nhttps://lnkd.in/dedCaMDh\\\\n.\\\\n‚Ü≥ To see all these components in action, check out my FREE ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ ùó∞ùóºùòÇùóøùòÄùó≤ & give it a ‚≠ê:  üîó\\\\nhttps://lnkd.in/dZgqtf8f\\\\nhashtag\\\\n#\\\\nmachinelearning\\\\nhashtag\\\\n#\\\\nmlops\\\\nhashtag\\\\n#\\\\ndatascience\",        \"image\": \"https://media.licdn.com/dms/image/D4D10AQHWQzZcToQQ1Q/image-shrink_800/0/1698388219549?e=1705082400&v=beta&t=9mrDC_NooJgD7u7Qk0PmrTGGaZtuwDIFKh3bEqeBsm0\"    }]The following features of the above post are not compatible with embedding models. We‚Äôll need to find some way of handling them in our preprocessing step:emojisbold, italic textother non-ASCII charactersURLscontent that exceeds the context window limit of the embedding modelEmojis and bolded and italic text are represented by Unicode characters that are not available in the vocabulary of the embedding model. Thus, these items cannot be tokenized and passed to the model; we have to remove them or normalize them to something that can be parsed by the tokenizer. The same holds true for all other non-ASCII characters.URLs take up space in the context window without providing much semantic value. Still, knowing that there‚Äôs a URL in the sentence may add context. For this reason, we replace all URLs with a [URL] token. This lets us ingest whatever value the URL‚Äôs presence conveys without it taking up valuable space.3. Streaming ingestion pipelineLet‚Äôs dive into the streaming pipeline, starting from the top and working our way to the bottom ‚Üì3.1. The Bytewax flowThe Bytewax flow transparently conveys all the steps of the streaming pipeline.The first step is ingesting every LinkedIn post from our JSON files. In the next steps, every map operation has a single responsibility:validate the ingested data using a RawPost pydantic modelclean the postschunk the posts; because chunking will output a list of ChunkedPost objects, we use a flat_map operation to flatten them outembed the postsload the posts to a Qdrant vector DBdef build_flow():    embedding_model = EmbeddingModelSingleton()    flow = Dataflow(\"flow\")    stream = op.input(\"input\", flow, JSONSource([\"data/paul.json\"]))    stream = op.map(\"raw_post\", stream, RawPost.from_source)    stream = op.map(\"cleaned_post\", stream, CleanedPost.from_raw_post)    stream = op.flat_map(        \"chunked_post\",        stream,        lambda cleaned_post: ChunkedPost.from_cleaned_post(            cleaned_post, embedding_model=embedding_model        ),    )    stream = op.map(        \"embedded_chunked_post\",        stream,        lambda chunked_post: EmbeddedChunkedPost.from_chunked_post(            chunked_post, embedding_model=embedding_model        ),    )    op.inspect(\"inspect\", stream, print)    op.output(        \"output\", stream, QdrantVectorOutput(vector_size=model.embedding_size)    )        return flow3.2. The processing stepsEvery processing step is incorporated into a pydantic model. This way, we can easily validate the data at each step and reuse the code in the retrieval module.We isolate every step of an ingestion pipeline into its own class:cleaningchunkingembeddingDoing so, we follow the separation of concerns good SWE practice. Thus, every class has its own responsibility.Now the code is easy to read and understand. Also, it‚Äôs future-proof, as it‚Äôs extremely easy to change or extend either of the 3 steps: cleaning, chunking and embedding.Here is the interface of the pydantic models:class RawPost(BaseModel):    post_id: str    text: str    image: Optional[str]    @classmethod    def from_source(cls, k_v: Tuple[str, dict]) -> \"RawPost\":        ... # Mapping a dictionary to a RawPost validated pydantic model.        return cls(...)class CleanedPost(BaseModel):    post_id: str    raw_text: str    text: str    image: Optional[str]    @classmethod    def from_raw_post(cls, raw_post: RawPost) -> \"CleanedPost\":        ... # Cleaning the raw post        return cls(...)class ChunkedPost(BaseModel):    post_id: str    chunk_id: str    full_raw_text: str    text: str    image: Optional[str]    @classmethod    def from_cleaned_post(        cls, cleaned_post: CleanedPost, embedding_model: EmbeddingModelSingleton    ) -> list[\"ChunkedPost\"]:        chunks = ... # Compute chunks        return [cls(...) for chunk in chunks]class EmbeddedChunkedPost(BaseModel):    post_id: str    chunk_id: str    full_raw_text: str    text: str    text_embedding: list    image: Optional[str] = None    score: Optional[float] = None    rerank_score: Optional[float] = None    @classmethod    def from_chunked_post(        cls, chunked_post: ChunkedPost, embedding_model: EmbeddingModelSingleton    ) -> \"EmbeddedChunkedPost\":        ... # Compute embedding.        return cls(...)Now, the data at each step is validated and has a clear structure.Note: Providing different types when instantiating a pydantic model will throw a validation error. For example, if the post_id is defined as a string, and we try to instantiate an EmbeddedChunkedPost with a None or int post_id, it will throw an error.Check out the full implementation on our üîó GitHub Articles Hub repository.3.3. Load to QdrantTo load the LinkedIn posts to Qdrant, you have to override Bytewax‚Äôs StatelessSinkPartition class (which acts as an output in a Bytewax flow):class QdrantVectorSink(StatelessSinkPartition):    def __init__(        self,        client: QdrantClient,        collection_name: str    ):        self._client = client        self._collection_name = collection_name    def write_batch(self, chunks: list[EmbeddedChunkedPost]):        ... # Map chunks to ids, embeddings, and metadata.        self._client.upsert(            collection_name=self._collection_name,            points=Batch(                ids=ids,                vectors=embeddings,                payloads=metadata,            ),        )Within this class, you must overwrite the write_batch() method, where we will serialize every EmbeddedChunkedPost to a format expected by Qdrant and load it to the vector DB.4. Retrieval clientHere, we focus on preprocessing a user‚Äôs query, searching the vector DB, and postprocessing the retrieved posts for maximum results.To design the retrieval step, we implement a QdrantVectorDBRetriever class to expose all the necessary features for our retrieval client.class QdrantVectorDBRetriever:    def __init__(        self,        embedding_model: EmbeddingModelSingleton,        vector_db_client: QdrantClient,        cross_encoder_model: CrossEncoderModelSingleton        vector_db_collection: str    ):        self._embedding_model = embedding_model        self._vector_db_client = vector_db_client        self._cross_encoder_model = cross_encoder_model        self._vector_db_collection = vector_db_collection    def search(        self, query: str, limit: int = 3, return_all: bool = False    ) -> Union[list[EmbeddedChunkedPost], dict[str, list]]:        ... # Search the Qdrant vector DB based on the given query.    def embed_query(self, query: str) -> list[list[float]]:        ... # Embed the given query.    def rerank(self, query: str, posts: list[EmbeddedChunkedPost]) -> list[EmbeddedChunkedPost]:        ... # Rerank the posts relative to the given query.    def render_as_html(self, post: EmbeddedChunkedPost) -> None:        ... # Map the embedded post to HTML to display it.4.1. Embed queryWe must embed the query in precisely the same way we ingested our posts into the vector DB. Because the streaming pipeline is written in Python (thanks to Bytewax), and every preprocessing operation is modular, we can quickly replicate all the steps necessary to embed the query.class QdrantVectorDBRetriever:    ...    def embed_query(self, query: str) -> list[list[float]]:        cleaned_query = CleanedPost.clean(query)        chunks = ChunkedPost.chunk(cleaned_query, self._embedding_model)        embdedded_queries = [            self._embedding_model(chunk, to_list=True) for chunk in chunks        ]        return embdedded_queriesCheck out the full implementation on our üîó GitHub repository.4.2. Plain retrievalLet‚Äôs try to retrieve a set of posts without using the rerank algorithm.vector_db_retriever = QdrantVectorDBRetriever(    embedding_model=EmbeddingModelSingleton(),    vector_db_client=build_qdrant_client())query = \"Posts about Qdrant\"retrieved_results = vector_db_retriever.search(query=query)for post in retrieved_results[\"posts\"]:    vector_db_retriever.render_as_html(post)Here are the top 2 retrieved results sorted using the cosine similarity score ‚ÜìResult 1:Result 1 for the ‚ÄúPosts about Qdrant‚Äù query (without using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]Result 2:Result 2 for the ‚ÄúPosts about Qdrant‚Äù query (without using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]You can see from the results above, that starting from the second post the results are irrelevant. Even though it has a cosine similarly score of ~0.69 the posts doesn‚Äôt contain any information about Qdrant or vector DBs.Note: We looked over the top 5 retrieved results. Nothing after the first post was relevant. We haven‚Äôt added them here as the article is already too long.4.3. Visualize retrievalTo visualize our retrieval, we implement a dedicated class that uses the UMAP dimensionality reduction algorithm. We have picked UMAP as it preserves the geometric properties between points (e.g., the distance) in higher dimensions when they are projected onto lower dimensions better than its peers (e.g., PCA, t-SNE).The RetrievalVisualizer computes the projected embeddings for the entire vector space once. Afterwards, it uses the render() method to project only the given query and retrieved posts, and plot them to a 2D graph.class RetrievalVisualizer:    def __init__(self, posts: list[EmbeddedChunkedPost]):        self._posts = posts        self._umap_transform = self._fit_model(self._posts)        self._projected_post_embeddings = self.project_posts(self._posts)    def _fit_model(self, posts: list[EmbeddedChunkedPost]) -> umap.UMAP:        umap_transform = ... # Fit a UMAP model on the given posts.        return umap_transform    def project_posts(self, posts: list[EmbeddedChunkedPost]) -> np.ndarray:        embeddings = np.array([post.text_embedding for post in posts])        return self._project(embeddings=embeddings)    def _project(self, embeddings: np.ndarray) -> np.ndarray:        ... # Project the embeddings to 2D using UMAP.        return umap_embeddings    def render(        self,        embedded_queries: list[list[float]],        retrieved_posts: list[EmbeddedChunkedPost],    ) -> None:      ... # Render the given queries & retrieved posts using matplotlib.Let‚Äôs take a look at the result to see how the ‚ÄúPosts about Qdrant‚Äù query looks ‚ÜìVisualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (without reranking) [Image by the Author ‚Äî in collaboration with VectorHub].Our results are not great. You can see how far the retrieved posts are from our query in the vector space.Can we improve the quality of our retrieval system using the rerank algorithm?4.4. RerankWe use the reranking algorithm to refine our retrieval for the initial query. Our initial retrieval step ‚Äî because it used cosine similarity (or similar distance metrics) to compute the distance between a query and post embeddings ‚Äî may have missed more complex (but essential) relationships between the query and the documents in the vector space. Reranking leverages the power of transformer models that are capable of understanding more nuanced semantic relationships.We use a cross-encoder model to implement the reranking step, so we can score the query relative to all retrieved posts individually. These scores take into consideration more complex relationships than cosine similarity can. Under the hood is a BERT classifier that outputs a number between 0 and 1 according to how similar the 2 given sentences are. The BERT classifier outputs 0 if they are entirely different and 1 if they are a perfect match.Bi-Encoder vs. Cross-Encoder [Image by the Author ‚Äî in collaboration with VectorHub]Bi-Encoder vs. Cross-Encoder [Image by the Author ‚Äî in collaboration with VectorHub]But, you might ask, ‚ÄúWhy not use the cross-encoder model from the start if it is that much better?‚ÄùThe answer, in a word, is speed. Using a cross-encoder model to search your whole collection is much slower than using cosine similarity. To optimize your retrieval, therefore, your reranking process should involve 2 steps:an initial rough retrieval step using cosine similarity, which retrieves the top N items as potential candidatesfiltering the rough search using the rerank strategy, which retrieves the top K items as your final resultsThe implementation is relatively straightforward. For each retrieved post, we create a pair consisting of the (cleaned) query and the text of the post. We do this for all retrieved posts, resulting in a list of pairs.Next, we call a cross-encoder/ms-marco-MiniLM-L-6-v2 model (from sentence-transformers) to give the retrieved posts their rerank score. We then sort the posts in descending order based on their rerank score.Check out the rerank algorithm implementation on our üîó GitHub repository.4.5. Visualize retrieval with rerankNow that we‚Äôve added the rerank pattern to our retrieval system, let‚Äôs see if it improves the results of our ‚ÄúPosts about Qdrant‚Äù query ‚ÜìResult 1Result 1 for the ‚ÄúPosts about Qdrant‚Äù query (using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]Result 2:Result 2 for the ‚ÄúPosts about Qdrant‚Äù query (using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]The improvement is remarkable! All our results are about Qdrant and vector DBs.Note: We looked over the top 5 retrieved results. The top 4 out of 5 posts are relevant to our query, which is incredible.Now, let‚Äôs look at the UMAP visualization:Visualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (with reranking) [Image by the Author ‚Äî in collaboration with VectorHub].While the returned posts aren‚Äôt very close to the query, they are a lot closer to the query compared to when we weren‚Äôt reranking the retrieved posts.5. ConclusionIn this article, we learned how to adapt a RAG retrieval pattern to improve LinkedIn post retrieval. To keep our database up to date with rapidly changing social media data, we implemented a real-time streaming pipeline that uses CDC to sync the raw LinkedIn posts data source with a vector DB. You also saw how to use Bytewax to write ‚Äî using only Python ‚Äî a streaming pipeline that cleans, chunks, and embeds LinkedIn posts.Finally, you learned how to implement a standard retrieval client for RAG and saw how to improve it using the rerank pattern. As retrieval is complex to evaluate, you saw how to visualize the retrieval for a given query by rendering all the posts, the query, and the retrieved posts in a 2D space using UMAP.This article is a summary of my contribution from VectorHub. Check out the full article here to dig into the details, the code and more experiments.‚Üí Join 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthMl System DesignArtificial IntelligenceMachine LearningStreaming PipelineData Science358358FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13See all from Paul IusztinSee all from Decoding MLRecommended from MediumMdabdullahalhasibinTowards AIA Complete Guide to Embedding For NLP & Generative AI/LLMUnderstand the concept of vector embedding, why it is needed, and implementation with LangChain.3d agoVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesChatGPT prompts 50 stories¬∑2121 savesTarun SinghinAI AdvancesAI-Powered OCR with Phi-3-Vision-128K: The Future of Document ProcessingIn the fast-evolving world of artificial intelligence, multimodal models are setting new standards for integrating visual and textual data‚Ä¶Oct 989916Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Kamal DhunganaImplementing Human-in-the-Loop with LangGraphStreamlit app\\u200a‚Äî\\u200aHIL (Agent Framework\\u200a‚Äî\\u200aLangGraph)Jul 16205Umair Ali KhaninTowards Data ScienceIntegrating Multimodal Data into a Large Language ModelDeveloping a context-retrieval, multimodal RAG using advanced parsing, semantic & keyword search, and re-ranking4d ago841See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on-social-media-data-9cc01d50a2a0'),\n",
       "  ArticleDocument(id=UUID('c647c345-aeb5-46f7-8f16-8a6345344069'), content={'Title': 'SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!', 'Subtitle': 'Use a Python streaming engine to populate a feature store from 4+ data sources', 'Content': \"Streaming Pipelines for LLMs and RAG | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICASOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!Use a Python streaming engine to populate a feature store from 4+ data sourcesPaul Iusztin¬∑FollowPublished inDecoding ML¬∑19 min read¬∑Apr 20, 20248241ListenShare‚Üí the 4th out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineerüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG AppsTo better understand the course‚Äôs goal, technical details, and system design ‚Üí Check out Lesson 1Let‚Äôs start with Lesson 4 ‚Üì‚Üì‚ÜìLesson 4: Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!In the 4th lesson, we will focus on the feature pipeline.The feature pipeline is the first pipeline presented in the 3 pipeline architecture: feature, training and inference pipelines.A feature pipeline is responsible for taking raw data as input, processing it into features, and storing it in a feature store, from which the training & inference pipelines will use it.The component is completely isolated from the training and inference code. All the communication is done through the feature store.To avoid repeating myself, if you are unfamiliar with the 3 pipeline architecture, check out Lesson 1 for a refresher.By the end of this article, you will learn to design and build a production-ready feature pipeline that:uses Bytewax as a stream engine to process data in real-time;ingests data from a RabbitMQ queue;uses SWE practices to process multiple data types: posts, articles, code;cleans, chunks, and embeds data for LLM fine-tuning and RAG;loads the features to a Qdrant vector DB.Note: In our use case, the feature pipeline is also a streaming pipeline, as we use a Bytewax streaming engine. Thus, we will use these words interchangeably.We will wrap up Lesson 4 by showing you how to deploy the feature pipeline to AWS and integrate it with the components from previous lessons: data collection pipeline, MongoDB, and CDC.In the 5th lesson, we will go through the vector DB retrieval client, where we will teach you how to query the vector DB and improve the accuracy of the results using advanced retrieval techniques.Excited? Let‚Äôs get started!The architecture of the feature/streaming pipeline.Table of ContentsWhy are we doing this?System design of the feature pipelineThe Bytewax streaming flowPydantic data modelsLoad data to QdrantThe dispatcher layerPreprocessing steps: Clean, chunk, embedThe AWS infrastructureRun the code locallyDeploy the code to AWS & Run it from the cloudConclusionüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. Why are we doing this?A quick reminder from previous lessonsTo give you some context, in Lesson 2, we crawl data from LinkedIn, Medium, and GitHub, normalize it, and load it to MongoDB.In Lesson 3, we are using CDC to listen to changes to the MongoDB database and emit events in a RabbitMQ queue based on any CRUD operation done on MongoDB.‚Ä¶and here we are in Lesson 4, where we are building the feature pipeline that listens 24/7 to the RabbitMQ queue for new events to process and load them to a Qdrant vector DB.The problem we are solvingIn our LLM Twin use case, the feature pipeline constantly syncs the MongoDB warehouse with the Qdrant vector DB while processing the raw data into features.Important: In our use case, the Qdrant vector DB will be our feature store.Why we are solving itThe feature store will be the central point of access for all the features used within the training and inference pipelines.For consistency and simplicity, we will refer to different formats of our text data as ‚Äúfeatures.‚Äù‚Üí The training pipeline will use the feature store to create fine-tuning datasets for your LLM twin.‚Üí The inference pipeline will use the feature store for RAG.For reliable results (especially for RAG), the data from the vector DB must always be in sync with the data from the data warehouse.The question is, what is the best way to sync these 2?Other potential solutionsThe most common solution is probably to use a batch pipeline that constantly polls from the warehouse, computes a difference between the 2 databases, and updates the target database.The issue with this technique is that computing the difference between the 2 databases is extremely slow and costly.Another solution is to use a push technique using a webhook. Thus, on any CRUD change in the warehouse, you also update the source DB.The biggest issue here is that if the webhook fails, you have to implement complex recovery logic.Lesson 3 on CDC covers more of this.2. System design of the feature pipeline: our solutionOur solution is based on CDC, a queue, a streaming engine, and a vector DB:‚Üí CDC adds any change made to the Mongo DB to the queue (read more in Lesson 3).‚Üí the RabbitMQ queue stores all the events until they are processed.‚Üí The Bytewax streaming engine cleans, chunks, and embeds the data.‚Üí A streaming engine works naturally with a queue-based system.‚Üí The data is uploaded to a Qdrant vector DB on the flyWhy is this powerful?Here are 4 core reasons:The data is processed in real-time.Out-of-the-box recovery system: If the streaming pipeline fails to process a message will be added back to the queueLightweight: No need for any diffs between databases or batching too many recordsNo I/O bottlenecks on the source database‚Üí It solves all our problems!The architecture of the feature/streaming pipeline.How is the data stored?We store 2 snapshots of our data in the feature store. Here is why ‚ÜìRemember that we said that the training and inference pipeline will access the features only from the feature store, which, in our case, is the Qdrant vector DB?Well, if we had stored only the chunked & embedded version of the data, that would have been useful only for RAG but not for fine-tuning.Thus, we make an additional snapshot of the cleaned data, which will be used by the training pipeline.Afterward, we pass it down the streaming flow for chunking & embedding.How do we process multiple data types?How do you process multiple types of data in a single streaming pipeline without writing spaghetti code?Yes, that is for you, data scientists! Joking‚Ä¶am I?We have 3 data types: posts, articles, and code.Each data type (and its state) will be modeled using Pydantic models.To process them we will write a dispatcher layer, which will use a creational factory pattern [9] to instantiate a handler implemented for that specific data type (post, article, code) and operation (cleaning, chunking, embedding).The handler follows the strategy behavioral pattern [10].Intuitively, you can see the combination between the factory and strategy patterns as follows:Initially, we know we want to clean the data, but as we don‚Äôt know the data type, we can‚Äôt know how to do so.What we can do, is write the whole code around the cleaning code and abstract away the login under a Handler() interface (aka the strategy).When we get a data point, the factory class creates the right cleaning handler based on its type.Ultimately the handler is injected into the rest of the system and executed.By doing so, we can easily isolate the logic for a given data type & operation while leveraging polymorphism to avoid filling up the code with 1000x ‚Äúif else‚Äù statements.We will dig into the implementation in future sections.Streaming over batchYou may ask why we need a streaming engine instead of implementing a batch job that polls the messages at a given frequency.That is a valid question.The thing is that‚Ä¶Nowadays, using tools such as Bytewax makes implementing streaming pipelines a lot more frictionless than using their JVM alternatives.The key aspect of choosing a streaming vs. a batch design is real-time synchronization between your source and destination DBs.In our particular case, we will process social media data, which changes fast and irregularly.Also, for our digital twin, it is important to do RAG on up-to-date data. We don‚Äôt want to have any delay between what happens in the real world and what your LLM twin sees.That being said choosing a streaming architecture seemed natural in our use case.3. The Bytewax streaming flowThe Bytewax flow is the central point of the streaming pipeline. It defines all the required steps, following the next simplified pattern: ‚Äúinput -> processing -> output‚Äù.As I come from the AI world, I like to see it as the ‚Äúgraph‚Äù of the streaming pipeline, where you use the input(), map(), and output() Bytewax functions to define your graph, which in the Bytewax world is called a ‚Äúflow‚Äù.As you can see in the code snippet below, we ingest posts, articles or code messages from a RabbitMQ queue. After we clean, chunk and embed them. Ultimately, we load the cleaned and embedded data to a Qdrant vector DB, which in our LLM twin use case will represent the feature store of our system.To structure and validate the data, between each Bytewax step, we map and pass a different Pydantic model based on its current state: raw, cleaned, chunked, or embedded.Bytewax flow ‚Üí GitHub Code ‚É™‚ÜêWe have a single streaming pipeline that processes everything.As we ingest multiple data types (posts, articles, or code snapshots), we have to process them differently.To do this the right way, we implemented a dispatcher layer that knows how to apply data-specific operations based on the type of message.More on this in the next sections ‚ÜìWhy Bytewax?Bytewax is an open-source streaming processing framework that:- is built in Rust ‚öôÔ∏è for performance- has Python üêç bindings for leveraging its powerful ML ecosystem‚Ä¶ so, for all the Python fanatics out there, no more JVM headaches for you.Jokes aside, here is why Bytewax is so powerful ‚Üì- Bytewax local setup is plug-and-play- can quickly be integrated into any Python project (you can go wild ‚Äî even use it in Notebooks)- can easily be integrated with other Python packages (NumPy, PyTorch, HuggingFace, OpenCV, SkLearn, you name it)- out-of-the-box connectors for Kafka and local files, or you can quickly implement your ownWe used Bytewax to build the streaming pipeline for the LLM Twin course and loved it.To learn more about Bytewax, go and check them out. They are open source, so no strings attached ‚Üí Bytewax [2] ‚Üê4. Pydantic data modelsLet‚Äôs take a look at what our Pydantic models look like.First, we defined a set of base abstract models for using the same parent class across all our components.Pydantic base model structure ‚Üí GitHub Code ‚É™‚ÜêAfterward, we defined a hierarchy of Pydantic models for:all our data types: posts, articles, or codeall our states: raw, cleaned, chunked, and embeddedThis is how the set of classes for the posts will look like ‚ÜìPydantic posts model structure ‚Üí GitHub Code ‚É™‚ÜêWe repeated the same process for the articles and code model hierarchy.Check out the other data classes on our GitHub.Why is keeping our data in Pydantic models so powerful?There are 4 main criteria:every field has an enforced type: you are ensured the data types are going to be correctthe fields are automatically validated based on their type: for example, if the field is a string and you pass an int, it will through an errorthe data structure is clear and verbose: no more clandestine dicts that you never know what is in themyou make your data the first-class citizen of your program5. Load data to QdrantThe first step is to implement our custom Bytewax DynamicSink class ‚ÜìQdrant DynamicSink ‚Üí GitHub Code ‚É™‚ÜêNext, for every type of operation we need (output cleaned or embedded data ) we have to subclass the StatelessSinkPartition Bytewax class (they also provide a stateful option ‚Üí more in their docs)An instance of the class will run on every partition defined within the Bytewax deployment.In the course, we are using a single partition per worker. But, by adding more partitions (and workers), you can quickly scale your Bytewax pipeline horizontally.Qdrant worker partitions ‚Üí GitHub Code ‚É™‚ÜêNote that we used Qdrant‚Äôs Batch method to upload all the available points at once. By doing so, we reduce the latency on the network I/O side: more on that here [8] ‚ÜêThe RabbitMQ streaming input follows a similar pattern. Check it out here ‚Üê6. The dispatcher layerNow that we have the Bytewax flow and all our data models.How do we map a raw data model to a cleaned data model?‚Üí All our domain logic is modeled by a set of Handler() classes.For example, this is how the handler used to map a PostsRawModel to a PostCleanedModel looks like ‚ÜìHandler hierarchy of classes ‚Üí GitHub Code ‚É™‚ÜêCheck out the other handlers on our GitHub:‚Üí ChunkingDataHandler and EmbeddingDataHandlerIn the next sections, we will explore the exact cleaning, chunking and embedding logic.Now, to build our dispatcher, we need 2 last components:a factory class: instantiates the right handler based on the type of the eventa dispatcher class: the glue code that calls the factory class and handlerHere is what the cleaning dispatcher and factory look like ‚ÜìThe dispatcher and factory classes ‚Üí GitHub Code ‚É™‚ÜêCheck out the other dispatchers on our GitHub.By repeating the same logic, we will end up with the following set of dispatchers:RawDispatcher (no factory class required as the data is not processed)CleaningDispatcher (with a ChunkingHandlerFactory class)ChunkingDispatcher (with a ChunkingHandlerFactory class)EmbeddingDispatcher (with an EmbeddingHandlerFactory class)7. Preprocessing steps: Clean, chunk, embedHere we will focus on the concrete logic used to clean, chunk, and embed a data point.Note that this logic is wrapped by our handler to be integrated into our dispatcher layer using the Strategy behavioral pattern [10].We already described that in the previous section. Thus, we will directly jump into the actual logic here, which can be found in the utils module of our GitHub repository.Note: These steps are experimental. Thus, what we present here is just the first iteration of the system. In a real-world scenario, you would experiment with different cleaning, chunking or model versions to improve it on your data.CleaningThis is the main utility function used to clean the text for our posts, articles, and code.Out of simplicity, we used the same logic for all the data types, but after more investigation, you would probably need to adapt it to your specific needs.For example, your posts might start containing some weird characters, and you don‚Äôt want to run the ‚Äúunbold_text()‚Äù or ‚Äúunitalic_text()‚Äù functions on your code data point as is completely redundant.Cleaning logic ‚Üí GitHub Code ‚É™‚ÜêMost of the functions above are from the unstructured [3] Python package. It is a great tool for quickly finding utilities to clean text data.üîó More examples of unstructured here [3] ‚ÜêOne key thing to notice is that at the cleaning step, we just want to remove all the weird, non-interpretable characters from the text.Also, we want to remove redundant data, such as extra whitespace or URLs, as they do not provide much value.These steps are critical for our tokenizer to understand and efficiently transform our string input into numbers that will be fed into the transformer models.Note that when using bigger models (transformers) + modern tokenization techniques, you don‚Äôt need to standardize your dataset too much.For example, it is redundant to apply lemmatization or stemming, as the tokenizer knows how to split your input into a commonly used sequence of characters efficiently, and the transformers can pick up the nuances of the words.üí° What is important at the cleaning step is to throw out the noise.ChunkingWe are using Langchain to chunk our text.We use a 2 step strategy using Langchain‚Äôs RecursiveCharacterTextSplitter [4] and SentenceTransformersTokenTextSplitter [5]. As seen below ‚ÜìChunking logic ‚Üí GitHub Code ‚É™‚ÜêOverlapping your chunks is a common pre-indexing RAG technique, which helps to cluster chunks from the same document semantically.Again, we are using the same chunking logic for all of our data types, but to get the most out of it, we would probably need to tweak the separators, chunk_size, and chunk_overlap parameters for our different use cases.But our dispatcher + handler architecture would easily allow us to configure the chunking step in future iterations.EmbeddingThe data preprocessing, aka the hard part is done.Now we just have to call an embedding model to create our vectors.Embedding logic ‚Üí GitHub Code ‚É™‚ÜêWe used the all-MiniLm-L6-v2 [6] from the sentence-transformers library to embed our articles and posts: a lightweight embedding model that can easily run in real-time on a 2 vCPU machine.As the code data points contain more complex relationships and specific jargon to embed, we used a more powerful embedding model: hkunlp/instructor-xl [7].This embedding model is unique as it can be customized on the fly with instructions based on your particular data. This allows the embedding model to specialize on your data without fine-tuning, which is handy for embedding pieces of code.8. The AWS infrastructureIn Lesson 2, we covered how to deploy the data collection pipeline that is triggered by a link to Medium, Substack, LinkedIn or GitHub ‚Üí crawls the given link ‚Üí saves the crawled information to a MongoDB.In Lesson 3, we explained how to deploy the CDC components that emit events to a RabbitMQ queue based on any CRUD operation done to MongoDB.What is left is to deploy the Bytewax streaming pipeline and Qdrant vector DB.We will use Qdrant‚Äôs self-hosted option, which is easy to set up and scale.To test things out, they offer a Free Tier plan for up to a 1GB cluster, which is more than enough for our course.‚Üí We explained in our GitHub repository how to configure Qdrant.AWS infrastructure of the feature/streaming pipeline.The last piece of the puzzle is the Bytewax streaming pipeline.As we don‚Äôt require a GPU and the streaming pipeline needs to run 24/7, we will deploy it to AWS Fargate, a cost-effective serverless solution from AWS.As a serverless solution, Fargate allows us to deploy our code quickly and scale it fast in case of high traffic.How do we deploy the streaming pipeline code to Fargate?Using GitHub Actions, we wrote a CD pipeline that builds a Docker image on every new commit made on the main branch.After, the Docker image is pushed to AWS ECR. Ultimately, Fargate pulls the latest version of the Docker image.This is a common CD pipeline to deploy your code to AWS services.Why not use lambda functions, as we did for the data pipeline?An AWS lambda function executes a function once and then closes down.This worked perfectly for the crawling logic, but it won't work for our streaming pipeline, which has to run 24/7.9. Run the code locallyTo quickly test things up, we wrote a docker-compose.yaml file to spin up the MongoDB, RabbitMQ queue and Qdrant vector db.You can spin up the Docker containers using our Makefile by running the following, which will start the CDC component and streaming pipeline:make local-startTo start the data collection pipeline, run the following:make local-test-githubThe documentation of our GitHub repository provides more details on how to run and set up everything.10. Deploy the code to AWS & Run it from the cloudThis article is already too long, so I won‚Äôt go into the details of how to deploy the AWS infrastructure described above and test it out here.But to give you some insights, we have used Pulumi as our infrastructure as a code (IaC) tool, which will allow you to spin it quickly with a few commands.Also, I won‚Äôt let you hang on to this one. We made a promise and‚Ä¶ ‚ÜìWe prepared step-by-step instructions in the README of our GitHub repository on how to use Pulumni to spin up the infrastructure and test it out.ConclusionNow you know how to write streaming pipelines like a PRO!In Lesson 4, you learned how to:design a feature pipeline using the 3-pipeline architecturewrite a streaming pipeline using Bytewax as a streaming engineuse a dispatcher layer to write a modular and flexible application to process multiple types of data (posts, articles, code)load the cleaned and embedded data to Qdrantdeploy the streaming pipeline to AWS‚Üí This is only the ingestion part used for fine-tuning LLMs and RAG.In Lesson 5, you will learn how to write a retrieval client for the 3 data types using good SWE practices and improve the retrieval accuracy using advanced retrieval & post-retrieval techniques. See you there!üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èEnjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE ‚ÜìDecoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Bytewax, Bytewax Landing Page[3] Unstructured Cleaning Examples, Unstructured Documentation[4] Recursively split by character, LangChain‚Äôs Documentation[5] Split by tokens, LangChain‚Äôs Documentation[6] sentence-transformers/all-MiniLM-L6-v2, HuggingFace[7] hkunlp/instructor-xl, HuggingFace[8] Qdrant, Qdrant Documentation[9] Abstract Factory Pattern, Refactoring Guru[10] Strategy Pattern, Refactoring GuruImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthMl System DesignMachine LearningArtificial IntelligenceData ScienceSoftware Engineering8248241FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13See all from Paul IusztinSee all from Decoding MLRecommended from MediumVipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Vishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesdata science and AI40 stories¬∑269 savesDerckData architecture for MLOps: Metadata storeIntroductionJul 17Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Tarun SinghinAI AdvancesMastering RAG Chunking Techniques for Enhanced Document ProcessingDividing large documents into smaller parts is a crucial yet intricate task that significantly impacts the performance of‚Ä¶Jun 182592Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87'),\n",
       "  ArticleDocument(id=UUID('649bd7d7-aa0e-4ada-b5e2-1c50fe7c95e6'), content={'Title': 'The 4 Advanced RAG Algorithms You Must Know to Implement', 'Subtitle': 'Implement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithm', 'Content': '4 Advanced RAG Algorithms You Must Know | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICAThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmPaul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑May 4, 20241.8K12ListenShare‚Üí the 5th out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineerüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG AppsTo better understand the course‚Äôs goal, technical details, and system design ‚Üí Check out Lesson 1Let‚Äôs start with Lesson 5 ‚Üì‚Üì‚ÜìLesson 5: The 4 Advanced RAG Algorithms You Must Know to ImplementIn Lesson 5, we will focus on building an advanced retrieval module used for RAG.We will show you how to implement 4 retrieval and post-retrieval advanced optimization techniques to improve the accuracy of your RAG retrieval step.In this lesson, we will focus only on the retrieval part of the RAG system.In Lesson 4, we showed you how to clean, chunk, embed, and load social media data to a Qdrant vector DB (the ingestion part of RAG).In future lessons, we will integrate this retrieval module into the inference pipeline for a full-fledged RAG system.Retrieval Python Module ArchitectureWe assume you are already familiar with what a naive RAG looks like. If not, check out the following article from Decoding ML, where we present in a 2-minute read what a naive RAG looks like:Why you must choose streaming over batch pipelines when doing RAG in LLM applicationsLesson 2: RAG, streaming pipelines, vector DBs, text processingmedium.comTable of ContentsOverview of advanced RAG optimization techniquesAdvanced RAG techniques applied to the LLM twinRetrieval optimization (1): Query expansionRetrieval optimization (2): Self queryRetrieval optimization (3): Hybrid & filtered vector searchImplement the advanced retrieval Python classPost-retrieval optimization: Rerank using GPT-4How to use the retrievalConclusionüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. Overview of advanced RAG optimization techniquesA production RAG system is split into 3 main components:ingestion: clean, chunk, embed, and load your data to a vector DBretrieval: query your vector DB for contextgeneration: attach the retrieved context to your prompt and pass it to an LLMThe ingestion component sits in the feature pipeline, while the retrieval and generation components are implemented inside the inference pipeline.You can also use the retrieval and generation components in your training pipeline to fine-tune your LLM further on domain-specific prompts.You can apply advanced techniques to optimize your RAG system for ingestion, retrieval and generation.That being said, there are 3 main types of advanced RAG techniques:Pre-retrieval optimization [ingestion]: tweak how you create the chunksRetrieval optimization [retrieval]: improve the queries to your vector DBPost-retrieval optimization [retrieval]: process the retrieved chunks to filter out the noiseThe generation step can be improved through fine-tuning or prompt engineering, which will be explained in future lessons.The pre-retrieval optimization techniques are explained in Lesson 4.In this lesson, we will show you some popular retrieval and post-retrieval optimization techniques.2. Advanced RAG techniques applied to the LLM twinRetrieval optimizationWe will combine 3 techniques:Query ExpansionSelf QueryFiltered vector searchPost-retrieval optimizationWe will use the rerank pattern using GPT-4 and prompt engineering instead of Cohere or an open-source re-ranker cross-encoder [4].I don‚Äôt want to spend too much time on the theoretical aspects. There are plenty of articles on that.So, we will jump straight to implementing and integrating these techniques in our LLM twin system.But before seeing the code, let‚Äôs clarify a few things ‚ÜìAdvanced RAG architecture2.1 Important Note!We will show you a custom implementation of the advanced techniques and NOT use LangChain.Our primary goal is to build your intuition about how they work behind the scenes. However, we will attach LangChain‚Äôs equivalent so you can use them in your apps.Customizing LangChain can be a real headache. Thus, understanding what happens behind its utilities can help you build real-world applications.Also, it is critical to know that if you don‚Äôt ingest the data using LangChain, you cannot use their retrievals either, as they expect the data to be in a specific format.We haven‚Äôt used LangChain‚Äôs ingestion function in Lesson 4 either (the feature pipeline that loads data to Qdrant) as we want to do everything ‚Äúby hand‚Äù.2.2. Why Qdrant?There are many vector DBs out there, too many‚Ä¶But since we discovered Qdrant, we loved it.Why?It is built in Rust.Apache-2.0 license ‚Äî open-source üî•It has a great and intuitive Python SDK.It has a freemium self-hosted version to build PoCs for free.It supports unlimited document sizes, and vector dims of up to 645536.It is production-ready. Companies such as Disney, Mozilla, and Microsoft already use it.It is one of the most popular vector DBs out there.To put that in perspective, Pinecone, one of its biggest competitors, supports only documents with up to 40k tokens and vectors with up to 20k dimensions‚Ä¶. and a proprietary license.I could go on and on‚Ä¶‚Ä¶but if you are curious to find out more, check out Qdrant ‚Üê3. Retrieval optimization (1): Query expansionThe problemIn a typical retrieval step, you query your vector DB using a single point.The issue with that approach is that by using a single vector, you cover only a small area of your embedding space.Thus, if your embedding doesn\\'t contain all the required information, your retrieved context will not be relevant.What if we could query the vector DB with multiple data points that are semantically related?That is what the ‚ÄúQuery expansion‚Äù technique is doing!The solutionQuery expansion is quite intuitive.You use an LLM to generate multiple queries based on your initial query.These queries should contain multiple perspectives of the initial query.Thus, when embedded, they hit different areas of your embedding space that are still relevant to our initial question.You can do query expansion with a detailed zero-shot prompt.Here is our simple & custom solution ‚ÜìQuery expansion template ‚Üí GitHub Code ‚ÜêHere is LangChain‚Äôs MultiQueryRetriever class [5] (their equivalent).4. Retrieval optimization (2): Self queryThe problemWhen embedding your query, you cannot guarantee that all the aspects required by your use case are present in the embedding vector.For example, you want to be 100% sure that your retrieval relies on the tags provided in the query.The issue is that by embedding the query prompt, you can never be sure that the tags are represented in the embedding vector or have enough signal when computing the distance against other vectors.The solutionWhat if you could extract the tags within the query and use them along the embedded query?That is what self-query is all about!You use an LLM to extract various metadata fields that are critical for your business use case (e.g., tags, author ID, number of comments, likes, shares, etc.)In our custom solution, we are extracting just the author ID. Thus, a zero-shot prompt engineering technique will do the job.But, when extracting multiple metadata types, you should also use few-shot learning to optimize the extraction step.Self-queries work hand-in-hand with vector filter searches, which we will explain in the next section.Here is our solution ‚ÜìSelf-query template ‚Üí GitHub Code ‚ÜêHere is LangChain‚Äôs SelfQueryRetriever class [6] equivalent and this is an example using Qdrant [8].5. Retrieval optimization (3): Hybrid & filtered vector searchThe problemEmbeddings are great for capturing the general semantics of a specific chunk.But they are not that great for querying specific keywords.For example, if we want to retrieve article chunks about LLMs from our Qdrant vector DB, embeddings would be enough.However, if we want to query for a specific LLM type (e.g., LLama 3), using only similarities between embeddings won‚Äôt be enough.Thus, embeddings are not great for finding exact phrase matching for specific terms.The solutionCombine the vector search technique with one (or more) complementary search strategy, which works great for finding exact words.It is not defined which algorithms are combined, but the most standard strategy for hybrid search is to combine the traditional keyword-based search and modern vector search.How are these combined?The first method is to merge the similarity scores of the 2 techniques as follows:hybrid_score = (1 - alpha) * sparse_score + alpha * dense_scoreWhere alpha takes a value between [0, 1], with:alpha = 1: Vector Searchalpha = 0: Keyword searchAlso, the similarity scores are defined as follows:sparse_score: is the result of the keyword search that, behind the scenes, uses a BM25 algorithm [7] that sits on top of TF-IDF.dense_score: is the result of the vector search that most commonly uses a similarity metric such as cosine distanceThe second method uses the vector search technique as usual and applies a filter based on your keywords on top of the metadata of retrieved results.‚Üí This is also known as filtered vector search.In this use case, the similar score is not changed based on the provided keywords.It is just a fancy word for a simple filter applied to the metadata of your vectors.But it is essential to understand the difference between the first and second methods:the first method combines the similarity score between the keywords and vectors using the alpha parameter;the second method is a simple filter on top of your vector search.How does this fit into our architecture?Remember that during the self-query step, we extracted the author_id as an exact field that we have to match.Thus, we will search for the author_id using the keyword search algorithm and attach it to the 5 queries generated by the query expansion step.As we want the most relevant chunks from a given author, it makes the most sense to use a filter using the author_id as follows (filtered vector search) ‚Üìself._qdrant_client.search(      collection_name=\"vector_posts\",      query_filter=models.Filter(          must=[              models.FieldCondition(                  key=\"author_id\",                  match=models.MatchValue(                      value=metadata_filter_value,                  ),              )          ]      ),      query_vector=self._embedder.encode(generated_query).tolist(),      limit=k,)Note that we can easily extend this with multiple keywords (e.g., tags), making the combination of self-query and hybrid search a powerful retrieval duo.The only question you have to ask yourself is whether we want to use a simple vector search filter or the more complex hybrid search strategy.Note that LangChain‚Äôs SelfQueryRetriever class combines the self-query and hybrid search techniques behind the scenes, as can be seen in their Qdrant example [8]. That is why we wanted to build everything from scratch.6. Implement the advanced retrieval Python classNow that you‚Äôve understood the advanced retrieval optimization techniques we\\'re using, let‚Äôs combine them into a Python retrieval class.Here is what the main retriever function looks like ‚ÜìVectorRetriever: main retriever function ‚Üí GitHub ‚ÜêUsing a Python ThreadPoolExecutor is extremely powerful for addressing I/O bottlenecks, as these types of operations are not blocked by Python‚Äôs GIL limitations.Here is how we wrapped every advanced retrieval step into its own class ‚ÜìQuery expansion chains wrapper ‚Üí GitHub ‚ÜêThe SelfQuery class looks very similar ‚Äî üîó access it here [1] ‚Üê.Now the final step is to call Qdrant for each query generated by the query expansion step ‚ÜìVectorRetriever: main search function ‚Üí GitHub ‚ÜêNote that we have 3 types of data: posts, articles, and code repositories.Thus, we have to make a query for each collection and combine the results in the end.The most performant method is to use multi-indexing techniques, which allow you to query multiple types of data at once.But at the time I am writing this article, this is not a solved problem at the production level.Thus, we gathered data from each collection individually and kept the best-retrieved results using rerank.Which is the final step of the article.7. Post-retrieval optimization: Rerank using GPT-4We made a different search in the Qdrant vector DB for N prompts generated by the query expansion step.Each search returns K results.Thus, we end up with N x K chunks.In our particular case, N = 5 & K = 3. Thus, we end up with 15 chunks.Post-retrieval optimization: rerankThe problemThe retrieved context may contain irrelevant chunks that only:add noise: the retrieved context might be irrelevantmake the prompt bigger: results in higher costs & the LLM is usually biased in looking only at the first and last pieces of context. Thus, if you add a big context, there is a big chance it will miss the essence.unaligned with your question: the chunks are retrieved based on the query and chunk embedding similarity. The issue is that the embedding model is not tuned to your particular question, which might result in high similarity scores that are not 100% relevant to your question.The solutionWe will use rerank to order all the N x K chunks based on their relevance relative to the initial question, where the first one will be the most relevant and the last chunk the least.Ultimately, we will pick the TOP K most relevant chunks.Rerank works really well when combined with query expansion.A natural flow when using rerank is as follows:Search for >K chunks >>> Reorder using rerank >>> Take top KThus, when combined with query expansion, we gather potential useful context from multiple points in space rather than just looking for more than K samples in a single location.Now the flow looks like:Search for N x K chunks >>> Reoder using rerank >>> Take top KA typical re-ranking solution uses open-source Cross-Encoder models from sentence transformers [4].These solutions take both the question and context as input and return a score from 0 to 1.In this article, we want to take a different approach and use GPT-4 + prompt engineering as our reranker.If you want to see how to apply rerank using open-source algorithms, check out this hands-on article from Decoding ML:A Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.medium.comNow let‚Äôs see our implementation using GPT-4 & prompt engineering.Similar to what we did for the expansion and self-query chains, we define a template and a chain builder ‚ÜìRerank chain ‚Üí GitHub ‚ÜêHere is how we integrate the rerank chain into the retriever:Retriever: rerank step ‚Üí GitHub ‚Üê‚Ä¶and that‚Äôs it!Note that this is an experimental process. Thus, you can further tune your prompts for better results, but the primary idea is the same.8. How to use the retrievalThe last step is to run the whole thing.But there is a catch.As we said in the beginning the retriever will not be used as a standalone component in the LLM system.It will be used as a layer between the data and the Qdrant vector DB by the:training pipeline to retrieve raw data for fine-tuning (we haven‚Äôt shown that as it‚Äôs a straightforward search operation ‚Äî no RAG involved)inference pipeline to do RAG‚Üí That is why, for this lesson, there is no infrastructure involved!But, to test the retrieval, we wrote a simple script ‚ÜìRetriever testing entry point ‚Üí GitHub ‚ÜêLook at how easy it is to call the whole chain with our custom retriever‚Äîno fancy LangChain involved!Now, to call this script, run the following Make command:make local-test-retriever‚Ä¶and that‚Äôs it!In future lessons, we will learn to integrate it into the training & inference pipelines.‚Üí Check out the LLM Twin GitHub repository and try it yourself! ‚Ä¶ Of course, don‚Äôt forget to give it a ‚≠êÔ∏è to stay updated with the latest changes.ConclusionCongratulations!In Lesson 5, you learned to build an advanced RAG retrieval module optimized for searching posts, articles, and code repositories from a Qdrant vector DB.First, you learned about where the RAG pipeline can be optimized:pre-retrievalretrievalpost-retrievalAfter you learn how to build from scratch (without using LangChain‚Äôs utilities) the following advanced RAG retrieval & post-retrieval optimization techniques:query expansionself queryhybrid searchrerankUltimately, you understood where the retrieval component sits in an RAG production LLM system, where the code is shared between multiple microservices and doesn‚Äôt sit in a single Notebook.In Lesson 6, we will move to the training pipeline and show you how to automatically transform the data crawled from LinkedIn, Substack, Medium, and GitHub into an instruction dataset using GPT-4 to fine-tune your LLM Twin.See you there! ü§óüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èEnjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE ‚ÜìDecoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Bytewax, Bytewax Landing Page[3] Qdrant, Qdrant Documentation[4] Retrieve & Re-Rank, Sentence Transformers Documentation[5] MultiQueryRetriever, LangChain‚Äôs Documentation[6] Self-querying, LangChain‚Äôs Documentation[7] Okapi BM25, Wikipedia[8] Qdrant Self Query Example, LangChain‚Äôs DocumentationImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthData ScienceMachine LearningArtificial IntelligenceRagGenerative Ai1.8K1.8K12FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72Austin StarksinDataDrivenInvestorI used OpenAI‚Äôs o1 model to develop a trading strategy. It is DESTROYING the marketIt literally took one try. I was shocked.Sep 154.3K119ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesAI Regulation6 stories¬∑593 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18Louis-Fran√ßois BouchardinTowards AIThe Best RAG Stack to Date(exploring every component)Sep 1473911Necati DemirAdvanced RAG: Implementing Advanced Techniques to Enhance Retrieval-Augmented Generation SystemsMay 16481See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'),\n",
       "  ArticleDocument(id=UUID('597ead2d-ae88-43f9-945d-d974630e858a'), content={'Title': 'Architect scalable and cost-effective LLM & RAG inference pipelines', 'Subtitle': 'Design, build and deploy RAG inference pipeline using LLMOps best practices.', 'Content': 'Architect LLM & RAG inference pipelines | Decoding MLOpen in appSign upSign inWriteSign upSign inLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICAArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑17 min read¬∑Jun 1, 20245601ListenShare‚Üí the 9th out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineerüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG AppsTo better understand the course‚Äôs goal, technical details, and system design ‚Üí Check out Lesson 1Let‚Äôs start with Lesson 9 ‚Üì‚Üì‚ÜìLesson 9: Architect scalable and cost-effective LLM & RAG inference pipelinesIn Lesson 9, we will focus on implementing and deploying the inference pipeline of the LLM twin system.First, we will design and implement a scalable LLM & RAG inference pipeline based on microservices, separating the ML and business logic into two layers.Secondly, we will use Comet ML to integrate a prompt monitoring service to capture all input prompts and LLM answers for further debugging and analysis.Ultimately, we will deploy the inference pipeline to Qwak and make the LLM twin service available worldwide.‚Üí Context from previous lessons. What you must know.This lesson is part of a more extensive series in which we learn to build an end-to-end LLM system using LLMOps best practices.In Lesson 4, we populated a Qdrant vector DB with cleaned, chunked, and embedded digital data (posts, articles, and code snippets).In Lesson 5, we implemented the advanced RAG retrieval module to query relevant digital data. Here, we will learn to integrate it into the final inference pipeline.In Lesson 7, we used Qwak to build a training pipeline to fine-tune an open-source LLM on our custom digital data. The LLM weights are available in a model registry.In Lesson 8, we evaluated the fine-tuned LLM to ensure the production candidate behaves accordingly.So‚Ä¶ What you must know from all of this?Don‚Äôt worry. If you don‚Äôt want to replicate the whole system, you can read this article independently from the previous lesson.Thus, the following assumptions are what you have to know. We have:a Qdrant vector DB populated with digital data (posts, articles, and code snippets)a vector DB retrieval module to do advanced RAGa fine-tuned open-source LLM available in a model registry from Comet ML‚Üí In this lesson, we will focus on gluing everything together into a scalable inference pipeline and deploying it to the cloud.Architect scalable and cost-effective LLM & RAG inference pipelinesTable of ContentsThe architecture of the inference pipelineThe training vs. the inference pipelineSettings Pydantic classThe RAG business moduleThe LLM microservicePrompt monitoringDeploying and running the inference pipelineConclusionüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. The architecture of the inference pipelineOur inference pipeline contains the following core elements:a fine-tuned LLMa RAG modulea monitoring serviceLet‚Äôs see how to hook these into a scalable and modular system.The interface of the inference pipelineAs we follow the feature/training/inference (FTI) pipeline architecture, the communication between the 3 core components is clear.Our LLM inference pipeline needs 2 things:a fine-tuned LLM: pulled from the model registryfeatures for RAG: pulled from a vector DB (which we modeled as a logical feature store)This perfectly aligns with the FTI architecture.‚Üí If you are unfamiliar with the FTI pipeline architecture, we recommend you review Lesson 1‚Äôs section on the 3-pipeline architecture.Monolithic vs. microservice inference pipelinesUsually, the inference steps can be split into 2 big layers:the LLM service: where the actual inference is being donethe business service: domain-specific logicWe can design our inference pipeline in 2 ways.Option 1: Monolithic LLM & business serviceIn a monolithic scenario, we implement everything into a single service.Pros:easy to implementeasy to maintainCons:harder to scale horizontally based on the specific requirements of each componentharder to split the work between multiple teamsnot being able to use different tech stacks for the two servicesMonolithic vs. microservice inference pipelinesOption 2: Different LLM & business microservicesThe LLM and business services are implemented as two different components that communicate with each other through the network, using protocols such as REST or gRPC.Pros:each component can scale horizontally individuallyeach component can use the best tech stack at handCons:harder to deployharder to maintainLet‚Äôs focus on the ‚Äúeach component can scale individually‚Äù part, as this is the most significant benefit of the pattern. Usually, LLM and business services require different types of computing. For example, an LLM service depends heavily on GPUs, while the business layer can do the job only with a CPU.As the LLM inference takes longer, you will often need more LLM service replicas to meet the demand. But remember that GPU VMs are really expensive.By decoupling the 2 components, you will run only what is required on the GPU machine and not block the GPU VM with other computing that can quickly be done on a much cheaper machine.Thus, by decoupling the components, you can scale horizontally as required, with minimal costs, providing a cost-effective solution to your system‚Äôs needs.Microservice architecture of the LLM twin inference pipelineLet‚Äôs understand how we applied the microservice pattern to our concrete LLM twin inference pipeline.As explained in the sections above, we have the following components:A business microserviceAn LLM microserviceA prompt monitoring microserviceThe business microservice is implemented as a Python module that:contains the advanced RAG logic, which calls the vector DB and GPT-4 API for advanced RAG operations;calls the LLM microservice through a REST API using the prompt computed utilizing the user‚Äôs query and retrieved contextsends the prompt and the answer generated by the LLM to the prompt monitoring microservice.As you can see, the business microservice is light. It glues all the domain steps together and delegates the computation to other services.The end goal of the business layer is to act as an interface for the end client. In our case, as we will ship the business layer as a Python module, the client will be a Streamlit application.However, you can quickly wrap the Python module with FastAPI and expose it as a REST API to make it accessible from the cloud.Microservice architecture of the LLM twin inference pipelineThe LLM microservice is deployed on Qwak. This component is wholly niched on hosting and calling the LLM. It runs on powerful GPU-enabled machines.How does the LLM microservice work?It loads the fine-tuned LLM twin model from Comet‚Äôs model registry [2].It exposes a REST API that takes in prompts and outputs the generated answer.When the REST API endpoint is called, it tokenizes the prompt, passes it to the LLM, decodes the generated tokens to a string and returns the answer.That‚Äôs it!The prompt monitoring microservice is based on Comet ML‚Äôs LLM dashboard. Here, we log all the prompts and generated answers into a centralized dashboard that allows us to evaluate, debug, and analyze the accuracy of the LLM.Remember that a prompt can get quite complex. When building complex LLM apps, the prompt usually results from a chain containing other prompts, templates, variables, and metadata.Thus, a prompt monitoring service, such as the one provided by Comet ML, differs from a standard logging service. It allows you to quickly dissect the prompt and understand how it was created. Also, by attaching metadata to it, such as the latency of the generated answer and the cost to generate the answer, you can quickly analyze and optimize your prompts.2. The training vs. the inference pipelineBefore diving into the code, let‚Äôs quickly clarify what is the difference between the training and inference pipelines.Along with the apparent reason that the training pipeline takes care of training while the inference pipeline takes care of inference (Duh!), there are some critical differences you have to understand.The input of the pipeline & How the data is accessedDo you remember our logical feature store based on the Qdrant vector DB and Comet ML artifacts? If not, consider checking out Lesson 6 for a refresher.The core idea is that during training, the data is accessed from an offline data storage in batch mode, optimized for throughput and data lineage.Our LLM twin architecture uses Comet ML artifacts to access, version, and track all our data.The data is accessed in batches and fed to the training loop.During inference, you need an online database optimized for low latency. As we directly query the Qdrant vector DB for RAG, that fits like a glove.During inference, you don‚Äôt care about data versioning and lineage. You just want to access your features quickly for a good user experience.The data comes directly from the user and is sent to the inference logic.The training vs. the inference pipelineThe output of the pipelineThe training pipeline‚Äôs final output is the trained weights stored in Comet‚Äôs model registry.The inference pipeline‚Äôs final output is the predictions served directly to the user.The infrastructureThe training pipeline requires more powerful machines with as many GPUs as possible.Why? During training, you batch your data and have to hold in memory all the gradients required for the optimization steps. Because of the optimization algorithm, the training is more compute-hungry than the inference.Thus, more computing and VRAM result in bigger batches, which means less training time and more experiments.The inference pipeline can do the job with less computation. During inference, you often pass a single sample or smaller batches to the model.If you run a batch pipeline, you will still pass batches to the model but don‚Äôt perform any optimization steps.If you run a real-time pipeline, as we do in the LLM twin architecture, you pass a single sample to the model or do some dynamic batching to optimize your inference step.Are there any overlaps?Yes! This is where the training-serving skew comes in.During training and inference, you must carefully apply the same preprocessing and postprocessing steps.If the preprocessing and postprocessing functions or hyperparameters don‚Äôt match, you will end up with the training-serving skew problem.Enough with the theory. Let‚Äôs dig into the RAG business microservice ‚Üì3. Settings Pydantic classFirst, let‚Äôs understand how we defined the settings to configure the inference pipeline components.We used pydantic_settings and inherited its BaseSettings class.This approach lets us quickly define a set of default settings variables and load sensitive values such as the API KEY from a .env file.from pydantic_settings import BaseSettings, SettingsConfigDictclass AppSettings(BaseSettings):    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\"    ... # Settings.    # CometML config    COMET_API_KEY: str    COMET_WORKSPACE: str    COMET_PROJECT: str = \"llm-twin-course\"    ... # More settings.settings = AppSettings()All the variables called settings.* (e.g., settings.Comet_API_KEY) come from this class.4. The RAG business moduleWe will define the RAG business module under the LLMTwin class. The LLM twin logic is directly correlated with our business logic.We don‚Äôt have to introduce the word ‚Äúbusiness‚Äù in the naming convention of the classes. What we presented so far was used for a clear separation of concern between the LLM and business layers.Initially, within the LLMTwin class, we define all the clients we need for our business logic ‚ÜìInference pipeline business module: __init__() method ‚Üí GitHub ‚ÜêNow let‚Äôs dig into the generate() method, where we:call the RAG module;create the prompt using the prompt template, query and context;call the LLM microservice;log the prompt, prompt template, and answer to Comet ML‚Äôs prompt monitoring service.Inference pipeline business module: generate() method ‚Üí GitHub ‚ÜêNow, let‚Äôs look at the complete code of the generate() method. It‚Äôs the same thing as what we presented above, but with all the nitty-little details.class LLMTwin:    def __init__(self) -> None:        ...    def generate(        self,        query: str,        enable_rag: bool = True,        enable_monitoring: bool = True,    ) -> dict:        prompt_template = self.template.create_template(enable_rag=enable_rag)        prompt_template_variables = {            \"question\": query,        }        if enable_rag is True:            retriever = VectorRetriever(query=query)            hits = retriever.retrieve_top_k(                k=settings.TOP_K,                 to_expand_to_n_queries=settings.EXPAND_N_QUERY            )            context = retriever.rerank(                hits=hits,                 keep_top_k=settings.KEEP_TOP_K            )            prompt_template_variables[\"context\"] = context            prompt = prompt_template.format(question=query, context=context)        else:            prompt = prompt_template.format(question=query)        input_ = pd.DataFrame([{\"instruction\": prompt}]).to_json()        response: list[dict] = self.qwak_client.predict(input_)        answer = response[0][\"content\"][0]        if enable_monitoring is True:            self.prompt_monitoring_manager.log(                prompt=prompt,                prompt_template=prompt_template.template,                prompt_template_variables=prompt_template_variables,                output=answer,                metadata=metadata,            )        return {\"answer\": answer}Let‚Äôs look at how our LLM microservice is implemented using Qwak.5. The LLM microserviceAs the LLM microservice is deployed on Qwak, we must first inherit from the QwakModel class and implement some specific functions.initialize_model(): where we load the fine-tuned model from the model registry at serving timeschema(): where we define the input and output schemapredict(): where we implement the actual inference logicNote: The build() function contains all the training logic, such as loading the dataset, training the LLM, and pushing it to a Comet experiment. To see the full implementation, consider checking out Lesson 7, where we detailed the training pipeline.LLM microservice ‚Üí GitHub ‚ÜêLet‚Äôs zoom into the implementation and the life cycle of the Qwak model.The schema() method is used to define how the input and output of the predict() method look like. This will automatically validate the structure and type of the predict() method. For example, the LLM microservice will throw an error if the variable instruction is a JSON instead of a string.The other Qwak-specific methods are called in the following order:__init__() ‚Üí when deploying the modelinitialize_model() ‚Üí when deploying the modelpredict() ‚Üí on every request to the LLM microservice>>> Note that these methods are called only during serving time (and not during training).Qwak exposes your model as a RESTful API, where the predict() method is called on each request.Inside the prediction method, we perform the following steps:map the input text to token IDs using the LLM-specific tokenizermove the token IDs to the provided device (GPU or CPU)pass the token IDs to the LLM and generate the answerextract only the generated tokens from the generated_ids variable by slicing it using the shape of the input_idsdecode the generated_ids back to textreturn the generated textHere is the complete code for the implementation of the Qwak LLM microservice:class CopywriterMistralModel(QwakModel):    def __init__(        self,        use_experiment_tracker: bool = True,        register_model_to_model_registry: bool = True,        model_type: str = \"mistralai/Mistral-7B-Instruct-v0.1\",        fine_tuned_llm_twin_model_type: str = settings.FINE_TUNED_LLM_TWIN_MODEL_TYPE,        dataset_artifact_name: str = settings.DATASET_ARTIFACT_NAME,        config_file: str = settings.CONFIG_FILE,        model_save_dir: str = settings.MODEL_SAVE_DIR,    ) -> None:        self.use_experiment_tracker = use_experiment_tracker        self.register_model_to_model_registry = register_model_to_model_registry        self.model_save_dir = model_save_dir        self.model_type = model_type        self.fine_tuned_llm_twin_model_type = fine_tuned_llm_twin_model_type        self.dataset_artifact_name = dataset_artifact_name        self.training_args_config_file = config_file  def build(self) -> None:      # Training logic      ...  def initialize_model(self) -> None:      self.model, self.tokenizer, _ = build_qlora_model(            pretrained_model_name_or_path=self.model_type,            peft_pretrained_model_name_or_path=self.fine_tuned_llm_twin_model_type,            bnb_config=self.nf4_config,            lora_config=self.qlora_config,            cache_dir=settings.CACHE_DIR,        )        self.model = self.model.to(self.device)      logging.info(f\"Successfully loaded model from {self.model_save_dir}\")  def schema(self) -> ModelSchema:      return ModelSchema(          inputs=[RequestInput(name=\"instruction\", type=str)],          outputs=[InferenceOutput(name=\"content\", type=str)],      )  @qwak.api(output_adapter=DefaultOutputAdapter())  def predict(self, df) -> pd.DataFrame:      input_text = list(df[\"instruction\"].values)      input_ids = self.tokenizer(          input_text, return_tensors=\"pt\", add_special_tokens=True      )      input_ids = input_ids.to(self.device)      generated_ids = self.model.generate(          **input_ids,          max_new_tokens=500,          do_sample=True,          pad_token_id=self.tokenizer.eos_token_id,      )      answer_start_idx = input_ids[\"input_ids\"].shape[1]      generated_answer_ids = generated_ids[:, answer_start_idx:]      decoded_output = self.tokenizer.batch_decode(generated_answer_ids)[0]      return pd.DataFrame([{\"content\": decoded_output}]) Where the settings used in the code above have the following values:class AppSettings(BaseSettings):    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")    ... # Other settings.        DATASET_ARTIFACT_NAME: str = \"posts-instruct-dataset\"    FINE_TUNED_LLM_TWIN_MODEL_TYPE: str = \"decodingml/llm-twin:1.0.0\"    CONFIG_FILE: str = \"./finetuning/config.yaml\"        MODEL_SAVE_DIR: str = \"./training_pipeline_output\"    CACHE_DIR: Path = Path(\"./.cache\")The most important one is the FINE_TUNED_LLM_TWIN_MODEL_TYPE setting, which reflects what model and version to load from the model registry.Access the code üîó here ‚ÜêThe final step is to look at Comet‚Äôs prompt monitoring service. ‚Üì6. Prompt monitoringComet makes prompt monitoring straightforward. There is just one API call where you connect to your project and workspace and send the following to a single function:the prompt and LLM outputthe prompt template and variables that created the final outputyour custom metadata specific to your use case ‚Äî here, you add information about the model, prompt token count, token generation costs, latency, etc.Prompt monitoring service ‚Üí GitHub ‚ÜêLet‚Äôs look at the logs in Comet ML‚ÄôsML‚Äôs LLMOps dashboard.Here is how you can quickly access them ‚Üìlog in to Comet (or create an account)go to your workspaceaccess the project with the ‚ÄúLLM‚Äù symbol attached to it. In our case, this is the ‚Äúllm-twin-course-monitoring‚Äù project.Note: Comet ML provides a free version which is enough to run these examples.Screenshot from Comet ML‚Äôs dashboardThis is how Comet ML‚Äôs prompt monitoring dashboard looks. Here, you can scroll through all the prompts that were ever sent to the LLM. ‚ÜìYou can click on any prompt and see everything we logged programmatically using the PromptMonitoringManager class.Screenshot from Comet ML‚Äôs dashboardBesides what we logged, adding various tags and the inference duration can be valuable.7. Deploying and running the inference pipelineQwak makes the deployment of the LLM microservice straightforward.During Lesson 7, we fine-tuned the LLM and built the Qwak model. As a quick refresher, we ran the following CLI command to build the Qwak model, where we used the build_config.yaml file with the build configuration:poetry run qwak models build -f build_config.yaml .After the build is finished, we can make various deployments based on the build. For example, we can deploy the LLM microservice using the following Qwak command:qwak models deploy realtime \\\\--model-id \"llm_twin\" \\\\--instance \"gpu.a10.2xl\" \\\\ --timeout 50000 \\\\ --replicas 2 \\\\--server-workers 2We deployed two replicas of the LLM twin. Each replica has access to a machine with x1 A10 GPU. Also, each replica has two workers running on it.üîó More on Qwak instance types ‚ÜêTwo replicas and two workers result in 4 microservices that run in parallel and can serve our users.You can scale the deployment to more replicas if you need to serve more clients. Qwak provides autoscaling mechanisms triggered by listening to the consumption of GPU, CPU or RAM.To conclude, you build the Qwak model once, and based on it, you can make multiple deployments with various strategies.You can quickly close the deployment by running the following:qwak models undeploy --model-id \"llm_twin\"We strongly recommend closing down the deployment when you are done, as GPU VMs are expensive.To run the LLM system with a predefined prompt example, you have to run the following Python file:poetry run python main.pyWithin the main.py file, we call the LLMTwin class, which calls the other services as explained during this lesson.Note: The ‚Üí complete installation & usage instructions ‚Üê are available in the README of the GitHub repository.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èConclusionCongratulations! You are close to the end of the LLM twin series.In Lesson 9 of the LLM twin course, you learned to build a scalable inference pipeline for serving LLMs and RAG systems.First, you learned how to architect an inference pipeline by understanding the difference between monolithic and microservice architectures. We also highlighted the difference in designing the training and inference pipelines.Secondly, we walked you through implementing the RAG business module and LLM twin microservice. Also, we showed you how to log all the prompts, answers, and metadata for Comet‚Äôs prompt monitoring service.Ultimately, we showed you how to deploy and run the LLM twin inference pipeline on the Qwak AI platform.In Lesson 10, we will show you how to evaluate the whole system by building an advanced RAG evaluation pipeline that analyzes the accuracy of the LLMs ‚Äô answers relative to the query and context.See you there! ü§óüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èEnjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE ‚ÜìDecoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Add your models to Model Registry (2024), Comet ML GuidesImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthMachine LearningProgrammingMl System DesignData ScienceArtificial Intelligence5605601FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13See all from Paul IusztinSee all from Decoding MLRecommended from MediumVipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Vishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesChatGPT21 stories¬∑846 savesDerckData architecture for MLOps: Metadata storeIntroductionJul 17Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922MdabdullahalhasibinTowards AIA Complete Guide to Embedding For NLP & Generative AI/LLMUnderstand the concept of vector embedding, why it is needed, and implementation with LangChain.3d agoNecati DemirAdvanced RAG: Implementing Advanced Techniques to Enhance Retrieval-Augmented Generation SystemsMay 16481See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99'),\n",
       "  ArticleDocument(id=UUID('d39ca560-21bf-4a6c-a080-064b1ad7996a'), content={'Title': 'Real-time feature pipelines for RAG - by Paul Iusztin', 'Subtitle': 'RAG hybrid search with transformers-based sparse vectors. CDC tech stack for event-driven architectures.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Real-time feature pipelines for RAG\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Real-time feature pipelines for RAG\\n\\n### RAG hybrid search with transformers-based sparse vectors. CDC tech stack\\nfor event-driven architectures.\\n\\nPaul Iusztin\\n\\nAug 17, 2024\\n\\n14\\n\\nShare this post\\n\\n#### Real-time feature pipelines for RAG\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n### **This week‚Äôs topics:**\\n\\n  * CDC tech stack for event-driven architectures\\n\\n  * Real-time feature pipelines with CDC\\n\\n  * RAG hybrid search with transformers-based sparse vectors\\n\\n* * *\\n\\n### CDC tech stack for event-driven architectures\\n\\nHere is the ùòÅùó≤ùó∞ùóµ ùòÄùòÅùóÆùó∞ùó∏ used to ùóØùòÇùó∂ùóπùó± a ùóñùóµùóÆùóªùó¥ùó≤ ùóóùóÆùòÅùóÆ ùóñùóÆùóΩùòÅùòÇùóøùó≤ (ùóñùóóùóñ) ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅ for\\nimplementing an ùó≤ùòÉùó≤ùóªùòÅ-ùó±ùóøùó∂ùòÉùó≤ùóª ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤ in our ùóüùóüùó† ùóßùòÑùó∂ùóª ùó∞ùóºùòÇùóøùòÄùó≤  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóñùóµùóÆùóªùó¥ùó≤ ùóóùóÆùòÅùóÆ ùóñùóÆùóΩùòÅùòÇùóøùó≤ (ùóñùóóùóñ)?  \\n  \\nThe purpose of CDC is to capture insertions, updates, and deletions applied to\\na database and to make this change data available in a format easily\\nconsumable by downstream applications.  \\n  \\nùó™ùóµùòÜ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùóñùóóùóñ ùóΩùóÆùòÅùòÅùó≤ùóøùóª?  \\n  \\n\\\\- Real-time Data Syncing  \\n\\\\- Efficient Data Pipelines  \\n\\\\- Minimized System Impact  \\n\\\\- Event-Driven Architectures  \\n  \\nùó™ùóµùóÆùòÅ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùó≥ùóºùóø ùóÆùóª ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅùóÆùòÅùó∂ùóºùóª ùóºùó≥ ùóñùóóùóñ?  \\n  \\nWe will take the tech stack used in our LLM Twin course as an example,\\nwhere...  \\n  \\n... we built a feature pipeline to gather cleaned data for fine-tuning and\\nchunked & embedded data for RAG  \\n  \\nùóòùòÉùó≤ùóøùòÜùòÅùóµùó∂ùóªùó¥ ùòÑùó∂ùóπùóπ ùóØùó≤ ùó±ùóºùóªùó≤ ùóºùóªùóπùòÜ ùó∂ùóª ùó£ùòÜùòÅùóµùóºùóª!  \\n  \\nùòèùò¶ùò≥ùò¶ ùòµùò©ùò¶ùò∫ ùò¢ùò≥ùò¶  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\n1\\\\. ùóßùóµùó≤ ùòÄùóºùòÇùóøùó∞ùó≤ ùó±ùóÆùòÅùóÆùóØùóÆùòÄùó≤: MongoDB (it (also works for most databases such as\\nMySQL, PostgreSQL, Oracle, etc.)  \\n  \\n2\\\\. ùóî ùòÅùóºùóºùóπ ùòÅùóº ùó∫ùóºùóªùó∂ùòÅùóºùóø ùòÅùóµùó≤ ùòÅùóøùóÆùóªùòÄùóÆùó∞ùòÅùó∂ùóºùóª ùóπùóºùó¥: MongoDB Watcher (also Debezium is a\\npopular & scalable solution)  \\n  \\n3\\\\. ùóî ùó±ùó∂ùòÄùòÅùóøùó∂ùóØùòÇùòÅùó≤ùó± ùóæùòÇùó≤ùòÇùó≤: RabbitMQ (another popular option is to use Kafka, but\\nit was overkill in our use case)  \\n  \\n4\\\\. ùóî ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó≤ùóªùó¥ùó∂ùóªùó≤: Bytewax (great streaming engine for the Python\\necosystem)  \\n  \\n5\\\\. ùóî ùòÄùóºùòÇùóøùó∞ùó≤ ùó±ùóÆùòÅùóÆùóØùóÆùòÄùó≤: Qdrant (this works with any other database, but we\\nneeded a vector DB to store our data for fine-tuning and RAG)\\n\\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶, ùò©ùò¶ùò≥ùò¶ ùò™ùò¥ ùò©ùò∞ùò∏ ùò¢ ùòûùòôùòêùòõùòå ùò∞ùò±ùò¶ùò≥ùò¢ùòµùò™ùò∞ùòØ ùò∏ùò™ùò≠ùò≠ ùò£ùò¶ ùò±ùò≥ùò∞ùò§ùò¶ùò¥ùò¥ùò¶ùò•:  \\n  \\n1\\\\. Write a post to the MongoDB warehouse  \\n2\\\\. A \"ùò§ùò≥ùò¶ùò¢ùòµùò¶\" operation is logged in the transaction log of Mongo  \\n3\\\\. The MongoDB watcher captures this and emits it to the RabbitMQ queue  \\n4\\\\. The Bytewax streaming pipelines read the event from the queue  \\n5\\\\. It cleans, chunks, and embeds it right away - in real time!  \\n6\\\\. The cleaned & embedded version of the post is written to Qdrant\\n\\n* * *\\n\\n### Real-time feature pipelines with CDC\\n\\nùóõùóºùòÑ to ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùóñùóóùóñ to ùòÄùòÜùóªùó∞ your ùó±ùóÆùòÅùóÆ ùòÑùóÆùóøùó≤ùóµùóºùòÇùòÄùó≤ and ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùòÄùòÅùóºùóøùó≤ using a\\nRabbitMQ ùóæùòÇùó≤ùòÇùó≤ and a Bytewax ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó≤ùóªùó¥ùó∂ùóªùó≤ ‚Üì  \\n  \\nùóôùó∂ùóøùòÄùòÅ, ùóπùó≤ùòÅ\\'ùòÄ ùòÇùóªùó±ùó≤ùóøùòÄùòÅùóÆùóªùó± ùòÑùóµùó≤ùóøùó≤ ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùòÅùóº ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùòÅùóµùó≤ ùóñùóµùóÆùóªùó¥ùó≤ ùóóùóÆùòÅùóÆ ùóñùóÆùóΩùòÅùòÇùóøùó≤\\n(ùóñùóóùóñ) ùóΩùóÆùòÅùòÅùó≤ùóøùóª:  \\n  \\nùòäùòãùòä ùò™ùò¥ ùò∂ùò¥ùò¶ùò• ùò∏ùò©ùò¶ùòØ ùò∫ùò∞ùò∂ ùò∏ùò¢ùòØùòµ ùòµùò∞ ùò¥ùò∫ùòØùò§ 2 ùò•ùò¢ùòµùò¢ùò£ùò¢ùò¥ùò¶ùò¥.  \\n  \\nThe destination can be a complete replica of the source database (e.g., one\\nfor transactional and the other for analytical applications)  \\n  \\n...or you can process the data from the source database before loading it to\\nthe destination DB (e.g., retrieve various documents and chunk & embed them\\nfor RAG).  \\n  \\nùòõùò©ùò¢ùòµ\\'ùò¥ ùò∏ùò©ùò¢ùòµ ùòê ùò¢ùòÆ ùò®ùò∞ùò™ùòØùò® ùòµùò∞ ùò¥ùò©ùò∞ùò∏ ùò∫ùò∞ùò∂:  \\n  \\n**How** to **use CDC** to **sync** a **MongoDB** & **Qdrant vector DB** to\\nstreamline real-time documents that must be ready for fine-tuning LLMs and\\nRAG.  \\n  \\n**MongoDB** is our data warehouse.  \\n  \\n**Qdrant** is our logical feature store.  \\n  \\n.  \\n  \\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùòÅùóµùó≤ ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅùóÆùòÅùó∂ùóºùóª ùóºùó≥ ùòÅùóµùó≤ ùóñùóóùóñ ùóΩùóÆùòÅùòÅùó≤ùóøùóª:  \\n  \\n1\\\\. Use Mongo\\'s ùò∏ùò¢ùòµùò§ùò©() method to listen for CRUD transactions  \\n  \\n2\\\\. For example, on a CREATE operation, along with saving it to Mongo, the\\nùò∏ùò¢ùòµùò§ùò©() method will trigger a change and return a JSON with all the\\ninformation.  \\n  \\n3\\\\. We standardize the JSON in our desired structure.  \\n  \\n4\\\\. We stringify the JSON and publish it to the RabbitMQ queue  \\n  \\nùóõùóºùòÑ ùó±ùóº ùòÑùó≤ ùòÄùó∞ùóÆùóπùó≤?  \\n  \\n‚Üí You can use Debezium instead of Mongo\\'s ùò∏ùò¢ùòµùò§ùò©() method for scaling up the\\nsystem, but the idea remains the same.  \\n  \\n‚Üí You can swap RabbitMQ with Kafka, but RabbitMQ can get you far.  \\n  \\nùó°ùóºùòÑ, ùòÑùóµùóÆùòÅ ùóµùóÆùóΩùóΩùó≤ùóªùòÄ ùóºùóª ùòÅùóµùó≤ ùóºùòÅùóµùó≤ùóø ùòÄùó∂ùó±ùó≤ ùóºùó≥ ùòÅùóµùó≤ ùóæùòÇùó≤ùòÇùó≤?  \\n  \\nYou have a Bytewax streaming pipeline - 100% written in Python that:  \\n  \\n5\\\\. Listens in real-time to new messages from the RabbitMQ queue  \\n  \\n6\\\\. It cleans, chunks, and embeds the events on the fly  \\n  \\n7\\\\. It loads the data to Qdrant for LLM fine-tuning & RAG\\n\\nMongoDB CDC example\\n\\n> Do you ùòÑùóÆùóªùòÅ to check out the ùó≥ùòÇùóπùóπ ùó∞ùóºùó±ùó≤?  \\n>  \\n> ...or even an ùó≤ùóªùòÅùó∂ùóøùó≤ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ about ùóñùóóùóñ?  \\n>  \\n> The CDC component is part of the ùóüùóüùó† ùóßùòÑùó∂ùóª FREE ùó∞ùóºùòÇùóøùòÄùó≤, made by Decoding ML.  \\n>  \\n> ‚Üì‚Üì‚Üì  \\n>  \\n> üîó ùòìùò¶ùò¥ùò¥ùò∞ùòØ 3: ùòäùò©ùò¢ùòØùò®ùò¶ ùòãùò¢ùòµùò¢ ùòäùò¢ùò±ùòµùò∂ùò≥ùò¶: ùòåùòØùò¢ùò£ùò≠ùò™ùòØùò® ùòåùò∑ùò¶ùòØùòµ-ùòãùò≥ùò™ùò∑ùò¶ùòØ ùòàùò≥ùò§ùò©ùò™ùòµùò¶ùò§ùòµùò∂ùò≥ùò¶ùò¥  \\n>  \\n> üîó ùòéùò™ùòµùòèùò∂ùò£\\n\\n* * *\\n\\n### RAG hybrid search with transformers-based sparse vectors\\n\\nùóõùòÜùóØùóøùó∂ùó± ùòÄùó≤ùóÆùóøùó∞ùóµ is standard in ùóÆùó±ùòÉùóÆùóªùó∞ùó≤ùó± ùó•ùóîùóö ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ. The ùòÅùóøùó∂ùó∞ùó∏ is to ùó∞ùóºùó∫ùóΩùòÇùòÅùó≤ the\\nsuitable ùòÄùóΩùóÆùóøùòÄùó≤ ùòÉùó≤ùó∞ùòÅùóºùóøùòÄ for it. Here is an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ that shows ùóµùóºùòÑ to use\\nùó¶ùó£ùóüùóîùóóùóò to ùó∞ùóºùó∫ùóΩùòÇùòÅùó≤ ùòÄùóΩùóÆùóøùòÄùó≤ ùòÉùó≤ùó∞ùòÅùóºùóøùòÄ using ùòÅùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùó≤ùóøùòÄ and integrate them into a\\nùóµùòÜùóØùóøùó∂ùó± ùòÄùó≤ùóÆùóøùó∞ùóµ ùóÆùóπùó¥ùóºùóøùó∂ùòÅùóµùó∫ using Qdrant.  \\n  \\nùôíùôùùôÆ ùôóùô§ùô©ùôùùôöùôß ùô¨ùôûùô©ùôù ùô®ùô•ùôñùôßùô®ùôö ùô´ùôöùôòùô©ùô§ùôßùô® ùô¨ùôùùôöùô£ ùô¨ùôö ùôùùôñùô´ùôö ùôôùôöùô£ùô®ùôö ùô´ùôöùôòùô©ùô§ùôßùô® (ùôöùô¢ùôóùôöùôôùôôùôûùô£ùôúùô®)?  \\n  \\nSparse vectors represent data by highlighting only the most relevant features\\n(like keywords), significantly reducing memory usage compared to dense\\nvectors.  \\n  \\nAlso, sparse vectors work great in finding specific keywords, which is why\\nthey work fantastic in combination with dense vectors used for finding\\nsimilarities in semantics but not particular words.  \\n  \\nùóßùóµùó≤ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ ùóµùó∂ùó¥ùóµùóπùó∂ùó¥ùóµùòÅùòÄ:  \\n  \\n\\\\- ùòöùò±ùò¢ùò≥ùò¥ùò¶ ùò∑ùò¥. ùò•ùò¶ùòØùò¥ùò¶ ùò∑ùò¶ùò§ùòµùò∞ùò≥ùò¥  \\n  \\n\\\\- ùòèùò∞ùò∏ ùòöùòóùòìùòàùòãùòå ùò∏ùò∞ùò≥ùò¨ùò¥: The SPLADE model leverages sparse vectors to perform\\nbetter than traditional methods like BM25 by computing it using transformer\\narchitectures.  \\n  \\n\\\\- ùòûùò©ùò∫ ùòöùòóùòìùòàùòãùòå ùò∏ùò∞ùò≥ùò¨ùò¥: It expands terms based on context rather than just\\nfrequency, offering a nuanced understanding of content relevancy.  \\n  \\n\\\\- ùòèùò∞ùò∏ ùòµùò∞ ùò™ùòÆùò±ùò≠ùò¶ùòÆùò¶ùòØùòµ ùò©ùò∫ùò£ùò≥ùò™ùò• ùò¥ùò¶ùò¢ùò≥ùò§ùò© ùò∂ùò¥ùò™ùòØùò® ùòöùòóùòìùòàùòãùòå with Qdrant: step-by-step code\\n\\nSparse vectors using transformers\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùòÅùóµùó≤ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòöùò±ùò¢ùò≥ùò¥ùò¶ ùòùùò¶ùò§ùòµùò∞ùò≥ùò¥ ùò™ùòØ ùòòùò•ùò≥ùò¢ùòØùòµ: ùòóùò∂ùò≥ùò¶ ùòùùò¶ùò§ùòµùò∞ùò≥-ùò£ùò¢ùò¥ùò¶ùò• ùòèùò∫ùò£ùò≥ùò™ùò• ùòöùò¶ùò¢ùò≥ùò§ùò©\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n14\\n\\nShare this post\\n\\n#### Real-time feature pipelines for RAG\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/real-time-feature-pipelines-with?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('4271a54f-6239-4f50-97e6-b3fa3a9a2fbd'), content={'Title': 'Building ML System Using the FTI Architecture', 'Subtitle': 'Introduction to the feature/training/inference (FTI) design pattern to build scalable and modular ML systems using MLOps best practices.', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Building ML systems the right way using the FTI architecture\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Building ML systems the right way using the FTI architecture\\n\\n### The fundamentals of the FTI architecture that will help you build modular\\nand scalable ML systems using MLOps best practices.\\n\\nPaul Iusztin\\n\\nAug 10, 2024\\n\\n12\\n\\nShare this post\\n\\n#### Building ML systems the right way using the FTI architecture\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nThe feature/training/inference (FTI) architecture builds scalable and modular\\nML systems using MLOps best practices.\\n\\nWe will start by discussing the problems of naively building ML systems. Then,\\nwe will examine other potential solutions and their problems.\\n\\nUltimately, we will present the feature/training/inference (FTI) design\\npattern and its benefits. We will also understand the benefits of using a\\nfeature store and model registry when architecting your ML system.\\n\\n### The problem with building ML systems\\n\\nBuilding production-ready ML systems is much more than just training a model.\\nFrom an engineering point of view, training the model is the most\\nstraightforward step in most use cases.\\n\\nHowever, training a model becomes complex when deciding on the correct\\narchitecture and hyperparameters. That‚Äôs not an engineering problem but a\\nresearch problem.\\n\\nAt this point, we want to focus on how to design a production-ready\\narchitecture. Training a model with high accuracy is extremely valuable, but\\njust by training it on a static dataset, you are far from deploying it\\nrobustly. We have to consider how to:\\n\\n  * ingest, clean and validate fresh data\\n\\n  * training vs. inference setups\\n\\n  * compute and serve features in the right environment\\n\\n  * serve the model in a cost-effective way\\n\\n  * version, track and share the datasets and models\\n\\n  * monitor your infrastructure and models\\n\\n  * deploy the model on a scalable infrastructure\\n\\n  * automate the deployments and training\\n\\nThese are the types of problems an ML or MLOps engineer must consider, while\\nthe research or data science team is often responsible for training the model.\\n\\nFigure 1: Components of an ML system. Photo from the Google Cloud Architecture\\ndocuments\\n\\nFigure 1 shows all the components the Google Cloud team suggests that a mature\\nML and MLOps system requires. Along with the ML code, there are many moving\\npieces. The rest of the system comprises configuration, automation, data\\ncollection, data verification, testing and debugging, resource management,\\nmodel analysis, process and metadata management, serving infrastructure, and\\nmonitoring. The point is that there are many components we must consider when\\nproductionizing an ML model.\\n\\n_Thus, the**critical question** is: ‚ÄúHow do we connect all these components\\ninto a single homogenous system‚Äù?_\\n\\nWe must create a boilerplate for clearly designing ML systems to answer that\\nquestion.\\n\\nSimilar solutions exist for classic software. For example, if you zoom out,\\nmost software applications can be split between a database, business logic and\\nUI layer. Every layer can be as complex as needed, but at a high-level\\noverview, the architecture of standard software can be boiled down to these\\nthree components.\\n\\nDo we have something similar for ML applications? The first step is to examine\\nprevious solutions and why they are unsuitable for building scalable ML\\nsystems.\\n\\n* * *\\n\\n### **The issue with previous solutions**\\n\\nIn Figure 2, you can observe the typical architecture present in most ML\\napplications. It is based on a monolithic batch architecture that couples the\\nfeature creation, model training, and inference into the same component.\\n\\nBy taking this approach, you quickly solve one critical problem in the ML\\nworld: the training-serving skew. The training-serving skew happens when the\\nfeatures passed to the model are computed differently at training and\\ninference time. In this architecture, the features are created using the same\\ncode. Hence, the training-serving skew issue is solved by default.\\n\\nThis pattern works fine when working with small data. The pipeline runs on a\\nschedule in batch mode, and the predictions are consumed by a third-party\\napplication such as a dashboard.\\n\\nFigure 2: Monolithic batch pipeline architecture\\n\\nUnfortunately, building a monolithic batch system raises many other issues,\\nsuch as:\\n\\n  * features are not reusable (by your system or others)\\n\\n  * if the data increases, you have to refactor the whole code to support PySpark or Ray\\n\\n  * hard to rewrite the prediction module in a more efficient language such as C++, Java or Rust\\n\\n  * hard to share the work between multiple teams between the features, training, and prediction modules\\n\\n  * impossible to switch to a streaming technology for real-time training\\n\\nIn Figure 3, we can see a similar scenario for a real-time system. This use\\ncase introduces another issue in addition to what we listed before. To make\\nthe predictions, we have to transfer the whole state through the client\\nrequest so the features can be computed and passed to the model.\\n\\nConsider the scenario of computing movie recommendations for a user. Instead\\nof simply passing the user ID, we must transmit the entire user state,\\nincluding their name, age, gender, movie history, and more. This approach is\\nfraught with potential errors, as the client must understand how to access\\nthis state, and it‚Äôs tightly coupled with the model service.\\n\\nAnother example would be when implementing an LLM with RAG support. The\\ndocuments we add as context along the query represent our external state. If\\nwe didn‚Äôt store the records in a vector DB, we would have to pass them with\\nthe user query. To do so, the client must know how to query and retrieve the\\ndocuments, which is not feasible. It is an antipattern for the client\\napplication to know how to access or compute the features. If you don‚Äôt\\nunderstand how RAG works, we will explain it in future chapters.\\n\\nFigure 3: Stateless real-time architecture\\n\\nIn conclusion, our problem is accessing the features to make predictions\\nwithout passing them at the client‚Äôs request. For example, based on our first\\nuser movie recommendation example, how can we predict the recommendations\\nsolely based on the user‚Äôs ID?\\n\\nRemember these questions, as we will answer them shortly.\\n\\n### **The solution: the FTI architecture**\\n\\nThe solution is based on creating a clear and straightforward mind map that\\nany team or person can follow to compute the features, train the model, and\\nmake predictions.\\n\\nBased on these three critical steps that any ML system requires, the pattern\\nis known as the FTI (feature, training, inference) pipelines. So, how does\\nthis differ from what we presented before?\\n\\nThe pattern suggests that any ML system can be boiled down to these three\\npipelines: feature, training, and inference (similar to the database, business\\nlogic and UI layers from classic software).\\n\\nThis is powerful, as we can clearly define the scope and interface of each\\npipeline. Also, it‚Äôs easier to understand how the three components interact.\\n\\nAs shown in Figure 4, we have the feature, training and inference pipelines.\\nWe will zoom in on each of them and understand their scope and interface.\\n\\nBefore going into the details, it is essential to understand that each\\npipeline is a different component that can run on a different process or\\nhardware. Thus, each pipeline can be written using a different technology, by\\na different team, or scaled differently. The key idea is that the design is\\nvery flexible to the needs of your team. It acts as a mind map for structuring\\nyour architecture.\\n\\nFigure 4: Feature/Training/Inference (FTI) pipelines architecture\\n\\n#### The feature pipeline\\n\\nThe feature pipelines take as input data and output features & labels used to\\ntrain the model.\\n\\nInstead of directly passing them to the model, the features and labels are\\nstored inside a feature store. Its responsibility is to store, version, track,\\nand share the features.\\n\\nBy saving the features into a feature store, we always have a state of our\\nfeatures. Thus, we can easily send the features to the training and inference\\npipeline(s).\\n\\nAs the data is versioned, we can always ensure that the training and inference\\ntime features match. Thus, we avoid the training-serving skew problem.\\n\\n#### The training pipeline\\n\\nThe training pipeline takes the features and labels from the features store as\\ninput and outputs a train model or models.\\n\\nThe models are stored in a model registry. Its role is similar to that of\\nfeature stores, but this time, the model is the first-class citizen. Thus, the\\nmodel registry will store, version, track, and share the model with the\\ninference pipeline.\\n\\nAlso, most modern model registries support a metadata store that allows you to\\nspecify essential aspects of how the model was trained. The most important are\\nthe features, labels and their version used to train the model. Thus, we will\\nalways know what data the model was trained on.\\n\\n#### The inference pipeline\\n\\nThe inference pipeline takes as input the features & labels from the feature\\nstore and the trained model from the model registry. With these two,\\npredictions can be easily made in either batch or real-time mode.\\n\\nAs this is a versatile pattern, it is up to you to decide what you do with\\nyour predictions. If it‚Äôs a batch system, they will probably be stored in a\\ndatabase. If it‚Äôs a real-time system, the predictions will be served to the\\nclient who requested them.\\n\\nAs the features, labels, and model are versioned. We can easily upgrade or\\nroll back the deployment of the model. For example, we will always know that\\nmodel v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus,\\nwe can quickly change the connections between the model and features.\\n\\n### Benefits of the FTI architecture\\n\\nTo conclude, the most important thing you must remember about the FTI\\npipelines is their interface:\\n\\n¬∑ The feature pipeline takes in data and outputs features & labels saved to\\nthe feature store.\\n\\n¬∑ The training pipelines query the features store for features & labels and\\noutput a model to the model registry.\\n\\n¬∑ The inference pipeline uses the features from the feature store and the\\nmodel from the model registry to make predictions.\\n\\nIt doesn‚Äôt matter how complex your ML system gets. These interfaces will\\nremain the same.\\n\\nNow that we better understand how the pattern works, we want to highlight the\\nmain benefits of using this pattern:\\n\\n  * as you have just three components, it is intuitive to use and easy to understand;\\n\\n  * each component can be written into its tech stack, so we can quickly adapt them to specific needs, such as big or streaming data. Also, it allows us to pick the best tools for the job;\\n\\n  * as there is a transparent interface between the three components, each one can be developed by a different team (if necessary), making the development more manageable and scalable;\\n\\n  * every component can be deployed, scaled, and monitored independently.\\n\\nThe final thing you must understand about the FTI pattern is that the system\\ndoesn‚Äôt have to contain only three pipelines. In most cases, it will include\\nmore. For example, the feature pipeline can be composed of a service that\\ncomputes the features and one that validates the data. Also, the training\\npipeline can be composed of the training and evaluation components.\\n\\nThe FTI pipelines act as logical layers. Thus, it is perfectly fine for each\\nto be complex and contain multiple services. However, what is essential is to\\nstick to the same interface on how the FTI pipelines interact with each other\\nthrough the feature store and model registries. By doing so, each FTI\\ncomponent can evolve differently, without knowing the details of each other\\nand without breaking the system on new changes.\\n\\n### Conclusion\\n\\nIn this article, we understood the fundamental problems when naively building\\nML systems.\\n\\nWe also looked at potential solutions and their downsides.\\n\\nUltimately, we presented the FTI architecture, its benefits, and how to apply\\nit to modern ML systems.\\n\\n* * *\\n\\n> My _**latest book** , ‚ÄúLLM Engineer‚Äôs Handbook,‚Äù _inspired me to write this\\n> article.\\n\\nIf you liked this article, consider supporting me by buying my book and enjoy\\na lot more similar content compressed into a single book:\\n\\nLLM Engineer's Handbook\\n\\nLLM Engineer‚Äôs Handbook Cover\\n\\n* * *\\n\\n### References\\n\\n### Literature\\n\\n[1] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference\\nPipelines [2023], Hopsworks blog\\n\\n### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n12\\n\\nShare this post\\n\\n#### Building ML systems the right way using the FTI architecture\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/building-ml-systems-the-right-way?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('2ce3c5d1-730b-4258-88ab-07009eddaf33'), content={'Title': 'Reduce your PyTorch code latency by 82% - by Paul Iusztin', 'Subtitle': 'How not to optimize the inference of your DL models. Computer science is dead.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Reduce your PyTorch code latency by 82%\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Reduce your PyTorch code latency by 82%\\n\\n### How not to optimize the inference of your DL models. Computer science is\\ndead.\\n\\nPaul Iusztin\\n\\nAug 03, 2024\\n\\n9\\n\\nShare this post\\n\\n#### Reduce your PyTorch code latency by 82%\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Reduce the latency of your PyTorch code by 82%\\n\\n  * How I failed to optimize the inference of my DL models\\n\\n  * Computer science is dead\\n\\n* * *\\n\\n> ùó°ùó≤ùòÑ ùóØùóºùóºùó∏ on engineering end-to-end LLM systems, from data collection and\\n> fine-tuning to LLMOps (deployment, monitoring).\\n\\nI kept this one a secret, but in the past months, in collaboration with Packt\\n, Alex Vesa and Maxime Labonne , we started working on the ùòìùòìùòî ùòåùòØùò®ùò™ùòØùò¶ùò¶ùò≥\\'ùò¥\\nùòèùò¢ùòØùò•ùò£ùò∞ùò∞ùò¨.  \\n  \\nùóî ùóØùóºùóºùó∏ that will walk you through everything you know to build a production-\\nready LLM project.\\n\\nI am a big advocate of learning with hands-on examples while being anchored in\\nreal-world use cases.  \\n  \\nThat is why this is not the standard theoretical book.  \\n  \\nWhile reading the book, you will learn to build a complex LLM project: an LLM\\nTwin. In contrast, theoretical aspects will back everything to understand why\\nwe make certain decisions.  \\n  \\nHowever, our ultimate goal is to present a framework that can be applied to\\nmost LLM projects.  \\n  \\n.  \\n  \\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùóÆ ùòÄùóªùó≤ùóÆùó∏ ùóΩùó≤ùó≤ùó∏ ùóºùó≥ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùòÑùó∂ùóπùóπ ùóπùó≤ùóÆùóøùóª ùòÑùó∂ùòÅùóµùó∂ùóª ùòÅùóµùó≤ ùóüùóüùó† ùóòùóªùó¥ùó∂ùóªùó≤ùó≤ùóø\\'ùòÄ\\nùóõùóÆùóªùó±ùóØùóºùóºùó∏:  \\n  \\n\\\\- collect unstructured data  \\n\\\\- create instruction datasets from raw data to fine-tune LLMs  \\n\\\\- SFT techniques such as LoRA and QLoRA  \\n\\\\- LLM evaluation techniques  \\n\\\\- Preference alignment using DPO  \\n\\\\- inference optimization methods (key optimization, model parallelism,\\nquantization, attention mechanisms)  \\n\\\\- advanced RAG algorithms using LangChain as our LLM framework and Qdrant as\\nour vector DB  \\n  \\n\\\\- design LLM systems using the FTI architecture  \\n\\\\- use AWS SageMaker to fine-tune and deploy open-source LLMs  \\n\\\\- use ZenML to orchestrate all the pipelines and track the data as artifacts  \\n\\\\- LLMOps patterns such as CT/CI/CD pipelines, model registries and using\\nComet for experiment tracking and prompt monitoring  \\n  \\n.  \\n  \\nThe book is still a work in progress, but we are very excited about it!  \\n  \\nThank you, Packt, for making this possible and Maxime and Alex for this\\nremarkable collaboration.  \\n  \\nIf you are curious, you can currently pre-order it from Amazon. The whole book\\nshould be released by the end of September 2024.  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòìùòìùòî ùòåùòØùò®ùò™ùòØùò¶ùò¶ùò≥\\'ùò¥ ùòèùò¢ùòØùò•ùò£ùò∞ùò∞ùò¨: ùòîùò¢ùò¥ùòµùò¶ùò≥ ùòµùò©ùò¶ ùò¢ùò≥ùòµ ùò∞ùòß ùò¶ùòØùò®ùò™ùòØùò¶ùò¶ùò≥ùò™ùòØùò® ùòìùò¢ùò≥ùò®ùò¶ ùòìùò¢ùòØùò®ùò∂ùò¢ùò®ùò¶ ùòîùò∞ùò•ùò¶ùò≠ùò¥\\nùòßùò≥ùò∞ùòÆ ùò§ùò∞ùòØùò§ùò¶ùò±ùòµ ùòµùò∞ ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ\\n\\n* * *\\n\\n### Reduce the latency of your PyTorch code by 82%\\n\\nThis is how I ùóøùó≤ùó±ùòÇùó∞ùó≤ùó± the ùóπùóÆùòÅùó≤ùóªùó∞ùòÜ of my ùó£ùòÜùóßùóºùóøùó∞ùóµ ùó∞ùóºùó±ùó≤ by ùü¥ùüÆ% ùòÇùòÄùó∂ùóªùó¥ ùóºùóªùóπùòÜ ùó£ùòÜùòÅùóµùóºùóª\\n& ùó£ùòÜùóßùóºùóøùó∞ùóµ. ùó°ùó¢ ùó≥ùóÆùóªùó∞ùòÜ ùòÅùóºùóºùóπùòÄ ùó∂ùóªùòÉùóºùóπùòÉùó≤ùó±!  \\n  \\nùôèùôùùôö ùô•ùôßùô§ùôóùô°ùôöùô¢?  \\n  \\nDuring inference, I am using 5 DL at ~25k images at once.  \\n  \\nThe script took around ~4 hours to run.  \\n  \\nThe problem is that this isn\\'t a batch job that runs over the night...  \\n  \\nVarious people across the company required it to run in \"real-time\" multiple\\ntimes a day.\\n\\nùôèùôùùôö ùô®ùô§ùô°ùô™ùô©ùôûùô§ùô£?  \\n  \\nThe first thing that might come to your mind is to start using some fancy\\noptimizer (e.g., TensorRT).  \\n  \\nEven though that should be done at some point...  \\n  \\nFirst, you should ùóÆùòÄùó∏ ùòÜùóºùòÇùóøùòÄùó≤ùóπùó≥:  \\n  \\n\\\\- I/O bottlenecks: reading & writing images  \\n\\\\- preprocessing & postprocessing - can it be parallelized?  \\n\\\\- are the CUDA cores used at their maximum potential?  \\n\\\\- is the bandwidth between the CPU & GPU throttled?  \\n\\\\- can we move more computation to the GPU?  \\n  \\nThat being said...  \\n  \\nùóõùó≤ùóøùó≤ is what I did I ùó±ùó≤ùó∞ùóøùó≤ùóÆùòÄùó≤ùó± the ùóπùóÆùòÅùó≤ùóªùó∞ùòÜ of the script by ùü¥ùüÆ%  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nùü≠\\\\. ùóïùóÆùòÅùó∞ùóµùó≤ùó± ùòÅùóµùó≤ ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùòÄùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nBatching is not only valuable for training but also mighty in speeding up your\\ninference time.  \\n  \\nOtherwise, you waste your GPU CUDA cores.  \\n  \\nInstead of passing through the models one sample at a time, I now process 64.  \\n  \\nùüÆ\\\\. ùóüùó≤ùòÉùó≤ùóøùóÆùó¥ùó≤ùó± ùó£ùòÜùóßùóºùóøùó∞ùóµ\\'ùòÄ ùóóùóÆùòÅùóÆùóüùóºùóÆùó±ùó≤ùóø  \\n  \\nThis has 2 main advantages:  \\n  \\n\\\\- parallel data loading & preprocessing on multiple processes (NOT threads)  \\n\\\\- copying your input images directly into the pinned memory (avoid a CPU ->\\nCPU copy operation)  \\n  \\nùüØ\\\\. ùó†ùóºùòÉùó≤ùó± ùóÆùòÄ ùó∫ùòÇùó∞ùóµ ùóºùó≥ ùòÅùóµùó≤ ùóΩùóºùòÄùòÅùóΩùóøùóºùó∞ùó≤ùòÄùòÄùó∂ùóªùó¥ ùóºùóª ùòÅùóµùó≤ ùóöùó£ùó®  \\n  \\nI saw that the tensor was moved too early on the CPU and mapped to a NumPy\\narray.  \\n  \\nI refactored the code to keep it on the GPU as much as possible, which had 2\\nmain advantages:  \\n  \\n\\\\- tensors are processed faster on the GPU  \\n\\\\- at the end of the logic, I had smaller tensors, resulting in smaller\\ntransfers between the CPU & GPU  \\n  \\nùü∞\\\\. ùó†ùòÇùóπùòÅùó∂ùòÅùóµùóøùó≤ùóÆùó±ùó∂ùóªùó¥ ùó≥ùóºùóø ùóÆùóπùóπ ùó∫ùòÜ ùóú/ùó¢ ùòÑùóøùó∂ùòÅùó≤ ùóºùóΩùó≤ùóøùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nFor I/O bottlenecks, using Python threads is extremely powerful.  \\n  \\nI moved all my writes under a ùòõùò©ùò≥ùò¶ùò¢ùò•ùòóùò∞ùò∞ùò≠ùòåùòπùò¶ùò§ùò∂ùòµùò∞ùò≥, batching my write\\noperations.  \\n  \\n.  \\n  \\nNote that I used only good old Python & PyTorch code.  \\n  \\n‚Üí When the code is poorly written, no tool can save you  \\n  \\nOnly now is the time to add fancy tooling, such as TensorRT.\\n\\n.\\n\\nSo remember...  \\n  \\nTo optimize the PyTorch code by 82%:  \\n  \\n1\\\\. Batched the inference samples  \\n2\\\\. Leveraged PyTorch\\'s DataLoader  \\n3\\\\. Moved as much of the postprocessing on the GPU  \\n4\\\\. Multithreading for all my I/O write operations  \\n  \\nWhat other methods do you have in mind? Leave them in the comments ‚Üì\\n\\n* * *\\n\\n### How I failed to optimize the inference of my DL models\\n\\nThis is how I FAILED to ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùó≤ the ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ of my ùóóùóü ùó∫ùóºùó±ùó≤ùóπùòÄ when ùóøùòÇùóªùóªùó∂ùóªùó¥\\nùòÅùóµùó≤ùó∫ on a ùó°ùòÉùó∂ùó±ùó∂ùóÆ ùóöùó£ùó®. Let me tell you ùòÑùóµùóÆùòÅ ùòÅùóº ùóÆùòÉùóºùó∂ùó± ‚Üì  \\n  \\nI had a simple task. To reduce the latency of the DL models used in\\nproduction.  \\n  \\nWe had 4 DL models that were running on Nvidia GPUs.  \\n  \\nAfter a first look at the inference code, I saw that the inputs to the models\\nweren\\'t batched.  \\n  \\nWe were processing one sample at a time.  \\n  \\nI said to myself: \"Ahaa! That\\'s it. I cracked it. We just have to batch as\\nmany samples as possible, and we are done.\"  \\n  \\nSo, I did just that...  \\n  \\nAfter 2-3 days of work adding the extra batch dimension to the PyTorch\\npreprocessing & postprocessing code, ùóú ùóøùó≤ùóÆùóπùó∂ùòáùó≤ùó± ùóú ùó™ùóîùó¶ ùó™ùó•ùó¢ùó°ùóö.\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùòÑùóµùòÜ  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nWe were using Nvidia GPUs from the A family (A6000, A5000, etc.).  \\n  \\nAs these GPUs have a lot of memory (>40GB), I managed to max out the VRAM and\\nsquash a batch of 256 images on the GPU.  \\n  \\nRelative to using a \"ùò£ùò¢ùòµùò§ùò© = 1\" it was faster, but not A LOT FASTER, as I\\nexpected.  \\n  \\nThen I tried batches of 128, 64, 32, 16, and 8.  \\n  \\n...and realized that everything > batch = 16 was running slower than using a\\nbatch of 16.  \\n  \\n‚Üí ùóî ùóØùóÆùòÅùó∞ùóµ ùóºùó≥ ùü≠ùü≤ ùòÑùóÆùòÄ ùòÅùóµùó≤ ùòÄùòÑùó≤ùó≤ùòÅ ùòÄùóΩùóºùòÅ.  \\n  \\nBut that is not good, as I was using only ~10% of the VRAM...  \\n  \\nùó™ùóµùòÜ ùó∂ùòÄ ùòÅùóµùóÆùòÅ?  \\n  \\nThe Nvidia A family of GPUs are known to:  \\n  \\n\\\\- having a lot of VRAM  \\n\\\\- not being very fast (the memory transfer between the CPU & GPU + the number\\nof CUDA cores isn\\'t that great)  \\n  \\nThat being said, my program was throttled.  \\n  \\nEven if my GPU could handle much more memory-wise, the memory transfer &\\nprocessing speeds weren\\'t keeping up.  \\n  \\nIn the end, it was a good optimization: ~75% faster  \\n  \\nùóïùòÇùòÅ ùòÅùóµùó≤ ùóπùó≤ùòÄùòÄùóºùóª ùóºùó≥ ùòÅùóµùó∂ùòÄ ùòÄùòÅùóºùóøùòÜ ùó∂ùòÄ:  \\n  \\n‚Üí ALWAYS KNOW YOUR HARDWARE ‚Üê  \\n  \\nMost probably, running a bigger batch on an A100 or V100 wouldn\\'t have the\\nsame problem.  \\n  \\nI plan to try that.  \\n  \\nBut that is why...  \\n  \\n‚Üí ùôÆùô§ùô™ ùôñùô°ùô¨ùôñùôÆùô® ùôùùôñùô´ùôö ùô©ùô§ ùô§ùô•ùô©ùôûùô¢ùôûùôØùôö ùô©ùôùùôö ùô•ùôñùôßùôñùô¢ùôöùô©ùôöùôßùô® ùô§ùôõ ùôÆùô§ùô™ùôß ùô®ùôÆùô®ùô©ùôöùô¢ ùôóùôñùô®ùôöùôô ùô§ùô£ ùôÆùô§ùô™ùôß\\nùôùùôñùôßùôôùô¨ùôñùôßùôö!\\n\\nIn theory, I knew this, but it is completely different when you encounter it\\nin production.  \\n  \\nLet me know in the comments if you want more similar stories on \"DO NOTs\" from\\nmy experience.\\n\\n* * *\\n\\n### Computer science is dead\\n\\nùóñùóºùó∫ùóΩùòÇùòÅùó≤ùóø ùòÄùó∞ùó∂ùó≤ùóªùó∞ùó≤ ùó∂ùòÄ ùó±ùó≤ùóÆùó±. Do this instead.  \\n  \\nIn a recent talk, Jensen Huang, CEO of Nvidia, said that kids shouldn\\'t learn\\nprogramming anymore.  \\n  \\nHe said that until now, most of us thought that everyone should learn to\\nprogram at some point.  \\n  \\nBut the actual opposite is the truth.  \\n  \\nWith the rise of AI, nobody should have or need to learn to program anymore.  \\n  \\nHe highlights that with AI tools, the technology divide between non-\\nprogrammers and engineers is closing.  \\n  \\n.  \\n  \\nùóîùòÄ ùóÆùóª ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóø, ùó∫ùòÜ ùó≤ùó¥ùóº ùó∂ùòÄ ùóµùòÇùóøùòÅ; ùó∫ùòÜ ùó≥ùó∂ùóøùòÄùòÅ ùóøùó≤ùóÆùó∞ùòÅùó∂ùóºùóª ùó∂ùòÄ ùòÅùóº ùòÄùóÆùòÜ ùó∂ùòÅ ùó∂ùòÄ ùòÄùòÅùòÇùóΩùó∂ùó±.  \\n  \\nBut after thinking about it more thoroughly, I tend to agree with him.  \\n  \\nAfter all, even now, almost anybody can work with AI.  \\n  \\nThis probably won\\'t happen in the next 10 years, but at some point, 100% will\\ndo.  \\n  \\nAt some point, we will ask our AI companion to write a program that does X for\\nus or whatever.  \\n  \\nBut, I think this is a great thing, as it will give us more time & energy to\\nfocus on what matters, such as:  \\n  \\n\\\\- solving real-world problems (not just tech problems)  \\n\\\\- moving to the next level of technology (Bioengineering, interplanetary\\ncolonization, etc.)  \\n\\\\- think about the grand scheme of things  \\n\\\\- be more creative  \\n\\\\- more time to connect with our family  \\n\\\\- more time to take care of our  \\n  \\nI personally think it is a significant step for humanity.  \\n  \\n.  \\n  \\nWhat do you think?  \\n  \\nAs an engineer, do you see your job still present in the next 10+ years?  \\n  \\nHere is the full talk  \\n  \\n‚Üì‚Üì‚Üì\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n9\\n\\nShare this post\\n\\n#### Reduce your PyTorch code latency by 82%\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| SorinAug 3Liked by Paul IusztinExcellent article, except the part CS is dead\\nis invalidExpand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/reduce-your-pytorchs-code-latency?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('7a276ac3-5c78-42d3-9ecf-05ff7f76fe31'), content={'Title': 'LLM Agents Demystified  - by Li - Decoding ML Newsletter ', 'Subtitle': 'Hands-on ReAct Agent implementation with AdalFlow library', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### LLM Agents Demystified\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# LLM Agents Demystified\\n\\n### Hands-on ReAct Agent implementation with AdalFlow library\\n\\nLi\\n\\nJul 27, 2024\\n\\n14\\n\\nShare this post\\n\\n#### LLM Agents Demystified\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nHi, all! I‚Äôm Li Yin, Author of AdalFlow and ex AI researcher @ MetaAI\\n\\nFind me on LinkedIn\\n\\nHandy links:\\n\\n  * AdalFlow Github\\n\\n  * Open in Colab\\n\\n _AdalFlow is an LLM library that not only helps developers build but also\\noptimizes LLM task pipelines. Embracing a design pattern similar to PyTorch,\\nAdalFlow is light, modular, and robust, with a 100% readable codebase._\\n\\n_There are many tutorials that show users how to call high-level agent APIs,\\nbut none of them explain how it really works in depth. This is where the\\nAdalFlow library aims to make a difference._\\n\\n_In this blog, you will not only learn how to use the ReAct Agent but more\\nimportantly, also understand how it was implemented and how you can customize\\nor build your own agent with AdalFlow._\\n\\n_Let‚Äôs get started!_\\n\\n_Image source , credits to Growtika_\\n\\n## Introduction\\n\\n _‚ÄúAn autonomous agent is a system situated within and a part of an\\nenvironment that senses that environment and acts on it, over time, in pursuit\\nof its own agenda and so as to effect what it senses in the future.‚Äù_\\n\\n _‚Äî Franklin and Graesser (1997)_\\n\\nAlongside the well-known RAGs, agents [1] are another popular family of LLM\\napplications. What makes agents stand out is their ability to reason, plan,\\nand act via accessible tools. When it comes to implementation, AdalFlow has\\nsimplified it down to a generator that can use tools, taking multiple steps\\n(sequential or parallel) to complete a user query.\\n\\n* * *\\n\\n### Table of Contents:\\n\\n  1. What is ReAct Agent\\n\\n  2. Introduction on tools/function calls\\n\\n  3. ReAct Agent implementation\\n\\n  4. ReAct Agent in action\\n\\n* * *\\n\\n### 1\\\\. What is ReAct Agent\\n\\nReAct [2] is a general paradigm for building agents that sequentially\\ninterleaves thought, action, and observation steps.\\n\\n  * **Thought** : The reasoning behind taking an action.\\n\\n  * **Action** : The action to take from a predefined set of actions. In particular, these are the tools/functional tools we have introduced in tools.\\n\\n  * **Observation** : The simplest scenario is the execution result of the action in string format. To be more robust, this can be defined in any way that provides the right amount of execution information for the LLM to plan the next step.\\n\\n#### **Prompt and Data Models**\\n\\n _The prompt is the most straightforward way to understand any LLM\\napplication. Always read the prompt._\\n\\nAdalFlow uses jinja2 syntax for the prompt.\\n\\nDEFAULT_REACT_AGENT_SYSTEM_PROMPT is the default prompt for the React agent‚Äôs\\nLLM planner. We can categorize the prompt template into four parts:\\n\\n  1. **Task description**\\n\\nThis part is the overall role setup and task description for the agent.\\n\\n    \\n    \\n    task_desc = r\"\"\"You are a helpful assistant.Answer the user\\'s query using the tools provided below with minimal steps and maximum accuracy.Each step you will read the previous Thought, Action, and Observation(execution result of the action) and then provide the next Thought and Action.\"\"\"\\n\\n  2. **Tools, output format, and example**\\n\\nThis part of the template is exactly the same as how we were calling functions\\nin the tools. The `output_format_str` is generated by `FunctionExpression` via\\n`JsonOutputParser`. It includes the actual output format and examples of a\\nlist of `FunctionExpression` instances. We use `thought` and `action` fields\\nof the `FunctionExpression` as the agent‚Äôs response. _You will be easily\\nvisualize the whole pipeline later by simply_`print(react).`\\n\\n    \\n    \\n    tools = r\"\"\"{% if tools %}\\n    <TOOLS>\\n    {% for tool in tools %}\\n    {{ loop.index }}.\\n    {{tool}}\\n    ------------------------\\n    {% endfor %}\\n    </TOOLS>\\n    {% endif %}\\n    {{output_format_str}}\"\"\"\\n\\n  3. **Task specification to teach the planner how to ‚Äúthink‚Äù.**\\n\\nWe provide more detailed instruction to ensure the agent will always end with\\n‚Äòfinish‚Äô action to complete the task. Additionally, we teach it how to handle\\nsimple queries and complex queries.\\n\\n  * For simple queries, we instruct the agent to finish with as few steps as possible.\\n\\n  * For complex queries, we teach the agent a ‚Äòdivide-and-conquer‚Äô strategy to solve the query step by step.\\n\\n    \\n    \\n    task_spec = r\"\"\"<TASK_SPEC>\\n    - For simple queries: Directly call the ``finish`` action and provide the answer.\\n    - For complex queries:\\n       - Step 1: Read the user query and potentially divide it into subqueries. And get started with the first subquery.\\n       - Call one available tool at a time to solve each subquery/subquestion. \\\\\\n       - At step \\'finish\\', join all subqueries answers and finish the task.\\n    Remember:\\n    - Action must call one of the above tools with name. It can not be empty.\\n    - You will always end with \\'finish\\' action to finish the task. The answer can be the final answer or failure message.\\n    </TASK_SPEC>\"\"\"\\n\\nWe put all these three parts together to be within the `<SYS></SYS>` tag.\\n\\n  4. **Agent step history.**\\n\\nWe use `StepOutput` to record the agent‚Äôs step history, including:\\n\\n  * `action`: This will be the `FunctionExpression` instance predicted by the agent.\\n\\n  * `observation`: The execution result of the action.\\n\\nIn particular, we format the steps history after the user query as follows:\\n\\n    \\n    \\n    step_history = r\"\"\"User query:\\n    {{ input_str }}\\n    {# Step History #}\\n    {% if step_history %}\\n    <STEPS>\\n    {% for history in step_history %}\\n    Step {{ loop.index }}.\\n    \"Thought\": \"{{history.action.thought}}\",\\n    \"Action\": \"{{history.action.action}}\",\\n    \"Observation\": \"{{history.observation}}\"\\n    ------------------------\\n    {% endfor %}\\n    </STEPS>\\n    {% endif %}\\n    You:\"\"\"\\n\\n### 2\\\\. Introduction on tools/function calls\\n\\nIn addition to the tools provided by users, by default, we add a new tool\\nnamed `finish` to allow the agent to stop and return the final answer.\\n\\n    \\n    \\n    def finish(answer: str) -> str:\\n       \"\"\"Finish the task with answer.\"\"\"\\n       return answer\\n\\nSimply returning a string might not fit all scenarios, and we might consider\\nallowing users to define their own finish function in the future for more\\ncomplex cases.\\n\\nAdditionally, since the provided tools cannot always solve user queries, we\\nallow users to configure if an LLM model should be used to solve a subquery\\nvia the `add_llm_as_fallback` parameter. This LLM will use the same model\\nclient and model arguments as the agent‚Äôs planner. Here is our code to specify\\nthe fallback LLM tool:\\n\\n    \\n    \\n    _additional_llm_tool = (\\n       Generator(model_client=model_client, model_kwargs=model_kwargs)\\n       if self.add_llm_as_fallback\\n       else None\\n    )\\n    \\n    def llm_tool(input: str) -> str:\\n       \"\"\"I answer any input query with llm\\'s world knowledge. Use me as a fallback tool or when the query is simple.\"\"\"\\n       # use the generator to answer the query\\n       try:\\n             output: GeneratorOutput = _additional_llm_tool(\\n                prompt_kwargs={\"input_str\": input}\\n             )\\n             response = output.data if output else None\\n             return response\\n       except Exception as e:\\n             log.error(f\"Error using the generator: {e}\")\\n             print(f\"Error using the generator: {e}\")\\n       return None\\n\\n### 3\\\\. ReAct Agent implementation\\n\\nWe define the class ReActAgent to put everything together. It will orchestrate\\ntwo components:\\n\\n  * `planner`: A `Generator` that works with a `JsonOutputParser` to parse the output format and examples of the function calls using `FunctionExpression`.\\n\\n  * `ToolManager`: Manages a given list of tools, the finish function, and the LLM tool. It is responsible for parsing and executing the functions.\\n\\nAdditionally, it manages step_history as a list of `StepOutput` instances for\\nthe agent‚Äôs internal state.\\n\\nPrompt the agent with an input query and process the steps to generate a\\nresponse.\\n\\n### 4\\\\. ReAct Agent in action\\n\\nWe will set up two sets of models, llama3‚Äì70b-8192 by Groq and gpt-3.5-turbo\\nby OpenAI, to test two queries. For comparison, we will compare these with a\\nvanilla LLM response without using the agent. Here are the code snippets:\\n\\n    \\n    \\n    from lightrag.components.agent import ReActAgent\\n    from lightrag.core import Generator, ModelClientType, ModelClient\\n    from lightrag.utils import setup_env\\n    \\n    setup_env()\\n    \\n    # Define tools\\n    def multiply(a: int, b: int) -> int:\\n       \"\"\"\\n       Multiply two numbers.\\n       \"\"\"\\n       return a * b\\n    def add(a: int, b: int) -> int:\\n       \"\"\"\\n       Add two numbers.\\n       \"\"\"\\n       return a + b\\n    def divide(a: float, b: float) -> float:\\n       \"\"\"\\n       Divide two numbers.\\n       \"\"\"\\n       return float(a) / b\\n    llama3_model_kwargs = {\\n       \"model\": \"llama3-70b-8192\",  # llama3 70b works better than 8b here.\\n       \"temperature\": 0.0,\\n    }\\n    gpt_model_kwargs = {\\n       \"model\": \"gpt-3.5-turbo\",\\n       \"temperature\": 0.0,\\n    }\\n    \\n    def test_react_agent(model_client: ModelClient, model_kwargs: dict):\\n       tools = [multiply, add, divide]\\n       queries = [\\n          \"What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2?\",\\n          \"Give me 5 words rhyming with cool, and make a 4-sentence poem using them\",\\n       ]\\n       # define a generator without tools for comparison\\n       generator = Generator(\\n          model_client=model_client,\\n          model_kwargs=model_kwargs,\\n       )\\n       react = ReActAgent(\\n          max_steps=6,\\n          add_llm_as_fallback=True,\\n          tools=tools,\\n          model_client=model_client,\\n          model_kwargs=model_kwargs,\\n       )\\n       # print(react)\\n       for query in queries:\\n          print(f\"Query: {query}\")\\n          agent_response = react.call(query)\\n          llm_response = generator.call(prompt_kwargs={\"input_str\": query})\\n          print(f\"Agent response: {agent_response}\")\\n          print(f\"LLM response: {llm_response}\")\\n          print(\"\")\\n\\nThe structure of React using `print(react)`, including the initialization\\narguments and two major components: `tool_manager` and `planner`. You can\\nvisualize the structure from our colab.\\n\\nNow, let‚Äôs run the test function to see the agent in action.\\n\\n    \\n    \\n    test_react_agent(ModelClientType.GROQ(), llama3_model_kwargs)\\n    test_react_agent(ModelClientType.OPENAI(), gpt_model_kwargs)\\n\\nOur agent will show the core steps for developers via colored printout,\\nincluding input_query, steps, and the final answer. The printout of the first\\nquery with llama3 is shown below (without the color here):\\n\\n    \\n    \\n    2024-07-10 16:48:47 - [react.py:287:call] - input_query: What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2\\n    \\n    2024-07-10 16:48:48 - [react.py:266:_run_one_step] - Step 1:\\n    StepOutput(step=1, action=FunctionExpression(thought=\"Let\\'s break down the query into subqueries and start with the first one.\", action=\\'llm_tool(input=\"What is the capital of France?\")\\'), function=Function(thought=None, name=\\'llm_tool\\', args=[], kwargs={\\'input\\': \\'What is the capital of France?\\'}), observation=\\'The capital of France is Paris!\\')\\n    _______\\n    2024-07-10 16:48:49 - [react.py:266:_run_one_step] - Step 2:\\n    StepOutput(step=2, action=FunctionExpression(thought=\"Now, let\\'s move on to the second subquery.\", action=\\'multiply(a=465, b=321)\\'), function=Function(thought=None, name=\\'multiply\\', args=[], kwargs={\\'a\\': 465, \\'b\\': 321}), observation=149265)\\n    _______\\n    2024-07-10 16:48:49 - [react.py:266:_run_one_step] - Step 3:\\n    StepOutput(step=3, action=FunctionExpression(thought=\"Now, let\\'s add 95297 to the result.\", action=\\'add(a=149265, b=95297)\\'), function=Function(thought=None, name=\\'add\\', args=[], kwargs={\\'a\\': 149265, \\'b\\': 95297}), observation=244562)\\n    _______\\n    2024-07-10 16:48:50 - [react.py:266:_run_one_step] - Step 4:\\n    StepOutput(step=4, action=FunctionExpression(thought=\"Now, let\\'s divide the result by 13.2.\", action=\\'divide(a=244562, b=13.2)\\'), function=Function(thought=None, name=\\'divide\\', args=[], kwargs={\\'a\\': 244562, \\'b\\': 13.2}), observation=18527.424242424244)\\n    _______\\n    2024-07-10 16:48:50 - [react.py:266:_run_one_step] - Step 5:\\n    StepOutput(step=5, action=FunctionExpression(thought=\"Now, let\\'s combine the answers of both subqueries.\", action=\\'finish(answer=\"The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\")\\'), function=Function(thought=None, name=\\'finish\\', args=[], kwargs={\\'answer\\': \\'The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\'}), observation=\\'The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\')\\n    _______\\n    2024-07-10 16:48:50 - [react.py:301:call] - answer:\\n    The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\n\\nThe comparison between the agent and the vanilla LLM response is shown below:\\n\\n    \\n    \\n    Answer with agent: The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\n    Answer without agent: GeneratorOutput(data=\"I\\'d be happy to help you with that!\\\\n\\\\nThe capital of France is Paris.\\\\n\\\\nNow, let\\'s tackle the math problem:\\\\n\\\\n1. 465 √ó 321 = 149,485\\\\n2. Add 95,297 to that result: 149,485 + 95,297 = 244,782\\\\n3. Divide the result by 13.2: 244,782 √∑ 13.2 = 18,544.09\\\\n\\\\nSo, the answer is 18,544.09!\", error=None, usage=None, raw_response=\"I\\'d be happy to help you with that!\\\\n\\\\nThe capital of France is Paris.\\\\n\\\\nNow, let\\'s tackle the math problem:\\\\n\\\\n1. 465 √ó 321 = 149,485\\\\n2. Add 95,297 to that result: 149,485 + 95,297 = 244,782\\\\n3. Divide the result by 13.2: 244,782 √∑ 13.2 = 18,544.09\\\\n\\\\nSo, the answer is 18,544.09!\", metadata=None)\\n\\nThe ReAct agent is particularly helpful for answering queries that require\\ncapabilities like computation or more complicated reasoning and planning.\\nHowever, using it on general queries might be an overkill, as it might take\\nmore steps than necessary to answer the query.\\n\\n### 5\\\\. [Optional] Customization\\n\\nPlease refer to our tutorial for how to customize ReAct to your use case.\\n\\n* * *\\n\\n## References\\n\\n[1] A survey on large language model based autonomous agents: Paitesanshi/LLM-\\nAgent-Survey\\n\\n[2]**** ReAct: https://arxiv.org/abs/2210.03629\\n\\n[3] Tool Tutorial: https://lightrag.sylph.ai/tutorials/tool_helper.html  \\n\\n## API References\\n\\n  * components.agent.react.ReActAgent\\n\\n  * core.types.StepOutput\\n\\n  * components.agent.react.DEFAULT_REACT_AGENT_SYSTEM_PROMPT\\n\\n14\\n\\nShare this post\\n\\n#### LLM Agents Demystified\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n| A guest post by| LiAuthor of AdalFlow, Founder at SylphAI, ex AI researcher\\nat MetaAI. Github: liyin2015| Subscribe to Li  \\n---|---  \\n  \\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/llm-agents-demystified?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('12ad5863-ba57-4f5c-9ab7-4600c7edbf5c'), content={'Title': 'Scalable RAG pipeline using 74.3% less code', 'Subtitle': 'Tutorial on building a scalable & modular advanced RAG feature pipeline to chunk, embed and ingest multiple data categories to a vector DB using Superlinked', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Scalable RAG ingestion pipeline using 74.3% less code\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Scalable RAG ingestion pipeline using 74.3% less code\\n\\n### End-to-end implementation for an advanced RAG feature pipeline\\n\\nPaul Iusztin\\n\\nJul 20, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Scalable RAG ingestion pipeline using 74.3% less code\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _‚Üí the 1st lesson of the Superlinked bonus series from**the LLM Twin** free\\ncourse_\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> _More**details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà_\\n\\n## Latest lessons of the LLM Twin course\\n\\n**Lesson 8:** Best practices when evaluating fine-tuned LLM models\\n\\n‚Üí Quantitative/Qualitative Evaluation Metrics, Human-in-the-Loop, LLM-Eval\\n\\n**Lesson 9:** Architect scalable and cost-effective LLM & RAG inference\\npipelines\\n\\n‚ÜíMonolithic vs. microservice, Qwak Deployment, RAG Pipeline Walkthrough\\n\\n**Lesson 10:** How to evaluate your RAG using RAGAs Framework\\n\\n‚Üí RAG evaluation best practic, RAGAs framework\\n\\n* * *\\n\\n## **Lesson 11: Build a scalable RAG ingestion pipeline using 74.3% less\\ncode**\\n\\n**Lessons 11** and **12** are part of a **bonus serie** s in which we will\\ntake the advanced RAG system from the **LLM Twin course** (written in\\nLangChain) and refactor it using Superlinked, a framework specialized in\\nvector computing for information retrieval.\\n\\nIn **Lesson 11** **(this article)** , we will learn to build a highly\\nscalable, real-time RAG feature pipeline that ingests multi-data categories\\ninto a Redis vector database.\\n\\nMore concretely we will take the ingestion pipeline implemented in Lesson 4\\nand swap the chunking, embedding, and vector DB logic with Superlinked.\\n\\n_You don‚Äôt have to readLesson 4 to read this article. We will give enough\\ncontext to make sense of it._\\n\\nIn the **12th lesson** , we will use Superlinked to implement a multi-index\\nquery strategy and further optimize the advanced RAG retrieval module\\n(initially built in Lesson 5).\\n\\n> _The value of this article lies in understanding how easy it is to build\\n> complex advanced RAG systems usingSuperlinked._\\n>\\n> _**Using Superlinked** , we **reduced** the number of RAG-related **lines of\\n> code** by **74.3%**. Powerful, right?_\\n\\nBy the **end of this article** , **you will learn** to build a production-\\nready feature pipeline built in Superlinked that:\\n\\n  * uses Bytewax as a stream engine to process data in real-time;\\n\\n  * ingests multiple data categories from a RabbitMQ queue;\\n\\n  * validates the data with Pydantic;\\n\\n  * chunks, and embeds data using Superlinked for doing RAG;\\n\\n  * loads the embedded vectors along their metadata to a Redis vector DB;\\n\\nUltimately, on the infrastructure side, we will show you how to deploy a\\nSuperlinked vector compute server.\\n\\n### **Quick intro in feature pipelines**\\n\\nThe **feature pipeline** is the **first** **pipeline** presented in the\\n**FTI** **pipeline architecture** : feature, training and inference pipelines.\\n\\nA **feature pipeline** takes raw data as input, processes it into features,\\nand stores it in a feature store, from which the training & inference\\npipelines will use it.\\n\\nThe component is completely isolated from the training and inference code. All\\nthe communication is done through the feature store.\\n\\n> _To avoid repeating myself, if you are**unfamiliar** with the **FTI**\\n> **pipeline architecture** , check out Lesson 1 for a refresher._\\n\\n* * *\\n\\n## **Table of Contents**\\n\\n  1. What is Superlinked?\\n\\n  2. The old architecture of the RAG feature pipeline\\n\\n  3. The new Superlinked architecture of the RAG feature pipeline\\n\\n  4. Understanding the streaming flow for real-time processing\\n\\n  5. Loading data to Superlinked\\n\\n  6. Exploring the RAG Superlinked server\\n\\n  7. Using Redis as a vector DB\\n\\n>  _üîó**Check out** the code on GitHub [1] and support us with a _\\n\\n* * *\\n\\n## **1\\\\. What is Superlinked?**\\n\\n_Superlinked is a computing framework for turning complex data into vectors._\\n\\nIt lets you quickly build multimodal vectors and define weights at query time,\\nso you don‚Äôt need a custom reranking algorithm to optimize results.\\n\\nIt‚Äôs focused on turning complex data into vector embeddings within your RAG,\\nSearch, RecSys and Analytics stack.\\n\\nI love how Daniel Svonava, the CEO of Superlinked, described the value of\\nvector compute and implicitly Superlinked:\\n\\n> _Daniel Svonava, CEO at Superlinked:_\\n>\\n> _‚ÄúVectors power most of what you already do online ‚Äî hailing a cab, finding\\n> a funny video, getting a date, scrolling through a feed or paying with a\\n> tap. And yet, building production systems powered by vectors is still too\\n> hard! Our goal is to help enterprises put vectors at the center of their\\n> data & compute infrastructure, to build smarter and more reliable\\n> software.‚Äù_\\n\\nTo conclude, Superlinked is a framework that puts the vectors in the center of\\ntheir universe and allows you to:\\n\\n  * chunk and embed embeddings;\\n\\n  * store multi-index vectors in a vector DB;\\n\\n  * do complex vector search queries on top of your data.\\n\\nScreenshot from Superlinked‚Äôs landing page\\n\\n* * *\\n\\n## **2\\\\. The old architecture of the RAG feature pipeline**\\n\\nHere is a quick recap of the critical aspects of the architecture of the RAG\\nfeature pipeline presented in the 4th lesson of the LLM Twin course.\\n\\n_We are working with**3 different data categories** :_\\n\\n  * posts (e.g., LinkedIn, Twitter)\\n\\n  * articles (e.g., Medium, Substack, or any other blog)\\n\\n  * repositories (e.g., GitHub, GitLab)\\n\\nEvery data category has to be preprocessed differently. For example, you want\\nto chunk the posts into smaller documents while keeping the articles in bigger\\nones.\\n\\n_The**solution** is based on **CDC** , a **queue,** a **streaming engine,**\\nand a **vector DB:**_\\n\\n-> The raw data is collected from multiple social platforms and is stored in MongoDB. (Lesson 2)\\n\\n‚Üí CDC adds any change made to the MongoDB to a RabbitMQ queue (Lesson 3).\\n\\n‚Üí the RabbitMQ queue stores all the events until they are processed.\\n\\n‚Üí The Bytewax streaming engine reads the messages from the RabbitMQ queue and\\ncleans, chunks, and embeds them.\\n\\n‚Üí The processed data is uploaded to a Qdrant vector DB.\\n\\nThe old feature/streaming pipeline architecture that was presented in Lesson\\n4.\\n\\n### **Why is this design robust?**\\n\\nHere are 4 core reasons:\\n\\n  1. The **data** is **processed** in **real-time**.\\n\\n  2. **Out-of-the-box recovery system:** If the streaming pipeline fails to process a message, it will be added back to the queue\\n\\n  3. **Lightweight:** No need for any diffs between databases or batching too many records\\n\\n  4. **No I/O bottlenecks** on the source database\\n\\n### **What is the issue with this design?**\\n\\nIn this architecture, we had to write custom logic to chunk, embed, and load\\nthe data to Qdrant.\\n\\nThe issue with this approach is that we had to leverage various libraries,\\nsuch as LangChain and unstructured, to get the job done.\\n\\nAlso, because we have 3 data categories, we had to write a dispatcher layer\\nthat calls the right function depending on its category, which resulted in\\ntons of boilerplate code.\\n\\nUltimately, as the chunking and embedding logic is implemented directly in the\\nstreaming pipeline, it is harder to scale horizontally. The embedding\\nalgorithm needs powerful GPU machines, while the rest of the operations\\nrequire a strong CPU.\\n\\nThis results in:\\n\\n  * more time spent on development;\\n\\n  * more code to maintain;\\n\\n  * the code can quickly become less readable;\\n\\n  * less freedom to scale.\\n\\nSuperlinked can speed up this process by providing a very intuitive and\\npowerful Python API that can speed up the development of our ingestion and\\nretrieval logic.\\n\\nThus, let‚Äôs see how to redesign the architecture using Superlinked ‚Üì\\n\\n## **3\\\\. The new Superlinked architecture of the RAG feature pipeline**\\n\\nThe core idea of the architecture will be the same. We still want to:\\n\\n  * use a Bytewax streaming engine for real-time processing;\\n\\n  * read new events from RabbitMQ;\\n\\n  * clean, chunk, and embed the new incoming raw data;\\n\\n  * load the processed data to a vector DB.\\n\\n**The question is** , how will we do this with Superlinked?\\n\\nAs you can see in the image below, Superlinked will replace the logic for the\\nfollowing operations:\\n\\n  * chunking;\\n\\n  * embedding;\\n\\n  * vector storage;\\n\\n  * queries.\\n\\nAlso, we have to swap Qdrant with a Redis vector DB because Superlinked didn‚Äôt\\nsupport Qdrant when I wrote this article. But they plan to add it in future\\nmonths (along with many other vector DBs).\\n\\nWhat will remain unchanged are the following:\\n\\n  * the Bytewax streaming layer;\\n\\n  * the RabbitMQ queue ingestion component;\\n\\n  * the cleaning logic.\\n\\n> _By seeing**what we must change** to the architecture to integrate\\n> Superlinked, we can **see** the **framework‚Äôs core features**._\\n\\nThe components that can be refactored into the Superlinked framework.\\n\\nNow, let‚Äôs take a deeper look at the new architecture.\\n\\nAll the Superlinked logic will sit on its own server, completely decoupling\\nthe vector compute component from the rest of the feature pipeline.\\n\\nWe can quickly scale the streaming pipeline or the Superlinked server\\nhorizontally based on our needs. Also, this makes it easier to run the\\nembedding models (from Superlinked) on a machine with a powerful GPU while\\nkeeping the streaming pipeline on a machine optimized for network I/O\\noperations.\\n\\nAll the communication to Superlinked (ingesting or query data) will be done\\nthrough a REST API, automatically generated based on the schemas and queries\\nyou define in your Superlinked application.\\n\\nThe **Bytewax streaming pipeline** will perform the following operations:\\n\\n  * will concurrently read messages from RabbitMQ;\\n\\n  * clean each message based on it‚Äôs data category;\\n\\n  * send the cleaned document to the Superlinked server through an HTTP request.\\n\\n**On the** **Superlinked server side** , we have defined an ingestion endpoint\\nfor each data category (article, post or code). Each endpoint will know how to\\nchunk embed and store every data point based on its category.\\n\\nAlso, we have a query endpoint (automatically generated) for each data\\ncategory that will take care of embedding the query and perform a vector\\nsemantic search operation to retrieve similar results.\\n\\nThe RAG feature pipeline architecture after refactoring.\\n\\nNow, let‚Äôs finally jump into the code ‚Üì\\n\\n* * *\\n\\n## **4\\\\. Understanding the streaming flow for real-time processing**\\n\\nThe **Bytewax flow** is the **central point** of the **streaming pipeline**.\\nIt defines all the required steps, following the next simplified pattern:\\n_‚Äúinput - > processing -> output‚Äù._\\n\\nHere is the Bytewax flow and its core steps ‚Üì\\n\\n    \\n    \\n    flow = Dataflow(\"Streaming RAG feature pipeline\")\\n    stream = op.input(\"input\", flow, RabbitMQSource())\\n    stream = op.map(\"raw\", stream, RawDispatcher.handle_mq_message)\\n    stream = op.map(\"clean\", stream, CleaningDispatcher.dispatch_cleaner)\\n    op.output(\\n        \"superlinked_output\",\\n        stream,\\n        SuperlinkedOutputSink(client=SuperlinkedClient()),\\n    )\\n\\n## **5\\\\. Loading data to Superlinked**\\n\\nBefore we explore the Superlinked application, let‚Äôs review our Bytewax\\n_SuperlinkedOutputSink()_ and _SuperlinkedClient() _classes.\\n\\nThe purpose of the _SuperlinkedOutputSink()_ class is to instantiate a new\\n_SuperlinkedSinkPartition()_ instance for each worker within the Bytewax\\ncluster. Thus, we can optimize the system for I/O operations by scaling our\\noutput workers horizontally.\\n\\n    \\n    \\n    class SuperlinkedOutputSink(DynamicSink):\\n        def __init__(self, client: SuperlinkedClient) -> None:\\n            self._client = client\\n    \\n        def build(self, worker_index: int, worker_count: int) -> StatelessSinkPartition:\\n            return SuperlinkedSinkPartition(client=self._client)\\n\\nThe _SuperlinkedSinkPartition()_ class inherits the _StatelessSinkPartition\\nBytewax base class_ used to create custom stateless partitions.\\n\\nThis class takes as input batches of items and sends them to Superlinked\\nthrough the _SuperlinkedClient()_.\\n\\n    \\n    \\n    class SuperlinkedSinkPartition(StatelessSinkPartition):\\n        def __init__(self, client: SuperlinkedClient):\\n            self._client = client\\n    \\n        def write_batch(self, items: list[Document]) -> None:\\n            for item in tqdm(items, desc=\"Sending items to Superlinked...\"):\\n                match item.type:\\n                    case \"repositories\":\\n                        self._client.ingest_repository(item)\\n                    case \"posts\":\\n                        self._client.ingest_post(item)\\n                    case \"articles\":\\n                        self._client.ingest_article(item)\\n                    case _:\\n                        logger.error(f\"Unknown item type: {item.type}\")\\n\\nThe _SuperlinkedClient() _is a basic wrapper that makes HTTP requests to the\\nSuperlinked server that contains all the RAG logic. We use _httpx_ to make __\\nPOST requests for ingesting or searching data.\\n\\n    \\n    \\n    class SuperlinkedClient:\\n        ...\\n    \\n        def ingest_repository(self, data: RepositoryDocument) -> None:\\n            self.__ingest(f\"{self.base_url}/api/v1/ingest/repository_schema\", data)\\n    \\n        def ingest_post(self, data: PostDocument) -> None:\\n            self.__ingest(f\"{self.base_url}/api/v1/ingest/post_schema\", data)\\n    \\n        def ingest_article(self, data: ArticleDocument) -> None:\\n            self.__ingest(f\"{self.base_url}/api/v1/ingest/article_schema\", data)\\n    \\n        def __ingest(self, url: str, data: T) -> None:\\n            ...\\n    \\n        def search_repository(\\n            self, search_query: str, platform: str, author_id: str, *, limit: int = 3\\n        ) -> list[RepositoryDocument]:\\n            return self.__search(\\n                f\"{self.base_url}/api/v1/search/repository_query\",\\n                RepositoryDocument,\\n                search_query,\\n                platform,\\n                author_id,\\n                limit=limit,\\n            )\\n    \\n        def search_post(\\n            self, search_query: str, platform: str, author_id: str, *, limit: int = 3\\n        ) -> list[PostDocument]:\\n            ... # URL: f\"{self.base_url}/api/v1/search/post_query\"\\n    \\n        def search_article(\\n            self, search_query: str, platform: str, author_id: str, *, limit: int = 3\\n        ) -> list[ArticleDocument]:\\n            ... # URL: f\"{self.base_url}/api/v1/search/article_query\"\\n    \\n        def __search(\\n            self, url: str, document_class: type[T], search_query: str, ...\\n        ) -> list[T]:\\n            ...\\n          \\n\\nThe Superlinked server URLs are automatically generated as follows:\\n\\n  * the ingestion URLs are generated based on the data schemas you defined (e.g., repository schema, post schema, etc.)\\n\\n  * the search URLs are created based on the Superlinked queries defined within the application\\n\\n## **6\\\\. Exploring the RAG Superlinked server**\\n\\nAs the RAG Superlinked server is a different component than the Bytewax one,\\nthe implementation sits under the server folder at _6-bonus-superlinked-\\nrag/server/src/app.py._\\n\\n_Here is a step-by-step implementation of the Superlinked application ‚Üì_\\n\\n### **Settings class**\\n\\nUse Pydantic settings to define a global configuration class.\\n\\n    \\n    \\n    class Settings(BaseSettings):\\n        EMBEDDING_MODEL_ID: str = \"sentence-transformers/all-mpnet-base-v2\"\\n    \\n        REDIS_HOSTNAME: str = \"redis\"\\n        REDIS_PORT: int = 6379\\n    \\n    \\n    settings = Settings()\\n\\n### **Schemas**\\n\\nSuperlinked requires you to define your data structure through a set of\\nschemas, which are very similar to data classes or Pydantic models.\\n\\nSuperlinked will use these schemas as ORMs to save your data to a specified\\nvector DB.\\n\\nIt will also use them to define ingestion URLs automatically as POST HTTP\\nmethods that expect the request body to have the same signature as the schema.\\n\\nSimple and effective. Cool, right?\\n\\n    \\n    \\n    @schema\\n    class PostSchema:\\n        id: IdField\\n        platform: String\\n        content: String\\n        author_id: String\\n        type: String\\n    \\n    \\n    @schema\\n    class ArticleSchema:\\n        id: IdField\\n        platform: String\\n        link: String\\n        content: String\\n        author_id: String\\n        type: String\\n    \\n    \\n    @schema\\n    class RepositorySchema:\\n        id: IdField\\n        platform: String\\n        name: String\\n        link: String\\n        content: String\\n        author_id: String\\n        type: String\\n    \\n    \\n    post = PostSchema()\\n    article = ArticleSchema()\\n    repository = RepositorySchema()\\n\\n### **Spaces**\\n\\nThe spaces are where you define your chunking and embedding logic.\\n\\nA space is scoped at the field of a schema. Thus, if you want to embed\\nmultiple attributes of a single schema, you must define multiple spaces and\\ncombine them later into a multi-index.\\n\\nLet‚Äôs take the spaces for the article category as an example:\\n\\n    \\n    \\n    articles_space_content = TextSimilaritySpace(\\n        text=chunk(article.content, chunk_size=500, chunk_overlap=50),\\n        model=settings.EMBEDDING_MODEL_ID,\\n    )\\n    articles_space_plaform = CategoricalSimilaritySpace(\\n        category_input=article.platform,\\n        categories=[\"medium\", \"superlinked\"],\\n        negative_filter=-5.0,\\n    )\\n\\nChunking is done simply by calling the _chunk()_ function on a given schema\\nfield and specifying standard parameters such as ‚Äú _chunk_size‚Äù_ and ‚Äú\\n_chunk_overlap‚Äù_.\\n\\nThe embedding is done through the _TextSimilaritySpace()_ and\\n_CategoricalSimilaritySpace()_ classes.\\n\\nAs the name suggests, the _**TextSimilaritySpace()** _embeds text data using\\nthe model specified within the _‚Äúmodel‚Äù_ parameter. It supports any\\nHuggingFace model. We are using _‚Äúsentence-transformers/all-mpnet-base-v2‚Äù._\\n\\nThe _**CategoricalSimilaritySpace()**_ class uses an _n-hot encoded vector_\\nwith the option to apply a negative filter for unmatched categories, enhancing\\nthe distinction between matching and non-matching category items.\\n\\nYou must also specify all the available categories through the ‚Äú _categories_\\n‚Äù parameter to encode them in n-hot.\\n\\n### **Indexes**\\n\\nThe indexes define how a collection can be queried. They take one or multiple\\nspaces from the same schema.\\n\\nHere is what the article index looks like:\\n\\n    \\n    \\n    article_index = Index(\\n        [articles_space_content, articles_space_plaform],\\n        fields=[article.author_id],\\n    )\\n\\nAs you can see, the vector index combines the article‚Äôs content and the posted\\nplatform. When the article collection is queried, both embeddings will be\\nconsidered.\\n\\nAlso, we index the ‚Äúauthor_id‚Äù field to filter articles written by a specific\\nauthor. It is nothing fancy‚Äîit is just a classic filter. However, indexing the\\nfields used in filters is often good practice.\\n\\n### **Queries**\\n\\nWe will quickly introduce what a query looks like. But in the 14th lesson, we\\nwill insist on the advanced retrieval part, hence on queries.\\n\\nHere is what the article query looks like:\\n\\n    \\n    \\n    article_query = (\\n        Query(\\n            article_index,\\n            weights={\\n                articles_space_content: Param(\"content_weight\"),\\n                articles_space_plaform: Param(\"platform_weight\"),\\n            },\\n        )\\n        .find(article)\\n        .similar(articles_space_content.text, Param(\"search_query\"))\\n        .similar(articles_space_plaform.category, Param(\"platform\"))\\n        .filter(article.author_id == Param(\"author_id\"))\\n        .limit(Param(\"limit\"))\\n    )\\n\\n‚Ä¶and here is what it does:\\n\\n  * it queries the _article_index_ using a weighted multi-index between the content and platform vectors (e.g., `0.9 * content_embedding + 0.1 * platform_embedding` );\\n\\n  * the search text used to compute query content embedding is specified through the ‚Äúsearch_query‚Äù parameter and similar for the platform embedding through the ‚Äúplatform‚Äù parameter;\\n\\n  * we filter the results based on the ‚Äúauthor_id‚Äù;\\n\\n  * take only the top results using the ‚Äúlimit‚Äù parameter.\\n\\nThese parameters are automatically exposed on the REST API endpoint, as seen\\nin the _SuperlinkedClient()_ class.\\n\\n### **Sources**\\n\\nThe sources wrap the schemas and allow you to save that schema in the\\ndatabase.\\n\\nIn reality, the source maps the schema to an ORM and automatically generates\\nREST API endpoints to ingest data points.\\n\\n    \\n    \\n    article_source = RestSource(article)\\n\\n### **Executor**\\n\\nThe last step is to define the executor that wraps all the sources, indices,\\nqueries and vector DB into a single entity:\\n\\n    \\n    \\n    executor = RestExecutor(\\n        sources=[article_source, repository_source, post_source],\\n        indices=[article_index, repository_index, post_index],\\n        queries=[\\n            RestQuery(RestDescriptor(\"article_query\"), article_query),\\n            RestQuery(RestDescriptor(\"repository_query\"), repository_query),\\n            RestQuery(RestDescriptor(\"post_query\"), post_query),\\n        ],\\n        vector_database=InMemoryVectorDatabase(),\\n    )\\n    \\n\\nNow, the last step is to register the executor to the Superlinked engine:\\n\\n    \\n    \\n    SuperlinkedRegistry.register(executor)\\n\\n‚Ä¶and that‚Äôs it!\\n\\nJoking‚Ä¶ there is something more. We have to use a Redis database instead of\\nthe in-memory one.\\n\\n## **7\\\\. Using Redis as a vector DB**\\n\\nFirst, we have to spin up a Redis vector database that we can work with.\\n\\nWe used Docker and attached a Redis image as a service in a _docker-compose_\\nfile along with the Superlinked poller and executor (which comprise the\\nSuperlinked server):\\n\\n    \\n    \\n    version: \"3\"\\n    \\n    services:\\n      poller:\\n        ...\\n    \\n      executor:\\n        ...\\n    \\n      redis:\\n        image: redis/redis-stack:latest\\n        ports:\\n          - \"6379:6379\"\\n          - \"8001:8001\"\\n        volumes:\\n          - redis-data:/data\\n    \\n    volumes:\\n      redis-data:\\n\\nNow, Superlinked makes everything easy. The last step is to define a\\nRedisVectorDatabase connector provided by Superlinked:\\n\\n    \\n    \\n    vector_database = RedisVectorDatabase(\\n        settings.REDIS_HOSTNAME,\\n        settings.REDIS_PORT\\n    )\\n\\n‚Ä¶and swap it in the executor with the _InMemoryVectorDatabase()_ one:\\n\\n    \\n    \\n    executor = RestExecutor(\\n        ...\\n        vector_database=vector_database,\\n    )\\n\\nNow we are done!\\n\\n* * *\\n\\n## **Conclusion**\\n\\n _Congratulations! You learned to write advanced RAG systems\\nusingSuperlinked._\\n\\nMore concretely, in **Lesson 11** , you learned:\\n\\n  * what is Superlinked;\\n\\n  * how to design a streaming pipeline using Bytewax;\\n\\n  * how to design a RAG server using Superlinked;\\n\\n  * how to take a standard RAG feature pipeline and refactor it using Superlinked;\\n\\n  * how to split the feature pipeline into 2 services, one that reads in real-time messages from RabbitMQ and one that chunks, embeds, and stores the data to a vector DB;\\n\\n  * how to use a Redis vector DB.\\n\\n**Lesson 12** will teach you how to implement multi-index queries to optimize\\nthe RAG retrieval layer further.\\n\\n> _üîó**Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è_\\n\\n* * *\\n\\n### Next Steps\\n\\n#### Step 1\\n\\nThis is just the **short version** of **Lesson 11** on **building scalable RAG\\ningestion pipelines.**\\n\\n‚Üí For‚Ä¶\\n\\n  * The full implementation.\\n\\n  * Full deep dive into the code.\\n\\n  * More on the RAG, Bytewax and Superlinked.\\n\\n**Check out** the **full version** of **Lesson 11** on our **Medium\\npublication**. It‚Äôs still FREE:\\n\\nLesson 11 on Medium\\n\\n#### Step 2\\n\\n‚Üí **Consider checking out theLLM Twin GitHub repository and try it yourself\\nü´µ**\\n\\n _Nothing compares with getting your hands dirty and doing it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### Scalable RAG ingestion pipeline using 74.3% less code\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/scalable-rag-ingestion-pipeline-using?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('0eae1447-70c8-40b2-a5c4-96f6de69f04b'), content={'Title': 'The ultimate MLOps tool - by Paul Iusztin', 'Subtitle': '6 steps to build your AWS infrastructure that will work for 90% of your projects. How to build a real-time news search engine', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The ultimate MLOps tool\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The ultimate MLOps tool\\n\\n### 6 steps to build your AWS infrastructure that will work for 90% of your\\nprojects. How to build a real-time news search engine\\n\\nPaul Iusztin\\n\\nJul 13, 2024\\n\\n18\\n\\nShare this post\\n\\n#### The ultimate MLOps tool\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nBased on your feedback from last week‚Äôs poll, we will post exclusively on\\nSaturdays starting now.\\n\\nEnjoy today‚Äôs article ü§ó\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * The ultimate MLOps tool\\n\\n  * 6 steps to build your AWS infrastructure that will work for 90% of your projects\\n\\n  * How to build a real-time news search engine\\n\\n* * *\\n\\n### The ultimate MLOps tool\\n\\nI tested this ùóºùóøùó∞ùóµùó≤ùòÄùòÅùóøùóÆùòÅùóºùóø ùòÅùóºùóºùóπ for my ùó†ùóü ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ùòÄ and ùóπùóºùòÉùó≤ùó± ùó∂ùòÅ! It is the\\nùòÇùóπùòÅùó∂ùó∫ùóÆùòÅùó≤ ùó†ùóüùó¢ùóΩùòÄ ùòÅùóºùóºùóπ to glue everything together for ùóøùó≤ùóΩùóøùóºùó±ùòÇùó∞ùó∂ùóØùó∂ùóπùó∂ùòÅùòÜ and\\nùó∞ùóºùóªùòÅùó∂ùóªùòÇùóºùòÇùòÄ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥.  \\n  \\nIn the past months, I have tested most of the top orchestrator tools out\\nthere: Airflow, Prefect, Argo, Kubeflow, Metaflow...  \\n  \\nYou name it!  \\n  \\nùóïùòÇùòÅ ùóºùóªùó≤ ùòÄùòÅùóºùóºùó± ùóºùòÇùòÅ ùòÅùóº ùó∫ùó≤.  \\n  \\nI am talking about ZenML!  \\n  \\nùó™ùóµùòÜ?  \\n  \\nThey realized they don\\'t have to compete with tools such as Airflow or AWS in\\nthe orchestrators and MLOps race, but join them!  \\n  \\nInstead of being yet another orchestrator tool, they have built an ùóÆùóØùòÄùòÅùóøùóÆùó∞ùòÅ\\nùóπùóÆùòÜùó≤ùóø ùóºùóª ùòÅùóºùóΩ ùóºùó≥ ùòÅùóµùó≤ ùó†ùóüùó¢ùóΩùòÄ ùó≤ùó∞ùóºùòÄùòÜùòÄùòÅùó≤ùó∫:  \\n  \\n\\\\- experiment trackers & model registries (e.g., Weights & Biases, Comet)  \\n\\\\- orchestrators (e.g., Apache Airflow, Kubeflow)  \\n\\\\- container registries for your Docker images  \\n\\\\- model deployers (Hugging Face , BentoML, Seldon)  \\n  \\nThey wrote a clever wrapper that integrated the whole MLOps ecosystem!  \\n  \\nùòàùò≠ùò¥ùò∞, ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò™ùòØùò® ùò™ùòµ ùò™ùòØùòµùò∞ ùò∫ùò∞ùò∂ùò≥ ùòóùò∫ùòµùò©ùò∞ùòØ ùò§ùò∞ùò•ùò¶ ùò™ùò¥ ùòØùò∞ùòµ ùò™ùòØùòµùò≥ùò∂ùò¥ùò™ùò∑ùò¶.  \\n  \\nAs long your code is modular (which should be anyway), you have to annotate\\nyour DAG:  \\n\\\\- steps with \"Stephen S.\"  \\n\\\\- entry point with james wang  \\n  \\nùòàùò¥ ùò∫ùò∞ùò∂ ùò§ùò¢ùòØ ùò¥ùò¶ùò¶ ùò™ùòØ ùòµùò©ùò¶ ùò§ùò∞ùò•ùò¶ ùò¥ùòØùò™ùò±ùò±ùò¶ùòµùò¥ ùò£ùò¶ùò≠ùò∞ùò∏ ‚Üì  \\n\\nZenML Pipelines\\n\\n.\\n\\nZenML Steps\\n\\n  \\nùóßùóµùó≤ùòÜ ùóÆùóπùòÄùóº ùóΩùóøùóºùòÉùó∂ùó±ùó≤ ùòÅùóµùó≤ ùó∞ùóºùóªùó∞ùó≤ùóΩùòÅ ùóºùó≥ ùóÆ \"ùòÄùòÅùóÆùó∞ùó∏\".  \\n  \\nThis allows you to configure multiple tools and infrastructure sets your\\npipeline can run on.  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶:  \\n  \\n\\\\- ùò¢ ùò≠ùò∞ùò§ùò¢ùò≠ ùò¥ùòµùò¢ùò§ùò¨: that uses a local orchestrator, artifact store, and compute\\nfor quick testing (so you don\\'t have to set up other dependencies)  \\n  \\n\\\\- ùò¢ùòØ ùòàùòûùòö ùò¥ùòµùò¢ùò§ùò¨: that uses AWS SageMaker Orchestrator, Comet, and Seldon\\n\\nZenML Stacks\\n\\n  \\nAs I am still learning ZenML, this was just an intro post to share my\\nexcitement.  \\n  \\nI plan to integrate it into Decoding ML\\'s LLM twin open-source project and\\nshare the process with you!  \\n  \\n.  \\n  \\nùó†ùó≤ùóÆùóªùòÑùóµùó∂ùóπùó≤, ùó∞ùóºùóªùòÄùó∂ùó±ùó≤ùóø ùó∞ùóµùó≤ùó∞ùó∏ùó∂ùóªùó¥ ùóºùòÇùòÅ ùòÅùóµùó≤ùó∂ùóø ùòÄùòÅùóÆùóøùòÅùó≤ùóø ùó¥ùòÇùó∂ùó±ùó≤ ‚Üì  \\n  \\nüîó ùòöùòµùò¢ùò≥ùòµùò¶ùò• ùò®ùò∂ùò™ùò•ùò¶: https://lnkd.in/dPzXHvjH\\n\\n* * *\\n\\n### 6 steps to build your AWS infrastructure that will work for 90% of your\\nprojects\\n\\nùü≤ ùòÄùòÅùó≤ùóΩùòÄ to ùóØùòÇùó∂ùóπùó± your ùóîùó™ùó¶ ùó∂ùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ (using ùóúùóÆùóñ) and a ùóñùóú/ùóñùóó ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ that\\nwill ùòÑùóºùóøùó∏ for ùüµùü¨% of your ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅùòÄ ‚Üì  \\n  \\nWe will use the data collection pipeline from our free digital twin course as\\nan example, but it can easily be extrapolated to most of your projects.  \\n  \\nùòçùò™ùò≥ùò¥ùòµ, ùò≠ùò¶ùòµ\\'ùò¥ ùò¥ùò¶ùò¶ ùò∏ùò©ùò¢ùòµ ùò™ùò¥ ùò™ùòØ ùò∞ùò∂ùò≥ ùòµùò∞ùò∞ùò≠ùò£ùò¶ùò≠ùòµ:  \\n  \\n\\\\- Docker  \\n\\\\- AWS ECR  \\n\\\\- AWS Lambda  \\n\\\\- MongoDB  \\n\\\\- Pulumni  \\n\\\\- GitHub Actions  \\n  \\nùòöùò¶ùò§ùò∞ùòØùò•ùò≠ùò∫, ùò≠ùò¶ùòµ\\'ùò¥ ùò≤ùò∂ùò™ùò§ùò¨ùò≠ùò∫ ùò∂ùòØùò•ùò¶ùò≥ùò¥ùòµùò¢ùòØùò• ùò∏ùò©ùò¢ùòµ ùòµùò©ùò¶ ùò•ùò¢ùòµùò¢ ùò§ùò∞ùò≠ùò≠ùò¶ùò§ùòµùò™ùò∞ùòØ ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùò™ùò¥ ùò•ùò∞ùò™ùòØùò®  \\n  \\nIt automates your digital data collection from LinkedIn, Medium, Substack, and\\nGitHub. The normalized data will be loaded into MongoDB.  \\n  \\nùòïùò∞ùò∏, ùò≠ùò¶ùòµ\\'ùò¥ ùò∂ùòØùò•ùò¶ùò≥ùò¥ùòµùò¢ùòØùò• ùò©ùò∞ùò∏ ùòµùò©ùò¶ ùòàùòûùòö ùò™ùòØùòßùò≥ùò¢ùò¥ùòµùò≥ùò∂ùò§ùòµùò∂ùò≥ùò¶ ùò¢ùòØùò• ùòäùòê/ùòäùòã ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùò∏ùò∞ùò≥ùò¨ùò¥ ‚Üì  \\n  \\n1\\\\. We wrap the application\\'s entry point with a `ùò©ùò¢ùòØùò•ùò≠ùò¶(ùò¶ùò∑ùò¶ùòØùòµ, ùò§ùò∞ùòØùòµùò¶ùòπùòµ:\\nùòìùò¢ùòÆùò£ùò•ùò¢ùòäùò∞ùòØùòµùò¶ùòπùòµ)` function. The AWS Lambda serverless computing service will\\ndefault to the `ùò©ùò¢ùòØùò•ùò≠ùò¶()` function.  \\n  \\n2\\\\. Build a Docker image of your application inheriting the\\n`ùò±ùò∂ùò£ùò≠ùò™ùò§.ùò¶ùò§ùò≥.ùò¢ùò∏ùò¥/ùò≠ùò¢ùòÆùò£ùò•ùò¢/ùò±ùò∫ùòµùò©ùò∞ùòØ:3.11` base Docker image  \\n  \\n‚Üí Now, you can quickly check your AWS Lambda function locally by making HTTP\\nrequests to your Docker container.  \\n  \\n3\\\\. Use Pulumni IaC to create your AWS infrastructure programmatically:  \\n  \\n\\\\- an ECR as your Docker registry  \\n\\\\- an AWS Lambda service  \\n\\\\- a MongoDB cluster  \\n\\\\- the VPC for the whole infrastructure  \\n  \\n4\\\\. Now that we have our Docker image and infrastructure, we can build our\\nCI/CD pipeline using GitHub Actions. The first step is to build the Docker\\nimage inside the CI and push it to ECR when a new PR is merged into the main\\nbranch.  \\n  \\n5\\\\. On the CD part, we will take the fresh Docker image from ECR and deploy it\\nto AWS Lambda.  \\n  \\n6\\\\. Repeat the same logic with the Pulumni code ‚Üí Add a CD GitHub Action that\\nupdates the infrastructure whenever the IaC changes.  \\n  \\nWith ùòÅùóµùó∂ùòÄ ùó≥ùóπùóºùòÑ, you will do fine for ùüµùü¨% of your ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅùòÄ üî•  \\n  \\n.  \\n  \\nùòõùò∞ ùò¥ùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶, ùòµùò©ùò¶ ùòäùòê/ùòäùòã ùò∏ùò™ùò≠ùò≠ ùò≠ùò∞ùò∞ùò¨ ùò≠ùò™ùò¨ùò¶ ùòµùò©ùò™ùò¥:  \\n  \\nfeature PR -> merged to main -> build Docker image -> push to ECR -> deploy to\\nAWS Lambda\\n\\nLLM Twin AWS architecture\\n\\n  \\n  \\nùó™ùóÆùóªùòÅ ùòÅùóº ùóøùòÇùóª ùòÅùóµùó≤ ùó∞ùóºùó±ùó≤ ùòÜùóºùòÇùóøùòÄùó≤ùóπùó≥?  \\n  \\nConsider checking out ùóüùó≤ùòÄùòÄùóºùóª ùüÆ from the FREE ùóüùóüùó† ùóßùòÑùó∂ùóª ùó∞ùóºùòÇùóøùòÄùó≤ hosted by:\\n\\nüîó _The Importance of Data Pipelines in the Era of Generative AI_\\n\\n* * *\\n\\n### How to build a real-time news search engine\\n\\nDecoding ML ùóøùó≤ùóπùó≤ùóÆùòÄùó≤ùó± an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ & ùó∞ùóºùó±ùó≤ on building a ùó•ùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ ùó°ùó≤ùòÑùòÄ ùó¶ùó≤ùóÆùóøùó∞ùóµ\\nùóòùóªùó¥ùó∂ùóªùó≤ using ùóûùóÆùó≥ùó∏ùóÆ, ùó©ùó≤ùó∞ùòÅùóºùóø ùóóùóïùòÄ and ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó≤ùóªùó¥ùó∂ùóªùó≤ùòÄ.  \\n  \\nùòåùò∑ùò¶ùò≥ùò∫ùòµùò©ùò™ùòØùò® ùò™ùòØ ùòóùò∫ùòµùò©ùò∞ùòØ!  \\n  \\nùóßùóµùó≤ ùó≤ùóªùó± ùó¥ùóºùóÆùóπ?  \\n  \\nLearn to build a production-ready semantic search engine for news that is\\nsynced in real-time with multiple news sources using:  \\n\\\\- a streaming engine  \\n\\\\- Kafka  \\n\\\\- a vector DB.  \\n  \\nùóßùóµùó≤ ùóΩùóøùóºùóØùóπùó≤ùó∫?  \\n  \\nAccording to a research study by earthweb.com, the daily influx of news\\narticles, both online and offline, is between 2 and 3 million.  \\n  \\nHow would you constantly sync these data sources with your vector DB to stay\\nin sync with the outside world?  \\n  \\nùóßùóµùó≤ ùòÄùóºùóπùòÇùòÅùó∂ùóºùóª!  \\n  \\n‚Üí Here is where the streaming pipeline kicks in.  \\n  \\nAs soon as a new data point is available, it is:  \\n\\\\- ingested  \\n\\\\- processed  \\n\\\\- loaded to a vector DB  \\n  \\n...in real-time by the streaming pipeline ‚Üê  \\n  \\n.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò™ùò¥ ùò∏ùò©ùò¢ùòµ ùò∫ùò∞ùò∂ ùò∏ùò™ùò≠ùò≠ ùò≠ùò¶ùò¢ùò≥ùòØ ùòßùò≥ùò∞ùòÆ ùòµùò©ùò¶ ùò¢ùò≥ùòµùò™ùò§ùò≠ùò¶ ‚Üì  \\n  \\n‚Üí Set up your own Upstash ùóûùóÆùó≥ùó∏ùóÆ & ùó©ùó≤ùó∞ùòÅùóºùóø ùóóùóï ùó∞ùóπùòÇùòÄùòÅùó≤ùóøùòÄ  \\n  \\n‚Üí ùó¶ùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ & ùòÉùóÆùóπùó∂ùó±ùóÆùòÅùó≤ your ùó±ùóÆùòÅùóÆ points using Pydantic  \\n  \\n‚Üí ùó¶ùó∂ùó∫ùòÇùóπùóÆùòÅùó≤ multiple ùóûùóÆùó≥ùó∏ùóÆ ùóñùóπùó∂ùó≤ùóªùòÅùòÄ using ùòõùò©ùò≥ùò¶ùò¢ùò•ùòóùò∞ùò∞ùò≠ùòåùòπùò¶ùò§ùò∂ùòµùò∞ùò≥ & ùòíùò¢ùòßùò¨ùò¢ùòóùò≥ùò∞ùò•ùò∂ùò§ùò¶ùò≥  \\n  \\n‚Üí ùó¶ùòÅùóøùó≤ùóÆùó∫ ùóΩùóøùóºùó∞ùó≤ùòÄùòÄùó∂ùóªùó¥ using Bytewax \\\\- learn to ùóØùòÇùó∂ùóπùó± ùóÆ ùóøùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ ùó•ùóîùóö ingestion\\nùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n‚Üí ùóïùóÆùòÅùó∞ùóµ-ùòÇùóΩùòÄùó≤ùóøùòÅùó∂ùóªùó¥ ùó≤ùó∫ùóØùó≤ùó±ùó±ùó∂ùóªùó¥ùòÄ + ùó∫ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ to Upstash Vector DB  \\n  \\n‚Üí Build a ùó§&ùóî ùó®I using Streamlit  \\n  \\n‚Üí ùó®ùóªùó∂ùòÅ ùóßùó≤ùòÄùòÅùó∂ùóªùó¥ - Yes, we even added unit testing!\\n\\n  \\nùóñùòÇùóøùó∂ùóºùòÇùòÄ ùòÅùóº ùóπùó≤ùòÉùó≤ùóπ ùòÇùóΩ ùòÜùóºùòÇùóø ùó£ùòÜùòÅùóµùóºùóª, ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ & ùó•ùóîùóö ùó¥ùóÆùó∫ùó≤ ü´µ  \\n  \\nThen, consider checking out ùòµùò©ùò¶ ùò¢ùò≥ùòµùò™ùò§ùò≠ùò¶ & ùò§ùò∞ùò•ùò¶. Everything is free.  \\n  \\n‚Üì‚Üì‚Üì\\n\\nüîó **[Article]** How to build a real-time News Search Engine using Vector DBs\\n\\nüîó ùóöùó∂ùòÅùóõùòÇùóØ ùó∞ùóºùó±ùó≤\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n18\\n\\nShare this post\\n\\n#### The ultimate MLOps tool\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-ultimate-mlops-tool?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('1436e3e5-eb7c-4632-a538-00fd69c01998'), content={'Title': 'The new king of Infrastructure as Code (IaC)', 'Subtitle': 'Monitoring your DL models while in production. How to build a scalable data collection pipeline', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The new king of Infrastructure as Code (IaC)\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The new king of Infrastructure as Code (IaC)\\n\\n### Monitoring your DL models while in production. How to build a scalable\\ndata collection pipeline\\n\\nPaul Iusztin\\n\\nJun 29, 2024\\n\\n11\\n\\nShare this post\\n\\n#### The new king of Infrastructure as Code (IaC)\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * The new king of Infrastructure as Code (IaC)\\n\\n  * How to build a scalable data collection pipeline\\n\\n  * Monitoring your DL models while in production\\n\\n* * *\\n\\n### The new king of Infrastructure as Code (IaC)\\n\\nThis is ùòÅùóµùó≤ ùóªùó≤ùòÑ ùó∏ùó∂ùóªùó¥ ùóºùó≥ ùóúùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ ùóÆùòÄ ùóñùóºùó±ùó≤ (ùóúùóÆùóñ). Here is ùòÑùóµùòÜ it is ùóØùó≤ùòÅùòÅùó≤ùóø\\nthan ùóßùó≤ùóøùóøùóÆùó≥ùóºùóøùó∫ or ùóñùóóùóû ‚Üì  \\n  \\n‚Üí I am talking about Pulumi ‚Üê  \\n  \\nLet's see what is made of  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùó£ùòÇùóπùòÇùó∫ùó∂ ùóÆùóªùó± ùóµùóºùòÑ ùó∂ùòÄ ùó∂ùòÅ ùó±ùó∂ùó≥ùó≥ùó≤ùóøùó≤ùóªùòÅ?  \\n  \\nUnlike other IaC tools that use YAML, JSON, or a Domain-Specific Language\\n(DSL), Pulumi lets you write code in languages like Python, TypeScript,\\nNode.js, etc.  \\n\\\\- This enables you to leverage existing programming knowledge and tooling for\\nIaC tasks.  \\n\\\\- Pulumi integrates with familiar testing libraries for unit and integration\\ntesting of your infrastructure code.  \\n\\\\- It integrates with most cloud providers (AWS, GCP, Azure, Oracle, etc.)  \\n  \\nùóïùó≤ùóªùó≤ùó≥ùó∂ùòÅùòÄ ùóºùó≥ ùòÇùòÄùó∂ùóªùó¥ ùó£ùòÇùóπùòÇùó∫ùó∂:  \\n  \\nùóôùóπùó≤ùòÖùó∂ùóØùó∂ùóπùó∂ùòÅùòÜ: Use your preferred programming language for IaC + it works for\\nmost clouds out there  \\nùóòùó≥ùó≥ùó∂ùó∞ùó∂ùó≤ùóªùó∞ùòÜ: Leverage existing programming skills and tooling.  \\nùóßùó≤ùòÄùòÅùóÆùóØùó∂ùóπùó∂ùòÅùòÜ: Write unit and integration tests for your infrastructure code.  \\nùóñùóºùóπùóπùóÆùóØùóºùóøùóÆùòÅùó∂ùóºùóª: Enables Dev and Ops to work together using the same language.  \\n  \\nIf you disagree, try to apply OOP or logic (if, for statements) to Terraform\\nHCL's syntax.  \\n  \\nIt works, but it quickly becomes a living hell.  \\n  \\nùóõùóºùòÑ ùó£ùòÇùóπùòÇùó∫ùó∂ ùòÑùóºùóøùó∏ùòÄ:  \\n  \\n\\\\- Pulumi uses a declarative approach. You define the desired state of your\\ninfrastructure.  \\n\\\\- It manages the state of your infrastructure using a state file.  \\n\\\\- When changes are made to the code, Pulumi compares the desired state with\\nthe current state and creates a plan to achieve the desired state.  \\n\\\\- The plan shows what resources will be created, updated, or deleted.  \\n\\\\- You can review and confirm the plan before Pulumi executes it.  \\n  \\n‚Üí It works similarly to Terraform but with all the benefits your favorite\\nprogramming language and existing tooling provides  \\n  \\n‚Üí It works similar to CDK, but faster and for your favorite cloud\\ninfrastructure (not only AWS)\\n\\nPulumi code example\\n\\n _What do you think? Have you used Pulumi?_  \\n  \\nWe started using it for the LLM Twin course, and so far, we love it! I will\\nprobably wholly migrate from Terraform to Pulumi in future projects.\\n\\n> üîó More on Pulumi\\n\\n* * *\\n\\n### How to build a scalable data collection pipeline\\n\\nùóïùòÇùó∂ùóπùó±, ùó±ùó≤ùóΩùóπùóºùòÜ to ùóîùó™ùó¶, ùóúùóÆùóñ, and ùóñùóú/ùóñùóó for a ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ that\\nùó∞ùóøùóÆùòÑùóπùòÄ your ùó±ùó∂ùó¥ùó∂ùòÅùóÆùóπ ùó±ùóÆùòÅùóÆ ‚Üí ùó™ùóµùóÆùòÅ do you need ü§î  \\n  \\nùóßùóµùó≤ ùó≤ùóªùó± ùó¥ùóºùóÆùóπ?  \\n  \\nùòà ùò¥ùò§ùò¢ùò≠ùò¢ùò£ùò≠ùò¶ ùò•ùò¢ùòµùò¢ ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùòµùò©ùò¢ùòµ ùò§ùò≥ùò¢ùò∏ùò≠ùò¥, ùò§ùò∞ùò≠ùò≠ùò¶ùò§ùòµùò¥, ùò¢ùòØùò• ùò¥ùòµùò∞ùò≥ùò¶ùò¥ ùò¢ùò≠ùò≠ ùò∫ùò∞ùò∂ùò≥ ùò•ùò™ùò®ùò™ùòµùò¢ùò≠\\nùò•ùò¢ùòµùò¢ ùòßùò≥ùò∞ùòÆ:  \\n  \\n\\\\- LinkedIn  \\n\\\\- Medium  \\n\\\\- Substack  \\n\\\\- Github  \\n  \\nùóßùóº ùóØùòÇùó∂ùóπùó± ùó∂ùòÅ - ùóµùó≤ùóøùó≤ ùó∂ùòÄ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ‚Üì  \\n  \\nùü≠\\\\. ùó¶ùó≤ùóπùó≤ùóªùó∂ùòÇùó∫: a Python tool for automating web browsers. It‚Äôs used here to\\ninteract with web pages programmatically (like logging into LinkedIn,\\nnavigating through profiles, etc.)  \\n  \\nùüÆ\\\\. ùóïùó≤ùóÆùòÇùòÅùó∂ùó≥ùòÇùóπùó¶ùóºùòÇùóΩ: a Python library for parsing HTML and XML documents. It\\ncreates parse trees that help us extract the data quickly.  \\n  \\nùüØ\\\\. ùó†ùóºùóªùó¥ùóºùóóùóï (ùóºùóø ùóÆùóªùòÜ ùóºùòÅùóµùó≤ùóø ùó°ùóºùó¶ùó§ùóü ùóóùóï): a NoSQL database fits like a glove on our\\nunstructured text data  \\n  \\nùü∞\\\\. ùóîùóª ùó¢ùóóùó†: a technique that maps between an object model in an application\\nand a document database  \\n  \\nùü±\\\\. ùóóùóºùó∞ùó∏ùó≤ùóø & ùóîùó™ùó¶ ùóòùóñùó•: to deploy our code, we have to containerize it, build an\\nimage for every change of the main branch, and push it to AWS ECR  \\n  \\nùü≤\\\\. ùóîùó™ùó¶ ùóüùóÆùó∫ùóØùó±ùóÆ: we will deploy our Docker image to AWS Lambda - a serverless\\ncomputing service that allows you to run code without provisioning or managing\\nservers. It executes your code only when needed and scales automatically, from\\na few daily requests to thousands per second  \\n  \\nùü≥\\\\. ùó£ùòÇùóπùòÇùó∫ùóªùó∂: IaC tool used to programmatically create the AWS infrastructure:\\nMongoDB instance, ECR, Lambdas and the VPC  \\n  \\nùü¥\\\\. ùóöùó∂ùòÅùóõùòÇùóØ ùóîùó∞ùòÅùó∂ùóºùóªùòÄ: used to build our CI/CD pipeline - on any merged PR to the\\nmain branch, it will build & push a new Docker image and deploy it to the AWS\\nLambda service\\n\\nETL architecture to collect digital data from social media platforms\\n\\nùòæùô™ùôßùôûùô§ùô™ùô® ùôùùô§ùô¨ ùô©ùôùùôöùô®ùôö ùô©ùô§ùô§ùô°ùô® ùô¨ùô§ùôßùô† ùô©ùô§ùôúùôöùô©ùôùùôöùôß?\\n\\n> Then...  \\n>  \\n> ‚Üì‚Üì‚Üì  \\n>  \\n> Check out ùóüùó≤ùòÄùòÄùóºùóª ùüÆ from the FREE ùóüùóüùó† ùóßùòÑùó∂ùóª ùóñùóºùòÇùóøùòÄùó≤ created by Decoding ML  \\n>  \\n> ...where we will walk you ùòÄùòÅùó≤ùóΩ-ùóØùòÜ-ùòÄùòÅùó≤ùóΩ through the ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤ and ùó∞ùóºùó±ùó≤ of\\n> the ùó±ùóÆùòÅùóÆ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤:\\n>\\n> üîó The Importance of Data Pipelines in the Era of Generative AI\\n\\n* * *\\n\\n### Monitoring your DL models while in production\\n\\nùó†ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ is ùóßùóõùóò ùó∏ùó≤ùòÜ ùó†ùóüùó¢ùóΩùòÄ ùó≤ùóπùó≤ùó∫ùó≤ùóªùòÅ in ensuring your ùó∫ùóºùó±ùó≤ùóπùòÄ in ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª are\\nùó≥ùóÆùó∂ùóπ-ùòÄùóÆùó≥ùó≤. Here is an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ on ùó†ùóü ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ using Triton, Prometheus and\\nGrafana ‚Üì  \\n  \\n\\nRazvant Alexandru\\n\\nwrote a fantastic ùòÄùòÅùó≤ùóΩ-ùóØùòÜ-ùòÄùòÅùó≤ùóΩ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ in the\\n\\nDecoding ML Newsletter\\n\\non ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ your ùóóùóü ùó∫ùóºùó±ùó≤ùóπùòÄ while in ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª.  \\n  \\nWithin his article, he started with an example where, in one of his projects,\\na main processing task was supposed to take <5 ùò©ùò∞ùò∂ùò≥ùò¥, but while in production,\\nit jumped to >8 ùò©ùò∞ùò∂ùò≥ùò¥.  \\n  \\n‚Üí ùòõùò©ùò™ùò¥ (ùò∞ùò≥ ùò¥ùò∞ùòÆùò¶ùòµùò©ùò™ùòØùò® ùò¥ùò™ùòÆùò™ùò≠ùò¢ùò≥) ùò∏ùò™ùò≠ùò≠ ùò©ùò¢ùò±ùò±ùò¶ùòØ ùòµùò∞ ùò¢ùò≠ùò≠ ùò∞ùòß ùò∂ùò¥.  \\n  \\nEven to the greatest.  \\n  \\nIt's impossible always to anticipate everything that will happen in production\\n(sometimes it is a waste of time even to try to).  \\n  \\nThat is why you always need eyes and years on your production ML system.  \\n  \\nOtherwise, imagine how much $$$ or users he would have lost if he hadn't\\ndetected the ~3-4 hours loss in performance as fast as possible.\\n\\nAfterward, he explained step-by-step how to use:  \\n  \\n\\\\- ùó∞ùóîùó±ùòÉùó∂ùòÄùóºùóø to scrape RAM/CPU usage per container  \\n  \\n\\\\- ùóßùóøùó∂ùòÅùóºùóª ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó¶ùó≤ùóøùòÉùó≤ùóø to serve ML models and yield GPU-specific metrics.  \\n  \\n\\\\- ùó£ùóøùóºùó∫ùó≤ùòÅùóµùó≤ùòÇùòÄ to bind between the metrics generators and the consumer.  \\n  \\n\\\\- ùóöùóøùóÆùó≥ùóÆùóªùóÆ to visualize the metrics\\n\\n> ùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ ùóºùóª ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü  \\n>  \\n> ‚Üì‚Üì‚Üì  \\n>  \\n> üîó How to ensure your models are fail-safe in production?\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n11\\n\\nShare this post\\n\\n#### The new king of Infrastructure as Code (IaC)\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-new-king-of-infrastructure-as?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('fd48444e-ab32-49b9-afdc-14fe8ecafd41'), content={'Title': 'Data Ingestion Architecture for ML and Marketing Intelligence', 'Subtitle': 'Building a highly scalable data collection pipeline for AI, ML and marketing intelligence leveraging the AWS cloud, Python, data\\xa0crawling, and Docker.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\n### Leveraging AWS Ecosystem and Data Crawling for Scalable and Adaptive Data\\nPipelines\\n\\nRares Istoc\\n\\nJun 27, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n**Today‚Äôs article** is **written** by our **guest** , **Rares Istoc** , a\\nveteran with over 7 years of experience building scalable software and data\\nengineering systems in the industry.\\n\\n‚Üí Here is his üîó LinkedIn.\\n\\nMachine learning without data is like a chef without ingredients - all the\\nskills but nothing to cook.\\n\\nThese days, everything circulates around data, from personalized ads to\\nstreaming recommendations. Data drives decisions in business, healthcare, and\\nsports. Without it, apps would be clueless, smart devices would be dumb, and\\npredictions would be nothing more than guesses. In this digital age, data is\\nthe lifeblood of innovation and efficiency.\\n\\n**Ok, but why another article about data ingestion?**\\n\\nThere are many ways to build data ingestion pipelines, and with all the new\\ntools created over the last decade, selecting the best ones can be\\nchallenging. The answer often depends on your project‚Äôs specific needs.\\n\\nIn this article, you‚Äôll explore an end-to-end solution for marketing\\nintelligence. Using AWS‚Äôs ecosystem, you can create a scalable data-ingestion\\npipeline for data crawling and integrate it into various analytical processes\\nlike sales, competitor analysis, market analysis, and customer insights.\\n\\nI‚Äôll also present the challenges encountered while building this solution.\\nFinding a complete working solution is tough, with most answers scattered\\nacross the Internet. You can access the full solution code on üîó **GitHub**.\\n\\n_**IMPORTANT NOTE:** Before diving into this solution, you must be aware of\\nthe legal implications of ingesting data from some data sources, like social\\nmedia pages, so we can make sure nobody goes to jail. Please read the terms\\nand conditions of each major platform; these will restrict you from crawling\\nuser profiles and private pages._\\n\\n* * *\\n\\n### Table of Contents:\\n\\n  1. Architecture Overview\\n\\n  2. Implementation\\n\\n  3. Challenges & Pitfalls\\n\\n  4. Local Testings\\n\\n  5. Deployment\\n\\n* * *\\n\\n### 1\\\\. Architecture Overview\\n\\nThis is what we are about to build:\\n\\nHere are some non-functional requirements I‚Äôve aimed to achieve with this\\narchitecture:\\n\\n**Scalability:** The solution can process many pages simultaneously and easily\\nadd more, handling growth at any time.\\n\\n**Maintainability & Adaptability:** Each component is designed for easy\\nmodification and expansion without significant development time.\\n\\n**Components Overview:**\\n\\n‚Ä¢ **Scheduler:** Triggers crawler lambdas for each page link.\\n\\n‚Ä¢ **Crawler:** Extracts various posts and information from the page link. If\\nunfamiliar with crawling, look it up before proceeding. Details will follow in\\nthe implementation part.\\n\\n‚Ä¢ **Database:** MongoDB is used for our data lake storage, housing posts for\\nlater use. It excels at handling semi-structured data.\\n\\nThe complete flow: the scheduler triggers a crawler lambda for each page,\\nsending the page name and link. The crawler extracts posts from the past week,\\nstoring the raw content, creation date, link, and name. The scheduler waits\\nfor all lambdas to finish, aggregates the posts from the database, and sends\\nthem to ChatGPT using prompt templates to generate reports.\\n\\n### 2\\\\. Implementation\\n\\nIn this section, I‚Äôll provide a detailed overview of the main components,\\nbreaking them down with code samples and explanations.\\n\\n#### 2.1. Scheduler\\n\\nI‚Äôll not focus much on the reporting part, though you can find it **here**\\nalong with all the code shared in this article. The main focus is the\\nscheduling part, the entry point of the system where the flow starts and is\\norchestrated:\\n\\n    \\n    \\n    import json\\n    import os\\n    import time\\n    from datetime import datetime, timedelta\\n    \\n    import boto3\\n    from aws_lambda_powertools import Logger\\n    from aws_lambda_powertools.utilities.typing import LambdaContext\\n    \\n    from src.constants import PAGE_LINK\\n    from src.db import database\\n    from src.utils import monitor\\n    \\n    logger = Logger(service=\"decodingml/scheduler\")\\n    \\n    _client = boto3.client(\"lambda\")\\n    \\n    \\n    def lambda_handler(event, context: LambdaContext):\\n        correlation_ids = []\\n    \\n        for link in PAGE_LINK:\\n            response = _client.invoke(\\n                FunctionName=\"lambda\",\\n                InvocationType=\"Event\",\\n                Payload=json.dumps({\"link\": link}),\\n            )\\n            logger.info(f\"Triggered crawler for: {link}\")\\n    \\n            correlation_ids.append(response[\"ResponseMetadata\"][\"RequestId\"])\\n    \\n        logger.info(f\"Monitoring: {len(correlation_ids)} crawler processes\")\\n    \\n        while True:\\n            time.sleep(15)\\n            completed = monitor(correlation_ids)\\n    \\n            correlation_ids = [c for c in correlation_ids if c not in completed]\\n    \\n            if not correlation_ids:\\n                break\\n    \\n            logger.info(f\"Still waiting for {len(correlation_ids)} crawlers to complete\")\\n    \\n        now = datetime.now()\\n        posts = list(\\n            database.profiles.find(\\n                {\\n                    \"date\": {\"$gte\": (now - timedelta(days=7)), \"$lte\": now},\\n                }\\n            )\\n        )\\n    \\n        logger.info(f\"Gathered {len(posts)} posts\")\\n    \\n        if not posts:\\n            logger.info(\"Cannot generate report, no new posts available\")\\n            return\\n    \\n        reports = generate_profiles_report(posts)\\n    \\n        logger.info(\"Generated new report!\")\\n\\nThe scheduler acts as a scatterer, iterating over a list of page links and\\ninvoking a crawler asynchronously with the InvocationType parameter set to\\nEvent, ensuring the scheduler won‚Äôt block for a single page. It stores each\\nlambda‚Äôs correlation ID in a list and waits for all lambdas to finish, with a\\n15-second wait time, adjustable based on your crawler‚Äôs average completion\\ntime. Finally, it finds all crawled posts and sends them to the report\\ngeneration phase.\\n\\n#### 2.2. Crawler\\n\\nHere I‚Äôll break down the actual crawling process:\\n\\n    \\n    \\n    import abc\\n    import os\\n    from datetime import datetime, timedelta\\n    from itertools import takewhile, dropwhile\\n    from typing import List, Dict, Any\\n    \\n    import instaloader\\n    \\n    from src.crawlers.base import BaseAbstractCrawler\\n    \\n    class BaseAbstractCrawler(abc.ABC):\\n    \\n        @abc.abstractmethod\\n        def extract(self, link: str, **kwargs) -> None: ...\\n    \\n    \\n    class InstagramCrawler(BaseAbstractCrawler):\\n    \\n        def __init__(self, link: str, proxy=None):\\n            self.link = link\\n            self.loader = instaloader.Instaloader()\\n            self._until = datetime.now()\\n            self._since = self._until - timedelta(days=7)\\n            self._proxy = proxy\\n    \\n        def extract(self, **kwargs) -> List[Dict[str, str | Any]]:\\n            parsed_url = urlparse(self.link)\\n    \\n            if self._proxy:\\n                os.environ[\\'https_proxy\\'] = self._proxy.__dict__().get(\\'http\\')\\n            profile = instaloader.Profile.from_username(self.loader.context, parsed_url.path.strip(\\'/\\').split(\\'/\\')[0])\\n            posts = takewhile(lambda p: p.date > self._since, dropwhile(lambda p: p.date > self._until, profile.get_posts()))\\n    \\n            return [\\n                {\\'content\\': post.caption, \\'date\\': post.date, \\'link\\': self.link}\\n                for post in posts\\n            ]\\n\\nI‚Äôve defined a main abstraction point for all crawlers, establishing a common\\ninterface that all derived crawlers must implement. Each subclass must provide\\nits implementation for the `extract()` method, ensuring reusability and\\nuniformity.\\n\\n    \\n    \\n    import re\\n    \\n    from src.crawlers.base import BaseAbstractCrawler\\n    from src.crawlers.instagram import InstagramCrawler\\n    \\n    \\n    class CrawlerDispatcher:\\n    \\n        def __init__(self) -> None:\\n            self._crawlers = {}\\n    \\n        def register(self, domain: str, crawler: type[BaseAbstractCrawler]) -> None:\\n            self._crawlers[r\"https://(www\\\\.)?{}.com/*\".format(re.escape(domain))] = crawler\\n    \\n        def get_crawler(self, url: str) -> BaseAbstractCrawler:\\n            for pattern, crawler in self._crawlers.items():\\n                if re.match(pattern, url):\\n                    return crawler()\\n            else:\\n                raise ValueError(\"No crawler found for the provided link\")\\n    \\n    \\n    dispatcher = CrawlerDispatcher()\\n    dispatcher.register(\\'instagram\\', InstagramCrawler)\\n\\nTo promote and call each crawler automatically, I‚Äôve built a dispatcher that\\nselects and instantiates the correct crawler class based on the provided link.\\nThis acts as a registry and factory for the crawlers, managed under a unified\\ninterface and structure.\\n\\nAdvantages:\\n\\n‚Ä¢ **Flexibility & Scalability:** Allows easy addition of new domains and\\nspecialized crawlers without modifying the existing codebase.\\n\\n‚Ä¢ **Encapsulation & Modularity:** The dispatcher encapsulates the logic for\\ndetermining which crawler to use, making the system modular and allowing each\\ncrawler to focus on its core business logic.\\n\\n    \\n    \\n    from datetime import datetime, timedelta\\n    \\n    from aws_lambda_powertools import Logger\\n    from aws_lambda_powertools.utilities.typing import LambdaContext\\n    \\n    from src.crawlers import dispatcher\\n    from src.db import database\\n    \\n    logger = Logger(service=\"decodingml/crawler\")\\n    \\n    \\n    def lambda_handler(event, context: LambdaContext):\\n    \\n        link = event.get(\\'link\\')\\n    \\n        logger.info(f\"Start extracting posts for {link}\")\\n    \\n        crawler = dispatcher.get_crawler(event.get(\\'link\\'))\\n    \\n        posts = [{**page, \\'correlation_id\\': context.aws_request_id} for page in crawler.extract()]\\n    \\n        now = datetime.now()\\n        existing_posts = database.profiles.find({\\n            \"date\": {\"$gte\": (now - timedelta(days=7)), \"$lte\": now},\\n            \"name\": link\\n        }, projection={\\'date\\': 1})\\n    \\n        existing_posts = [post.get(\\'date\\') for post in list(existing_posts)]\\n    \\n        posts = [post for post in posts if post.get(\\'date\\') not in existing_posts]\\n    \\n        if not posts:\\n            logger.info(\"No new posts on page\")\\n            return\\n    \\n        logger.info(f\"Successfully extracted {len(posts)} posts\")\\n        database.profiles.insert_many(posts)\\n        logger.info(f\"Successfully inserted data in db\")\\n\\nThe main entry point assembles the link from the event body, selects the\\ncorrect crawler, and starts extraction jobs. After extraction, it checks for\\nexisting posts to avoid duplicates and adds new posts to the database.\\n\\n### 3\\\\. Challenges & Pitfalls\\n\\n#### 3.1. Running headless browser instance with selenium in lambda runtime\\nenvironment\\n\\nThis caused the most headaches. The Lambda execution environment is read-only,\\nso writing to disk requires using a temporary file, complicating automatic\\nbinary driver installation. Therefore, you need to install the driver directly\\nin the Docker image and reference it manually in Selenium‚Äôs driver options.\\nThe only usable driver for this setup was the Google binary driver in my case.\\n\\n    \\n    \\n    FROM  public.ecr.aws/lambda/python:3.11 as build\\n    \\n    # Download chrome driver and browser and manually unpack them in their folders\\n    RUN yum install -y unzip && \\\\\\n        curl -Lo \"/tmp/chromedriver-linux64.zip\" \"https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/119.0.6045.105/linux64/chromedriver-linux64.zip\" && \\\\\\n        curl -Lo \"/tmp/chrome-linux64.zip\" \"https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/119.0.6045.105/linux64/chrome-linux64.zip\" && \\\\\\n        unzip /tmp/chromedriver-linux64.zip -d /opt/ && \\\\\\n        unzip /tmp/chrome-linux64.zip -d /opt/\\n    \\n    \\n    FROM  public.ecr.aws/lambda/python:3.11\\n    \\n    # Install the function\\'s OS dependencies using yum\\n    RUN yum install -y \\\\\\n        atk \\\\\\n        cups-libs \\\\\\n        gtk3 \\\\\\n        libXcomposite \\\\\\n        alsa-lib \\\\\\n        libXcursor \\\\\\n        libXdamage \\\\\\n        libXext \\\\\\n        libXi \\\\\\n        libXrandr \\\\\\n        libXScrnSaver \\\\\\n        libXtst \\\\\\n        pango \\\\\\n        at-spi2-atk \\\\\\n        libXt \\\\\\n        xorg-x11-server-Xvfb \\\\\\n        xorg-x11-xauth \\\\\\n        dbus-glib \\\\\\n        dbus-glib-devel \\\\\\n        nss \\\\\\n        mesa-libgbm \\\\\\n        ffmpeg \\\\\\n        libxext6 \\\\\\n        libssl-dev \\\\\\n        libcurl4-openssl-dev \\\\\\n        libpq-dev\\n    \\n    COPY --from=build /opt/chrome-linux64 /opt/chrome\\n    COPY --from=build /opt/chromedriver-linux64 /opt/\\n    \\n    COPY ./pyproject.toml ./poetry.lock ./\\n    \\n    # Install Poetry, export dependencies to requirements.txt, and install dependencies\\n    # in the Lambda task directory, finally cleanup manifest files.\\n    RUN python3 -m pip install --upgrade pip && pip install poetry\\n    RUN poetry export -f requirements.txt > requirements.txt && \\\\\\n        pip3 install  --no-cache-dir -r requirements.txt --target \"${LAMBDA_TASK_ROOT}\" && \\\\\\n        rm requirements.txt pyproject.toml poetry.lock\\n    \\n    # Copy function code\\n    COPY ./src ${LAMBDA_TASK_ROOT}/src\\n\\nThe main idea in this Dockerfile is that I manually downloaded the Chrome\\ndriver and browser and unpacked them in a location where they can be accessed\\nby Selenium, which usually would‚Äôve done this directly.\\n\\nThis is a mandatory step for the Lambda environment. Since everything is read-\\nonly, in the next code sample I‚Äôll show you how point Selenium to the correct\\ndriver and browser locations:\\n\\n    \\n    \\n    from tempfile import mkdtemp\\n    \\n    def init_driver(self):\\n        options = Options()\\n        # Setup drover binary location manually\\n        options.binary_location = \\'/opt/chrome/chrome\\'\\n        # Run browser in headless mode\\n        options.add_argument(\\'--headless=new\\')\\n        options.add_argument(\\'--no-sandbox\\')\\n        options.add_argument(\\'--single-process\\')\\n        options.add_argument(\\'--window-size=1420,1080\\')\\n        options.add_argument(\\'--disable-dev-shm-usage\\')\\n        options.add_argument(\\'--disable-gpu\\')\\n        options.add_argument(\\'--disable-popup-blocking\\')\\n        options.add_argument(\\'--disable-notifications\\')\\n        options.add_argument(\\'--disable-dev-tools\\')\\n        options.add_argument(\\'--log-level=3\\')\\n        options.add_argument(\\'--ignore-certificate-errors\\')\\n        options.add_argument(\"--no-zygote\")\\n        options.add_argument(f\"--user-data-dir={mkdtemp()}\")\\n        options.add_argument(f\"--data-path={mkdtemp()}\")\\n        options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\\n        options.add_argument(\\'--remote-debugging-port=9222\\')\\n    \\n    \\n        self._driver = webdriver.Chrome(\\n            service=Service(\"/opt/chromedriver\"),\\n            options=options,\\n        )\\n\\nI hardcoded the driver and browser locations in the Dockerfile. Additionally,\\nI pointed several folders (e.g., user-data-dir, disk-cache-dir) to temporary\\ndirectories to prevent Selenium from creating them automatically, which would\\ncause errors due to Lambda‚Äôs disk limitations.\\n\\n#### 3.2. Aggregate Empty Pages\\n\\nMy initial monitoring algorithm was basic, looping over lambda invocation\\ncorrelation IDs and checking the database for generated posts. However, it\\nencountered an infinite loop when no new posts were created for some pages.\\n\\n    \\n    \\n    import datetime\\n    import re\\n    from typing import List\\n    \\n    import boto3\\n    \\n    _client = boto3.client(\\'logs\\')\\n    \\n    \\n    def monitor(correlation_ids: List[str]):\\n        finished = []\\n    \\n        now = int((datetime.datetime.now() datetime.timedelta(days=1)).timestamp() * 1000)\\n    \\n        response = _client.filter_log_events(\\n            logGroupName=\\'/aws/lambda/crawler\\',\\n            startTime=now,\\n            filterPattern=\"REPORT RequestId\"\\n        )\\n    \\n        for event in response[\\'events\\']:\\n            match = re.search(r\\'REPORT RequestId: ([^\\\\s]+)\\', event.get(\\'message\\'))\\n            if match:\\n                correlation_id = match.group(1)\\n                if correlation_id in correlation_ids:\\n                    finished.append(correlation_id)\\n    \\n        return finished\\n\\nHere, I search through all log streams for each lambda generated in that\\ncurrent day and look for the message, which usually has this format: _**REPORT\\nRequestId:**_ <correlation_id>. This indicates that the lambda has reached the\\nend of its execution, and I can mark which correlation IDs have finished.\\n\\n#### 3.3. Avoid being blocked by social media platforms\\n\\nThis was a pity error‚Äîthe kind you would‚Äôve spent days on‚Äîand the solution was\\nto watch it from a different perspective. Popular social media platforms\\nimplement many anti-bot protection mechanisms to prevent crawling, from\\nrequest header analysis to rate limiting to IP blocking.\\n\\nAnd because we run our browser in headless mode to mimic realistic user-\\nbrowser interaction, and all our crawlers send requests under the same IP\\naddress to multiple pages at the same time repeatedly, this screams, please\\nblock me.\\n\\nTo address this, I‚Äôve used a proxy to mask my IP address and location:\\n\\n    \\n    \\n    import os\\n    \\n    \\n    class ProxyConnection:\\n    \\n        def __init__(\\n            self,\\n            host: str = None,\\n            port: str = None,\\n            username: str = None,\\n            password: str = None,\\n            verify_ssl: bool = False\\n        ):\\n            self.host = host or os.getenv(\\'PROXY_HOST\\')\\n            self.port = port or os.getenv(\\'PROXY_PORT\\')\\n            self.username = username or os.getenv(\\'PROXY_USERNAME\\')\\n            self.password = password or os.getenv(\\'PROXY_PASSWORD\\')\\n            self.verify_ssl = verify_ssl\\n            self._url = f\"{self.username}:{self.password}@{self.host}:{self.port}\"\\n    \\n        def __dict__(self):\\n            return {\\n                \\'https\\': \\'https://{}\\'.format(self._url.replace(\" \", \"\")),\\n                \\'http\\': \\'http://{}\\'.format(self._url.replace(\" \", \"\")),\\n                \\'no_proxy\\': \\'localhost, 127.0.0.1\\',\\n                \\'verify_ssl\\': self.verify_ssl\\n            }\\n\\nTo address this, I used a proxy to mask my IP and location. Paid proxies like\\nSmartProxy offer a pool of rotating IPs, assigning a different IP to each\\ncrawler, mimicking regular user behavior. Additionally, using a proxy allows\\nfinding a country without access restrictions to public pages, ensuring smooth\\ncrawling.\\n\\n### 4\\\\. Local Testings\\n\\nTo prove this works, I wrote a makefile containing some simple commands for\\ncrawler and lambda. The problem is that I‚Äôve only managed to test the crawler\\nlocally. Since the scheduler spins up crawlers, they should be already\\ndeployed on AWS.\\n\\n    \\n    \\n    local-test-crawler: # Send test command on local to test  the lambda\\n     curl -X POST \"http://localhost:9000/2015-03-31/functions/function/invocations\" \\\\\\n      -d \\'{\"link\": \"https://www.instagram.com/mcdonalds\"}\\'\\n    \\n    local-test-scheduler: # Send test command on local to test  the lambda\\n     curl -X POST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\n\\nNow, most people, when testing lambda functions on a local environment, use\\nAWS Lambda **RIE (Runtime Interface Emulator)** , which allows you to test\\nyour lambda function packages in a container. Basically, this emulates a\\nlambda execution environment on your local machine. As you can see, I‚Äôve\\nmanaged to do this without using the emulator, which slightly simplified my\\nenvironment.\\n\\nYou can use these commands to test each component. For example, if you would\\nlike to test the crawler, go into your terminal and use this command:\\n\\n    \\n    \\n    > make local-test-crawler\\n\\nAs you can see, the crawling process has started, and for this page, we‚Äôve\\nfound three new posts in the last seven days:\\n\\n### 5\\\\. Deployment\\n\\nThe deployment process is defined in **our GitHub** repository under the\\n**ops** folder, where you can explore the whole solution written in Pulumi.\\n\\nYou can play with the Makefile. It contains all the necessary commands to make\\nyour infrastructure up and running.\\n\\n* * *\\n\\n### Conclusion\\n\\nIn this article, we‚Äôve explored a complete end-to-end robust solution for\\nbuilding a Highly Scalable Data Ingestion pipeline that can leverage existing\\ndata from multiple crawlable sources for various processes like ML training,\\ndata analysis, etc.\\n\\nWe‚Äôve gone through specific challenges you might face and how to overcome them\\nin this process.\\n\\n| _üîó**Check out** the code on GitHub [1] and support us with a _‚≠êÔ∏è\\n\\n* * *\\n\\nWithin our newsletter, we keep things short and sweet.\\n\\nIf you enjoyed reading this article, consider checking out the full version on\\nMedium. It‚Äôs still free ‚Üì\\n\\nFull article on Medium\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/highly-scalable-data-ingestion-architecture?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('9c6f5239-fc76-4fe9-a8e2-77f662d0c69f'), content={'Title': '2 Key LLMOps Concepts - by Alex Razvant', 'Subtitle': 'How to monitor LLM & RAG applications. Evaluate your RAG like a pro. Learn about memory/compute requirements on LLMs.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### 2 Key LLMOps Concepts\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# 2 Key LLMOps Concepts\\n\\n### How to monitor LLM & RAG applications. Evaluate your RAG like a pro. Learn\\nabout memory/compute requirements on LLMs.\\n\\nAlex Razvant\\n\\nJun 22, 2024\\n\\n10\\n\\nShare this post\\n\\n#### 2 Key LLMOps Concepts\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * A powerful framework to evaluate RAG pipelines\\n\\n  * Why do LLMs require so much VRAM?\\n\\n  * LLMOps Chain Monitoring\\n\\n* * *\\n\\n### ùó¢ùóªùó≤ ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ ùòÅùóº ùó≤ùòÉùóÆùóπùòÇùóÆùòÅùó≤ ùòÜùóºùòÇùóø ùó•ùóîùóö - ùó•ùóîùóöùóîùòÄ\\n\\nBuilding an RAG pipeline is fairly simple. You just need a Vector-DB knowledge\\nbase, an LLM to process your prompts, plus additional logic for interactions\\nbetween these modules.\\n\\nLesson 10: Evaluating the RAG pipeline. (Image by Author)\\n\\nHowever, reaching a satisfying performance level imposes its challenges due to\\nthe ‚Äúseparate‚Äù components:\\n\\n**Decoding ML Newsletter** is a reader-supported publication. If you enjoy our\\ncontent, please consider becoming a paid subscriber.\\n\\nSubscribe\\n\\n  1. **Retriever** ‚Äî which takes care of querying the Knowledge DB and retrieves additional context that matches the user‚Äôs query. \\n\\n  2. **Generator** ‚Äî which encompasses the LLM module, generating an answer based on the context-augmented prompt. When evaluating a RAG pipeline, we must evaluate both components separately and together. \\n\\nüî∏ **What is RAGAs?**\\n\\nA framework that helps you evaluate your Retrieval Augmented Generation (RAG)\\npipelines. One of the core concepts of RAGAs is Metric-Driven-Development\\n(MDD) which is a product development approach that relies on data to make\\nwell-informed decisions.\\n\\nüî∏ **What metrics do RAGAs expose?**\\n\\nüîΩ For ùó•ùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ Stage :\\n\\n‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùó£ùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª Evaluates the precision of the context used to generate an\\nanswer, ensuring relevant information is selected from the context  \\n‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùó•ùó≤ùóπùó≤ùòÉùóÆùóªùó∞ùòÜ Measures how relevant the selected context is to the\\nquestion. ‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùó•ùó≤ùó∞ùóÆùóπùóπ Measures if all the relevant information required\\nto answer the question was retrieved.  \\n‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùóòùóªùòÅùó∂ùòÅùó∂ùó≤ùòÄ ùó•ùó≤ùó∞ùóÆùóπùóπ Evaluates the recall of entities within the context,\\nensuring that no important entities are overlooked.\\n\\nüîΩ For ùóöùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóºùóª Stage :\\n\\n‚Ü≥ ùóôùóÆùó∂ùòÅùóµùó≥ùòÇùóπùóªùó≤ùòÄùòÄ Measures how accurately the generated answer reflects the\\nsource content, ensuring the generated content is truthful and reliable.  \\n‚Ü≥ ùóîùóªùòÄùòÑùó≤ùóø ùó•ùó≤ùóπùó≤ùòÉùóÆùóªùó∞ùó≤ It is validating that the response directly addresses the\\nuser‚Äôs query.  \\n‚Ü≥ ùóîùóªùòÄùòÑùó≤ùóø ùó¶ùó≤ùó∫ùóÆùóªùòÅùó∂ùó∞ ùó¶ùó∂ùó∫ùó∂ùóπùóÆùóøùó∂ùòÅùòÜ Shows that the generated content is semantically\\naligned with expected responses.  \\n‚Ü≥ ùóîùóªùòÄùòÑùó≤ùóø ùóñùóºùóøùóøùó≤ùó∞ùòÅùóªùó≤ùòÄùòÄ Focuses on fact-checking, assessing the factual accuracy\\nof the generated answer.  \\n  \\nüî∏ **How to evaluate using RAGAs?**\\n\\n1\\\\. Prepare your ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØùò¥,ùò¢ùòØùò¥ùò∏ùò¶ùò≥ùò¥,ùò§ùò∞ùòØùòµùò¶ùòπùòµùò¥ and ùò®ùò≥ùò∞ùò∂ùòØùò•_ùòµùò≥ùò∂ùòµùò©ùò¥  \\n2\\\\. Compose a Dataset object  \\n3\\\\. Select metrics  \\n4\\\\. Evaluate  \\n5\\\\. Monitor scores or log the entire evaluation chain to a platform like\\nCometML.\\n\\nFor a full end-to-end workflow of RAGAs evaluation in practice, I\\'ve described\\nit in this LLM-Twin Course Article üëá:\\n\\nHow to Evaluate RAGs Medium Article\\n\\n* * *\\n\\n### Why are LLMs so Memory-hungry?\\n\\nLLMs require lots of GPU memory, but let\\'s see why that\\'s the case. üëá\\n\\nüî∏ What is an LLM parameter?\\n\\nLLMs, like Mistral 7B or LLama3-8B, have billions of parameters. ùóòùóÆùó∞ùóµ\\nùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùó∂ùòÄ ùóÆ ùòÑùó≤ùó∂ùó¥ùóµùòÅ stored and accessed during computation.\\n\\nüî∏ How much GPU VRAM is required? There are three popular precision formats\\nthat LLMs are trained in:\\n\\n‚Üí FP32 - 32bits floating point  \\n‚Üí FP16/BFP16 - 16 bits floating point\\n\\nMost use mixed precision, e.g., matmul in BFP16 and accumulations in FP32.\\n\\nFor this example, we\\'ll use half-precision BFP16.\\n\\nHere\\'s a deeper dive on this topic:  \\nüîó Google BFloat16  \\nüîó LLMs Precision Benchmark\\n\\nüîπ Let\\'s calculate the VRAM required:\\n\\n\\\\\\\\(\\\\begin{align*} \\\\text{VRAM} &= \\\\text{Size}(\\\\text{params}) +\\n\\\\text{Size}(\\\\text{activations}) \\\\\\\\\\\\ \\\\text{Size}(\\\\text{params}) &=\\n\\\\text{Params} \\\\times \\\\text{Precision}(\\\\text{bytes}) \\\\end{align*}\\\\\\\\)\\n\\nAs 1byte=8bits, we\\'ve got:  \\n‚Üí FP32 = 32 bits = 4 bytes  \\n‚Üí FP16/BFP16 = 16bits = 2 bytes\\n\\nNow, for a 7B model, we would require:  \\n‚Üí VRAM = 7 * 10^9 (billion) * 2 bytes = 14 * 10^9 bytes\\n\\nKnowing that 1GB = 10 ^ 9 bytes we have ùü≠ùü∞ùóöùóï as the required VRAM to load a ùü≥ùóï\\nùó∫ùóºùó±ùó≤ùóπ ùó≥ùóºùóø ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ in half BF16 precision.\\n\\nùóßùóµùó∂ùòÄ ùó∂ùòÄ ùóΩùòÇùóøùó≤ùóπùòÜ ùó≥ùóºùóø ùóπùóºùóÆùó±ùó∂ùóªùó¥ ùòÅùóµùó≤ ùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóøùòÄ.  \\n  \\nEver encountered the ùóñùó®ùóóùóî ùó¢ùó¢ùó† Error e.g \"ùòõùò≥ùò™ùò¶ùò• ùòµùò∞ ùò¢ùò≠ùò≠ùò∞ùò§ùò¢ùòµùò¶ +56ùòîùòâ ...\" when\\ninferencing? here\\'s the most plausible cause for that:\\n\\n‚≠ï No GPU VRAM left for the activations. Let\\'s figure out the activation size\\nrequired by using ùóüùóüùóÆùó∫ùóÆùüÆ-ùü≥ùóï as an example.\\n\\nüî∏ Activations are a combination of the following model parameters:  \\n\\\\- Context Length (N)  \\n\\\\- Hidden Size (H)  \\n\\\\- Precision (P)\\n\\nAfter a quick look at the LLama2-7b model configuration, we get these values:  \\n\\\\- Context Length (N) = 4096 tokens  \\n\\\\- Hidden Size (H) = 4096 dims  \\n\\\\- Precision (P) = BF16 = 2bytes  \\nüîó ùóüùóüùóÆùó∫ùóÆùüÆ-ùü≥ùóØ ùó†ùóºùó±ùó≤ùóπ ùó£ùóÆùóøùóÆùó∫ùòÄ: shorturl.at/CWOJ9\\n\\nConsult this interactive LLM-VRAM calculator to check on the different memory\\nsegments reserved when inferencing/training LLMs.\\n\\nüü¢ Inference/Training VRAM Calculator  \\n  \\nüü° For training, things stay a little different, as more factors come into\\nplay, as memory is allocated for:  \\n‚Ü≥ Full Activations considering N(Heads) and N( Layers)  \\n‚Ü≥ Optimizer States which differ based on the optimizer type  \\n‚Ü≥ Gradients\\n\\nHere\\'s a tutorial on PEFT, QLoRA fine-tuning in action üëá:\\n\\nLLM Fine Tuning Medium Article\\n\\nOther Resources:  \\nüìî Model Anatomy: shorturl.at/nJeu0  \\nüìî VRAM for Serving: shorturl.at/9UPBE  \\nüìî LLM VRAM Explorer: shorturl.at/yAcTU\\n\\n* * *\\n\\n### One key LLMOps concept - Chain Monitoring\\n\\nIn traditional ML systems, it is easier to backtrack to a problem compared to\\nGenerative AI ones based on LLMs. When working with LLMs, their generative\\nnature can lead to complex and sometimes unpredictable behavior.\\n\\nüîπ ùóî ùòÄùóºùóπùòÇùòÅùó∂ùóºùóª ùó≥ùóºùóø ùòÅùóµùóÆùòÅ?\\n\\n\"Log prompts or entire chains with representative metadata when\\ntesting/evaluating your LLM.\" ùòñùòØùò¶ ùò±ùò≠ùò¢ùòµùòßùò∞ùò≥ùòÆ ùòµùò©ùò¢ùòµ ùòê ùò≠ùò™ùò¨ùò¶ ùò¢ùòØùò• ùòê\\'ùò∑ùò¶ ùò£ùò¶ùò¶ùòØ ùò∂ùò¥ùò™ùòØùò® ùòßùò∞ùò≥\\nùòµùò©ùò™ùò¥ ùòµùò¢ùò¥ùò¨ ùò™ùò¥ ùóñùóºùó∫ùó≤ùòÅùó†ùóü - ùóüùóüùó†.\\n\\n**üî∏** ùóõùó≤ùóøùó≤ ùóÆùóøùó≤ ùóÆ ùó≥ùó≤ùòÑ ùó∞ùóÆùòÄùó≤ùòÄ ùòÑùóµùó≤ùóøùó≤ ùó∂ùòÅ ùóΩùóøùóºùòÉùó≤ùòÄ ùóØùó≤ùóªùó≤ùó≥ùó∂ùó∞ùó∂ùóÆùóπ**:**\\n\\n‚Üí ùóôùóºùóø ùó¶ùòÇùó∫ùó∫ùóÆùóøùó∂ùòÄùóÆùòÅùó∂ùóºùóª ùóßùóÆùòÄùó∏ùòÄ\\n\\nHere you might have a query that represents the larger text, the LLMs response\\nwhich is the summary, and you could calculate the ROUGE score inline between\\nquery & response and add it to the metadata field. Then you can compose a JSON\\nwith query, response, and rouge_score and log it to comet.\\n\\n‚Üí ùóôùóºùóø ùó§&ùóî ùóßùóÆùòÄùó∏ùòÄ Here, you could log the Q&A pairs separately, or even add an\\nevaluation step using a larger model to evaluate the response. Each pair would\\nbe composed of Q, A, GT, and True/False to mark the evaluation.\\n\\n‚Ü≥ ùóôùóºùóø ùóöùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóºùóª ùóßùóÆùòÄùó∏ùòÄ You could log the query and response, and append in the\\nmetadata a few qualitative metrics (e.g. relevance, cohesiveness).\\n\\n‚Ü≥ùóôùóºùóø ùó•ùóîùóö If you have complex chains within your RAG application, you could log\\nprompt structures (sys_prompt, query), and LLM responses and track the chain\\nexecution step by step.\\n\\n‚Ü≥ ùóôùóºùóø ùó°ùóòùó• You could define the entity fields and log the query, response,\\nentities_list, and extracted_entities in the same prompt payload.\\n\\n‚Ü≥ùóôùóºùóø ùó©ùó∂ùòÄùó∂ùóºùóª ùóßùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùó≤ùóøùòÄ CometML LLM also allows you to log images associated\\nwith a prompt or a chain. If you‚Äôre working with GPT4-Vision for example, you\\ncould log the query and the generated image in the same payload.\\n\\nAlso, besides the actual prompt payload, you could inspect the processing time\\nper each step of a chain.\\n\\nFor example, a 3-step chain in an RAG application might query the Vector DB,\\ncompose the prompt, and pass it to the LLM, and when logging the chain to\\nCometML, you could see the processing time/chain step.\\n\\nüîπ ùóßùóº ùòÄùó≤ùòÅ ùó∂ùòÅ ùòÇùóΩ, ùòÜùóºùòÇ\\'ùóπùóπ ùóªùó≤ùó≤ùó±:\\n\\n\\\\- CometML pip package  \\n\\\\- CometML API key - Workspace name and Project Name\\n\\nI\\'ve used this approach when evaluating a fine-tuned LLM on a custom\\ninstruction dataset. For a detailed walkthrough üëá\\n\\nEvaluating LLMs Medium Article\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n10\\n\\nShare this post\\n\\n#### 2 Key LLMOps Concepts\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/2-key-llmops-concepts?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('87f34471-9a5b-4641-8272-15b6a18a9be7'), content={'Title': 'The LLM-Twin Free Course on Production-Ready RAG applications.', 'Subtitle': 'Learn how to build a full end-to-end LLM & RAG production-ready system, follow and code along each component by yourself.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\n### Learn how to build a full end-to-end LLM & RAG production-ready system,\\nfollow and code along each component by yourself.\\n\\nAlex Razvant\\n\\nJun 20, 2024\\n\\n13\\n\\nShare this post\\n\\n#### The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n‚Üí the **last lesson** of the LLM Twin free course\\n\\n**What is your LLM Twin?** It is an AI character that writes like yourself by\\nincorporating your style, personality, and voice into an LLM.\\n\\n**Decoding ML Newsletter** is a reader-supported publication. If you enjoy our\\nwork, please consider becoming a paid subscriber.\\n\\nSubscribe\\n\\nImage by DALL-E\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> _More**details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà_\\n\\n# **The LLM-Twin Free Course**\\n\\nThis course teaches you how to design, build, and deploy a production-ready\\nLLM-RAG system. It covers all the components, system design, data ingestion,\\nstreaming pipeline, fine-tuning pipeline, inference pipeline alongside\\nproduction monitoring, and more.\\n\\n## **What is the course about?**\\n\\nWe‚Äôre building a production-ready RAG system, able to write content based on\\nyour unique style, by scrapping previous posts/articles and code snippets\\nwritten by you to construct a fresh and continuously updated knowledge base,\\ngenerate a dataset to fine-tune a capable and efficient open-source LLM, and\\nthen interconnect all components for a full end-to-end deployment while\\nintegrating evaluation and post-deployment monitoring.\\n\\nThis course follows best MLOps & LLMOps practices, focusing on the 3-pipeline-\\ndesign pattern for building ML-centered applications.\\n\\n## **Lesson 1: Presenting the Architecture**\\n\\nPresenting and describing each component, the tooling used, and the intended\\nworkflow of implementation. The first lesson will prepare the ground by\\noffering a wide overview of each component and consideration.\\n\\n**We recommend you start here.**\\n\\nüîó **Lesson 1:** An End-to-End Framework for Production-Ready LLM Systems by\\nBuilding Your LLM Twin\\n\\nLLM twin system architecture [Image by the Author]\\n\\n## **Lesson 2: Data Pipelines**\\n\\nIn this lesson, we‚Äôll start by explaining what a data pipeline is, and the key\\nconcepts of data processing and streaming, and then dive into the data\\nscrapping and processing logic.\\n\\nüîó **Lesson 2:** The Importance of Data Pipelines in the Era of Generative AI\\n\\nLesson 2: The Data Collection Pipeline [Image by author]\\n\\n## **Lesson 3: Change Data Capture and Data Processing**\\n\\nIn this lesson, we‚Äôre showcasing the CDC(Change Data Capture) integration\\nwithin the LLM-Twin data pipeline. We‚Äôre showing how to set up MongoDB, the\\nCDC approach for event-driven processing, RabbitMQ for message queuing, and\\nefficient low-latency database querying using the MongoDB Oplog.\\n\\nüîó **Lesson 3:** CDC Enabling Event-Driven Architectures\\n\\nLesson 3: Event-Driven Processing using RabbitMQ, CDC, and MongoDB (Image by\\nAuthor)\\n\\n## **Lesson 4: Efficient Data Streaming Pipelines**\\n\\nIn this lesson, we‚Äôll focus on the feature pipeline. Here, we‚Äôre showcasing\\nhow we ingest data that we‚Äôve gathered in the previous lesson, and how we‚Äôve\\nbuilt a stream-processing workflow with **Bytewax **that fetches raw samples,\\nstructures them using Pydantic Models, cleans, chunks, encodes, and stores\\nthem in our **Qdrant** Vector Database.\\n\\nüîó **Lesson 4:** SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî\\nin Real-Time!\\n\\nLesson 4: Efficient Data Streaming Pipelines using Bytewax and Qdrant Vector\\nDB. (Image by Author)\\n\\n## **Lesson 5: Advanced RAG Optimization Techniques**\\n\\nIn this lesson, we‚Äôll showcase a few advanced techniques to increase the\\nsimilarity and accuracy of the embedded data samples from our **Qdrant**\\nVector Database. The contents of this lesson could make a significant\\ndifference between a naive RAG application and a production-ready one.\\n\\nüîó **Lesson 5:** The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\nLesson 5: Advanced RAG Optimization Techniques. (Image by Author)\\n\\n## **Lesson 6: Dataset preparation for LLM fine-tuning**\\n\\nIn this lesson, we‚Äôll discuss the core concepts to consider when creating\\ntask-specific custom datasets to fine-tune LLMs. We‚Äôll use our cleaned data\\nfrom our Vector Database, and engineer specific Prompt Templates alongside\\nusing GPT3.5-Turbo API to generate our custom dataset and version it on\\n**Comet ML**.\\n\\nüîó **Lesson 6:** The Role of Feature Stores in Fine-Tuning LLMs\\n\\nLesson 6: Generate custom datasets using Knowledge Distillation.\\n\\n## **Lesson 7: Fine-tuning LLMs on custom datasets**\\n\\nWe‚Äôll show how to implement a fine-tuning workflow for a Mistral7B-Instruct\\nmodel while using the custom dataset we‚Äôve versioned previously. We‚Äôll present\\nin-depth the key concepts including LoRA Adapters, PEFT, Quantisation, and how\\nto deploy on Qwak.\\n\\nüîó **Lesson 7:**How to fine-tune LLMs on custom datasets at Scale using Qwak\\nand CometML\\n\\nLesson 7: Fine-tuning LLMs on custom datasets using Qwak and CometML. (Image\\nby Author)\\n\\n## **Lesson 8: Evaluating the fine-tuned LLM**\\n\\nIn this lesson, we‚Äôre discussing one core concept of ML - **Evaluation**.  \\nWe‚Äôll present the evaluation workflow we‚Äôll showcase the full process of\\nassessing the model‚Äôs performance using the GPT3.5-Turbo model and custom-\\nengineered evaluation templates.\\n\\nüîó **Lesson 8:**Best Practices When Evaluating Fine-Tuned LLMs\\n\\nLesson 8: Evaluating the quality of our custom fine-tuned LLM. (Image by\\nAuthor)\\n\\n## **Lesson 9: Deploying the Inference Pipeline Stack**\\n\\nIn this lesson, we‚Äôll showcase how to design and implement the LLM & RAG\\ninference pipeline based on a set of detached Python microservices. We‚Äôll\\nsplit the ML and business logic into two components, describe each one in\\npart, and show how to wrap up and deploy the inference pipeline on **Qwak** as\\na scalable and reproducible system.\\n\\nüîó **Lesson 9:**Architect scalable and cost-effective LLM & RAG inference\\npipelines\\n\\nLesson 9: Architecturing LLM & RAG inference pipeline. (Image by Author)\\n\\n## **Lesson 10: RAG Pipeline Evaluation**\\n\\nIn this lesson, we‚Äôre covering RAG evaluation ‚Äî which is one of great\\nimportance. If no proper evaluation metrics are monitored or techniques are\\nused, the RAG systems might underperform and hallucinate badly.\\n\\nHere, we‚Äôll describe the workflow of evaluating RAG pipelines using the\\npowerful RAGAs framework, compose the expected RAGAs evaluation format, and\\ncapture eval scores which will be included in full LLM execution chains and\\nlogged on **Comet ML LLM**.\\n\\nüîó **Lesson 10:**Evaluating RAG Systems using the RAGAs Framework\\n\\nLesson 10: Evaluating the RAG pipeline. (Image by Author)\\n\\n### Next Steps\\n\\n#### Step 1\\n\\n**Check out** the **full versions** of all **Lessons 1-11** on our **Medium\\npublication** , under the LLM-Twin Course group tag. _It‚Äôs still FREE:_\\n\\nThe LLM-Twin Course\\n\\n#### Step 2\\n\\n‚Üí **Check out theLLM Twin GitHub repository and try it yourself ü´µ**\\n\\n _Nothing compares with getting your hands dirty and building it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-llm-twin-free-course-on-production?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('d3cb26a9-45fe-42e0-9a79-7a2f358fc875'), content={'Title': 'A blueprint for designing production LLM systems: From Notebooks to production ', 'Subtitle': 'How to get a GitHub Copilot subscription for FREE (to 5x writing code). Learn to build production ML systems by building an LLM application.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\n### How to get a GitHub Copilot subscription for FREE (to 5x writing code).\\nLearn to build production ML systems by building an LLM application.\\n\\nPaul Iusztin\\n\\nJun 15, 2024\\n\\n13\\n\\nShare this post\\n\\n#### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * How to get a GitHub Copilot subscription for FREE (to 5x writing code)\\n\\n  * A blueprint for designing production LLM systems: From Notebooks to production\\n\\n  * Learn to build production ML systems by building an LLM application\\n\\n* * *\\n\\n### How to get a GitHub Copilot subscription for FREE (to 5x writing code)\\n\\nùóõùóºùòÑ to get a ùóöùó∂ùòÅùóõùòÇùóØ ùóñùóºùóΩùó∂ùóπùóºùòÅ ùòÄùòÇùóØùòÄùó∞ùóøùó∂ùóΩùòÅùó∂ùóºùóª for ùóôùó•ùóòùóò (to 5x writing code) ‚Üì  \\n  \\nThere are other alternatives, but GitHub Copilot is still the leading solution\\ndue to 2 factors: performance & convenience.  \\n  \\nIf you can get it for free, there are 0 reasons not to use it (sneaky move\\nMicrosoft) ‚Üì  \\n  \\nùó¶ùóº ùòÑùóµùóÆùòÅ ùó∂ùòÄ ùòÅùóµùó≤ ùòÄùóºùóπùòÇùòÅùó∂ùóºùóª?  \\n  \\nThere is no secret.  \\n  \\nAs stated in their docs: \"Verified students, teachers, and maintainers of\\npopular open source projects on GitHub are eligible to use Copilot Individual\\nfor free. \"  \\n  \\nüîó Docs  \\n  \\nTo become a student or teacher when you are not is not a solution.  \\n  \\nBut...  \\n  \\nTo become a maintainer of a popular open-source project is!\\n\\nùó¶ùóº ùòÑùóµùóÆùòÅ ùóÆùóøùó≤ ùòÅùóµùó≤ ùó∞ùóøùó∂ùòÅùó≤ùóøùó∂ùóÆ ùó≥ùóºùóø ùóØùó≤ùó∞ùóºùó∫ùó∂ùóªùó¥ ùóÆ \"ùó∫ùóÆùó∂ùóªùòÅùóÆùó∂ùóªùó≤ùóø ùóºùó≥ ùóÆ ùóΩùóºùóΩùòÇùóπùóÆùóø ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤\\nùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ\"?  \\n  \\nI don\\'t know the exact formula, but here are some examples.  \\n  \\nI am eligible for it because I am the owner of a GitHub repository with ~2.2k\\nstars & 350 forks: üîó Hands-on LLMs Course  \\n  \\nAfter digging into some Reddit threads, a dude said that for a repo with ~520\\nstars & 299 forks, you got the free subscription.  \\n  \\nThe idea is that you don\\'t have to be a maintainer of Pandas or PyTorch to\\nbecome eligible.  \\n  \\n.  \\n  \\nùóßùóµùó≤ ùó∞ùóºùóªùó∞ùóπùòÇùòÄùó∂ùóºùóª ùó∂ùòÄ ùòÅùóº...  \\n  \\n‚Üí start contributing to open-source or creating your cool project, which will\\ncomplete the job!  \\n  \\n.  \\n  \\nùòêùòß ùò∫ùò∞ùò∂ ùò£ùò¶ùòµùòµùò¶ùò≥ ùò¨ùòØùò∞ùò∏ ùòµùò©ùò¶ \"ùò¥ùò¶ùò§ùò≥ùò¶ùòµ ùòßùò∞ùò≥ùòÆùò∂ùò≠ùò¢/ùò§ùò≥ùò™ùòµùò¶ùò≥ùò™ùò¢,\" ùò±ùò≠ùò¶ùò¢ùò¥ùò¶ ùò≠ùò¶ùò¢ùò∑ùò¶ ùò™ùòµ ùò™ùòØ ùòµùò©ùò¶\\nùò§ùò∞ùòÆùòÆùò¶ùòØùòµùò¥ ùòßùò∞ùò≥ ùò∞ùòµùò©ùò¶ùò≥ùò¥ ùòµùò∞ ùò¨ùòØùò∞ùò∏.  \\n  \\nAlso, let me know if you know that when contributing to open-source, you must\\ncontribute by \"how much\" until you become eligible.\\n\\n* * *\\n\\n### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\nI am ùóæùòÇùó∂ùòÅùòÅùó∂ùóªùó¥ ùó∞ùóøùó≤ùóÆùòÅùó∂ùóªùó¥ ùó∞ùóºùóªùòÅùó≤ùóªùòÅ... ùóùùóºùó∏ùó∂ùóªùó¥, but here is ùóµùóºùòÑ to ùóØùòÇùó∂ùóπùó± your ùóüùóüùó†\\nùòÅùòÑùó∂ùóª for ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ posts or articles ùòÇùòÄùó∂ùóªùó¥ ùòÜùóºùòÇùóø ùòÉùóºùó∂ùó∞ùó≤ ‚Üì  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóÆùóª ùóüùóüùó† ùòÅùòÑùó∂ùóª?  \\n  \\nIt\\'s an AI character who writes like you, using your writing style and\\npersonality.  \\n  \\nùó™ùóµùòÜ ùóªùóºùòÅ ùó±ùó∂ùóøùó≤ùó∞ùòÅùóπùòÜ ùòÇùòÄùó≤ ùóñùóµùóÆùòÅùóöùó£ùóß? ùó¨ùóºùòÇ ùó∫ùóÆùòÜ ùóÆùòÄùó∏...  \\n  \\nWhen generating content using an LLM, the results tend to:  \\n  \\n\\\\- be very generic and unarticulated,  \\n\\\\- contain misinformation (due to hallucination),  \\n\\\\- require tedious prompting to achieve the desired result.  \\n  \\nùóßùóµùóÆùòÅ ùó∂ùòÄ ùòÑùóµùòÜ, ùó≥ùóºùóø ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ ùó∞ùóºùóªùòÅùó≤ùóªùòÅ, ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùóÆ ùòÄùóΩùó≤ùó∞ùó∂ùóÆùóπùó∂ùòáùó≤ùó± ùòÅùóºùóºùóπ ùòÅùóµùóÆùòÅ:  \\n  \\n‚Üí is fine-tuned on your digital content to replicate your persona  \\n  \\n‚Üí has access to a vector DB (with relevant data) to avoid hallucinating and\\nwrite only about concrete facts\\n\\nùóõùó≤ùóøùó≤ ùóÆùóøùó≤ ùòÅùóµùó≤ ùó∫ùóÆùó∂ùóª ùòÄùòÅùó≤ùóΩùòÄ ùóøùó≤ùóæùòÇùó∂ùóøùó≤ùó± ùòÅùóº ùóØùòÇùó∂ùóπùó± ùòÜùóºùòÇùóø ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÅùòÑùó∂ùóª:  \\n  \\n1\\\\. A data collection pipeline will gather your digital data from Medium,\\nSubstack, LinkedIn and GitHub. It will be normalized and saved to a Mongo DB.  \\n  \\n2\\\\. Using CDC, you listen to any changes made to the Mongo DB and add them as\\nevents to a RabbitMQ queue.  \\n  \\n3\\\\. A Bytewax streaming ingestion pipeline will listen to the queue to clean,\\nchunk, and embed the data in real time.  \\n  \\n4\\\\. The cleaned and embedded data is loaded to a Qdrant vector DB.  \\n  \\n5\\\\. On the training pipeline side, you use a vector DB retrieval client to\\nbuild your training dataset, which consists of the cleaned data (augmented\\nusing RAG).  \\n  \\n6\\\\. You fine-tune an open-source Mistral LLM using QLoRA and push all the\\nexperiment artifacts to a Comet experiment tracker.  \\n  \\n7\\\\. Based on the best experiment, you push the LLM candidate to Comet\\'s model\\nregistry. You carefully evaluate the LLM candidate using Comet\\'s prompt\\nmonitoring dashboard. If the evaluation passes, you tag it as accepted.  \\n  \\n8\\\\. On the inference pipeline side, you deploy the new LLM model by pulling it\\nfrom the model registry, loading it, and quantizing it.  \\n  \\n9\\\\. The inference pipeline is wrapped by a REST API, which allows users to\\nmake ChatGPT-like requests.\\n\\n* * *\\n\\n### Learn to build production ML systems by building an LLM application\\n\\nTaking in mind the _blueprint for designing production LLM systems presented\\nabove_ , we want to let you know that:\\n\\n_‚Üí We are close to wrapping our LLM twin course lessons and code._\\n\\nTo give more context for newcomers, in the past weeks we started ùóøùó≤ùóπùó≤ùóÆùòÄùó∂ùóªùó¥ an\\nùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó∞ùóºùòÇùóøùòÄùó≤ on ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª ùóüùóüùó†ùòÄ by teaching you how to ùóØùòÇùó∂ùóπùó± an ùóüùóüùó† ùòÅùòÑùó∂ùóª:\\nùò†ùò∞ùò∂ùò≥ ùòóùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ-ùòôùò¶ùò¢ùò•ùò∫ ùòàùòê ùòôùò¶ùò±ùò≠ùò™ùò§ùò¢\\n\\nSo‚Ä¶\\n\\nIf you are looking for an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùóôùó•ùóòùóò ùó∞ùóºùòÇùóøùòÄùó≤ on ùóµùóºùòÑ to ùóØùòÇùó∂ùóπùó± ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-\\nùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ, consider checking the course\\'s **first** FREE **lesson**.  \\n  \\nùòõùò©ùò¶ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùò∏ùò™ùò≠ùò≠ ùò∏ùò¢ùò≠ùò¨ ùò∫ùò∞ùò∂ ùòµùò©ùò≥ùò∞ùò∂ùò®ùò© ùò¢ ùòßùò∂ùò≠ùò≠-ùò¥ùòµùò¢ùò§ùò¨ ùò±ùò≥ùò∞ùò§ùò¶ùò¥ùò¥:  \\n  \\n‚Üí from data gathering...  \\n  \\n...until deploying and monitoring your LLM twin using LLMOps ‚Üê  \\n  \\n.  \\n  \\nWith that in mind...  \\n  \\nThe ùü≠ùòÄùòÅ ùóπùó≤ùòÄùòÄùóºùóª will walk you through:  \\n  \\n\\\\- the issues of generating content using ChatGPT (or other similar solutions)  \\n\\\\- the 3-pipeline design  \\n\\\\- the system design and architecture of the LLM twin  \\n  \\n.  \\n  \\nWithin the ùòÄùòÜùòÄùòÅùó≤ùó∫ ùó±ùó≤ùòÄùó∂ùó¥ùóª ùòÄùó≤ùó∞ùòÅùó∂ùóºùóª, we will present all the ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùóÆùóπ\\nùó±ùó≤ùó∞ùó∂ùòÄùó∂ùóºùóªùòÄ on ùóµùóºùòÑ to ùóØùòÇùó∂ùóπùó±:  \\n  \\n\\\\- a data collection pipeline  \\n\\\\- a real-time feature pipeline using a streaming engine  \\n\\\\- hook the data and feature pipelines using the CDC pattern  \\n\\\\- a continuous fine-tuning pipeline  \\n\\\\- an inference pipeline deployed as a REST API  \\n  \\n  \\nA ùóΩùóÆùóøùòÅùó∂ùó∞ùòÇùóπùóÆùóø ùó≥ùóºùó∞ùòÇùòÄ will be on ùó∂ùóªùòÅùó≤ùó¥ùóøùóÆùòÅùó∂ùóªùó¥ ùó†ùóüùó¢ùóΩùòÄ & ùóüùóüùó†ùó¢ùóΩùòÄ ùó¥ùóºùóºùó± ùóΩùóøùóÆùó∞ùòÅùó∂ùó∞ùó≤ùòÄ:  \\n  \\n\\\\- prompt versioning  \\n\\\\- model registries  \\n\\\\- experiment tracker  \\n\\\\- prompt monitoring  \\n\\\\- CI/CD  \\n\\\\- IaC  \\n\\\\- Docker  \\n  \\n.  \\n  \\nùôíùôñùô£ùô© ùô©ùô§ ùôôùôûùôú ùôûùô£ùô©ùô§ ùô©ùôùùôö 1ùô®ùô© ùô°ùôöùô®ùô®ùô§ùô£?  \\n  \\nùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ. It\\'s FREE, and no registration is required  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòìùò¶ùò¥ùò¥ùò∞ùòØ 1 - ùòàùòØ ùòåùòØùò•-ùòµùò∞-ùòåùòØùò• ùòçùò≥ùò¢ùòÆùò¶ùò∏ùò∞ùò≥ùò¨ ùòßùò∞ùò≥ ùòóùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ-ùòôùò¶ùò¢ùò•ùò∫ ùòìùòìùòî ùòöùò∫ùò¥ùòµùò¶ùòÆùò¥ ùò£ùò∫\\nùòâùò∂ùò™ùò≠ùò•ùò™ùòØùò® ùò†ùò∞ùò∂ùò≥ ùòìùòìùòî ùòõùò∏ùò™ùòØ\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/a-blueprint-for-designing-production?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('9d858911-52d4-4240-8d6e-91f6b426baa0'), content={'Title': 'The difference between development and continuous training ML environments', 'Subtitle': 'Looking to become a PRO in LangChain? How to write a streaming retrieval system for RAG on social media data.', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The difference between development and continuous training ML\\nenvironments\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The difference between development and continuous training ML environments\\n\\n### Looking to become a PRO in LangChain? How to write a streaming retrieval\\nsystem for RAG on social media data.\\n\\nPaul Iusztin\\n\\nJun 08, 2024\\n\\n7\\n\\nShare this post\\n\\n#### The difference between development and continuous training ML\\nenvironments\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Looking to become a PRO in LangChain?\\n\\n  * The difference between development and continuous training ML environments\\n\\n  * How to write a streaming retrieval system for RAG on social media data\\n\\n* * *\\n\\n _**First** , I want to thank everyone who supported our Hands-on LLMs course\\nrepo_ üôèüèª\\n\\nThe ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ FREE ùó∞ùóºùòÇùóøùòÄùó≤ passed 2.1k+ ‚≠êÔ∏è on GitHub - the place to ùóπùó≤ùóÆùóøùóª\\nthe ùó≥ùòÇùóªùó±ùóÆùó∫ùó≤ùóªùòÅùóÆùóπùòÄ of ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ & ùóüùóüùó†ùó¢ùóΩùòÄ  \\n  \\nùòõùò©ùò¶ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùò™ùò¥ ùòµùò©ùò¶ ùò®ùò∞-ùòµùò∞ ùò©ùò∂ùò£ ùòßùò∞ùò≥ ùò≠ùò¶ùò¢ùò≥ùòØùò™ùòØùò® ùòµùò©ùò¶ ùòßùò∂ùòØùò•ùò¢ùòÆùò¶ùòØùòµùò¢ùò≠ùò¥ ùò∞ùòß ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ-ùò≥ùò¶ùò¢ùò•ùò∫\\nùòìùòìùòîùò¥ & ùòìùòìùòîùòñùò±ùò¥  \\n  \\nIt will walk you through an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùóΩùóøùóºùó∞ùó≤ùòÄùòÄ...  \\n  \\n...from data preparation to deployment & monitoring:  \\n  \\n\\\\- the 3-pipeline design  \\n\\\\- building your custom financial dataset using GPT-4  \\n\\\\- a streaming pipeline to ingest financial news in real-time  \\n\\\\- fine-tuning an LLM using QLoRA  \\n\\\\- building a custom RAG pipeline  \\n\\\\- deploying the streaming pipeline to AWS  \\n\\\\- deploying the training & inference pipelines to Beam  \\n\\\\- using MLOps components: model registries, experiment trackers, prompt\\nmonitoring  \\n  \\n\\nùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòèùò¢ùòØùò•ùò¥-ùò∞ùòØ ùòìùòìùòîùò¥ ùòäùò∞ùò∂ùò≥ùò¥ùò¶ - ùòìùò¶ùò¢ùò≥ùòØ ùòµùò∞ ùòõùò≥ùò¢ùò™ùòØ ùò¢ùòØùò• ùòãùò¶ùò±ùò≠ùò∞ùò∫ ùò¢ ùòôùò¶ùò¢ùò≠-ùòõùò™ùòÆùò¶ ùòçùò™ùòØùò¢ùòØùò§ùò™ùò¢ùò≠\\nùòàùò•ùò∑ùò™ùò¥ùò∞ùò≥\\n\\n* * *\\n\\n### Looking to become a PRO in LangChain?\\n\\nThen ùó∞ùóµùó≤ùó∞ùó∏ ùóºùòÇùòÅ this ùóØùóºùóºùó∏ on ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóÆùóªùó¥ùóñùóµùóÆùó∂ùóª: from ùóØùó≤ùó¥ùó∂ùóªùóªùó≤ùóø to ùóÆùó±ùòÉùóÆùóªùó∞ùó≤ùó± ‚Üì  \\n  \\n‚Üí It's called: ùòéùò¶ùòØùò¶ùò≥ùò¢ùòµùò™ùò∑ùò¶ ùòàùòê ùò∏ùò™ùòµùò© ùòìùò¢ùòØùò®ùòäùò©ùò¢ùò™ùòØ: ùòâùò∂ùò™ùò≠ùò• ùòìùòìùòî ùò¢ùò±ùò±ùò¥ ùò∏ùò™ùòµùò© ùòóùò∫ùòµùò©ùò∞ùòØ,\\nùòäùò©ùò¢ùòµùòéùòóùòõ, ùò¢ùòØùò• ùò∞ùòµùò©ùò¶ùò≥ ùòìùòìùòîùò¥ by Ben Auffarth , published by Packt  \\n  \\nùòèùò¶ùò≥ùò¶ ùò™ùò¥ ùò¢ ùò¥ùò©ùò∞ùò≥ùòµ ùò£ùò≥ùò¶ùò¢ùò¨ùò•ùò∞ùò∏ùòØ:  \\n  \\n\\\\- It begins with some theoretical chapters on LLMs & LangChain  \\n  \\n\\\\- It explores the critical components of LangChain: chains, agents, memory,\\ntools  \\n  \\nùóßùóµùó≤ùóª, ùó∫ùòÜ ùó≥ùóÆùòÉùóºùóøùó∂ùòÅùó≤ ùóΩùóÆùóøùòÅ...  \\n  \\nùóúùòÅ ùó∑ùòÇùó∫ùóΩùòÄ ùó±ùó∂ùóøùó≤ùó∞ùòÅùóπùòÜ ùó∂ùóªùòÅùóº ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ - ùó™ùóúùóßùóõ ùó£ùó¨ùóßùóõùó¢ùó° ùóñùó¢ùóóùóò ‚Üì  \\n  \\n\\\\- takes off with beginner-friendly examples of using LangChain with agents,\\nHuggingFace, GCP/VertexAI, Azure, Anthropic, etc.  \\n  \\n\\\\- shows an end-to-end example of building a customer services application\\nwith LangChain & VertexAI  \\n  \\n\\\\- how to mitigate hallucinations using the ùòìùòìùòîùòäùò©ùò¶ùò§ùò¨ùò¶ùò≥ùòäùò©ùò¢ùò™ùòØ class  \\n  \\n\\\\- how to implement map-reduce pipelines  \\n  \\n\\\\- how to monitor token usage & costs  \\n  \\n\\\\- how to extract information from documents such as PDFs  \\n  \\n\\\\- building a Streamlit interface  \\n  \\n\\\\- how reasoning works in agent  \\n  \\n\\\\- building a chatbot like ChatGPT from SCRATCH  \\n  \\n.  \\n  \\nI haven't finished it yet, but I love it so far ‚ÄîI plan to finish it soon.  \\n  \\n.  \\n  \\nùó™ùóµùóº ùó∂ùòÄ ùòÅùóµùó∂ùòÄ ùó≥ùóºùóø?  \\n  \\nIf you are ùòÄùòÅùóÆùóøùòÅùó∂ùóªùó¥ ùóºùòÇùòÅ in the LLM world, this is a great book to ùóøùó≤ùóÆùó± ùó≤ùóªùó±-ùòÅùóº-\\nùó≤ùóªùó±.  \\n  \\nEven if you are ùó≤ùòÖùóΩùó≤ùóøùó∂ùó≤ùóªùó∞ùó≤ùó±, I think it is ùó≤ùòÖùòÅùóøùó≤ùó∫ùó≤ùóπùòÜ ùòÇùòÄùó≤ùó≥ùòÇùóπ to ùòÄùó∏ùó∂ùó∫ ùó∂ùòÅ to\\nrefresh the fundamentals, learn new details, and see how everything is\\nimplemented in LangChain.\\n\\nGenerative AI with LangChain [By Ben Auffarth]\\n\\nùóúùòÄ ùòÅùóµùó∂ùòÄ ùó≥ùóºùóø ùòÜùóºùòÇ? ü´µ  \\n  \\nüîó ùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ: Generative AI with LangChain [By Ben Auffarth]\\n\\n* * *\\n\\n### The difference between development and continuous training ML environments\\n\\nThey might do the same thing, but their design is entirely different ‚Üì  \\n  \\nùó†ùóü ùóóùó≤ùòÉùó≤ùóπùóºùóΩùó∫ùó≤ùóªùòÅ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nAt this point, your main goal is to ingest the raw and preprocessed data\\nthrough versioned artifacts (or a feature store), analyze it & generate as\\nmany experiments as possible to find the best:  \\n\\\\- model  \\n\\\\- hyperparameters  \\n\\\\- augmentations  \\n  \\nBased on your business requirements, you must maximize some specific metrics,\\nfind the best latency-accuracy trade-offs, etc.  \\n  \\nYou will use an experiment tracker to compare all these experiments.  \\n  \\nAfter you settle on the best one, the output of your ML development\\nenvironment will be:  \\n\\\\- a new version of the code  \\n\\\\- a new version of the configuration artifact  \\n  \\nHere is where the research happens. Thus, you need flexibility.  \\n  \\nThat is why we decouple it from the rest of the ML systems through artifacts\\n(data, config, & code artifacts).\\n\\nThe difference between ML development & continuous training environments\\n\\nùóñùóºùóªùòÅùó∂ùóªùòÇùóºùòÇùòÄ ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nHere is where you want to take the data, code, and config artifacts and:  \\n  \\n\\\\- train the model on all the required data  \\n\\\\- output a staging versioned model artifact  \\n\\\\- test the staging model artifact  \\n\\\\- if the test passes, label it as the new production model artifact  \\n\\\\- deploy it to the inference services  \\n  \\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub\\nActions):  \\n  \\n\\\\- builds a docker image from the code artifact (e.g., triggered manually or\\nwhen a new artifact version is created)  \\n\\\\- start the training pipeline inside the docker container that pulls the\\nfeature and config artifacts and outputs the staging model artifact  \\n\\\\- manually look over the training report -> If everything went fine, manually\\ntrigger the testing pipeline  \\n\\\\- manually look over the testing report -> if everything worked fine (e.g.,\\nthe model is better than the previous one), manually trigger the CD pipeline\\nthat deploys the new model to your inference services  \\n  \\nNote how the model registry quickly helps you to decouple all the components.  \\n  \\nAlso, because training and testing metrics are not always black and white, it\\nis challenging to automate the CI/CD pipeline 100%.  \\n  \\nThus, you need a human in the loop when deploying ML models.  \\n  \\nTo conclude...  \\n  \\nThe ML development environment is where you do your research to find better\\nmodels.  \\n  \\nThe continuous training environment is used to train & test the production\\nmodel at scale.\\n\\n* * *\\n\\n### How to write a streaming retrieval system for RAG on social media data\\n\\nùóïùóÆùòÅùó∞ùóµ ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ are the ùóΩùóÆùòÄùòÅ. Here is how to ùòÑùóøùó∂ùòÅùó≤ a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫\\nfor ùó•ùóîùóö on ùòÄùóºùó∞ùó∂ùóÆùóπ ùó∫ùó≤ùó±ùó∂ùóÆ ùó±ùóÆùòÅùóÆ ‚Üì  \\n  \\nùó™ùóµùòÜ ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóºùòÉùó≤ùóø ùóØùóÆùòÅùó∞ùóµ?  \\n  \\nIn environments where data evolves quickly (e.g., social media platforms), the\\nsystem's response time is critical for your application's user experience.  \\n  \\nThat is why TikTok is so addicting. Its recommender system adapts in real-time\\nbased on your interaction with the app.  \\n  \\nHow would it be if the recommendations were updated daily or hourly?  \\n  \\nWell, it would work, but you would probably get bored of the app much faster.  \\n  \\nThe same applies to RAG for highly intensive data sources...  \\n  \\n‚Üí where you must sync your source and vector DB in real time for up-to-date\\nretrievals.  \\n  \\nùòìùò¶ùòµ'ùò¥ ùò¥ùò¶ùò¶ ùò©ùò∞ùò∏ ùò™ùòµ ùò∏ùò∞ùò≥ùò¨ùò¥.  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nI wrote an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ on how to ùóØùòÇùó∂ùóπùó± a ùóøùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫ for ùó•ùóîùóö on\\nùóüùó∂ùóªùó∏ùó≤ùó±ùóúùóª ùó±ùóÆùòÅùóÆ in collaboration with Superlinked .  \\n  \\nThe ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫ is based on ùüÆ ùó±ùó≤ùòÅùóÆùó∞ùóµùó≤ùó± ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅùòÄ:  \\n\\\\- the streaming ingestion pipeline  \\n\\\\- the retrieval client  \\n  \\nThe ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó∂ùóªùó¥ùó≤ùòÄùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ runs 24/7 to keep the vector DB synced with\\nthe current raw LinkedIn posts data source.  \\n  \\nThe ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùó∞ùóπùó∂ùó≤ùóªùòÅ is used in RAG applications to query the vector DB.  \\n  \\n‚Üí These 2 components are completely decoupled and communicate with each other\\nthrough the vector DB.  \\n  \\n#ùü≠. ùóßùóµùó≤ ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó∂ùóªùó¥ùó≤ùòÄùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n‚Üí Implemented in Bytewax \\\\- a streaming engine built in Rust (speed&\\nreliability) that exposes a Python interface  \\n  \\nùòîùò¢ùò™ùòØ ùòßùò≠ùò∞ùò∏:  \\n  \\n\\\\- uses CDC to add changes from the source DB to a queue  \\n\\\\- listens to the queue for new events  \\n\\\\- cleans, chunks, and embeds the LI posts  \\n\\\\- loads them to a Qdrant vector DB  \\n  \\nand... everything in real-time!\\n\\nAdvanced RAG architecture [source from Superlinked Vectorhub]\\n\\n#ùüÆ. ùóßùóµùó≤ ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùó∞ùóπùó∂ùó≤ùóªùòÅ  \\n  \\n‚Üí A standard Python module.  \\n  \\nThe goal is to retrieve similar posts using various query types, such as\\nposts, questions, and sentences.  \\n  \\nùòîùò¢ùò™ùòØ ùòßùò≠ùò∞ùò∏:  \\n  \\n\\\\- preprocess user queries (the same way as they were ingested)  \\n\\\\- search the Qdrant vector DB for the most similar results  \\n\\\\- use rerank to improve the retrieval system's accuracy  \\n\\\\- visualize the results on a 2D plot using UMAP  \\n  \\n.  \\n  \\nYou don't believe me? ü´µ  \\n  \\nùóñùóµùó≤ùó∞ùó∏ ùóºùòÇùòÅ ùòÅùóµùó≤ ùó≥ùòÇùóπùóπ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ & ùó∞ùóºùó±ùó≤ ùóºùóª ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ‚Üì  \\n  \\nüîó ùòà ùòôùò¶ùò¢ùò≠-ùòµùò™ùòÆùò¶ ùòôùò¶ùòµùò≥ùò™ùò¶ùò∑ùò¢ùò≠ ùòöùò∫ùò¥ùòµùò¶ùòÆ ùòßùò∞ùò≥ ùòôùòàùòé ùò∞ùòØ ùòöùò∞ùò§ùò™ùò¢ùò≠ ùòîùò¶ùò•ùò™ùò¢ ùòãùò¢ùòµùò¢\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n7\\n\\nShare this post\\n\\n#### The difference between development and continuous training ML\\nenvironments\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-difference-between-development?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('20beb560-6063-4158-b7b5-c2083b299ec5'), content={'Title': 'Architect LLM & RAG inference pipelines - by Paul Iusztin', 'Subtitle': 'Design, build, deploy and monitor LLM and RAG inference pipelines using LLMOps best practices. Integrate it with a model registry and vector DB.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\n### Design, build and deploy RAG inference pipeline using LLMOps best\\npractices.\\n\\nPaul Iusztin\\n\\nJun 06, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n‚Üí the **9th** out of **11 lessons** of the **LLM Twin free course**\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> _More**details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà_\\n\\n### Latest Lessons of the LLM Twin Course\\n\\n**Lesson 6:** The Role of Feature Stores in Fine-Tuning LLMs\\n\\n‚Üí Custom Dataset Generation, Artifact Versioning, GPT3.5-Turbo Distillation,\\nQdrant\\n\\n**Lesson 7:** How to fine-tune LLMs on custom datasets at Scale using Qwak and\\nCometML\\n\\n‚ÜíQLoRA, PEFT, Fine-tuning Mistral-7b-Instruct on custom dataset, Qwak, Comet\\nML\\n\\n**Lesson 8:** Best practices when evaluating fine-tuned LLM models\\n\\n‚Üí LLM Evaluation techniques: Does and don‚Äôts, Quantitive and manual LLM\\nevaluation techniques\\n\\n* * *\\n\\n## **Lesson 9: Architect scalable and cost-effective LLM & RAG inference\\npipelines**\\n\\nIn **Lesson 9,** we will focus on implementing and deploying the inference\\npipeline of the LLM twin system.\\n\\n**First** , we will design and implement a scalable LLM & RAG inference\\npipeline based on microservices, separating the ML and business logic into two\\nlayers.\\n\\n**Secondly** , we will use Comet ML to integrate a prompt monitoring service\\nto capture all input prompts and LLM answers for further debugging and\\nanalysis.\\n\\n**Ultimately** , we will deploy the inference pipeline to Qwak and make the\\nLLM twin service available worldwide.\\n\\n#### **‚Üí Context from previous lessons. What you must know.**\\n\\nThis lesson is part of a more extensive series in which we learn to build an\\nend-to-end LLM system using LLMOps best practices.\\n\\n_If you haven‚Äôt read the whole series, for this one to make sense, you have to\\nknow that we have a:_\\n\\n  * Qdrant vector DB populated with digital data (posts, articles, and code snippets)\\n\\n  * vector DB retrieval module to do advanced RAG\\n\\n  * fine-tuned open-source LLM available in a model registry from Comet ML\\n\\n>  _‚Üí In this lesson, we will focus on gluing everything together into a\\n> scalable inference pipeline and deploying it to the cloud._\\n\\n* * *\\n\\n### **Table of Contents**\\n\\n  1. The architecture of the inference pipeline\\n\\n  2. The training vs. the inference pipeline\\n\\n  3. The RAG business module\\n\\n  4. The LLM microservice\\n\\n  5. Prompt monitoring\\n\\n  6. Deploying and running the inference pipeline\\n\\n  7. Conclusion\\n\\n* * *\\n\\n## 1\\\\. The architecture of the inference pipeline\\n\\nOur inference pipeline contains the following core elements:\\n\\n  * a fine-tuned LLM\\n\\n  * a RAG module\\n\\n  * a monitoring service\\n\\nLet‚Äôs see how to hook these into a scalable and modular system.\\n\\n### **The interface of the inference pipeline**\\n\\nAs we follow the feature/training/inference (FTI) pipeline architecture, the\\ncommunication between the 3 core components is clear.\\n\\nOur LLM inference pipeline needs 2 things:\\n\\n  * a fine-tuned LLM: pulled from the model registry\\n\\n  * features for RAG: pulled from a vector DB (which we modeled as a logical feature store)\\n\\nThis perfectly aligns with the FTI architecture.\\n\\n> _‚Üí If you are unfamiliar with the FTI pipeline architecture, we recommend\\n> you reviewLesson 1‚Äôs section on the 3-pipeline architecture._\\n\\n### **Monolithic vs. microservice inference pipelines**\\n\\nUsually, the inference steps can be split into 2 big layers:\\n\\n  * t**he LLM service:** where the actual inference is being done\\n\\n  * **the business service:** domain-specific logic\\n\\nWe can design our inference pipeline in 2 ways.\\n\\n#### **Option 1: Monolithic LLM & business service**\\n\\nIn a monolithic scenario, we implement everything into a single service.\\n\\n_Pros:_\\n\\n  * easy to implement\\n\\n  * easy to maintain\\n\\n _Cons:_\\n\\n  * harder to scale horizontally based on the specific requirements of each component\\n\\n  * harder to split the work between multiple teams\\n\\n  * not being able to use different tech stacks for the two services\\n\\nMonolithic vs. microservice inference pipelines\\n\\n#### **Option 2: Different LLM & business microservices**\\n\\nThe LLM and business services are implemented as two different components that\\ncommunicate with each other through the network, using protocols such as REST\\nor gRPC.\\n\\n_Pros:_\\n\\n  * each component can scale horizontally individually\\n\\n  * each component can use the best tech stack at hand\\n\\n _Cons:_\\n\\n  * harder to deploy\\n\\n  * harder to maintain\\n\\nLet‚Äôs focus on the ‚Äúeach component can scale individually‚Äù part, as this is\\nthe most significant benefit of the pattern. Usually, LLM and business\\nservices require different types of computing. For example, an LLM service\\ndepends heavily on GPUs, while the business layer can do the job only with a\\nCPU.\\n\\n### **Microservice architecture of the LLM twin inference pipeline**\\n\\nLet‚Äôs understand how we applied the microservice pattern to our concrete LLM\\ntwin inference pipeline.\\n\\nAs explained in the sections above, we have the following components:\\n\\n  1. A business microservice\\n\\n  2. An LLM microservice\\n\\n  3. A prompt monitoring microservice\\n\\n**The business microservice** is implemented as a Python module that:\\n\\n  * contains the advanced RAG logic, which calls the vector DB and GPT-4 API for advanced RAG operations;\\n\\n  * calls the LLM microservice through a REST API using the prompt computed utilizing the user‚Äôs query and retrieved context\\n\\n  * sends the prompt and the answer generated by the LLM to the prompt monitoring microservice.\\n\\nAs you can see, the business microservice is light. It glues all the domain\\nsteps together and delegates the computation to other services.\\n\\nThe end goal of the business layer is to act as an interface for the end\\nclient. In our case, as we will ship the business layer as a Python module,\\nthe client will be a Streamlit application.\\n\\nHowever, you can quickly wrap the Python module with FastAPI and expose it as\\na REST API to make it accessible from the cloud.\\n\\nMicroservice architecture of the LLM twin inference pipeline\\n\\n**The LLM microservice** is deployed on Qwak. This component is wholly niched\\non hosting and calling the LLM. It runs on powerful GPU-enabled machines.\\n\\nHow does the LLM microservice work?\\n\\n  * It loads the fine-tuned LLM twin model from Comet‚Äôs model registry [2].\\n\\n  * It exposes a REST API that takes in prompts and outputs the generated answer.\\n\\n  * When the REST API endpoint is called, it tokenizes the prompt, passes it to the LLM, decodes the generated tokens to a string and returns the answer.\\n\\nThat‚Äôs it!\\n\\n**The prompt monitoring microservice** is based on Comet ML‚Äôs LLM dashboard.\\nHere, we log all the prompts and generated answers into a centralized\\ndashboard that allows us to evaluate, debug, and analyze the accuracy of the\\nLLM.\\n\\n## **2\\\\. The training vs. the inference pipeline**\\n\\nAlong with the obvious reason that the training pipeline takes care of\\ntraining while the inference pipeline takes care of inference (Duh!), there\\nare some critical differences you have to understand.\\n\\n### **The input of the pipeline & How the data is accessed**\\n\\nDo you remember our logical feature store based on the Qdrant vector DB and\\nComet ML artifacts? If not, consider checking out Lesson 6 for a refresher.\\n\\nThe core idea is that **during training** , the data is accessed from an\\noffline data storage in batch mode, optimized for throughput and data lineage.\\n\\nOur LLM twin architecture uses Comet ML artifacts to access, version, and\\ntrack all our data.\\n\\nThe data is accessed in batches and fed to the training loop.\\n\\n**During inference** , you need an online database optimized for low latency.\\nAs we directly query the Qdrant vector DB for RAG, that fits like a glove.\\n\\nDuring inference, you don‚Äôt care about data versioning and lineage. You just\\nwant to access your features quickly for a good user experience.\\n\\nThe data comes directly from the user and is sent to the inference logic.\\n\\nThe training vs. the inference pipeline\\n\\n### **The output of the pipeline**\\n\\nThe **training pipeline‚Äôs** final output is the trained weights stored in\\nComet‚Äôs model registry.\\n\\nThe **inference pipeline‚Äôs** final output is the predictions served directly\\nto the user.\\n\\n### **The infrastructure**\\n\\nThe training pipeline requires more powerful machines with as many GPUs as\\npossible.\\n\\n_Why?_ During training, you batch your data and have to hold in memory all the\\ngradients required for the optimization steps. Because of the optimization\\nalgorithm, the training is more compute-hungry than the inference.\\n\\nThus, more computing and VRAM result in bigger batches, which means less\\ntraining time and more experiments.\\n\\nIf you run a batch pipeline, you will still pass batches to the model but\\ndon‚Äôt perform any optimization steps.\\n\\nIf you run a real-time pipeline, as we do in the LLM twin architecture, you\\npass a single sample to the model or do some dynamic batching to optimize your\\ninference step.\\n\\n### **Are there any overlaps?**\\n\\nYes! This is where the training-serving skew comes in.\\n\\nTo avoid the training-serving skew, you must carefully apply the same\\npreprocessing and postprocessing steps during training and inference.\\n\\n## **3\\\\. The RAG business module**\\n\\nWe will define the RAG business module under the _LLMTwin_ class. The LLM twin\\nlogic is directly correlated with our business logic.\\n\\nWe don‚Äôt have to introduce the word ‚Äúbusiness‚Äù in the naming convention of the\\nclasses.\\n\\nLet‚Äôs dig into the _generate()_ method of the _LLMTwin_ class, where we:\\n\\n  * call the RAG module;\\n\\n  * create the prompt using the prompt template, query and context;\\n\\n  * call the LLM microservice;\\n\\n  * log the prompt, prompt template, and answer to Comet ML‚Äôs prompt monitoring service.\\n\\nInference pipeline business module: generate() method ‚Üí GitHub ‚Üê\\n\\nLet‚Äôs look at how our LLM microservice is implemented using Qwak.\\n\\n## **4\\\\. The LLM microservice**\\n\\nAs the LLM microservice is deployed on Qwak, we must first inherit from the\\n_QwakModel_ class and implement some specific functions.\\n\\n  * _initialize_model()_ : where we load the fine-tuned model from the model registry at serving time\\n\\n  *  _schema():_ where we define the input and output schema\\n\\n  *  _predict()_ : where we implement the actual inference logic\\n\\n**Note:** The _build()_ function contains all the training logic, such as\\nloading the dataset, training the LLM, and pushing it to a Comet experiment.\\nTo see the full implementation, consider checking out Lesson 7, where we\\ndetailed the training pipeline.\\n\\nLLM microservice ‚Üí GitHub ‚Üê\\n\\nLet‚Äôs zoom into the implementation and the life cycle of the Qwak model.\\n\\nThe _schema()_ method is used to define how the input and output of the\\n_predict()_ method look like. This will automatically validate the structure\\nand type of the _predict()_ method. For example, the LLM microservice will\\nthrow an error if the variable instruction is a JSON instead of a string.\\n\\nThe other Qwak-specific methods are called in the following order:\\n\\n  1. ___init__()_ ‚Üí when deploying the model\\n\\n  2.  _initialize_model()_ ‚Üí when deploying the model\\n\\n  3.  _predict()_ ‚Üí on every request to the LLM microservice\\n\\n**> >>** Note that these methods are called only during serving time (and not\\nduring training).\\n\\nQwak exposes your model as a RESTful API, where the _predict()_ method is\\ncalled on each request.\\n\\nInside the prediction method, we perform the following steps:\\n\\n  * map the input text to token IDs using the LLM-specific tokenizer\\n\\n  * move the token IDs to the provided device (GPU or CPU)\\n\\n  * pass the token IDs to the LLM and generate the answer\\n\\n  * extract only the generated tokens from the _generated_ids_ variable by slicing it using the shape of the _input_ids_\\n\\n  * decode the _generated_ids_ back to text\\n\\n  * return the generated text\\n\\nThe final step is to look at Comet‚Äôs prompt monitoring service. ‚Üì\\n\\n## **5\\\\. Prompt monitoring**\\n\\nComet makes prompt monitoring straightforward. There is just one API call\\nwhere you connect to your project and workspace and send the following to a\\nsingle function:\\n\\n  * the prompt and LLM output\\n\\n  * the prompt template and variables that created the final output\\n\\n  * your custom metadata specific to your use case ‚Äî here, you add information about the model, prompt token count, token generation costs, latency, etc.\\n\\n    \\n    \\n    class PromptMonitoringManager:\\n        @classmethod\\n        def log(\\n            cls, prompt: str, output: str,\\n            prompt_template: str | None = None,\\n            prompt_template_variables: dict | None = None,\\n            metadata: dict | None = None,\\n        ) -> None:\\n            metadata = {\\n                \"model\": settings.MODEL_TYPE,\\n                **metadata,\\n            } or {\"model\": settings.MODEL_TYPE}\\n    \\n            comet_llm.log_prompt(\\n                workspace=settings.COMET_WORKSPACE,\\n                project=f\"{settings.COMET_PROJECT}-monitoring\",\\n                api_key=settings.COMET_API_KEY,\\n                prompt=prompt, prompt_template=prompt_template,\\n                prompt_template_variables=prompt_template_variables,\\n                output=output, metadata=metadata,\\n            )\\n\\nThis is how Comet ML‚Äôs prompt monitoring dashboard looks. Here, you can scroll\\nthrough all the prompts that were ever sent to the LLM. ‚Üì\\n\\nYou can click on any prompt and see everything we logged programmatically\\nusing the _PromptMonitoringManager_ class.\\n\\nScreenshot from Comet ML‚Äôs dashboard\\n\\nBesides what we logged, adding various tags and the inference duration can be\\nvaluable.\\n\\n## **6\\\\. Deploying and running the inference pipeline**\\n\\nWe can deploy the LLM microservice using the following Qwak command:\\n\\n    \\n    \\n    qwak models deploy realtime \\\\\\n    --model-id \"llm_twin\" \\\\\\n    --instance \"gpu.a10.2xl\" \\\\ \\n    --timeout 50000 \\\\ \\n    --replicas 2 \\\\\\n    --server-workers 2\\n\\nWe deployed two replicas of the LLM twin. Each replica has access to a machine\\nwith x1 A10 GPU. Also, each replica has two workers running on it.\\n\\nüîó More on Qwak instance types ‚Üê\\n\\nTwo replicas and two workers result in 4 microservices that run in parallel\\nand can serve our users.\\n\\nYou can scale the deployment to more replicas if you need to serve more\\nclients. Qwak provides autoscaling mechanisms triggered by listening to the\\nconsumption of GPU, CPU or RAM.\\n\\nTo conclude, you build the Qwak model once, and based on it, you can make\\nmultiple deployments with various strategies.\\n\\n* * *\\n\\n## **Conclusion**\\n\\n _Congratulations! You are close to the end of the LLM twin series._\\n\\nIn **Lesson 9** of the LLM twin course, you learned to **build** a scalable\\ninference pipeline for serving LLMs and RAG systems.\\n\\n**First** , you learned how to architect an inference pipeline by\\nunderstanding the difference between monolithic and microservice\\narchitectures. We also highlighted the difference in designing the training\\nand inference pipelines.\\n\\n**Secondly** , we walked you through implementing the RAG business module and\\nLLM twin microservice. Also, we showed you how to log all the prompts,\\nanswers, and metadata for Comet‚Äôs prompt monitoring service.\\n\\n**Ultimately** , we showed you how to deploy and run the LLM twin inference\\npipeline on the Qwak AI platform.\\n\\nIn **Lesson 10** , we will show you how to evaluate the whole system by\\nbuilding an advanced RAG evaluation pipeline that analyzes the accuracy of the\\nLLMs ‚Äô answers relative to the query and context.\\n\\nSee you there! ü§ó\\n\\n>  _üîó**Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è_\\n\\n* * *\\n\\n### Next Steps\\n\\n#### Step 1\\n\\nThis is just the **short version** of **Lesson 9** on **architecting scalable\\nand cost-effective LLM & RAG inference pipelines.**\\n\\n‚Üí For‚Ä¶\\n\\n  * The full implementation.\\n\\n  * Full deep dive into the code.\\n\\n  * More on the RAG, LLM and monitoring services.\\n\\n**Check out** the **full version** of **Lesson 9** on our **Medium\\npublication**. It‚Äôs still FREE:\\n\\nLesson 9 on Medium\\n\\n#### Step 2\\n\\n‚Üí **Consider checking out theLLM Twin GitHub repository and try it yourself\\nü´µ**\\n\\n _Nothing compares with getting your hands dirty and doing it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/architect-scalable-and-cost-effective?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('95d64d1d-83f2-47e9-8eda-9a687b98e6eb'), content={'Title': '7 tips to reduce your VRAM when training LLMs ', 'Subtitle': '3 techniques you must know to evaluate your LLMs. Introduction to deploying private LLMs with AWS SageMaker.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### 7 tips to reduce your VRAM when training LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# 7 tips to reduce your VRAM when training LLMs\\n\\n### 3 techniques you must know to evaluate your LLMs. Introduction to\\ndeploying private LLMs with AWS SageMaker.\\n\\nPaul Iusztin\\n\\nMay 18, 2024\\n\\n4\\n\\nShare this post\\n\\n#### 7 tips to reduce your VRAM when training LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * 3 techniques you must know to evaluate your LLMs\\n\\n  * 7 tips you must know to reduce your VRAM consumption of your LLMs during training\\n\\n  * Introduction to deploying private LLMs with AWS SageMaker\\n\\n* * *\\n\\nOn the 3rd of May, I ùóµùóºùòÄùòÅùó≤ùó± a ùó≥ùóøùó≤ùó≤ ùòÄùó≤ùòÄùòÄùó∂ùóºùóª on Maven for ùüµùü∞ ùóΩùó≤ùóºùóΩùóπùó≤ on how to\\nùóîùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅ ùó¨ùóºùòÇùóø ùóüùóüùó† ùóßùòÑùó∂ùóª. If you missed it, here is ùóµùóºùòÑ you can ùóÆùó∞ùó∞ùó≤ùòÄùòÄ ùó∂ùòÅ for\\nùó≥ùóøùó≤ùó≤ ‚Üì  \\n  \\n.  \\n  \\nùòíùò¶ùò∫ ùòµùò¢ùò¨ùò¶ùò¢ùò∏ùò¢ùò∫ùò¥ ùò∏ùò¶ùò≥ùò¶:  \\n  \\n‚Üí Why I started building my LLM Twin  \\n  \\n‚Üí The 3 pipeline design / The FTI pipeline architecture  \\n  \\n‚Üí System design of the LLM Twin Architecture  \\n  \\n‚Üí Break down the RAG system of the LLM Twin Architecture  \\n  \\n‚Üí Live Demo  \\n  \\n.  \\n  \\nIf you want the recording, you can watch it for free here:\\nhttps://bit.ly/3PZGV0S  \\n  \\nùòàùò≠ùò¥ùò∞, ùò©ùò¶ùò≥ùò¶ ùò¢ùò≥ùò¶ ùò∞ùòµùò©ùò¶ùò≥ ùò∂ùò¥ùò¶ùòßùò∂ùò≠ ùò≠ùò™ùòØùò¨ùò¥:  \\n  \\n\\\\- ùò¥ùò≠ùò™ùò•ùò¶ùò¥: üîó https://lnkd.in/d_MdqGwS  \\n  \\n\\\\- ùòìùòìùòî ùòõùò∏ùò™ùòØ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùòéùò™ùòµùòèùò∂ùò£: üîó https://lnkd.in/dzat6PB6  \\n  \\n\\\\- ùòìùòìùòî ùòõùò∏ùò™ùòØ ùòçùòôùòåùòå ùò≠ùò¶ùò¥ùò¥ùò∞ùòØùò¥: üîó https://lnkd.in/dX__4mhX\\n\\n* * *\\n\\n### 3 techniques you must know to evaluate your LLMs\\n\\nHere are 3 techniques you must know to evaluate your LLMs quickly.  \\n  \\nManually testing the output of your LLMs is a tedious and painful process ‚Üí\\nyou need to automate it.  \\n  \\nIn generative AI, most of the time, you cannot leverage standard metrics.  \\n  \\nThus, the real question is, how do you evaluate the outputs of an LLM?  \\n  \\n#ùü≠. ùó¶ùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùóÆùóªùòÄùòÑùó≤ùóøùòÄ - ùòÜùóºùòÇ ùó∏ùóªùóºùòÑ ùó≤ùòÖùóÆùó∞ùòÅùóπùòÜ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùòÑùóÆùóªùòÅ ùòÅùóº ùó¥ùó≤ùòÅ  \\n  \\nEven if you use an LLM to generate text, you can ask it to generate a response\\nin a structured format (e.g., JSON) that can be parsed.  \\n  \\nYou know exactly what you want (e.g., a list of products extracted from the\\nuser\\'s question).  \\n  \\nThus, you can easily compare the generated and ideal answers using classic\\napproaches.  \\n  \\nFor example, when extracting the list of products from the user\\'s input, you\\ncan do the following:  \\n\\\\- check if the LLM outputs a valid JSON structure  \\n\\\\- use a classic method to compare the generated and real answers  \\n  \\n#ùüÆ. ùó°ùóº \"ùóøùó∂ùó¥ùóµùòÅ\" ùóÆùóªùòÄùòÑùó≤ùóø (ùó≤.ùó¥., ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ ùó±ùó≤ùòÄùó∞ùóøùó∂ùóΩùòÅùó∂ùóºùóªùòÄ, ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùó≤ùòÄ, ùó≤ùòÅùó∞.)  \\n  \\nWhen generating sentences, the LLM can use different styles, words, etc. Thus,\\ntraditional metrics (e.g., BLUE score) are too rigid to be useful.  \\n  \\nYou can leverage another LLM to test the output of our initial LLM. The trick\\nis in what questions to ask.  \\n  \\nHere, we have another 2 sub scenarios:  \\n  \\n‚Ü≥ ùüÆ.ùü≠ ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº (ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ\\nùóµùóÆùòÉùó≤ ùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nYou don\\'t have access to an expert to write an ideal answer for a given\\nquestion to compare it to.  \\n  \\nBased on the initial prompt and generated answer, you can compile a set of\\nquestions and pass them to an LLM. Usually, these are Y/N questions that you\\ncan easily quantify and check the validity of the generated answer.  \\n  \\nThis is known as \"Rubric Evaluation\"  \\n  \\nFor example:  \\n\"\"\"  \\n\\\\- Is there any disagreement between the response and the context? (Y or N)  \\n\\\\- Count how many questions the user asked. (output a number)  \\n...  \\n\"\"\"  \\n  \\nThis strategy is intuitive, as you can ask the LLM any question you are\\ninterested in as long it can output a quantifiable answer (Y/N or a number).  \\n  \\n‚Ü≥ ùüÆ.ùüÆ. ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóº ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóøùó≤ùòÄùóΩùóºùóªùòÄùó≤ ùòÅùóº (ùòÜùóºùòÇ ùóµùóÆùòÉùó≤\\nùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nWhen you have access to an answer manually created by a group of experts,\\nthings are easier.  \\n  \\nYou will use an LLM to compare the generated and ideal answers based on\\nsemantics, not structure.  \\n  \\nFor example:  \\n\"\"\"  \\n(A) The submitted answer is a subset of the expert answer and entirely\\nconsistent.  \\n...  \\n(E) The answers differ, but these differences don\\'t matter.  \\n\"\"\"\\n\\n* * *\\n\\n### 7 tips you must know to reduce your VRAM consumption of your LLMs during\\ntraining\\n\\nHere are ùü≥ ùòÅùó∂ùóΩùòÄ you must know to ùóøùó≤ùó±ùòÇùó∞ùó≤ your ùó©ùó•ùóîùó† ùó∞ùóºùóªùòÄùòÇùó∫ùóΩùòÅùó∂ùóºùóª of your ùóüùóüùó†ùòÄ\\nduring ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ so you can ùó≥ùó∂ùòÅ it on ùòÖùü≠ ùóöùó£ùó®.  \\n  \\nùü≠\\\\. ùó†ùó∂ùòÖùó≤ùó±-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: During training you use both FP32 and FP16 in the\\nfollowing way: \"FP32 weights\" -> \"FP16 weights\" -> \"FP16 gradients\" -> \"FP32\\ngradients\" -> \"Update weights\" -> \"FP32 weights\" (and repeat). As you can see,\\nthe forward & backward passes are done in FP16, and only the optimization step\\nis done in FP32, which reduces both the VRAM and runtime.  \\n  \\nùüÆ\\\\. ùóüùóºùòÑùó≤ùóø-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: All your computations are done in FP16 instead of FP32.\\nBut the key is using bfloat16 (\"Brain Floating Point\"), a numerical\\nrepresentation Google developed for deep learning. It allows you to represent\\nvery large and small numbers, avoiding overflowing or underflowing scenarios.  \\n  \\nùüØ\\\\. ùó•ùó≤ùó±ùòÇùó∞ùó∂ùóªùó¥ ùòÅùóµùó≤ ùóØùóÆùòÅùó∞ùóµ ùòÄùó∂ùòáùó≤: This one is straightforward. Fewer samples per\\ntraining iteration result in smaller VRAM requirements. The downside of this\\nmethod is that you can\\'t go too low with your batch size without impacting\\nyour model\\'s performance.  \\n  \\nùü∞\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ ùóÆùó∞ùó∞ùòÇùó∫ùòÇùóπùóÆùòÅùó∂ùóºùóª: It is a simple & powerful trick to increase your\\nbatch size virtually. You compute the gradients for \"micro\" batches (forward +\\nbackward passes). Once the accumulated gradients reach the given \"virtual\"\\ntarget, the model weights are updated with the accumulated gradients. For\\nexample, you have a batch size of 4 and a micro-batch size of 1. Then, the\\nforward & backward passes will be done using only x1 sample, and the\\noptimization step will be done using the aggregated gradient of the 4 samples.  \\n  \\nùü±\\\\. ùó®ùòÄùó≤ ùóÆ ùòÄùòÅùóÆùòÅùó≤ùóπùó≤ùòÄùòÄ ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùó≤ùóø: Adam is the most popular optimizer. It is one\\nof the most stable optimizers, but the downside is that it has 2 additional\\nparameters (a mean & variance) for every model parameter. If you use a\\nstateless optimizer, such as SGD, you can reduce the number of parameters by\\n2/3, which is significant for LLMs.  \\n  \\nùü≤\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ (ùóºùóø ùóÆùó∞ùòÅùó∂ùòÉùóÆùòÅùó∂ùóºùóª) ùó∞ùóµùó≤ùó∞ùó∏ùóΩùóºùó∂ùóªùòÅùó∂ùóªùó¥: It drops specific activations\\nduring the forward pass and recomputes them during the backward pass. Thus, it\\neliminates the need to hold all activations simultaneously in VRAM. This\\ntechnique reduces VRAM consumption but makes the training slower.  \\n  \\nùü≥\\\\. ùóñùó£ùó® ùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùóºùó≥ùó≥ùóπùóºùóÆùó±ùó∂ùóªùó¥: The parameters that do not fit on your GPU\\'s\\nVRAM are loaded on the CPU. Intuitively, you can see it as a model parallelism\\nbetween your GPU & CPU.\\n\\nImage by DALL-E\\n\\nMost of these methods are orthogonal, so you can combine them and drastically\\nreduce your VRAM requirements during training.\\n\\n* * *\\n\\n### Introduction to deploying private LLMs with AWS SageMaker\\n\\nEver wondered ùóµùóºùòÑ to ùó±ùó≤ùóΩùóπùóºùòÜ in <ùüØùü¨ ùó∫ùó∂ùóªùòÇùòÅùó≤ùòÄ ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†ùòÄ, such as ùóüùóπùóÆùó∫ùóÆùüÆ,\\non ùóîùó™ùó¶ ùó¶ùóÆùó¥ùó≤ùó†ùóÆùó∏ùó≤ùóø? Then wonder no more ‚Üì\\n\\n#### Step 1: Deploy the LLM to AWS SageMaker\\n\\nThe sweet thing about SageMaker is that it accelerates the development\\nprocess, enabling a more efficient and rapid transition to the production\\nstage.  \\n  \\n\\nVesa Alexandru\\n\\nsmashed with his first article on DML about showing step-by-step how to deploy\\nan LLM from HuggingFace to AWS SageMaker using good practices, such as:  \\n  \\n\\\\- designing a config class for the deployment of the LLM  \\n\\\\- set up AWS and deploy the LLM to SageMaker  \\n\\\\- implement an inference class to call the deployed LLM in real-time through\\na web endpoint  \\n\\\\- define a prompt template function to ensure reproducibility & consistency  \\n  \\n...and, ultimately, how to play yourself with your freshly deployed LLM.\\n\\n_Here is the full article explaining how to deploy the LLM to AWS SageMaker_ ‚Üì\\n\\n#### DML: Introduction to Deploying Private LLMs with AWS SageMaker: Focus on\\nLlama2-7b-chat\\n\\nVesa Alexandru\\n\\n¬∑\\n\\nJan 18\\n\\nRead full story\\n\\n#### Step 2: Call the SageMaker inference endpoint\\n\\nYou\\'ve just deployed your Mistral LLM to SageMaker.  \\n  \\nùòïùò∞ùò∏ ùò∏ùò©ùò¢ùòµ?  \\n  \\nUnfortunately, you are not done.  \\n  \\nThat was just the beginning of the journey.  \\n  \\n‚Üí Now, you have to write a Python client that calls the LLM.  \\n  \\nùóüùó≤ùòÅ\\'ùòÄ ùòÇùòÄùó≤ ùóÆ ùó±ùóºùó∞ùòÇùó∫ùó≤ùóªùòÅ ùòÄùòÇùó∫ùó∫ùóÆùóøùòÜ ùòÅùóÆùòÄùó∏ ùóÆùòÄ ùóÆùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤.  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: Define a Settings object using ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: Create an inference interface that inherits from ùòàùòâùòä  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: Implement an ùòàùòûùòö ùòöùò¢ùò®ùò¶ùòîùò¢ùò¨ùò¶ùò≥ version of the inference interface by\\nspecifying how to construct the HTTP payload and call the SageMaker endpoint.\\nWe want to keep this class independent from the summarization prompt!  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü∞: Create the summarization prompt.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü±: Encapsulate the summarization prompt and Python SageMaker client into\\na ùòöùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶ùòöùò©ùò∞ùò≥ùòµùòãùò∞ùò§ùò∂ùòÆùò¶ùòØùòµ task.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≤: Wrap the ùòöùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶ùòöùò©ùò∞ùò≥ùòµùòãùò∞ùò§ùò∂ùòÆùò¶ùòØùòµ task with a FastAPI endpoint.  \\n  \\n...and bam!  \\n  \\nYou have an LLM for summarizing any document.  \\n  \\n.  \\n  \\nùóõùó≤ùóøùó≤ ùóÆùóøùó≤ ùòÄùóºùó∫ùó≤ ùóÆùó±ùòÉùóÆùóªùòÅùóÆùó¥ùó≤ùòÄ ùóºùó≥ ùòÅùóµùó≤ ùó±ùó≤ùòÄùó∂ùó¥ùóª ùó±ùó≤ùòÄùó∞ùóøùó∂ùóØùó≤ùó± ùóÆùóØùóºùòÉùó≤:  \\n  \\n\\\\- by using an inference interface, you can quickly swap the LLM\\nimplementation  \\n  \\n\\\\- by decoupling the prompt construction logic from the inference class, you\\ncan reuse the inference client with any prompt  \\n  \\n\\\\- by wrapping everything with a ùòöùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶ùòöùò©ùò∞ùò≥ùòµùòãùò∞ùò§ùò∂ùòÆùò¶ùòØùòµ task you can quickly\\ndefine & configure multiple types of tasks and leverage polymorphism to run\\nthem  \\n  \\n_Here is the full article explaining how to design the inference module_ ‚Üì\\n\\n#### Steal my code to solve real-world problems\\n\\nVesa Alexandru\\n\\n¬∑\\n\\nFeb 29\\n\\nRead full story\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n4\\n\\nShare this post\\n\\n#### 7 tips to reduce your VRAM when training LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/7-tips-to-reduce-your-vram-when-training?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('d0c592eb-82bc-46c4-9632-388f9dd144ce'), content={'Title': 'Using this Python package, you can x10 your text preprocessing pipelines', 'Subtitle': 'End-to-end framework for production-ready LLMs. Top 6 ML platform features you must know and use in your ML system.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Using this Python package, you can x10 your text preprocessing pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Using this Python package, you can x10 your text preprocessing pipelines\\n\\n### End-to-end framework for production-ready LLMs. Top 6 ML platform features\\nyou must know and use in your ML system.\\n\\nPaul Iusztin\\n\\nMay 11, 2024\\n\\n9\\n\\nShare this post\\n\\n#### Using this Python package, you can x10 your text preprocessing pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Top 6 ML platform features you must know and use in your ML system.\\n\\n  * Using this Python package, you can x10 your text preprocessing pipelines\\n\\n  * End-to-end framework for production-ready LLMs\\n\\n* * *\\n\\n### Top 6 ML platform features you must know and use in your ML system\\n\\nHere they are ‚Üì  \\n  \\n#ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥  \\n  \\nIn your ML development phase, you generate lots of experiments.  \\n  \\nTracking and comparing the metrics between them is crucial in finding the\\noptimal model.  \\n  \\n#ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤  \\n  \\nIts primary purpose is reproducibility.  \\n  \\nTo know how a model was generated, you need to know:  \\n\\\\- the version of the code  \\n\\\\- the version of the packages  \\n\\\\- hyperparameters/config  \\n\\\\- total compute  \\n\\\\- version of the dataset  \\n... and more  \\n  \\n#ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nMost of the time, along with the metrics, you must log a set of visualizations\\nfor your experiment.  \\n  \\nSuch as:  \\n\\\\- images  \\n\\\\- videos  \\n\\\\- prompts  \\n\\\\- t-SNE graphs  \\n\\\\- 3D point clouds  \\n... and more  \\n  \\n#ùü∞. ùó•ùó≤ùóΩùóºùóøùòÅùòÄ  \\n  \\nYou don\\'t work in a vacuum.  \\n  \\nYou have to present your work to other colleges or clients.  \\n  \\nA report lets you take the metadata and visualizations from your experiment...  \\n  \\n...and create, deliver and share a targeted presentation for your clients or\\npeers.  \\n  \\n#ùü±. ùóîùóøùòÅùó∂ùó≥ùóÆùó∞ùòÅùòÄ  \\n  \\nThe most powerful feature out of them all.  \\n  \\nAn artifact is a versioned object that is an input or output for your task.  \\n  \\nEverything can be an artifact, but the most common cases are:  \\n\\\\- data  \\n\\\\- model  \\n\\\\- code  \\n  \\nWrapping your assets around an artifact ensures reproducibility.  \\n  \\nFor example, you wrap your features into an artifact (e.g., features:3.1.2),\\nwhich you can consume into your ML development step.  \\n  \\nThe ML development step will generate config (e.g., config:1.2.4) and code\\n(e.g., code:1.0.2) artifacts used in the continuous training pipeline.  \\n  \\nDoing so lets you quickly respond to questions such as \"What I used to\\ngenerate the model?\" and \"What Version?\"  \\n  \\n#ùü≤. ùó†ùóºùó±ùó≤ùóπ ùó•ùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ  \\n  \\nThe model registry is the ultimate way to make your model accessible to your\\nproduction ecosystem.  \\n  \\nFor example, in your continuous training pipeline, after the model is trained,\\nyou load the weights as an artifact into the model registry (e.g.,\\nmodel:1.2.4).  \\n  \\nYou label this model as \"staging\" under a new version and prepare it for\\ntesting. If the tests pass, mark it as \"production\" under a new version and\\nprepare it for deployment (e.g., model:2.1.5).\\n\\nAll of these features are used in a mature ML system. What is your favorite\\none?\\n\\n* * *\\n\\n### Using this Python package, you can x10 your text preprocessing pipelines\\n\\nAny text preprocessing pipeline has to clean, partition, extract, or chunk\\ntext data to feed it into your LLMs.  \\n  \\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± offers a ùóøùó∂ùó∞ùóµ and ùó∞ùóπùó≤ùóÆùóª ùóîùó£ùóú that allows you to quickly:  \\n  \\n\\\\- ùò±ùò¢ùò≥ùòµùò™ùòµùò™ùò∞ùòØ your data into smaller segments from various data sources (e.g.,\\nHTML, CSV, PDFs, even images, etc.)  \\n\\\\- ùò§ùò≠ùò¶ùò¢ùòØùò™ùòØùò® the text of anomalies (e.g., wrong ASCII characters), any\\nirrelevant information (e.g., white spaces, bullets, etc.), and filling\\nmissing values  \\n\\\\- ùò¶ùòπùòµùò≥ùò¢ùò§ùòµùò™ùòØùò® information from pieces of text (e.g., datetimes, addresses, IP\\naddresses, etc.)  \\n\\\\- ùò§ùò©ùò∂ùòØùò¨ùò™ùòØùò® your text segments into pieces of text that can be inserted into\\nyour embedding model  \\n\\\\- ùò¶ùòÆùò£ùò¶ùò•ùò•ùò™ùòØùò® data (e.g., wrapper over OpenAIEmbeddingEncoder,\\nHuggingFaceEmbeddingEncoders, etc.)  \\n\\\\- ùò¥ùòµùò¢ùò®ùò¶ your data to be fed into various tools (e.g., Label Studio, Label\\nBox, etc.)  \\n  \\nùóîùóπùóπ ùòÅùóµùó≤ùòÄùó≤ ùòÄùòÅùó≤ùóΩùòÄ ùóÆùóøùó≤ ùó≤ùòÄùòÄùó≤ùóªùòÅùó∂ùóÆùóπ ùó≥ùóºùóø:  \\n  \\n\\\\- feeding your data into your LLMs  \\n\\\\- embedding the data and ingesting it into a vector DB  \\n\\\\- doing RAG  \\n\\\\- labeling  \\n\\\\- recommender systems  \\n  \\n... basically for any LLM or multimodal applications  \\n  \\n.  \\n  \\nImplementing all these steps from scratch will take a lot of time.  \\n  \\nI know some Python packages already do this, but the functionality is\\nscattered across multiple packages.\\n\\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± packages everything together under a nice, clean API.\\n\\n* * *\\n\\n### End-to-end framework for production-ready LLMs\\n\\nWant to ùóπùó≤ùóÆùóøùóª to ùóØùòÇùó∂ùóπùó± ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª ùóüùóüùó†ùòÄ in a ùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùòÑùóÆùòÜ? For ùóôùó•ùóòùóò? Then ùòÜùóºùòÇ\\nùòÄùóµùóºùòÇùóπùó± ùòÅùóÆùó∏ùó≤ our ùó°ùóòùó™ ùó∞ùóºùòÇùóøùòÄùó≤ on how to ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ for\\nùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ ‚Üì  \\n  \\nüß† Decoding ML and I are ùòÄùòÅùóÆùóøùòÅùó∂ùóªùó¥ a ùóªùó≤ùòÑ ùóôùó•ùóòùóò ùó∞ùóºùòÇùóøùòÄùó≤ on ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ how to\\nùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅ and ùóØùòÇùó∂ùóπùó± a ùóøùó≤ùóÆùóπ-ùòÑùóºùóøùóπùó± ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ by ùóØùòÇùó∂ùóπùó±ùó∂ùóªùó¥ an ùóüùóüùó† ùóßùòÑùó∂ùóª:  \\n  \\n‚Üí from start to finish - from  \\n‚Üí from data collection to deployment  \\n‚Üí production-ready  \\n‚Üí from NO MLOps to experiment trackers, model registries, prompt monitoring,\\nand versioning\\n\\nThe course is called: ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ  \\n  \\n...and here is what you will learn to build  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüêç 4 ùòóùò∫ùòµùò©ùò∞ùòØ ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥:  \\n  \\n‚Üí ùóßùóµùó≤ ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Crawl your digital data from various social media platforms.  \\n\\\\- Clean, normalize and load the data to a NoSQL DB through a series of ETL\\npipelines.  \\n\\\\- Send database changes to a queue using the CDC pattern.  \\n  \\n‚òÅ Deployed on AWS.\\n\\n  \\n  \\n‚Üí ùóßùóµùó≤ ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Consume messages from a queue through a Bytewax streaming pipeline.  \\n\\\\- Every message will be cleaned, chunked, embedded and loaded into a Qdrant\\nvector DB in real-time.  \\n  \\n‚òÅ Deployed on AWS.  \\n  \\n  \\n‚Üí ùóßùóµùó≤ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Create a custom dataset based on your digital data.  \\n\\\\- Fine-tune an LLM using QLoRA.  \\n\\\\- Use Comet ML\\'s experiment tracker to monitor the experiments.  \\n\\\\- Evaluate and save the best model to Comet\\'s model registry.  \\n  \\n‚òÅ Deployed on Qwak.  \\n  \\n  \\n‚Üí ùóßùóµùó≤ ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Load and quantize the fine-tuned LLM from Comet\\'s model registry.  \\n\\\\- Deploy it as a REST API  \\n\\\\- Enhance the prompts using RAG  \\n\\\\- Generate content using your LLM twin  \\n\\\\- Monitor the LLM using Comet\\'s prompt monitoring dashboard  \\n  \\n‚òÅ Deployed on Qwak.  \\n  \\n.  \\n  \\nùòàùò≠ùò∞ùòØùò® ùòµùò©ùò¶ 4 ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥, ùò∫ùò∞ùò∂ ùò∏ùò™ùò≠ùò≠ ùò≠ùò¶ùò¢ùò≥ùòØ ùòµùò∞ ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò¶ 3 ùò¥ùò¶ùò≥ùò∑ùò¶ùò≥ùò≠ùò¶ùò¥ùò¥ ùòµùò∞ùò∞ùò≠ùò¥:  \\n  \\n\\\\- Comet as your ML Platform  \\n\\\\- Qdrant as your vector DB  \\n\\\\- Qwak as your ML infrastructure  \\n  \\n.  \\n  \\nTo stay updated on ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ\\ncourse...  \\n  \\nùòæùôùùôöùôòùô† ùôûùô© ùô§ùô™ùô© ùôÇùôûùô©ùôÉùô™ùôó ùôñùô£ùôô ùô®ùô™ùô•ùô•ùô§ùôßùô© ùô™ùô® ùô¨ùôûùô©ùôù ùôñ ‚≠êÔ∏è  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n9\\n\\nShare this post\\n\\n#### Using this Python package, you can x10 your text preprocessing pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/using-this-python-package-you-can?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('46f9a4cc-cf3b-43c6-9026-6c9cddf8674a'), content={'Title': '4 Advanced RAG Algorithms You Must Know - by Paul Iusztin', 'Subtitle': 'Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\n### Implement from scratch 4 advanced RAG methods to optimize your retrieval\\nand post-retrieval algorithm\\n\\nPaul Iusztin\\n\\nMay 09, 2024\\n\\n17\\n\\nShare this post\\n\\n#### The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n1\\n\\nShare\\n\\n _‚Üí the 5th out of 11 lessons of the LLM Twin free course_\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> More **details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà\\n\\n* * *\\n\\n### Latest Lessons of the LLM Twin Course\\n\\n**Lesson 2** : The importance of Data Pipeline in the era of Generative AI\\n\\n‚Üí Data crawling, ETL pipelines, ODM, NoSQL Database\\n\\n**Lesson 3:** CDC: Enabling Event-Driven Architectures\\n\\n‚Üí Change Data Capture (CDC), MongoDB Watcher, RabbitMQ queue\\n\\n**Lesson 4:** Python Streaming Pipelines for Fine-tuning LLMs and RAG - in\\nReal-Time!\\n\\n‚Üí Feature pipeline, Bytewax streaming engine, Pydantic models, The dispatcher\\nlayer\\n\\n* * *\\n\\n### Lesson 5: **The 4 Advanced RAG Algorithms You Must Know to Implement**\\n\\nIn **Lesson 5** , we will focus on building an advanced retrieval module used\\nfor RAG.\\n\\nWe will show you how to implement 4 **retrieval** and **post-retrieval\\nadvanced optimization techniques** to **improve** the **accuracy** of your\\n**RAG retrieval step**.\\n\\nIn this lesson, we will focus only on the retrieval part of the RAG system.\\n\\nIn **Lesson 4** , we showed you how to clean, chunk, embed, and load social\\nmedia data to a Qdrant vector DB (the ingestion part of RAG).\\n\\nIn future lessons, we will integrate this retrieval module into the inference\\npipeline for a full-fledged RAG system.\\n\\nRetrieval Python Module Architecture\\n\\n* * *\\n\\n### 1\\\\. Overview of advanced RAG optimization techniques\\n\\nA production RAG system is split into **3 main components** :\\n\\n  * **ingestion:** clean, chunk, embed, and load your data to a vector DB\\n\\n  * **retrieval:** query your vector DB for context\\n\\n  * **generation:** attach the retrieved context to your prompt and pass it to an LLM\\n\\nThe **ingestion component** sits in the _feature pipeline_ , while the\\n**retrieval** and **generation** **components** are implemented inside the\\n_inference pipeline_.\\n\\nYou can **also** **use** the **retrieval** and **generation** **components**\\nin your _training pipeline_ to fine-tune your LLM further on domain-specific\\nprompts.\\n\\nYou can apply advanced techniques to optimize your RAG system for ingestion,\\nretrieval and generation.\\n\\n_That being said, there are 3 main types of advanced RAG techniques:_\\n\\n  * **Pre-retrieval optimization**[ingestion]: tweak how you create the chunks\\n\\n  * **Retrieval optimization**[retrieval]:**** improve the queries to your vector DB\\n\\n  * **Post-retrieval optimization**[retrieval]**:** process the retrieved chunks to filter out the noise\\n\\n> The **generation step** can be **improved** through fine-tuning or prompt\\n> engineering, which will be explained in future lessons.\\n\\nThe **pre-retrieval optimization techniques** are explained in Lesson 4.\\n\\nIn this lesson, we will show you some **popular** **retrieval** and **post-\\nretrieval** **optimization techniques**.\\n\\n* * *\\n\\n### 2\\\\. Advanced RAG techniques applied to the LLM twin\\n\\n#### **Retrieval optimization**\\n\\n _We will combine 3 techniques:_\\n\\n  * Query Expansion\\n\\n  * Self Query\\n\\n  * Filtered vector search\\n\\n#### **Post-retrieval optimization**\\n\\nWe will **use** the **rerank** pattern **using** **GPT-4** and **prompt\\nengineering** instead of Cohere or an open-source re-ranker cross-encoder [4].\\n\\nI don‚Äôt want to spend too much time on the theoretical aspects. There are\\nplenty of articles on that.\\n\\n_So, we will**jump** straight to **implementing** and **integrating** these\\ntechniques in our LLM twin system._\\n\\nBut first, let‚Äôs clarify why we picked Qdrant as our vector DB ‚Üì\\n\\n#### 2.1. Why Qdrant?\\n\\nThere are many vector DBs out there, too many‚Ä¶\\n\\nBut since we discovered Qdrant, we loved it.\\n\\n**Why?**\\n\\n  * It is built in Rust.\\n\\n  * Apache-2.0 license ‚Äî open-source üî•\\n\\n  * It has a great and intuitive Python SDK.\\n\\n  * It has a freemium self-hosted version to build PoCs for free.\\n\\n  * It supports unlimited document sizes, and vector dims of up to 645536.\\n\\n  * It is production-ready. Companies such as Disney, Mozilla, and Microsoft already use it.\\n\\n  * It is one of the most popular vector DBs out there.\\n\\n_**To** **put that in perspective,**_ Pinecone, one of its biggest\\ncompetitors, supports only documents with up to 40k tokens and vectors with up\\nto 20k dimensions‚Ä¶. and a proprietary license.\\n\\nI could go on and on‚Ä¶\\n\\n‚Ä¶but if you are **curious to find out more** , _check out Qdrant _‚Üê\\n\\n* * *\\n\\n### 3\\\\. Retrieval optimization (1): Query expansion\\n\\nQuery expansion is quite intuitive.\\n\\nYou use an LLM to generate multiple queries based on your initial query.\\n\\nThese queries should contain multiple perspectives of the initial query.\\n\\nThus, when embedded, they hit different areas of your embedding space that are\\nstill relevant to our initial question.\\n\\nYou can do query expansion with a detailed zero-shot prompt.\\n\\nQuery expansion template ‚Üí GitHub Code ‚Üê\\n\\n### 4\\\\. Retrieval optimization (2): Self query\\n\\nWhat if you could extract the tags within the query and use them along the\\nembedded query?\\n\\nThat is what self-query is all about!\\n\\nYou use an LLM to extract various metadata fields that are critical for your\\nbusiness use case (e.g., tags, author ID, number of comments, likes, shares,\\netc.)\\n\\nIn our custom solution, we are extracting just the author ID. Thus, a zero-\\nshot prompt engineering technique will do the job.\\n\\n_Self-queries work hand-in-hand with vector filter searches, which we will\\nexplain in the next section._\\n\\nTo define the _**SelfQueryTemplate**_ , we have to:\\n\\n  * Subclass the base abstract class\\n\\n  * Define the self-query prompt\\n\\n  * Create the LangChain PromptTemplate wrapper\\n\\n    \\n    \\n    class **SelfQueryTemplate**(BasePromptTemplate):\\n        prompt: str = \"\"\"\\n        You are an AI language model assistant. \\n        Your task is to extract information from a user question.\\n        The required information that needs to be extracted is the user id. \\n        Your response should consists of only the extracted id (e.g. 1345256), nothing else.\\n        User question: {question}\\n        \"\"\"\\n    \\n        def create_template(self) -> PromptTemplate:\\n            return PromptTemplate(\\n                template=self.prompt, input_variables=[\"question\"], verbose=True\\n            )\\n\\n### 5\\\\. Retrieval optimization (3): Hybrid & filtered vector search\\n\\nCombine the vector search technique with one (or more) complementary search\\nstrategy, which works great for finding exact words.\\n\\nIt is not defined which algorithms are combined, but the most standard\\nstrategy for hybrid search is to combine the traditional keyword-based search\\nand modern vector search.\\n\\n_How are these combined?_\\n\\n_The**first method** is to merge the similarity scores of the 2 techniques as\\nfollows:_\\n\\n    \\n    \\n    hybrid_score = (1 - alpha) * sparse_score + alpha * dense_score\\n\\nWhere **alpha** takes a value between [0, 1], with:\\n\\n  * **alpha = 1** : Vector Search\\n\\n  * **alpha = 0** : Keyword search\\n\\nAlso, the similarity scores are defined as follows:\\n\\n  * **sparse_score:** is the result of the _keyword search_ that, behind the scenes, uses a BM25 algorithm [7] that sits on top of TF-IDF.\\n\\n  * **dense_score:** is the result of the _vector search_ that most commonly uses a similarity metric such as cosine distance\\n\\n _The**second method** uses the vector search technique as usual and applies a\\nfilter based on your keywords on top of the metadata of retrieved results._\\n\\n> ‚Üí This is also known as**filtered vector search**.\\n\\nIn this use case, the **similar score** is **not changed based** on the\\n**provided** **keywords**.\\n\\nIt is just a fancy word for a simple filter applied to the metadata of your\\nvectors.\\n\\nBut it is **essential** to **understand** the **difference** **between** the\\n**first** and **second** **methods** :\\n\\n  * the**first method** combines the similarity score between the keywords and vectors using the alpha parameter;\\n\\n  * the **second method** is a simple filter on top of your vector search.\\n\\n#### How does this fit into our architecture?\\n\\nRemember that during the self-query step, we extracted the **author_id** as an\\nexact field that we have to match.\\n\\nThus, we will search for the **author_id** using the keyword search algorithm\\nand attach it to the 5 queries generated by the query expansion step.\\n\\n_As we want the**most relevant chunks** from a **given author,** it makes the\\nmost sense to use a **filter** **using** the **author_id** as follows\\n(**filtered vector search**)_ ‚Üì\\n\\n    \\n    \\n    self._qdrant_client.search(\\n          collection_name=\"vector_posts\",\\n          query_filter=models.Filter(\\n              must=[\\n                  models.FieldCondition(\\n                      key=\"author_id\",\\n                      match=models.MatchValue(\\n                          value=metadata_filter_value,\\n                      ),\\n                  )\\n              ]\\n          ),\\n          query_vector=self._embedder.encode(generated_query).tolist(),\\n          limit=k,\\n\\nNote that we can easily extend this with multiple keywords (e.g., tags),\\nmaking the combination of self-query and hybrid search a powerful retrieval\\nduo.\\n\\nThe only **question** you have to **ask yourself** is whether we want to\\n**use** a simple **vector search filter** or the more complex **hybrid\\nsearch** strategy.\\n\\n### 6\\\\. Implement the advanced retrieval Python class\\n\\n _Now that you‚Äôve understood the**advanced retrieval optimization techniques**\\nwe\\'re using, let‚Äôs **combine** them into a **Python retrieval class**._\\n\\nQuery expansion chains wrapper ‚Üí GitHub ‚Üê\\n\\nNow the final step is to call Qdrant for each query generated by the query\\nexpansion step ‚Üì\\n\\nVectorRetriever: main search function ‚Üí GitHub ‚Üê\\n\\n _Note that we have**3 types of data** : posts, articles, and code\\nrepositories._\\n\\nThus, we have to make a query for each collection and combine the results in\\nthe end.\\n\\nWe gathered data from each collection individually and kept the best-retrieved\\nresults using rerank.\\n\\nWhich is the final step of the article.\\n\\n### 7\\\\. Post-retrieval optimization: Rerank using GPT-4\\n\\nWe made a **different search** in the Qdrant vector DB for **N prompts**\\n**generated** by the **query expansion step**.\\n\\n**Each** **search** returns **K results**.\\n\\nThus, we **end up with** **N x K chunks**.\\n\\nIn our particular case, **N = 5** & **K = 3.** Thus, we end up with 15 chunks.\\n\\nPost-retrieval optimization: rerank\\n\\nWe will use **rerank** to order all the **N x K** chunks based on their\\nrelevance relative to the initial question, where the first one will be the\\nmost relevant and the last chunk the least.\\n\\nUltimately, we will pick the TOP K most relevant chunks.\\n\\nRerank works really well when combined with query expansion.\\n\\n_A natural flow when using rerank is as follows:_\\n\\n    \\n    \\n    Search for >K chunks >>> Reorder using rerank >>> Take top K\\n\\nThus, when combined with query expansion, we gather potential useful context\\nfrom multiple points in space rather than just looking for more than K samples\\nin a single location.\\n\\n _Now the flow looks like:_\\n\\n    \\n    \\n    Search for N x K chunks >>> Reoder using rerank >>> Take top K\\n\\nA typical solution for reranking is to use open-source Bi-Encoders from\\nsentence transformers [4].\\n\\nThese solutions take both the question and context as input and return a score\\nfrom 0 to 1.\\n\\nIn this article, we want to take a different approach and use GPT-4 + prompt\\nengineering as our reranker.\\n\\nIf you want to see how to apply rerank using open-source algorithms, check out\\nthis hands-on article from Decoding ML:\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\nPaul Iusztin\\n\\n¬∑\\n\\nMar 7\\n\\nRead full story\\n\\nNow let‚Äôs see our implementation using GPT-4 & prompt engineering.\\n\\nSimilar to what we did for the expansion and self-query chains, we define a\\ntemplate and a chain builder ‚Üì\\n\\n    \\n    \\n    class RerankingTemplate(BasePromptTemplate):\\n        prompt: str = \"\"\"\\n        You are an AI language model assistant. \\n        Your task is to rerank passages related to a query\\n        based on their relevance. The most relevant passages \\n        should be put at the beginning. \\n        You should only pick at max {k} passages.\\n        The following are passages related to this query: {question}.\\n        Passages: {passages}\\n        \"\"\"\\n    \\n        def create_template(self) -> PromptTemplate:\\n            return PromptTemplate(\\n                template=self.prompt, \\n                input_variables=[\"question\", \"passages\"])\\n\\n‚Ä¶and that‚Äôs it!\\n\\n* * *\\n\\n### Conclusion\\n\\n _Congratulations!_\\n\\nIn **Lesson 5** , you learned to **build** an **advanced RAG retrieval\\nmodule** optimized for searching posts, articles, and code repositories from a\\nQdrant vector DB.\\n\\n**First** , you learned about where the RAG pipeline can be optimized:\\n\\n  * pre-retrieval\\n\\n  * retrieval\\n\\n  * post-retrieval\\n\\n**After** you learn how to build from scratch (without using LangChain‚Äôs\\nutilities) the following advanced RAG retrieval & post-retrieval optimization\\ntechniques:\\n\\n  * query expansion\\n\\n  * self query\\n\\n  * hybrid search\\n\\n  * rerank\\n\\n**Ultimately** , you understood where the retrieval component sits in an RAG\\nproduction LLM system, where the code is shared between multiple microservices\\nand doesn‚Äôt sit in a single Notebook.\\n\\n_**Next week** , in **Lesson 6** , we will move to the training pipeline and\\nshow you how to automatically transform the data crawled from LinkedIn,\\nSubstack, Medium, and GitHub into an instruction dataset using GPT-4 to fine-\\ntune your LLM Twin._\\n\\nSee you there! ü§ó\\n\\n* * *\\n\\n### Next Steps\\n\\n#### Step 1\\n\\nThis is just the **short version** of **Lesson 5** on the **advanced RAG\\nretrieval module**.\\n\\n‚Üí For‚Ä¶\\n\\n  * The full implementation.\\n\\n  * Discussion on our custom implementation vs. LangChain.\\n\\n  * More on the problems these 4 advanced RAG techniques solve.\\n\\n  * How to use the retrieval module.\\n\\n**Check out** the **full version** of **Lesson 5** on our **Medium\\npublication**. It‚Äôs still FREE:\\n\\nLesson 5 - FREE Medium Article\\n\\n#### Step 2\\n\\n‚Üí **Check out theLLM Twin GitHub repository and try it yourself ü´µ**\\n\\n _Nothing compares with getting your hands dirty and building it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n17\\n\\nShare this post\\n\\n#### The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n1\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Meng LiAI Disruption May 17Great, thanks for sharing!Expand full\\ncommentReplyShare  \\n---|---  \\n  \\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('037e6362-8be7-4860-992f-1f075921a669'), content={'Title': 'Problems deploying your ML models? Here is your solution!', 'Subtitle': 'PyTorch + CUDA ultimate guide. Synthetic data generation. Serverless infrastructure.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Problems deploying your ML models? Here is your solution!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Problems deploying your ML models? Here is your solution!\\n\\n### PyTorch + CUDA ultimate guide. Synthetic data generation. Serverless\\ninfrastructure.\\n\\nPaul Iusztin\\n\\nApr 27, 2024\\n\\n10\\n\\nShare this post\\n\\n#### Problems deploying your ML models? Here is your solution!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * The ultimate guide on installing PyTorch with CUDA support in all possible ways\\n\\n  * Generate a synthetic domain-specific Q&A dataset in <30 minutes\\n\\n  * The power of serverless in the world of ML\\n\\n* * *\\n\\nExciting news üî• I was invited by Maven to speak in their Lighting Lesson\\nseries about how to ùóîùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅ ùó¨ùóºùòÇùóø ùóüùóüùó† ùóßùòÑùó∂ùóª.\\n\\nRegister here (it‚Äôs free) ‚Üê\\n\\nThis 30-min session is for ML & MLOps engineers who want to learn:\\n\\nùóüùóüùó† ùó¶ùòÜùòÄùòÅùó≤ùó∫ ùó±ùó≤ùòÄùó∂ùó¥ùóª ùóºùó≥ ùòÜùóºùòÇùóø ùóüùóüùó† ùóßùòÑùó∂ùóª\\n\\n‚Üí Using the 3-pipeline architecture & MLOps good practices\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆ ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí data crawling, ETLs, CDC, AWS\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆ ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí streaming engine in Python, data ingestion for fine-tuning & RAG, vector DBs\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí create a custom dataset, fine-tuning, model registries, experiment trackers,\\nLLM evaluation\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆùóª ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí real-time deployment, REST API, RAG, LLM monitoring\\n\\n‚Üì‚Üì‚Üì\\n\\n> Join LIVE on ùòçùò≥ùò™, ùòîùò¢ùò∫ 3!\\n>\\n> Register here (it‚Äôs free) ‚Üê\\n\\n* * *\\n\\n### The ultimate guide on installing PyTorch with CUDA support in all possible\\nways\\n\\nEver wanted to quit ML while wrestling with ùóñùó®ùóóùóî ùó≤ùóøùóøùóºùóøùòÄ? I know I did. ‚Üí\\nDiscover ùóµùóºùòÑ to install ùóñùó®ùóóùóî & ùó£ùòÜùóßùóºùóøùó∞ùóµ ùóΩùóÆùó∂ùóªùóπùó≤ùòÄùòÄùóπùòÜ in all possible ways.  \\n  \\nHere is the story of most ML people:  \\n  \\n1\\\\. You just got excited about a new model that came out.  \\n  \\n2\\\\. You want to try it out.  \\n  \\n3\\\\. You install everything.  \\n  \\n4\\\\. You run the model.  \\n  \\n5\\\\. Bam... CUDA error.  \\n  \\n6\\\\. You fix the error.  \\n  \\n7\\\\. Bam... Another CUDA error  \\n  \\n7\\\\. You fix the error.  \\n  \\n8\\\\. ...Yet another CUDA error.  \\n  \\nYou get the idea.  \\n  \\n‚Üí Now it is 3:00 am, and you finally solved all your CUDA errors and ran your\\nmodel.  \\n  \\nNow, it\\'s time to do your actual work.  \\n  \\nDo you relate?  \\n  \\nIf so...  \\n  \\nI started a Medium article where I documented good practices and step-by-step\\ninstructions on how to install CUDA & PyTorch with:  \\n  \\n\\\\- Pip  \\n\\\\- Conda (or Mamba)  \\n\\\\- Poetry  \\n\\\\- Docker\\n\\nDocker entry point - bash template\\n\\n> **Check it out** ‚Üì  \\n>  \\n> üîó _**The ultimate guide on installing PyTorch with CUDA support in all\\n> possible ways**_\\n\\nùó°ùóºùòÅùó≤: Feel free to comment with any improvements on how to install CUDA +\\nPyTorch. Let\\'s make the ultimate tutorial on installing these 2 beasts üî•\\n\\n* * *\\n\\n### Generate a synthetic domain-specific Q&A dataset in <30 minutes\\n\\nHow do you ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ a ùòÄùòÜùóªùòÅùóµùó≤ùòÅùó∂ùó∞ ùó±ùóºùó∫ùóÆùó∂ùóª-ùòÄùóΩùó≤ùó∞ùó∂ùó≥ùó∂ùó∞ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ in <ùüØùü¨ ùó∫ùó∂ùóªùòÇùòÅùó≤ùòÄ to\\nùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ your ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†?  \\n  \\nThis method is also known as ùó≥ùó∂ùóªùó≤ùòÅùòÇùóªùó∂ùóªùó¥ ùòÑùó∂ùòÅùóµ ùó±ùó∂ùòÄùòÅùó∂ùóπùóπùóÆùòÅùó∂ùóºùóª. Here are its 3 ùòÆùò¢ùò™ùòØ\\nùò¥ùòµùò¶ùò±ùò¥ ‚Üì  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶, ùò≠ùò¶ùòµ\\'ùò¥ ùò®ùò¶ùòØùò¶ùò≥ùò¢ùòµùò¶ ùò¢ ùòò&ùòà ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùò•ùò¢ùòµùò¢ùò¥ùò¶ùòµ ùò∂ùò¥ùò¶ùò• ùòµùò∞ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò¶ ùò¢\\nùòßùò™ùòØùò¢ùòØùò§ùò™ùò¢ùò≠ ùò¢ùò•ùò∑ùò™ùò¥ùò∞ùò≥ ùòìùòìùòî.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: ùó†ùóÆùóªùòÇùóÆùóπùóπùòÜ ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóÆ ùó≥ùó≤ùòÑ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nGenerate a few input samples (~3) that have the following structure:  \\n\\\\- ùò∂ùò¥ùò¶ùò≥_ùò§ùò∞ùòØùòµùò¶ùòπùòµ: describe the type of investor (e.g., \"I am a 28-year-old\\nmarketing professional\")  \\n\\\\- ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØ: describe the user\\'s intention (e.g., \"Is Bitcoin a good\\ninvestment option?\")  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: ùóòùòÖùóΩùóÆùóªùó± ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ùóµùó≤ùóπùóΩ ùóºùó≥ ùóÆ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó†  \\n  \\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up\\nto +N similar input examples.  \\n  \\nWe generated 100 input examples in our use case, but you can generate more.  \\n  \\nYou will use the manually filled input examples to do few-shot prompting.  \\n  \\nThis will guide the LLM to give you domain-specific samples.  \\n  \\nùòõùò©ùò¶ ùò±ùò≥ùò∞ùòÆùò±ùòµ ùò∏ùò™ùò≠ùò≠ ùò≠ùò∞ùò∞ùò¨ ùò≠ùò™ùò¨ùò¶ ùòµùò©ùò™ùò¥:  \\n\"\"\"  \\n...  \\nGenerate 100 more examples with the following pattern:  \\n  \\n# USER CONTEXT 1  \\n...  \\n  \\n# QUESTION 1  \\n...  \\n  \\n# USER CONTEXT 2  \\n...  \\n\"\"\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: ùó®ùòÄùó≤ ùòÅùóµùó≤ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó† ùòÅùóº ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóºùòÇùòÅùóΩùòÇùòÅùòÄ ùó≥ùóºùóø ùóÆùóπùóπ ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nNow, you will have the same powerful LLM as a teacher, but this time, it will\\nanswer all your N input examples.  \\n  \\nBut first, to introduce more variance, we will use RAG to enrich the input\\nexamples with news context.  \\n  \\nAfterward, we will use the teacher LLM to answer all N input examples.  \\n  \\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual\\nwork.  \\n  \\n.  \\n  \\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a\\nniched task, such as financial advising.  \\n  \\nThis technique is known as finetuning with distillation because you use a\\npowerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data,\\nwhich will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as\\nthe student.\\n\\nGenerate a Q&A dataset in <30 minutes\\n\\n  \\n‚úíÔ∏è ùòïùò∞ùòµùò¶: To ensure that the generated data is of high quality, you can hire a\\ndomain expert to check & refine it.\\n\\n* * *\\n\\n### The power of serverless in the world of ML\\n\\nùóóùó≤ùóΩùóπùóºùòÜùó∂ùóªùó¥ & ùó∫ùóÆùóªùóÆùó¥ùó∂ùóªùó¥ ML models is ùóµùóÆùóøùó±, especially when running your models on\\nGPUs.  \\n  \\nBut ùòÄùó≤ùóøùòÉùó≤ùóøùóπùó≤ùòÄùòÄ makes things ùó≤ùóÆùòÄùòÜ.  \\n  \\nUsing Beam as your serverless provider, deploying & managing ML models can be\\nas easy as ‚Üì  \\n  \\nùóóùó≤ùó≥ùó∂ùóªùó≤ ùòÜùóºùòÇùóø ùó∂ùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ & ùó±ùó≤ùóΩùó≤ùóªùó±ùó≤ùóªùó∞ùó∂ùó≤ùòÄ  \\n  \\nIn a few lines of code, you define the application that contains:  \\n  \\n\\\\- the requirements of your infrastructure, such as the CPU, RAM, and GPU  \\n\\\\- the dependencies of your application  \\n\\\\- the volumes from where you can load your data and store your artifacts  \\n  \\nùóóùó≤ùóΩùóπùóºùòÜ ùòÜùóºùòÇùóø ùó∑ùóºùóØùòÄ  \\n  \\nUsing the Beam application, you can quickly decorate your Python functions to:  \\n  \\n\\\\- run them once on the given serverless application  \\n\\\\- put your task/job in a queue to be processed or even schedule it using a\\nCRON-based syntax  \\n\\\\- even deploy it as a RESTful API endpoint  \\n  \\n.  \\n  \\nAs you can see in the image below, you can have one central function for\\ntraining or inference, and with minimal effort, you can switch from all these\\ndeployment methods.  \\n  \\nAlso, you don\\'t have to bother at all with managing the infrastructure on\\nwhich your jobs run. You specify what you need, and Beam takes care of the\\nrest.  \\n  \\nBy doing so, you can directly start to focus on your application and stop\\ncarrying about the infrastructure.  \\n  \\nThis is the power of serverless!\\n\\nBeam example\\n\\n> ‚Ü≥üîó ùòäùò©ùò¶ùò§ùò¨ ùò∞ùò∂ùòµ ùòâùò¶ùò¢ùòÆ ùòµùò∞ ùò≠ùò¶ùò¢ùò≥ùòØ ùòÆùò∞ùò≥ùò¶\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n10\\n\\nShare this post\\n\\n#### Problems deploying your ML models? Here is your solution!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/problems-deploying-your-ml-models?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('c91e76e3-774c-43e7-91db-01c0c6bff57a'), content={'Title': 'Streaming Pipelines for LLMs and RAG - by Paul Iusztin', 'Subtitle': 'SOTA streaming pipeline in Python to clean, chunk, embed and load data to a vector DB (feature store)  in real time: for fine-tuning LLMs and RAG (on AWS).', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-\\nTime!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-Time!\\n\\n### Use a Python streaming engine to populate a feature store from 4+ data\\nsources\\n\\nPaul Iusztin\\n\\nApr 25, 2024\\n\\n11\\n\\nShare this post\\n\\n#### SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-\\nTime!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n‚Üí the 4th out of 11 lessons of the LLM Twin free course\\n\\n**What is your LLM Twin?** It is an AI character that writes like yourself by\\nincorporating your style, personality, and voice into an LLM.\\n\\nImage by DALL-E\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> More **details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà\\n\\n* * *\\n\\n### Latest Lessons of the LLM Twin Course\\n\\n**Lesson 1:**` `An End-to-End Framework for Production-Ready LLM Systems by\\nBuilding Your LLM Twin\\n\\n‚Üí LLM Twin Concept, 3-Pipeline Architecture, System Design for LLM Twin\\n\\n**Lesson 2** : The importance of Data Pipeline in the era of Generative AI\\n\\n‚Üí Data crawling, ETL pipelines, ODM, NoSQL Database\\n\\n**Lesson 3:** CDC: Enabling Event-Driven Architectures\\n\\n‚Üí Change Data Capture (CDC), MongoDB Watcher, RabbitMQ queue\\n\\n* * *\\n\\n## Lesson 4: **Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-\\nTime!**\\n\\nIn the **4th lesson** , we will focus on the **feature pipeline.**\\n\\nThe **feature pipeline** is the **first** **pipeline** presented in the **3\\npipeline architecture** : feature, training and inference pipelines.\\n\\nA **feature pipeline** takes raw data as input, processes it into features,\\nand stores it in a feature store, from which the training & inference\\npipelines will use it.\\n\\nThe component is completely isolated from the training and inference code. All\\nthe communication is done through the feature store.\\n\\nBy the **end of this** **article** , you will **learn** to **design** and\\n**build** a **production-ready feature pipeline** that:\\n\\n  * uses Bytewax as a stream engine to process data in real-time;\\n\\n  * ingests data from a RabbitMQ queue;\\n\\n  * uses SWE practices to process multiple data types: posts, articles, code;\\n\\n  * cleans, chunks, and embeds data for LLM fine-tuning and RAG;\\n\\n  * loads the features to a Qdrant vector DB.\\n\\n> Note that we will only cover the **vector DB retrieval client** and\\n> **advanced retrieval techniques** in the **5th lesson**!\\n\\n_Excited? Let‚Äôs get started!_\\n\\n* * *\\n\\n### Table of Contents:\\n\\n  1. Why are we doing this?\\n\\n  2. System design of the feature pipeline\\n\\n  3. The Bytewax streaming flow\\n\\n  4. Pydantic data models\\n\\n  5. Load data to Qdrant (our feature store)\\n\\n  6. The dispatcher layer\\n\\n> üîó **Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è\\n\\n* * *\\n\\n### 1\\\\. Why are we doing this?\\n\\n#### A quick reminder from previous lessons\\n\\nTo give you some context, in Lesson 2, we crawl data from LinkedIn, Medium,\\nand GitHub, normalize it, and load it to MongoDB.\\n\\nIn Lesson 3, we are using CDC to listen to changes to the MongoDB database and\\nemit events in a RabbitMQ queue based on any CRUD operation done on MongoDB.\\n\\n#### The problem we are solving\\n\\nIn our LLM Twin use case, the **feature pipeline** constantly syncs the\\nMongoDB warehouse with the Qdrant vector DB (our feature store) while\\nprocessing the raw data into features.\\n\\n#### Why we are solving it\\n\\nThe **feature store** will be the **central point of access** for all the\\nfeatures used within the training and inference pipelines.\\n\\n‚Üí The **training pipeline** will use the feature store to create **fine-\\ntunin** g datasets for your **LLM** **twin**.\\n\\n‚Üí The **inference pipeline** will use the feature store for **RAG**.\\n\\n### 2\\\\. System design of the feature pipeline: our solution\\n\\n _Our**solution** is based on **CDC** , a **queue,** a **streaming engine,**\\nand a **vector DB:**_\\n\\n‚Üí CDC adds any change made to the Mongo DB to the queue (read more in Lesson\\n3).\\n\\n‚Üí the RabbitMQ queue stores all the events until they are processed.\\n\\n‚Üí The Bytewax streaming engine cleans, chunks, and embeds the data.\\n\\n‚Üí A streaming engine works naturally with a queue-based system.\\n\\n‚Üí The data is uploaded to a Qdrant vector DB on the fly\\n\\n#### **Why is this powerful?**\\n\\nHere are 4 core reasons:\\n\\n  1. The **data** is **processed** in **real-time**.\\n\\n  2. **Out-of-the-box recovery system:** If the streaming pipeline fails to process a message will be added back to the queue \\n\\n  3. **Lightweight:** No need for any diffs between databases or batching too many records\\n\\n  4. **No I/O bottlenecks** on the source database\\n\\n‚Üí **It solves all our problems!**\\n\\nStreaming ingestion pipeline architecture and integration with the rest of the\\ncomponents\\n\\n#### How do we process multiple data types?\\n\\nHow do you **process multiple types** **of** **data** in a **single streaming\\npipeline** **without** **writing** **spaghetti code**?\\n\\nYes, that is for you, data scientists! **Joking‚Ä¶** am I**?**\\n\\nWe have **3 data types** : posts, articles, and code.\\n\\n**Each data type** (and its state) will be **modeled** using **Pydantic**\\n**models**.\\n\\nTo **process** them, we will write a **dispatcher layer** , which will use a\\n**creational** **factory** **pattern **to **instantiate** a **handler**\\nimplemented for that **specific data type** (post, article, code) and\\n**operation** (cleaning, chunking, embedding).\\n\\nThe **handler** follows the **strategy behavioral pattern.**\\n\\n#### Streaming over batch\\n\\nNowadays, using tools such as Bytewax makes implementing streaming pipelines a\\nlot more frictionless than using their JVM alternatives.\\n\\nThe key aspect of choosing a streaming vs. a batch design is real-time\\nsynchronization between your source and destination DBs.\\n\\nIn our particular case, we will process social media data, which changes fast\\nand irregularly.\\n\\nAlso, for our digital twin, it is important to do RAG on up-to-date data. We\\ndon‚Äôt want to have any delay between what happens in the real world and what\\nyour LLM twin sees.\\n\\nThat being said, choosing a streaming architecture seemed natural in our use\\ncase.\\n\\n* * *\\n\\n### 3\\\\. The Bytewax streaming flow\\n\\nThe **Bytewax flow** is the **central point** of the **streaming pipeline**.\\nIt defines all the required steps, following the next simplified pattern:\\n_‚Äúinput - > processing -> output‚Äù._\\n\\nAs I come from the AI world, I like to see it as the **‚Äúgraph‚Äù** of the\\n**streaming pipeline** , where you use the _input()_ , _map()_ , and\\n_output()_ Bytewax functions to define your graph, which in the **Bytewax\\nworld** is **called** a _**‚Äúflow‚Äù**_.\\n\\nAs you can see in the code snippet below, we ingest posts, articles or code\\nmessages from a RabbitMQ queue. After we clean, chunk and embed them.\\nUltimately, we load the cleaned and embedded data to a Qdrant vector DB, which\\nin our LLM twin use case will represent the feature store of our system.\\n\\nTo structure and validate the data, between each Bytewax step, we map and pass\\na different Pydantic model based on its current state: raw, cleaned, chunked,\\nor embedded.\\n\\nBytewax flow ‚Üí GitHub Code  ‚Üê\\n\\nWe have a single streaming pipeline that processes everything.\\n\\nAs we ingest multiple data types (posts, articles, or code snapshots), we have\\nto process them differently.\\n\\nTo do this the right way, we implemented a dispatcher layer that knows how to\\napply data-specific operations based on the type of message.\\n\\nMore on this in the next sections ‚Üì\\n\\n#### Why Bytewax?\\n\\n_Bytewax is an open-source streaming processing framework that:_  \\n\\\\- is built in **Rust** ‚öôÔ∏è for **performance**  \\n\\\\- has **Python** üêç bindings for leveraging its powerful ML ecosystem\\n\\n‚Ä¶ so, for all the Python fanatics out there, no more JVM headaches for you.\\n\\nJokes aside, here is why Bytewax is so powerful ‚Üì\\n\\n\\\\- Bytewax local setup is plug-and-play  \\n\\\\- can quickly be integrated into any Python project (you can go wild ‚Äî even\\nuse it in Notebooks)  \\n\\\\- can easily be integrated with other Python packages (NumPy, PyTorch,\\nHuggingFace, OpenCV, SkLearn, you name it)  \\n\\\\- out-of-the-box connectors for Kafka and local files, or you can quickly\\nimplement your own\\n\\nWe used Bytewax to build the streaming pipeline for the LLM Twin course and\\nloved it.\\n\\n> To **learn more** about **Bytewax** , check out their **Substack** , where\\n> you have the chance to **dive deeper** into **streaming engines**. In\\n> Python. For FREE:\\n>\\n> ‚Üí Bytewax Newsletter ‚Üê\\n\\n* * *\\n\\n### 4\\\\. Pydantic data models\\n\\nLet‚Äôs take a look at what our Pydantic models look like.\\n\\nWe defined a hierarchy of Pydantic models for:\\n\\n  * all our data types: posts, articles, or code\\n\\n  * all our states: raw, cleaned, chunked, and embedded\\n\\nThis is how the set of classes for the posts will look like ‚Üì\\n\\nPydantic posts model structure ‚Üí GitHub Code ‚Üê\\n\\nWe **repeated** the s**ame process** for the **articles** and **code** model\\n**hierarchy**.\\n\\n### 5\\\\. Load data to Qdrant (our feature store)\\n\\nThe first step is to implement our custom Bytewax _DynamicSink_ class ‚Üì\\n\\nQdrant DynamicSink ‚Üí GitHub Code ‚Üê\\n\\nNext, for every type of operation we need (output cleaned or embedded data ),\\nwe have to subclass the _StatelessSinkPartition_ Bytewax class (they also\\nprovide a stateful option ‚Üí more in their docs)\\n\\nAn instance of the class will run on every partition defined within the\\nBytewax deployment.\\n\\nIn the course, we are using a single partition per worker. But, by adding more\\npartitions (and workers), you can quickly scale your Bytewax pipeline\\nhorizontally.\\n\\n**Remember** **why** we upload the **data** to Qdrant in **two stages** , as\\nthe **Qdrant vector DB** will act as our **feature store** :\\n\\n  1. The _cleaned data_ will be used for _LLM fine-tuning_(used by the training pipeline)\\n\\n  2. The _chunked & embedded_ data will be used for _RAG (used by the inference pipeline)_\\n\\nQdrant worker partitions ‚Üí GitHub Code ‚Üê\\n\\nNote that we used**Qdrant‚Äôs** **Batch** method to upload all the available\\npoints simultaneously. By doing so, we **reduce** the **latency** on the\\n**network I/O** side: more on that here\\n\\n### 6\\\\. The dispatcher layer\\n\\nNow that we have the Bytewax flow and all our data models.\\n\\n**How do we map a raw data model to a cleaned data model?**\\n\\n> All our domain logic is modeled by a set of _Handler()_ classes:\\n>\\n> ‚Üí CleaningDataHandler\\n>\\n> ‚Üí ChunkingDataHandler\\n>\\n> ‚Üí EmbeddingDataHandler\\n\\n**Now, to build our dispatcher, we need 2 last components:**\\n\\n  * **a factory class:** instantiates the right handler based on the type of the event\\n\\n  * **a dispatcher class:** the glue code that calls the factory class and handler\\n\\n**Here is what the cleaning dispatcher and factory look like** ‚Üì\\n\\nThe dispatcher and factory classes ‚Üí GitHub Code ‚Üê\\n\\nNote that we will have a different **Handler()** for every (data_type, state)\\npair ‚Äî resulting in 3 x 3 = 9 different handlers.\\n\\nFor Example, we will have 3 handlers based on their data type for the cleaned\\npost state: PostCleaningHandler, ArticleCleaningHandler, and\\nRepositoryCleaningHandler.\\n\\n**By repeating the same logic, we will end up with the following set of\\ndispatchers:**\\n\\n  * _RawDispatcher_ (no factory class required as the data is not processed)\\n\\n  * _CleaningDispatcher_ (with a _ChunkingHandlerFactory_ class)\\n\\n  * _ChunkingDispatcher_ (with a _ChunkingHandlerFactory_ class)\\n\\n  * _EmbeddingDispatcher_ (with an _EmbeddingHandlerFactory_ class)\\n\\n* * *\\n\\n### To Summarize\\n\\nIn **Lesson 4** of the LLM Twin course, we learned how to:\\n\\n  * Design a streaming pipeline in Python using Bytewax\\n\\n  * Load data to a Qdrant vector DB\\n\\n  * Use Pydantic models to add types and validation to the data points\\n\\n  * Implement a dispatcher layer to process multiple data types in a modular way\\n\\n _‚Üí In**Lesson 5, which will be held in two weeks,** we will focus on the\\nvector DB retrieval client and advanced retrieval techniques._\\n\\n* * *\\n\\n### Next Steps\\n\\nTo **dig** **into** the **details** of the **streaming pipeline** and **how**\\nto:\\n\\n  * **implement** **cleaning** , **chunking** , and **embedding** **strategies** for digital data\\n\\n  * **design** the **AWS infrastructure** for the streaming pipeline\\n\\n  * understand how to **run the component**\\n\\n**Check out** the **full-fledged version** of the **article** on our **Medium\\npublication**.\\n\\n‚Üì‚Üì‚Üì\\n\\nLesson 4 - FREE Medium Article\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n11\\n\\nShare this post\\n\\n#### SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-\\nTime!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/sota-python-streaming-pipelines-for?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('53bc94d1-8cfd-4e65-b55c-9b3582f6ed64'), content={'Title': 'Ready for production ML? Here are the 4 pillars to build production ML systems', 'Subtitle': 'ML Platforms & MLOps Components. RAG:RAG: What problems does it solve, and how is it integrated into LLM-powered applications', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\n### ML Platforms & MLOps Components. RAG:RAG: What problems does it solve, and\\nhow is it integrated into LLM-powered applications\\n\\nPaul Iusztin\\n\\nApr 13, 2024\\n\\n8\\n\\nShare this post\\n\\n#### Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Using an ML Platform is critical to integrating MLOps into your project\\n\\n  * The 4 pillars to build production ML systems\\n\\n  * RAG: What problems does it solve, and how is it integrated into LLM-powered applications?\\n\\n* * *\\n\\n### Using an ML Platform is critical to integrating MLOps into your project\\n\\nHere are 6 ML platform features you must know & use ‚Üì  \\n  \\n...and let\\'s use Comet ML as a concrete example.  \\n  \\n#ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥  \\n  \\nIn your ML development phase, you generate lots of experiments.  \\n  \\nTracking and comparing the metrics between them is crucial in finding the\\noptimal model & hyperparameters.  \\n  \\n#ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤  \\n  \\nIts primary purpose is reproducibility.  \\n  \\nTo know how a model from a specific experiment was generated, you must know:  \\n\\\\- the version of the code  \\n\\\\- version of the dataset  \\n\\\\- hyperparameters/config  \\n\\\\- total compute  \\n... and more  \\n  \\n#ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nMost of the time, along with the scalar metrics, you must log visual results,\\nsuch as:  \\n\\\\- images  \\n\\\\- videos  \\n\\\\- prompts  \\n\\\\- t-SNE graphs  \\n\\\\- 3D point clouds  \\n... and more  \\n  \\n#4. ùêÄùê´ùê≠ùê¢ùêüùêöùêúùê≠ùê¨  \\n  \\nThe most powerful feature out of them all.  \\n  \\nAn artifact is a versioned object that acts as an input or output for your\\njob.  \\n  \\nEverything can be an artifact (data, model, code), but the most common case is\\nfor your data.  \\n  \\nWrapping your assets around an artifact ensures reproducibility and\\nshareability.  \\n  \\nFor example, you wrap your features into an artifact (e.g., features:3.1.2),\\nwhich you can consume and share across multiple ML environments (development\\nor continuous training).  \\n  \\nUsing an artifact to wrap your data allows you to quickly respond to questions\\nsuch as \"What data have I used to generate the model?\" and \"What Version?\"  \\n  \\n#5. ùêåùê®ùêùùêûùê• ùêëùêûùê†ùê¢ùê¨ùê≠ùê´ùê≤  \\n  \\nThe model registry is the ultimate way to version your models and make them\\naccessible to all your services.  \\n  \\nFor example, your continuous training pipeline will log the weights as an\\nartifact into the model registry after it trains the model.  \\n  \\nYou label this model as \"v:1.1.5:staging\" and prepare it for testing. If the\\ntests pass, mark it as \"v:1.1.0:production\" and trigger the CI/CD pipeline to\\ndeploy it to production.  \\n  \\n#6. ùêñùêûùêõùê°ùê®ùê®ùê§ùê¨  \\n  \\nWebhooks lets you integrate the Comet model registry with your CI/CD pipeline.  \\n  \\nFor example, when the model status changes from \"Staging\" to \"Production,\" a\\nPOST request triggers a GitHub Actions workflow to deploy your new model.\\n\\nImage by the Author\\n\\n‚Ü≥üîó Check out **Comet** to learn more\\n\\n* * *\\n\\n### The 4 pillars to build production ML systems\\n\\nBefore building a production-ready system, it is critical to consider a set of\\nquestions that will later determine the nature of your ML system architecture.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò¢ùò≥ùò¶ ùòµùò©ùò¶ 4 ùò±ùò™ùò≠ùò≠ùò¢ùò≥ùò¥ ùòµùò©ùò¢ùòµ ùò∫ùò∞ùò∂ ùò¢ùò≠ùò∏ùò¢ùò∫ùò¥ ùò©ùò¢ùò∑ùò¶ ùòµùò∞ ùò§ùò∞ùòØùò¥ùò™ùò•ùò¶ùò≥ ùò£ùò¶ùòßùò∞ùò≥ùò¶ ùò•ùò¶ùò¥ùò™ùò®ùòØùò™ùòØùò® ùò¢ùòØùò∫\\nùò¥ùò∫ùò¥ùòµùò¶ùòÆ ‚Üì  \\n  \\n‚ûî ùóóùóÆùòÅùóÆ  \\n  \\n\\\\- What data types do you have? (e.g., tabular data, images, text, etc.)  \\n\\\\- What does the data look like? (e.g., for text data, is it in a single\\nlanguage or multiple?)  \\n\\\\- How do you collect the data?  \\n\\\\- At what frequency do you have to collect the data?  \\n\\\\- How do you collect labels for the data? (crucial for how you plan to\\nevaluate and monitor the model in production)  \\n  \\n‚ûî ùóßùóµùóøùóºùòÇùó¥ùóµùóΩùòÇùòÅ  \\n  \\n\\\\- What are the throughput requirements? You must know at least the\\nthroughput\\'s minimum, average, and maximum statistics.  \\n\\\\- How many requests the system must handle simultaneously? (1, 10, 1k, 1\\nmillion, etc.)  \\n  \\n‚ûî ùóüùóÆùòÅùó≤ùóªùó∞ùòÜ  \\n  \\n\\\\- What are the latency requirements? (1 millisecond, 10 milliseconds, 1\\nsecond, etc.)  \\n\\\\- Throughput vs. latency trade-off  \\n\\\\- Accuracy vs. speed trade-off  \\n  \\n‚ûî ùóúùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤  \\n  \\n\\\\- Batch vs. real-time architecture (closely related to the throughput vs.\\nlatency trade-off)  \\n\\\\- How should the system scale? (e.g., based on CPU workload, # of requests,\\nqueue size, data size, etc.)  \\n\\\\- Cost requirements  \\n  \\n.  \\n  \\nDo you see how we shifted the focus from model performance towards how it is\\nintegrated into a more extensive system?  \\n  \\nWhen building production-ready ML, the model\\'s accuracy is no longer the holy\\ngrail but a bullet point in a grander scheme.  \\n  \\n.  \\n  \\nùóßùóº ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùòáùó≤, the 4 pillars to keep in mind before designing an ML\\narchitecture are:  \\n\\\\- Data  \\n\\\\- Throughput  \\n\\\\- Latency  \\n\\\\- Infrastructure\\n\\nImage by the Author\\n\\n* * *\\n\\n### RAG: What problems does it solve, and how is it integrated into LLM-\\npowered applications?\\n\\nLet\\'s find out ‚Üì  \\n  \\nRAG is a popular strategy when building LLMs to add external data to your\\nprompt.  \\n  \\n=== ùó£ùóøùóºùóØùóπùó≤ùó∫ ===  \\n  \\nWorking with LLMs has 3 main issues:  \\n  \\n1\\\\. The world moves fast  \\n  \\nLLMs learn an internal knowledge base. However, the issue is that its\\nknowledge is limited to its training dataset.  \\n  \\nThe world moves fast. New data flows on the internet every second. Thus, the\\nmodel\\'s knowledge base can quickly become obsolete.  \\n  \\nOne solution is to fine-tune the model every minute or day...  \\n  \\nIf you have some billions to spend around, go for it.  \\n  \\n2\\\\. Hallucinations  \\n  \\nAn LLM is full of testosterone and likes to be blindly confident.  \\n  \\nEven if the answer looks 100% legit, you can never fully trust it.  \\n  \\n3\\\\. Lack of reference links  \\n  \\nIt is hard to trust the response of the LLM if we can\\'t see the source of its\\ndecisions.  \\n  \\nEspecially for important decisions (e.g., health, financials)  \\n  \\n=== ùó¶ùóºùóπùòÇùòÅùó∂ùóºùóª ===  \\n  \\n‚Üí Surprize! It is RAG.  \\n  \\n1\\\\. Avoid fine-tuning  \\n  \\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge\\nbase as the main memory (e.g., vector DB).  \\n  \\nThe memory is volatile, so you can quickly introduce or remove data.  \\n  \\n2\\\\. Avoid hallucinations  \\n  \\nBy forcing the LLM to answer solely based on the given context, the LLM will\\nprovide an answer as follows:  \\n  \\n\\\\- use the external data to respond to the user\\'s question if it contains the\\nnecessary insights  \\n\\\\- \"I don\\'t know\" if not  \\n  \\n3\\\\. Add reference links  \\n  \\nUsing RAG, you can easily track the source of the data and highlight it to the\\nuser.  \\n  \\n=== ùóõùóºùòÑ ùó±ùóºùó≤ùòÄ ùó•ùóîùóö ùòÑùóºùóøùó∏? ===  \\n  \\nLet\\'s say we want to use RAG to build a financial assistant.  \\n  \\nùòûùò©ùò¢ùòµ ùò•ùò∞ ùò∏ùò¶ ùòØùò¶ùò¶ùò•?  \\n  \\n\\\\- a data source with historical and real-time financial news (e.g. Alpaca)  \\n\\\\- a stream processing engine (eg. Bytewax)  \\n\\\\- an encoder-only model for embedding the docs (e.g., pick one from\\n`sentence-transformers`)  \\n\\\\- a vector DB (e.g., Qdrant)  \\n  \\nùòèùò∞ùò∏ ùò•ùò∞ùò¶ùò¥ ùò™ùòµ ùò∏ùò∞ùò≥ùò¨?  \\n  \\n‚Ü≥ On the feature pipeline side:  \\n  \\n1\\\\. using Bytewax, you ingest the financial news and clean them  \\n2\\\\. you chunk the news documents and embed them  \\n3\\\\. you insert the embedding of the docs along with their metadata (e.g., the\\ninitial text, source_url, etc.) to Qdrant  \\n  \\n‚Ü≥ On the inference pipeline side:  \\n  \\n4\\\\. the user question is embedded (using the same embedding model)  \\n5\\\\. using this embedding, you extract the top K most similar news documents\\nfrom Qdrant  \\n6\\\\. along with the user question, you inject the necessary metadata from the\\nextracted top K documents into the prompt template (e.g., the text of\\ndocuments & its source_url)  \\n7\\\\. you pass the whole prompt to the LLM for the final answer\\n\\nImage by the Author\\n\\n8\\n\\nShare this post\\n\\n#### Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Dr. Jody-Ann S. JonesThe Data Sensei Apr 13Liked by Paul IusztinExcellent\\narticle Paul! Thank you so much for sharing üôèExpand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/ready-for-production-ml-here-are?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('20a85606-a880-4894-bfb7-6b0cad8b3f1f'), content={'Title': 'My monthly recommendations for leveling up in ML', 'Subtitle': 'In Vector DBs, RAG, MLOps, and LLMs', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### My monthly recommendations for leveling up in ML\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# My monthly recommendations for leveling up in ML\\n\\n### In Vector DBs, RAG, MLOps, and LLMs\\n\\nPaul Iusztin\\n\\nApr 06, 2024\\n\\n12\\n\\nShare this post\\n\\n#### My monthly recommendations for leveling up in ML\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n**Today is about learning.**\\n\\nHere is a list of learning resources I used and filtered in the past months.\\n\\nIt is one of the most helpful content on Vector DBs, RAG, MLOps and LLMs out\\nthere.\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * Pick the right vector DB for your exact use case\\n\\n  * 4 video lectures on hands-on LLMs\\n\\n  * 7 steps you have to achieve 100% MLOps maturity\\n\\n  * Advanced RAG\\n\\n* * *\\n\\n### Pick the right vector DB for your exact use case\\n\\nThis is the ùóºùóªùóπùòÜ ùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± to ùóΩùó∂ùó∞ùó∏ the ùóøùó∂ùó¥ùóµùòÅ ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï for your exact\\nùòÇùòÄùó≤ ùó∞ùóÆùòÄùó≤.  \\n  \\nSince ChatGPT made AI cool, besides the millions of ChatGPT posts you got\\ntired of and blocked, you realized that a new type of tool started to hit the\\nscene: Vector DBs.  \\n  \\nAs vector DBs play a crucial role in most LLM applications, they popped out\\neverywhere.  \\n  \\nOn this day, there are 37 vector DB solutions that are constantly changing and\\nadding features.  \\n  \\nùòïùò∞ùò∏, ùò©ùò∞ùò∏ ùòµùò©ùò¶ ùò©**ùò≠ ùò¥ùò©ùò∞ùò∂ùò≠ùò• ùòê ùò±ùò™ùò§ùò¨ ùò∞ùòØùò¶?\\n\\nSS from Superlinked\\n\\nùôÉùôöùôßùôö ùôûùô® ùô¨ùôùùôöùôßùôö ùô©ùôùùôö \"ùôëùôöùôòùô©ùô§ùôß ùòøùòΩ ùòæùô§ùô¢ùô•ùôñùôßùôûùô®ùô§ùô£\" ùô†ùôûùôòùô†ùô® ùôûùô£.  \\n  \\nIt is an effort managed by Superlinked, where they carefully compared all\\nthese 37 vector DBs across 29 features, such as:  \\n  \\n\\\\- License  \\n\\\\- GitHub ‚≠ê  \\n\\\\- support for text, image or struct models  \\n\\\\- RAG, RecSys, LangChain or LllamaIndex APIs  \\n\\\\- pricing  \\n\\\\- sharding  \\n\\\\- document size  \\n\\\\- vector dims  \\n  \\n...and more!  \\n  \\nI won\\'t list all 29 features.  \\n  \\nYou have to check it out to see them for yourself ‚Üì\\n\\nVector DB Comparison\\n\\nùó°ùóºùòÅùó≤: To keep the table updated or add more features, you can contribute to it\\nyourself.\\n\\n* * *\\n\\n### 4 video lectures on hands-on LLMs\\n\\nWant to build your first ùóüùóüùó† ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ but don\\'t know where to start?  \\n  \\nHere are ùü∞ ùóôùó•ùóòùóò ùóπùó≤ùó∞ùòÅùòÇùóøùó≤ùòÄ, made by\\n\\nPau Labarta Bajo\\n\\nfrom\\n\\nReal-World Machine Learning\\n\\n, to put you on the right track ‚Üì  \\n  \\n#1. ùêÖùê¢ùêßùêû-ùê≠ùêÆùêßùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû ùêüùê®ùê´ ùê®ùê©ùêûùêß-ùê¨ùê®ùêÆùê´ùêúùêû ùêãùêãùêåùê¨  \\n  \\nYou will learn:  \\n\\\\- What is model fine-tuning?  \\n\\\\- Why is it useful?  \\n\\\\- When to use it?  \\n\\\\- Why to fine-tune an LLM using QLoRA  \\n\\\\- How to architect a fine-tuning pipeline in a real-world project\\n\\n#2. ùêáùêöùêßùêùùê¨-ùê®ùêß ùêüùê¢ùêßùêû-ùê≠ùêÆùêßùê¢ùêßùê†  \\n  \\nLet\\'s apply what we learned in lesson 1 to build our first fine-tuning\\npipeline.\\n\\n#3. ùêÅùêÆùê¢ùê•ùêù & ùêùùêûùê©ùê•ùê®ùê≤ ùêö ùê´ùêûùêöùê•-ùê≠ùê¢ùê¶ùêû ùê¨ùê≠ùê´ùêûùêöùê¶ùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû  \\n  \\nYou will learn:  \\n\\\\- How to transform HTML docs into vector embeddings.  \\n\\\\- How to process data in real-time  \\n\\\\- How to store & retrieve embeddings from a vector DB  \\n\\\\- How to deploy it to AWS.\\n\\n#4. ùêàùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû  \\n  \\nFinally, you will learn how to use LangChain to glue together your fine-tuned\\nLLM and your financial news stored as embeddings in a vector DB to serve\\npredictions behind a RESTful API.\\n\\n* * *\\n\\n### 7 steps you have to achieve 100% MLOps maturity\\n\\nOne of the most ùó∞ùóºùóªùó≥ùòÇùòÄùó∂ùóªùó¥ ùòÑùóºùóøùó±ùòÄ in the ùó†ùóü ùòÑùóºùóøùóπùó± is \"ùó†ùóüùó¢ùóΩùòÄ\", a new &\\ninterdisciplinary process that isn\\'t fully defined yet.  \\n  \\nThe good news is that there is a strong movement in ùó±ùó≤ùó≥ùó∂ùóªùó∂ùóªùó¥ a ùó∞ùóπùó≤ùóÆùóø ùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤\\nin ùòÄùó∞ùóºùóøùó∂ùóªùó¥ the ùóπùó≤ùòÉùó≤ùóπ of ùó†ùóüùó¢ùóΩùòÄ ùó∫ùóÆùòÅùòÇùóøùó∂ùòÅùòÜ within your ùóºùóøùó¥ùóÆùóªùó∂ùòáùóÆùòÅùó∂ùóºùóª or ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ.  \\n  \\n‚Ü≥ Here are ùü≥ ùòÄùòÅùó≤ùóΩùòÄ you have to ùó∞ùóµùó≤ùó∞ùó∏ to ùóÆùó∞ùóµùó∂ùó≤ùòÉùó≤ ùü≠ùü¨ùü¨% ùó†ùóüùó¢ùóΩùòÄ ùó∫ùóÆùòÅùòÇùóøùó∂ùòÅùòÜ ‚Üì  \\n  \\nNo one other than\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\nhas proposed it.  \\n  \\nùóõùó≤ùóøùó≤ ùòÅùóµùó≤ùòÜ ùóÆùóøùó≤ ‚Üì  \\n  \\n=== ùòîùò∂ùò¥ùòµ ùò©ùò¢ùò∑ùò¶ùò¥ ===  \\n  \\nùü≠\\\\. ùóóùóºùó∞ùòÇùó∫ùó≤ùóªùòÅùóÆùòÅùó∂ùóºùóª: project, ML model, and technical documentation  \\n  \\nùüÆ\\\\. ùóßùóøùóÆùó∞ùó≤ùóÆùóØùó∂ùóπùó∂ùòÅùòÜ ùóÆùóªùó± ùóøùó≤ùóΩùóøùóºùó±ùòÇùó∞ùó∂ùóØùó∂ùóπùó∂ùòÅùòÜ: Infrastructure traceability and\\nreproducibility (versioned IaC under CI/CD) and ML code traceability and\\nreproducibility (versioned code, data, and models along with metadata &\\nlineage attached to the data & model)  \\n  \\nùüØ\\\\. ùóñùóºùó±ùó≤ ùóæùòÇùóÆùóπùó∂ùòÅùòÜ: infrastructure code & ML model code quality requirements\\n(tests ran on PRs under the CI pipeline, PR reviews, formatting checks)  \\n  \\nùü∞\\\\. ùó†ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ & ùòÄùòÇùóΩùóΩùóºùóøùòÅ: infrastructure, application, model performance,\\nbusiness KPIs, data drift and outliers monitoring  \\n  \\n=== ùòâùò¶ùò∫ùò∞ùòØùò• ùò£ùò¢ùò¥ùò™ùò§ ùòîùòìùòñùò±ùò¥ ===  \\n  \\nùü±\\\\. ùóóùóÆùòÅùóÆ ùòÅùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùóÆùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ùòÄ & ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùòÄùòÅùóºùóøùó≤: all the features are shared\\n& versioned from a central feature store  \\n  \\nùü≤\\\\. ùó†ùóºùó±ùó≤ùóπ ùóòùòÖùóΩùóπùóÆùó∂ùóªùóÆùóØùó∂ùóπùó∂ùòÅùòÜ: a human can understand the reasoning of the model\\nand not treat it as a black box  \\n  \\nùü≥\\\\. ùóî/ùóï ùòÅùó≤ùòÄùòÅùó∂ùóªùó¥ & ùó≥ùó≤ùó≤ùó±ùóØùóÆùó∞ùó∏ ùóπùóºùóºùóΩ: inputs & outputs of the model are stored\\nautomatically and A/B testing is performed regularly  \\n  \\n.  \\n  \\n‚Ü≥ Check out the entire questionnaire on the\\n\\nMarvelousMLOps\\n\\nblog: üîó MLOps maturity assessment\\n\\n**MLOps Maturity Assessment by Marvelous MLOps**\\n\\nWhat level of MLOps maturity is your organization at? For now, you will rarely\\nsee 100%.\\n\\n* * *\\n\\n### Advanced RAG\\n\\nRAG systems are far from perfect ‚Üí This free course teaches you how to improve\\nyour RAG system.  \\n  \\nI recently finished the ùóîùó±ùòÉùóÆùóªùó∞ùó≤ùó± ùó•ùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùó≥ùóºùóø ùóîùóú ùòÑùó∂ùòÅùóµ ùóñùóµùóøùóºùó∫ùóÆ free course from\\nDeepLearning.AI\\n\\nSS from the Advanced Retrieval for AI with Chroma course\\n\\nIf you are into RAG, I find it among the most valuable learning sources.  \\n  \\nThe course already assumes you know what RAG is.  \\n  \\nIts primary focus is to show you all the current issues of RAG and why it is\\nfar from perfect.  \\n  \\nAfterward, it shows you the latest SoTA techniques to improve your RAG system,\\nsuch as:  \\n\\\\- query expansion  \\n\\\\- cross-encoder re-ranking  \\n\\\\- embedding adaptors  \\n  \\nI am not affiliated with DeepLearning.AI (I wouldn\\'t mind though).  \\n  \\nThis is a great course you should take if you are into RAG systems.  \\n  \\nThe good news is that it is free and takes only 1 hour.  \\n  \\nCheck it out ‚Üì\\n\\nAdvanced Retrieval for AI with Chroma\\n\\n12\\n\\nShare this post\\n\\n#### My monthly recommendations for leveling up in ML\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/my-ml-monthly-learning-resource-recommendations?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('ab66f3dc-2957-4ab9-9ed7-ece653d3f725'), content={'Title': 'End-to-End Framework for Production-Ready LLMs', 'Subtitle': 'FREE course on designing, training, deploying, and monitoring a production-ready LLM system powered by LLMs, vector DBs & LLMOps by building your LLM twin.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\n### From data gathering to productionizing LLMs using LLMOps good practices.\\n\\nPaul Iusztin\\n\\nMar 28, 2024\\n\\n35\\n\\nShare this post\\n\\n#### An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _‚Üí the 1st out of 11 lessons of**the LLM Twin** free course_\\n\\n**What is your LLM Twin?** It is an AI character that writes like yourself by\\nincorporating your style, personality and voice into an LLM.\\n\\nImage by DALL-E\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> More **details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà\\n\\nAre you ready to build your AI replica? ü´¢\\n\\n**Let‚Äôs start** with **Lesson 1** ‚Üì‚Üì‚Üì\\n\\n* * *\\n\\n### **Lesson 1: End-to-end framework for production-ready LLM systems**\\n\\nIn the **first lesson** , we will present**** the **project** you will\\n**build** **during** **the** **course** :  _your production-ready LLM Twin/AI\\nreplica._\\n\\n**Afterward** , we will **dig into** the **LLM project system design**.\\n\\nWe will **present** all our **architectural decisions** regarding the design\\nof the _data collection pipeline_ for social media data and how we applied\\n_the 3-pipeline architecture_ to our LLM microservices.\\n\\nIn the **following lessons** , we will **examine** **each component‚Äôs code**\\nand learn **how** to **implement** and **deploy** **it** to AWS and Qwak.\\n\\nLLM twin system architecture [Image by the Author] ‚Üí What you will learn to\\nbuild during this course.\\n\\n### **Table of Contents**\\n\\n  1. What are you going to build? The LLM twin concept\\n\\n  2. LLM twin system design\\n\\n* * *\\n\\n### **1\\\\. What are you going to build? The LLM twin concept**\\n\\nThe **outcome** of this **course** is to learn to **build** your **own AI\\nreplica**. We will use an LLM to do that, hence the name of the course: _**LLM\\nTwin: Building Your Production-Ready AI Replica.**_\\n\\n**But what is an LLM twin?**\\n\\nShortly, your LLM twin will be an AI character who writes like you, using your\\nwriting style and personality.\\n\\nIt will not be you. It will be your writing copycat.\\n\\nMore concretely, you will build an AI replica that writes social media posts\\nor technical articles (like this one) using your own voice.\\n\\n**Why not directly use ChatGPT? You may ask‚Ä¶**\\n\\nWhen trying to generate an article or post using an LLM, the results tend to\\nbe:\\n\\n  * very generic and unarticulated,\\n\\n  * contain misinformation (due to hallucination),\\n\\n  * require tedious prompting to achieve the desired result.\\n\\n_**But here is what we are going to do to fix that** _‚Üì‚Üì‚Üì\\n\\n**First** , we will fine-tune an LLM on your digital data gathered from\\nLinkedIn, Medium, Substack and GitHub.\\n\\nBy doing so, the LLM will align with your writing style and online\\npersonality. It will teach the LLM to talk like the online version of\\nyourself.\\n\\nOur use case will focus on an LLM twin who writes social media posts or\\narticles that reflect and articulate your voice.\\n\\n**Secondly** , we will give the LLM access to a vector DB to access external\\ninformation to avoid hallucinating.\\n\\n**Ultimately** , in addition to accessing the vector DB for information, you\\ncan provide external links that will act as the building block of the\\ngeneration process.\\n\\nExcited? Let‚Äôs get started üî•\\n\\n* * *\\n\\n### **2\\\\. LLM Twin System design**\\n\\nLet‚Äôs understand how to **apply the 3-pipeline architecture** to **our LLM\\nsystem**.\\n\\nThe **architecture** of the **LLM twin** is split into **4 Python\\nmicroservices** :\\n\\n  1. The data collection pipeline\\n\\n  2. The feature pipeline\\n\\n  3. The training pipeline\\n\\n  4. The inference pipeline\\n\\nLLM twin system architecture [Image by the Author]\\n\\n_Now,**let‚Äôs zoom in** on **each component** to understand how they work\\nindividually and interact with each other. ‚Üì‚Üì‚Üì_\\n\\n### **2.1. The data collection pipeline**\\n\\nIts scope is to **crawl data** for **a given user** from:\\n\\n  * Medium (articles)\\n\\n  * Substack (articles)\\n\\n  * LinkedIn (posts)\\n\\n  * GitHub (code)\\n\\nAs every platform is unique, we implemented a different Extract Transform Load\\n(ETL) pipeline for each website.\\n\\nHowever, the **baseline steps** are the **same** for **each platform**.\\n\\n_Thus, for each ETL pipeline, we can abstract away the following baseline\\nsteps:_\\n\\n  * log in using your credentials\\n\\n  * use _selenium_ to crawl your profile\\n\\n  * use _BeatifulSoup_ to parse the HTML\\n\\n  * clean & normalize the extracted HTML\\n\\n  * save the normalized (but still raw) data to Mongo DB\\n\\n> **Important note:** We are crawling only our data, as most platforms do not\\n> allow us to access other people‚Äôs data due to privacy issues. But this is\\n> perfect for us, as to build our LLM twin, we need only our own digital data.\\n\\n**Why Mongo DB?**\\n\\nWe wanted a NoSQL database that quickly allows us to store unstructured data\\n(aka text).\\n\\n**How will the data pipeline communicate with the feature pipeline?**\\n\\nWe will use the **Change Data Capture (CDC) pattern** to inform the feature\\npipeline of any change on our Mongo DB.\\n\\nTo **explain** the **CDC** briefly, a watcher listens 24/7 for any CRUD\\noperation that happens to the Mongo DB.\\n\\nThe watcher will issue an event informing us what has been modified. We will\\nadd that event to a RabbitMQ queue.\\n\\nThe feature pipeline will constantly listen to the queue, process the\\nmessages, and add them to the Qdrant vector DB.\\n\\nFor example, when we write a new document to the Mongo DB, the watcher creates\\na new event. The event is added to the RabbitMQ queue; ultimately, the feature\\npipeline consumes and processes it.\\n\\n**Where will the data pipeline be deployed?**\\n\\nThe data collection pipeline and RabbitMQ service will be deployed to AWS. We\\nwill also use the freemium serverless version of Mongo DB.\\n\\n### **2.2. The feature pipeline**\\n\\nThe feature pipeline is **implemented usingBytewax** (a Rust streaming engine\\nwith a Python interface). Thus, in **our** specific **use case** , we will\\nalso **refer to it** as a **streaming ingestion pipeline**.\\n\\nIt is an **entirely different service** than the data collection pipeline.\\n\\n**How does it communicate with the data pipeline?**\\n\\nAs explained above, the **feature pipeline communicates** with the **data**\\n**pipeline** through a RabbitMQ **queue**.\\n\\nCurrently, the streaming pipeline doesn‚Äôt care how the data is generated or\\nwhere it comes from.\\n\\nIt knows it has to listen to a given queue, consume messages from there and\\nprocess them.\\n\\nBy doing so, we **decouple** **the two components** entirely.\\n\\n**What is the scope of the feature pipeline?**\\n\\nIt represents the **ingestion component** of the **RAG system**.\\n\\nIt will **take** the **raw data** passed through the queue and:\\n\\n  * clean the data;\\n\\n  * chunk it;\\n\\n  * embed it using the embedding models from Superlinked;\\n\\n  * load it to the Qdrant vector DB.\\n\\n**What data will be stored?**\\n\\nThe **training pipeline** will have **access** **only** to the **feature\\nstore** , which, in our case, is represented by the Qdrant vector DB.\\n\\n_With this in mind, we will**store** in Qdrant **2 snapshots of our data:**_\\n\\n1\\\\. The **cleaned data** (without using vectors as indexes ‚Äî store them in a\\nNoSQL fashion).\\n\\n2\\\\. The **cleaned, chunked, and embedded data** (leveraging the vector indexes\\nof Qdrant)\\n\\nThe **training pipeline** needs **access** to the **data** in**both formats**\\nas we want to fine-tune the LLM on standard and augmented prompts.\\n\\n**Why implement a streaming pipeline instead of a batch pipeline?**\\n\\nThere are **2 main reasons.**\\n\\nThe first one is that, coupled with the **CDC pattern** , it is the most\\n**efficient** way to **sync two DBs** between each other.\\n\\nUsing CDC + a streaming pipeline, you process only the changes to the source\\nDB without any overhead.\\n\\nThe second reason is that by doing so, your **source** and **vector DB** will\\n**always be in sync**. Thus, you will always have access to the latest data\\nwhen doing RAG.\\n\\n**Why Bytewax?**\\n\\n**Bytewax** is a streaming engine built in Rust that exposes a Python\\ninterface. We use Bytewax because it combines Rust‚Äôs impressive speed and\\nreliability with the ease of use and ecosystem of Python. It is incredibly\\nlight, powerful, and easy for a Python developer.\\n\\n**Where will the feature pipeline be deployed?**\\n\\nThe feature pipeline will be deployed to AWS. We will also use the freemium\\nserverless version of Qdrant.\\n\\n### **2.3. The training pipeline**\\n\\n**How do we have access to the training features?**\\n\\nAs section 2.2 highlights, all the **training data** will be **accessed** from\\nthe **feature store**. In our case, the feature store is the **Qdrant vector\\nDB** that contains:\\n\\n  * the cleaned digital data from which we will create prompts & answers;\\n\\n  * we will use the chunked & embedded data for RAG to augment the cleaned data.\\n\\n_We will implement a different vector DB retrieval client for each of our main\\ntypes of data (posts, articles, code)._\\n\\n**What will the training pipeline do?**\\n\\nThe training pipeline contains a **data-to-prompt layer** that will preprocess\\nthe data retrieved from the vector DB into prompts.\\n\\nIt will also contain an **LLM fine-tuning module** that inputs a HuggingFace\\ndataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral).\\n\\nAll the experiments will be logged into Comet ML‚Äôs **experiment tracker**.\\n\\nWe will use a bigger LLM (e.g., GPT4) to **evaluate** the results of our fine-\\ntuned LLM. These results will be logged into Comet‚Äôs experiment tracker.\\n\\n**Where will the production candidate LLM be stored?**\\n\\nWe will compare multiple experiments, pick the best one, and issue an LLM\\nproduction candidate for the model registry.\\n\\nAfter, we will inspect the LLM production candidate manually using Comet‚Äôs\\nprompt monitoring dashboard.\\n\\n**Where will the training pipeline be deployed?**\\n\\nThe training pipeline will be deployed to Qwak.\\n\\nQwak is a serverless solution for training and deploying ML models. It makes\\nscaling your operation easy while you can focus on building.\\n\\nAlso, we will use the freemium version of Comet ML for the following:\\n\\n  * experiment tracker;\\n\\n  * model registry;\\n\\n  * prompt monitoring.\\n\\n### **2.4. The inference pipeline**\\n\\nThe inference pipeline is the **final component** of the **LLM system**. It is\\nthe one the **clients** will **interact with**.\\n\\nIt will be **wrapped** under a **REST API**. The clients can call it through\\nHTTP requests, similar to your experience with ChatGPT or similar tools.\\n\\n**How do we access the features?**\\n\\nWe will grab the features solely from the feature store. We will use the same\\nQdrant vector DB retrieval clients as in the training pipeline to use the\\nfeatures we need for RAG.\\n\\n**How do we access the fine-tuned LLM?**\\n\\nThe fine-tuned LLM will always be downloaded from the model registry based on\\nits tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).\\n\\n**What are the components of the inference pipeline?**\\n\\nThe first one is the **retrieval client** used to access the vector DB to do\\nRAG.\\n\\nAfter we have a **query to prompt the layer,** that will map the prompt and\\nretrieved documents from Qdrant into a prompt.\\n\\nAfter the LLM generates its answer, we will log it to Comet‚Äôs **prompt\\nmonitoring dashboard** and return it to the clients.\\n\\nFor example, the client will request the inference pipeline to:\\n\\n‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will\\ngo through all the steps above to return the generated post.\\n\\n**Where will the inference pipeline be deployed?**\\n\\nThe inference pipeline will be deployed to Qwak.\\n\\nAs for the training pipeline, we will use a serverless freemium version of\\nComet for its prompt monitoring dashboard.\\n\\n* * *\\n\\n### **Conclusion**\\n\\nThis is the 1st article of the****_**LLM Twin: Building Your Production-Ready\\nAI Replica**_**** free**** course.\\n\\nIn this lesson, we presented what **you will build** during the course.\\n\\nUltimately, we went through the **system design** of the course and presented\\nthe **architecture** of **each microservice** and how they **interact with\\neach other** :\\n\\n  1. The data collection pipeline\\n\\n  2. The feature pipeline\\n\\n  3. The training pipeline\\n\\n  4. The inference pipeline\\n\\nIn **Lesson 2** , we will dive deeper into the **data collection pipeline** ,\\nlearn how to implement crawlers for various social media platforms, clean the\\ngathered data, store it in a Mongo DB, and finally, show you how to deploy it\\nto AWS.\\n\\n> _üîó**Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è_\\n\\n* * *\\n\\n#### This is how we can further help you ü´µ\\n\\nIn the **Decoding ML newsletter** , we want to keep things **short & sweet**.\\n\\nTo **dive deeper** into all the **concepts** presented in this article‚Ä¶\\n\\n**Check out** the **full-fledged version** of the **article** on our **Medium\\npublication**.\\n\\n**It‚Äôs FREE** ‚Üì‚Üì‚Üì\\n\\n> üîó Detailed Lesson 1 [on Medium]\\n\\n35\\n\\nShare this post\\n\\n#### An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/an-end-to-end-framework-for-production?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('c4ad61cb-4875-41f6-a9d9-f0da74303586'), content={'Title': 'Upskill your LLM knowledge base with these tools.', 'Subtitle': 'Speed-up your LLM inference and dissect the Attention Mechanism with step-by-step animation.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Upskill your LLM knowledge base with these tools.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Upskill your LLM knowledge base with these tools.\\n\\n### Speed-up your LLM inference and dissect the Attention Mechanism with step-\\nby-step animation.\\n\\nAlex Razvant\\n\\nMar 23, 2024\\n\\n10\\n\\nShare this post\\n\\n#### Upskill your LLM knowledge base with these tools.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nThe **LLM-Twin Course** development has taken off! üöÄ\\n\\nJoin aboard and learn how to design, build, and implement an end-to-end LLM\\nreplica, by following along in a step-by-step hands-on manner with the\\ndevelopment of data pipelines, ingestion, LLM fine-tuning, serving,\\nmonitoring, and more.\\n\\nDecoding ML Newsletter is a reader-supported publication. To receive new posts\\nand support my work, consider becoming a free or paid subscriber.\\n\\nSubscribe\\n\\nThe first 2/11 lessons are out, make sure to check them out here:\\n\\n  * Lesson 1: **An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin**\\n\\n  * Lesson 2: **The Importance of Data Pipelines in the Era of Generative AI**\\n\\n* * *\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * **Fast inference on LLMs**\\n\\n  * **Visualize attention mechanism**\\n\\n  * **A commonly misunderstood CUDA issue!**\\n\\n* * *\\n\\n### Fast inference LLMs\\n\\nFor the last few years, LLMs have been a hot topic - new models, RAGs, new\\npapers, the rise of OpenSource models, etc.  \\nThe attention mechanism is easy to understand, but ‚Äúhungry‚Äù to compute - thus\\nmultiple methods aim to fill the performance gap in model-serving.\\n\\nHere are the top 4 LLM inference solutions:\\n\\n  1. ùòÉùóüùóüùó†  \\nA fast and easy-to-use library for LLM inference and serving.\\n\\nùôÜùôöùôÆ ùôñùô®ùô•ùôöùôòùô©ùô® ùôñùôßùôö:\\n\\n     * ‚ûù is open-source \\n\\n     * ‚ûù state-of-the-art serving throughput \\n\\n     * ‚ûù fast model execution with optimized CUDA kernels/graph. \\n\\n     * ‚ûù efficient memory management using PagedAttention \\n\\n     * ‚ûù support for AMD GPUs (ROCm) ‚ûù deploy support with NVIDIA Triton, KServe, Docker\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/nAFPW\\n\\n  2. ùóßùó≤ùóªùòÄùóºùóøùó•ùóß-ùóüùóüùó†  \\nA library that accelerates and optimizes inference performance of the latest\\nLLMs.\\n\\nùôÜùôöùôÆ ùôñùô®ùô•ùôöùôòùô©ùô® ùôñùôßùôö:\\n\\n     * ‚ûù is open-source \\n\\n     * ‚ûù built on a strong TensorRT foundation \\n\\n     * ‚ûù leverages custom-optimized CUDA kernels for transformers ‚ûù enhances customization \\n\\n     * ‚ûù supports various optimization (quant, tensor parallelism) \\n\\n     * ‚ûù takes advantage of the NVIDIA Toolkit (perf-analyzer, Triton)\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/dluMX\\n\\n  3. ùó¢ùóπùóπùóÆùó∫ùóÆ   \\nA tool that allows you to run open-source language models locally.\\n\\nùóûùó≤ùòÜ ùóÆùòÄùóΩùó≤ùó∞ùòÅùòÄ ùóÆùóøùó≤:\\n\\n     * ‚ûù multi-modal model support \\n\\n     * ‚ûù optimizes setup and configuration details, including GPU usage \\n\\n     * ‚ûù bundles weights, configuration, and data into a single Modelfile package\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/dGZ46\\n\\n  4. ùóñùóµùóÆùòÅ ùòÑùó∂ùòÅùóµ ùó•ùóßùó´\\n\\nA solution from NVIDIA that allows users to build their own personalized\\nchatbot experience.\\n\\nùôÜùôöùôÆ ùôñùô®ùô•ùôöùôòùô©ùô® ùôñùôßùôö:\\n\\n     * ‚ûù emphasizes no-code, ChatGPT-like interface \\n\\n     * ‚ûù one can connect custom documents, videos, notes, and PDFs ‚ûù easy to set up RAG (Retrieval Augmented Generation) \\n\\n     * ‚ûù support for the latest LLMs \\n\\n     * ‚ûù leverages TensorRT-LLM and RTX acceleration \\n\\n     * ‚ûù downloadable installer (35GB), out-of-the-box Mistral & LLaMA 7b versions\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/ekuK6\\n\\n* * *\\n\\n### Visualize attention mechanism\\n\\nùóüùóüùó† models are complex - the key to understanding the process is the ùóÆùòÅùòÅùó≤ùóªùòÅùó∂ùóºùóª\\nùó∫ùó≤ùó∞ùóµùóÆùóªùó∂ùòÄùó∫.\\n\\nHere are ùüØ ùòÅùóºùóºùóπùòÄ to help you interactively visualize attention:\\n\\n  1. ùóîùòÅùòÅùó≤ùóªùòÅùó∂ùóºùóªùó©ùó∂ùòá : shorturl.at/DSY58\\n\\n    1. ùò§ùò∞ùòØùòßùò™ùò®ùò∂ùò≥ùò¢ùò£ùò≠ùò¶ ùòØùò∂ùòÆ ùò©ùò¶ùò¢ùò•ùò¥.\\n\\n    2. ùò§ùò∞ùòØùòßùò™ùò®ùò∂ùò≥ùò¢ùò£ùò≠ùò¶ ùòØùò∂ùòÆ ùò≠ùò¢ùò∫ùò¶ùò≥ùò¥.\\n\\n    3. ùò©ùò¢ùò¥ ùòùùò™ùòõ, ùòâùòåùòôùòõ, ùòéùòóùòõ2 ùò™ùòØùò§ùò≠ùò∂ùò•ùò¶ùò•.\\n\\n    4. ùüÆùóó visualization + ùüØùóó ùòªùò∞ùò∞ùòÆ-ùò™ùòØùò¥ ùò∞ùòØ ùò¥ùò¶ùò≠ùò¶ùò§ùòµùò¶ùò• ùò≠ùò¢ùò∫ùò¶ùò≥ùò¥.\\n\\n  2. ùó£ùòÜùóßùóºùóøùó∞ùóµ ùó†ùó†: shorturl.at/lqJQY\\n\\n     * ùò§ùò∂ùò¥ùòµùò∞ùòÆ ùò∞ùò±ùò¶ùò≥ùò¢ùòµùò™ùò∞ùòØùò¥.\\n\\n     * ùò¶ùòπùòµùò¶ùòØùò¥ùò™ùò£ùò≠ùò¶ ùò™ùòØ ùò®ùò≥ùò¢ùò±ùò©-ùò≠ùò™ùò¨ùò¶ ùòßùò¢ùò¥ùò©ùò™ùò∞ùòØ.\\n\\n     * ùò©ùò¢ùò¥ ùòéùòóùòõ2-ùòØùò¢ùòØùò∞, ùòìùò∞ùòôùòà ùòõùò¶ùò§ùò©ùòØùò™ùò≤ùò∂ùò¶ ùò™ùòØùò§ùò≠ùò∂ùò•ùò¶ùò•.\\n\\n     * 3D\\n\\n  3. ùóïùóïùòÜùóñùóøùóºùó≥ùòÅ: shorturl.at/ivCR1\\n\\n     * ùò™ùòØùò¥ùò±ùò¶ùò§ùòµ ùò¥ùòµùò¶ùò±-ùò£ùò∫-ùò¥ùòµùò¶ùò± 1 ùòµùò∞ùò¨ùò¶ùòØ ùò±ùò≥ùò¶ùò•ùò™ùò§ùòµùò™ùò∞ùòØ.\\n\\n     * ùò©ùò¢ùò¥ ùòéùòóùòõ2-ùò¥ùòÆùò¢ùò≠ùò≠, ùòéùòóùòõ3, ùòéùòóùòõ-ùòØùò¢ùòØùò∞, ùòéùòóùòõ2-ùòüùòì ùò™ùòØùò§ùò≠ùò∂ùò•ùò¶ùò•.\\n\\n     * straight-forward\\n\\n* * *\\n\\n### A commonly misunderstood CUDA issue!\\n\\nThe problem was that ùóªùòÉùó∂ùó±ùó∂ùóÆ-ùòÄùó∫ùó∂ was showing a ùó±ùó∂ùó≥ùó≥ùó≤ùóøùó≤ùóªùòÅ ùóöùó£ùó® ùó±ùó≤ùòÉùó∂ùó∞ùó≤ ùóºùóøùó±ùó≤ùóø\\ncompared to docker or Python. Thus, errors regarding the disjoint memory\\nregions appeared.\\n\\nùóõùó≤ùóøùó≤\\'ùòÄ ùòÅùóµùó≤ ùòÅùóøùó∂ùó∞ùó∏:\\n\\n  * ùó¶ùòÜùòÄùòÅùó≤ùó∫ ùóüùóÆùòÜùó≤ùóø\\n\\n    * ùô£ùô´ùôûùôôùôûùôñ-ùô®ùô¢ùôû works at the system level and orders GPU ùôßùôöùô®ùô•ùôöùôòùô©ùôûùô£ùôú ùô©ùôùùôö ùô©ùô§ùô•-ùôôùô§ùô¨ùô£ ùô§ùôßùôôùôöùôß ùô§ùôõ ùôùùô§ùô¨ ùô©ùôùùôö ùô•ùôùùôÆùô®ùôûùôòùôñùô° ùô´ùôûùôôùôöùô§ ùôòùôñùôßùôô ùôûùô® ùôûùô£ùô®ùôöùôßùô©ùôöùôô ùôûùô£ùô©ùô§ ùô©ùôùùôö ùôãùòæùôÑ_ùôÄùôìùôãùôçùôÄùôéùôé ùô®ùô°ùô§ùô©ùô® ùô§ùô£ ùô©ùôùùôö ùô¢ùô§ùô©ùôùùôöùôßùôóùô§ùôñùôßùôô.\\n\\n  * ùó¶ùóºùó≥ùòÅùòÑùóÆùóøùó≤ ùóüùóÆùòÜùó≤ùóø\\n\\n    * At this layer, python/docker or any other program, by default is seeing the ùôÇùôãùôêùô® ùôûùô£ ùô©ùôùùôö \"ùôÅùòºùôéùôèùôÄùôéùôè_ùôÅùôÑùôçùôéùôè\" ùô§ùôßùôôùôöùôß, meaning it will take the ùôÇùôãùôê ùô¨ùôûùô©ùôù ùô©ùôùùôö ùôùùôûùôúùôùùôöùô®ùô© ùòæùòæ (ùôòùô™ùôôùôñ ùôòùôñùô•ùôñùôóùôûùô°ùôûùô©ùôÆ) ùô§ùô£ ùô©ùôùùôö ùôõùôûùôßùô®ùô© ùôûùô£ùôôùôöùô≠.\\n\\nThe solution here is to condition the applications at the Software Layer to\\nrespect the System Layer ordering by setting the env variable:\\n\\n    \\n    \\n    ùòæùôêùòøùòº_ùòøùôÄùôëùôÑùòæùôÄùôé_ùôäùôçùòøùôÄùôç = \"ùôãùòæùôÑ_ùòΩùôêùôé_ùôÑùòø\"\\n\\nDecoding ML Newsletter is a reader-supported publication. To receive new posts\\nand support my work, consider becoming a free or paid subscriber.\\n\\nSubscribe\\n\\n10\\n\\nShare this post\\n\\n#### Upskill your LLM knowledge base with these tools.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/upskill-your-llm-knowledge-base-with?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('4d1d7d1c-ebd2-445e-a8d7-bdfc1c90cfc6'), content={'Title': 'An end-to-end framework for production-ready LLM systems', 'Subtitle': 'Learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Learn an end-to-end framework for production-ready LLM systems by\\nbuilding your LLM twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Learn an end-to-end framework for production-ready LLM systems by building\\nyour LLM twin\\n\\n### Why you should take our new production-ready LLMs course\\n\\nPaul Iusztin\\n\\nMar 16, 2024\\n\\n18\\n\\nShare this post\\n\\n#### Learn an end-to-end framework for production-ready LLM systems by\\nbuilding your LLM twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nWant to ùóπùó≤ùóÆùóøùóª an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ for ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ by\\nùóØùòÇùó∂ùóπùó±ùó∂ùóªùó¥ your ùóüùóüùó† ùòÅùòÑùó∂ùóª?\\n\\nThen you are in luck.\\n\\n‚Üì‚Üì‚Üì\\n\\nThe Decoding ML team and I will ùóøùó≤ùóπùó≤ùóÆùòÄùó≤ (in a few days) a ùóôùó•ùóòùóò ùó∞ùóºùòÇùóøùòÄùó≤ called\\nthe ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ.\\n\\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóÆùóª ùóüùóüùó† ùóßùòÑùó∂ùóª? It is an AI character that learns to write like somebody\\nby incorporating its style and personality into an LLM.\\n\\n> **Within** the**course,** you**** will**learn how** to**:**\\n>\\n>   * architect\\n>\\n>   * train\\n>\\n>   * deploy\\n>\\n>\\n\\n>\\n> ...a ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÅùòÑùó∂ùóª of yourself powered by LLMs, vector DBs, and\\n> LLMOps good practices, such as:\\n>\\n>   * experiment trackers\\n>\\n>   * model registries\\n>\\n>   * prompt monitoring\\n>\\n>   * versioning\\n>\\n>   * deploying LLMs\\n>\\n>\\n\\n>\\n> ...and more!\\n\\nIt is an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùóüùóüùó† ùó∞ùóºùòÇùóøùòÄùó≤ where you will ùóØùòÇùó∂ùóπùó± a ùóøùó≤ùóÆùóπ-ùòÑùóºùóøùóπùó± ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫:\\n\\n‚Üí from start to finish\\n\\n‚Üí from data collection to deployment\\n\\n‚Üí production-ready\\n\\n‚Üí from NO MLOps to experiment trackers, model registries, prompt monitoring,\\nand versioning\\n\\nImage by DALL-E\\n\\n* * *\\n\\n### Who is this for?\\n\\n**Audience:** MLE, DE, DS, or SWE who want to learn to engineer production-\\nready LLM systems using LLMOps good principles.\\n\\n**Level:** intermediate\\n\\n**Prerequisites:** basic knowledge of Python, ML, and the cloud\\n\\n### **How will you learn?**\\n\\nThe course contains **11 hands-on written lessons** and the **open-source\\ncode** you can access on GitHub (WIP).\\n\\nYou can read everything at your own pace.\\n\\n### Costs?\\n\\nThe **articles** and **code** are **completely free**. They will always remain\\nfree.\\n\\nThis time, the Medium articles won't be under any paid wall. I want to make\\nthem entirely available to everyone.\\n\\n### **Meet your teachers!**\\n\\nThe course is created under the Decoding ML umbrella by:\\n\\n  * Paul Iusztin | Senior ML & MLOps Engineer\\n\\n  * Alex Vesa | Senior AI Engineer\\n\\n  * Alex Razvant | Senior ML & MLOps Engineer\\n\\n* * *\\n\\n## What will you learn to build?\\n\\nLM twin system architecture [Image by the Author]\\n\\nüêç ùòõùò©ùò¶ ùòìùòìùòî ùò¢ùò≥ùò§ùò©ùò™ùòµùò¶ùò§ùòµùò∂ùò≥ùò¶ ùò∞ùòß ùòµùò©ùò¶ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùò™ùò¥ ùò¥ùò±ùò≠ùò™ùòµ ùò™ùòØùòµùò∞ 4 ùòóùò∫ùòµùò©ùò∞ùòØ ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥:\\n\\nùóßùóµùó≤ ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Crawl your digital data from various social media platforms.\\n\\n\\\\- Clean, normalize and load the data to a NoSQL DB through a series of ETL\\npipelines.\\n\\n\\\\- Send database changes to a queue using the CDC pattern.\\n\\n‚òÅ Deployed on AWS.\\n\\nùóßùóµùó≤ ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Consume messages from a queue through a Bytewax streaming pipeline.\\n\\n\\\\- Every message will be cleaned, chunked, embedded (using Superlinked), and\\nloaded into a Qdrant vector DB in real-time.\\n\\n‚òÅ Deployed on AWS.\\n\\nùóßùóµùó≤ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Create a custom dataset based on your digital data.\\n\\n\\\\- Fine-tune an LLM using QLoRA.\\n\\n\\\\- Use Comet ML's experiment tracker to monitor the experiments.\\n\\n\\\\- Evaluate and save the best model to Comet's model registry.\\n\\n‚òÅ Deployed on Qwak.\\n\\nùóßùóµùó≤ ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Load and quantize the fine-tuned LLM from Comet's model registry.\\n\\n\\\\- Deploy it as a REST API.\\n\\n\\\\- Enhance the prompts using RAG.\\n\\n\\\\- Generate content using your LLM twin.\\n\\n\\\\- Monitor the LLM using Comet's prompt monitoring dashboard .\\n\\n‚òÅ Deployed on Qwak.\\n\\n.\\n\\nùòàùò≠ùò∞ùòØùò® ùòµùò©ùò¶ 4 ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥, ùò∫ùò∞ùò∂ ùò∏ùò™ùò≠ùò≠ ùò≠ùò¶ùò¢ùò≥ùòØ ùòµùò∞ ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò¶ 3 ùò¥ùò¶ùò≥ùò∑ùò¶ùò≥ùò≠ùò¶ùò¥ùò¥ ùòµùò∞ùò∞ùò≠ùò¥:\\n\\n\\\\- Comet ML as your ML Platform\\n\\n\\\\- Qdrant as your vector DB\\n\\n\\\\- Qwak as your ML infrastructure\\n\\n* * *\\n\\nSoon, we will release the first lesson from the ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø\\nùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ\\n\\nTo stay updated...\\n\\nùòæùôùùôöùôòùô† ùôûùô© ùô§ùô™ùô© ùôÇùôûùô©ùôÉùô™ùôó ùôñùô£ùôô ùô®ùô™ùô•ùô•ùô§ùôßùô© ùô™ùô® ùô¨ùôûùô©ùôù ùôñ ‚≠êÔ∏è\\n\\n‚Üì‚Üì‚Üì\\n\\nüîó _**LLM Twin: Building Your Production-Ready AI Replica** Course GitHub\\nRepository_\\n\\n18\\n\\nShare this post\\n\\n#### Learn an end-to-end framework for production-ready LLM systems by\\nbuilding your LLM twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/want-to-learn-an-end-to-end-framework?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('1dbefe69-acbf-4b86-8b52-0670b28dbab4'), content={'Title': 'Fix your messy ML configs in your Python projects', 'Subtitle': '2024 MLOps learning roadmap. Python syntax sugar that will help you write cleaner code.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Fix your messy ML configs in your Python projects\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Fix your messy ML configs in your Python projects\\n\\n### 2024 MLOps learning roadmap. Python syntax sugar that will help you write\\ncleaner code.\\n\\nPaul Iusztin\\n\\nMar 09, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Fix your messy ML configs in your Python projects\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nThis week our main focus will be a classic.\\n\\n> We will discuss Python.\\n>\\n> More concretely how to write cleaner code and applications in Python. üî•\\n\\nIs that even possible? üíÄ\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * My favorite way to implement a configuration layer in Python\\n\\n  * Some Python syntax sugar that will help you write cleaner code\\n\\n  * 2024 MLOps learning roadmap\\n\\n* * *\\n\\nSince creating content, I learned one crucial thing: \"ùòåùò∑ùò¶ùò≥ùò∫ùò£ùò∞ùò•ùò∫ ùò≠ùò™ùò¨ùò¶ùò¥ ùòµùò∞ ùò≥ùò¶ùò¢ùò•\\nùò¢ùòØùò• ùò≠ùò¶ùò¢ùò≥ùòØ ùò•ùò™ùòßùòßùò¶ùò≥ùò¶ùòØùòµùò≠ùò∫.\"\\n\\n> Do you prefer to read content on Medium?\\n\\nThen, you are in luck.\\n\\nDecoding ML is also on Medium.\\n\\n**Substack vs. Medium?**\\n\\nOn Medium, we plan to post more extended and detailed content, while on\\nSubstack, we will write on the same topics but in a shorter and more\\nconcentrated manner.\\n\\nIf you want more code and less talking‚Ä¶\\n\\n _Check out our Medium publication_ üëÄ\\n\\n‚Üì‚Üì‚Üì\\n\\n‚ûî üîó Decoding ML Medium publication\\n\\nüîó Decoding ML Medium publication\\n\\n* * *\\n\\n### My favorite way to implement a configuration layer in Python\\n\\nThis is my favorite way to ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ a ùó∞ùóºùóªùó≥ùó∂ùó¥ùòÇùóøùóÆùòÅùó∂ùóºùóª/ùòÄùó≤ùòÅùòÅùó∂ùóªùó¥ùòÄ ùòÄùòÜùòÄùòÅùó≤ùó∫ in ùó£ùòÜùòÅùóµùóºùóª\\nfor all my apps ‚Üì\\n\\nThe core is based on ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§, a data validation library for Python.\\n\\nMore precisely, on their ùòâùò¢ùò¥ùò¶ùòöùò¶ùòµùòµùò™ùòØùò®ùò¥ class.\\n\\nùó™ùóµùòÜ ùòÇùòÄùó≤ ùòÅùóµùó≤ ùóΩùòÜùó±ùóÆùóªùòÅùó∂ùó∞ ùóïùóÆùòÄùó≤ùó¶ùó≤ùòÅùòÅùó∂ùóªùó¥ùòÄ ùó∞ùóπùóÆùòÄùòÄ?\\n\\n\\\\- you can quickly load values from .ùò¶ùòØùò∑ files (or even ùòëùòöùòñùòï or ùò†ùòàùòîùòì)\\n\\n\\\\- add default values for the configuration of your application\\n\\n\\\\- the MOST IMPORTANT one ‚Üí It validates the type of the loaded variables.\\nThus, you will always be ensured you use the correct variables to configure\\nyour system.\\n\\nùóõùóºùòÑ ùó±ùóº ùòÜùóºùòÇ ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùó∂ùòÅ?\\n\\nIt is pretty straightforward.\\n\\nYou subclass the ùòâùò¢ùò¥ùò¶ùòöùò¶ùòµùòµùò™ùòØùò®ùò¥ class and define all your settings at the class\\nlevel.\\n\\nIt is similar to a Python ùò•ùò¢ùòµùò¢ùò§ùò≠ùò¢ùò¥ùò¥ but with an extra layer of data validation\\nand factory methods.\\n\\nIf you assign a value to the variable, it makes it optional.\\n\\nIf you leave it empty, providing it in your .ùôöùô£ùô´ file is mandatory.\\n\\nùóõùóºùòÑ ùó±ùóº ùòÜùóºùòÇ ùó∂ùóªùòÅùó≤ùó¥ùóøùóÆùòÅùó≤ ùó∂ùòÅ ùòÑùó∂ùòÅùóµ ùòÜùóºùòÇùóø ùó†ùóü ùó∞ùóºùó±ùó≤?\\n\\nYou often have a training configuration file (or inference) into a JSON or\\nYAML file (I prefer YAML files as they are easier to read).\\n\\nYou shouldn\\'t pollute your ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§ settings class with all the\\nhyperparameters related to the module (as they are a lot, A LOT).\\n\\nAlso, to isolate the application & ML settings, the easiest way is to add the\\nùòµùò≥ùò¢ùò™ùòØùò™ùòØùò®_ùò§ùò∞ùòØùòßùò™ùò®_ùò±ùò¢ùòµùò© in your settings and use a ùòõùò≥ùò¢ùò™ùòØùò™ùòØùò®ùòäùò∞ùòØùòßùò™ùò® class to load\\nit independently.\\n\\nDoing so lets you leverage your favorite way (probably the one you already\\nhave in your ML code) of loading a config file for the ML configuration: plain\\nYAML or JSON files, hydra, or other fancier methods.\\n\\nAnother plus is that you can\\'t hardcode the path anywhere on your system. That\\nis a nightmare when you start using git with multiple people.\\n\\npydantic BaseSettings example [Image by the Author]\\n\\nWhat do you say? Would you start using the ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§ ùòâùò¢ùò¥ùò¶ùòöùò¶ùòµùòµùò™ùòØùò®ùò¥ class in your\\nML applications?\\n\\n* * *\\n\\n### Some Python syntax sugar that will help you write cleaner code\\n\\nHere is some ùó£ùòÜùòÅùóµùóºùóª ùòÄùòÜùóªùòÅùóÆùòÖ ùòÄùòÇùó¥ùóÆùóø that will help you ùòÑùóøùó∂ùòÅùó≤ ùó∞ùóπùó≤ùóÆùóªùó≤ùóø ùó∞ùóºùó±ùó≤ ‚Üì\\n\\nI am talking about the ùò∏ùò¢ùò≠ùò≥ùò∂ùò¥ ùò∞ùò±ùò¶ùò≥ùò¢ùòµùò∞ùò≥ denoted by the `:=` symbol.\\n\\nIt was introduced in Python 3.8, but I rarely see it used.\\n\\nThus, as a \"clean code\" freak, I wanted to dedicate a post to it.\\n\\nùó™ùóµùóÆùòÅ ùó±ùóºùó≤ùòÄ ùòÅùóµùó≤ ùòÑùóÆùóπùóøùòÇùòÄ ùóºùóΩùó≤ùóøùóÆùòÅùóºùóø ùó±ùóº?\\n\\nIt\\'s an assignment expression that allows you to assign and return a value in\\nthe same expression.\\n\\nùó™ùóµùòÜ ùòÄùóµùóºùòÇùóπùó± ùòÜùóºùòÇ ùòÇùòÄùó≤ ùó∂ùòÅ?\\n\\nùòäùò∞ùòØùò§ùò™ùò¥ùò¶ùòØùò¶ùò¥ùò¥: It reduces the number of lines needed for variable assignment and\\nchecking, making code more concise.\\n\\nùòôùò¶ùò¢ùò•ùò¢ùò£ùò™ùò≠ùò™ùòµùò∫: It can enhance readability by keeping related logic close,\\nalthough this depends on the context and the reader\\'s familiarity with exotic\\nPython syntax.\\n\\nùôÉùôöùôßùôö ùôñùôßùôö ùô®ùô§ùô¢ùôö ùôöùô≠ùôñùô¢ùô•ùô°ùôöùô®\\n\\n‚Üì‚Üì‚Üì\\n\\n1\\\\. Using the walrus operator, you can directly assign the result of the ùò≠ùò¶ùòØ()\\nfunction inside an if statement.\\n\\n2\\\\. Avoid calling the same function twice in a while loop. The benefit is less\\ncode and makes everything more readable.\\n\\n3\\\\. Another use case arises in list comprehensions where a value computed in a\\nfiltering condition is also needed in the expression body. Before the ùò∏ùò¢ùò≠ùò≥ùò∂ùò¥\\nùò∞ùò±ùò¶ùò≥ùò¢ùòµùò∞ùò≥, if you had to apply a function to an item from a list and filter it\\nbased on some criteria, you had to refactor it to a standard for loop.\\n\\n.\\n\\nWhen writing clean code, the detail matters.\\n\\nThe details make the difference between a codebase that can be read like a\\nbook or one with 10 WTFs / seconds.\\n\\nThe walrus operator examples [Image by the Author]\\n\\nWhat do you think? Does the walrus operator make the Python code more readable\\nand concise?\\n\\n* * *\\n\\n### 2024 MLOps learning roadmap\\n\\nùó™ùóÆùóªùòÅ to ùóπùó≤ùóÆùóøùóª ùó†ùóüùó¢ùóΩùòÄ but got stuck at the 100th tool you think you must know?\\nHere is the ùó†ùóüùó¢ùóΩùòÄ ùóøùóºùóÆùó±ùó∫ùóÆùóΩ ùó≥ùóºùóø ùüÆùü¨ùüÆùü∞ ‚Üì  \\n  \\nùòîùòìùòñùò±ùò¥ ùò∑ùò¥. ùòîùòì ùò¶ùòØùò®ùò™ùòØùò¶ùò¶ùò≥  \\n  \\nIn theory, MLEs focus on deploying models to production while MLOps engineers\\nbuild the platform used by MLEs.  \\n  \\nI think this is heavily dependent on the scale of the company. As the company\\ngets smaller, these 2 roles start to overlap more.  \\n  \\nThis roadmap will teach you how to build such a platform, from programming\\nskills to MLOps components and infrastructure as code.  \\n  \\n.  \\n  \\nHere is the MLOps roadmap for 2024 suggested by\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\n:  \\n  \\nùü≠\\\\. ùó£ùóøùóºùó¥ùóøùóÆùó∫ùó∫ùó∂ùóªùó¥  \\n\\\\- Python & IDEs  \\n\\\\- Bash basics & command line editors  \\n  \\nùüÆ\\\\. ùóñùóºùóªùòÅùóÆùó∂ùóªùó≤ùóøùó∂ùòáùóÆùòÅùó∂ùóºùóª ùóÆùóªùó± ùóûùòÇùóØùó≤ùóøùóªùó≤ùòÅùó≤ùòÄ  \\n\\\\- Docker  \\n\\\\- Kubernetes  \\n  \\nùüØ\\\\. ùó†ùóÆùó∞ùóµùó∂ùóªùó≤ ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ ùó≥ùòÇùóªùó±ùóÆùó∫ùó≤ùóªùòÅùóÆùóπùòÄ  \\n  \\n...until now we laid down the fundamentals. Now let\\'s get into MLOps üî•  \\n  \\nùü∞\\\\. ùó†ùóüùó¢ùóΩùòÄ ùóΩùóøùó∂ùóªùó∞ùó∂ùóΩùóπùó≤ùòÄ  \\n\\\\- reproducible,  \\n\\\\- testable, and  \\n\\\\- evolvable ML-powered software  \\n  \\nùü±\\\\. ùó†ùóüùó¢ùóΩùòÄ ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅùòÄ  \\n\\\\- Version control & CI/CD pipelines  \\n\\\\- Orchestration  \\n\\\\- Experiment tracking and model registries  \\n\\\\- Data lineage and feature stores  \\n\\\\- Model training & serving  \\n\\\\- Monitoring & observability  \\n  \\nùü≤\\\\. ùóúùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ ùóÆùòÄ ùó∞ùóºùó±ùó≤  \\n\\\\- Terraform\\n\\n2024 MLOps Learning Roadmap [Image by the Author]\\n\\nAs a self-learner, I wish I had access to this step-by-step plan when I\\nstarted learning MLOps.  \\n  \\nRemember, you should pick up and tailor this roadmap at the level you are\\ncurrently at.  \\n  \\nFind more details about the roadmap in\\n\\nMaria Vechtomova\\n\\narticle ‚Üì  \\n  \\n‚ûî üîó MLOps roadmap 2024\\n\\n13\\n\\nShare this post\\n\\n#### Fix your messy ML configs in your Python projects\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/my-favorite-way-to-implement-a-configuration?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('ba6ba94f-b2d0-4ad8-9dbc-638f5eb1a081'), content={'Title': 'A Real-time Retrieval System for RAG on Social Media Data', 'Subtitle': 'Use a Bytewax streaming engine to build a real-time ingestion pipeline to populate a Qdrant vector DB. Implement a RAG retrieval client using rerank.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# A Real-time Retrieval System for RAG on Social Media Data\\n\\n### Use a streaming engine to populate a vector DB in real time. Use rerank &\\nUMAP to improve the accuracy of your retrieved documents.\\n\\nPaul Iusztin\\n\\nMar 07, 2024\\n\\n31\\n\\nShare this post\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n4\\n\\nShare\\n\\n> We are putting in a lot of time to create high-quality content. Thus, we\\n> want to make it as convenient as possible for you to read our content.\\n>\\n> That is why we will experiment with the **posting time** and **move** it to\\n> **Thursday** at **3:00 PM CET**.\\n\\nIn this article, you will learn how to build a real-time retrieval system for\\nsocial media data. In our example, we will use only my LinkedIn posts, but our\\nimplementation can easily be extended to other platforms supporting written\\ncontent, such as X, Instagram, or Medium.\\n\\n**In this article, you will learn how to:**\\n\\n  * build a streaming pipeline that ingests LinkedIn posts into a vector DB in real-time\\n\\n  * clean, chunk, and embed LinkedIn posts\\n\\n  * build a retrieval client to query LinkedIn posts\\n\\n  * use a rerank pattern to improve retrieval accuracy\\n\\n  * visualize content retrieved for a given query in a 2D plot using UMAP\\n\\nOur implementation focuses on just the retrieval part of an RAG system. But\\nyou can quickly hook the retrieved LinkedIn posts to an LLM for post analysis\\nor personalized content generation.\\n\\n* * *\\n\\n## Table of Contents:\\n\\n  1. System Design\\n\\n  2. Data\\n\\n  3. Streaming ingestion pipeline\\n\\n  4. Retrieval client\\n\\n  5. Conclusion\\n\\n* * *\\n\\n### 1\\\\. System Design\\n\\nThe architecture of the retrieval system [Image by the Author - in\\ncollaboration with VectorHub].\\n\\nThe retrieval system is based on 2 detached components:\\n\\n  1. the streaming ingestion pipeline\\n\\n  2. the retrieval client\\n\\nThe **streaming ingestion pipeline** runs 24/7 to keep the vector DB synced up\\nwith current raw LinkedIn posts data source, while the **retrieval client** is\\nused in RAG applications to query the vector DB. These 2 components\\n**communicate with each other only through the vector DB**.\\n\\n#### **1.1. The streaming ingestion pipeline**\\n\\nThe streaming ingestion pipeline implements the Change Data Capture (CDC)\\npattern between a data source containing the raw LinkedIn posts and the vector\\nDB used for retrieval.\\n\\nIn a real-world scenario, the streaming pipeline listens to a queue populated\\nby all the changes made to the source database. But because we are focusing\\nprimarily on the retrieval system, we simulate the data within the queue with\\na couple of JSON files.\\n\\nThe streaming pipeline is built in Python using Bytewax, and cleans, chunks,\\nand embeds the LinkedIn posts before loading them into a Qdrant vector DB.\\n\\n**Why do we need a stream engine?**\\n\\nBecause LinkedIn posts (or any other social media data) evolve frequently,\\nyour vector DB can quickly get out of sync. To handle this, you can build a\\nbatch pipeline that runs every minute. But to really minimize data lag, to\\n**make sure your vector DB stays current with new social media posts** , you\\nneed to use a streaming pipeline that **immediately** takes every new item the\\nmoment it\\'s posted, preprocesses it, and loads it into the vector DB.\\n\\n**Why Bytewax?**\\n\\nBytewax is a streaming engine built in Rust that exposes a Python interface.\\nWe use Bytewax because it combines the impressive speed and reliability of\\nRust with the ease of use and ecosystem of Python.\\n\\n#### 1.2. The retrieval client\\n\\nOur retrieval client is a standard Python module that preprocesses user\\nqueries and searches the vector DB for most similar results. Qdrant vector DB\\nlets us decouple the retrieval client from the streaming ingestion pipeline.\\n\\nUsing a semantic-based retrieval system lets us query our LinkedIn post\\ncollection very flexibly. For example, we can retrieve similar posts using a\\nvariety of query types - e.g., posts, questions, sentences.\\n\\nAlso, to improve the retrieval system\\'s accuracy, we use a rerank pattern.\\n\\nLastly, to better understand and explain the retrieval process for particular\\nqueries, we visualize our results on a 2D plot using UMAP.\\n\\n### 2\\\\. Data\\n\\nWe will ingest 215 LinkedIn posts from my Linked profile - Paul Iusztin.\\nThough we simulate the post ingestion step using JSON files, the posts\\nthemselves are authentic.\\n\\nBefore diving into the code, let\\'s take a look at an example LinkedIn post to\\nfamiliarize ourselves with the challenges it will introduce ‚Üì\\n\\n    \\n    \\n    [\\n        {\\n            \"text\": \"ùó™ùóµùóÆùòÅ do you need to ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ an open-source ùóüùóüùó† to create your own ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùó±ùòÉùó∂ùòÄùóºùóø?\\\\nThis is the ùóüùóüùó† ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùó∏ùó∂ùòÅ you must know ‚Üì\\\\nùóóùóÆùòÅùóÆùòÄùó≤ùòÅ\\\\nThe key component of any successful ML project is the data.\\\\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\\\\nThe best approach is to hire a bunch of experts to create it manually.\\\\nBut, for a PoC, that might get expensive & slow.\\\\nThe good news is that a method called \\\\\"ùòçùò™ùòØùò¶ùòµùò∂ùòØùò™ùòØùò® ùò∏ùò™ùòµùò© ùò•ùò™ùò¥ùòµùò™ùò≠ùò≠ùò¢ùòµùò™ùò∞ùòØ\\\\\" exists.\\\\n \\n    ...\\n    Along with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\\\\n‚Ü≥ Beam: üîó\\\\nhttps://lnkd.in/dedCaMDh\\\\n.\\\\n‚Ü≥ To see all these components in action, check out my FREE ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ ùó∞ùóºùòÇùóøùòÄùó≤ & give it a ‚≠ê:  üîó\\\\nhttps://lnkd.in/dZgqtf8f\\\\nhashtag\\\\n#\\\\nmachinelearning\\\\nhashtag\\\\n#\\\\nmlops\\\\nhashtag\\\\n#\\\\ndatascience\",\\n            \"image\": \"https://media.licdn.com/dms/image/D4D10AQHWQzZcToQQ1Q/image-shrink_800/0/1698388219549?e=1705082400&v=beta&t=9mrDC_NooJgD7u7Qk0PmrTGGaZtuwDIFKh3bEqeBsm0\"\\n        }\\n    ]\\n\\nThe following features of the above post are not compatible with embedding\\nmodels. We\\'ll need to find some way of handling them in our preprocessing\\nstep:\\n\\n  * emojis\\n\\n  * bold, italic text\\n\\n  * other non-ASCII characters\\n\\n  * URLs\\n\\n  * content that exceeds the context window limit of the embedding model\\n\\nEmojis and bolded and italic text are represented by Unicode characters that\\nare not available in the vocabulary of the embedding model. Thus, these items\\ncannot be tokenized and passed to the model; we have to remove them or\\nnormalize them to something that can be parsed by the tokenizer. The same\\nholds true for all other non-ASCII characters.\\n\\nURLs take up space in the context window without providing much semantic\\nvalue. Still, knowing that there\\'s a URL in the sentence may add context. For\\nthis reason, we replace all URLs with a [URL] token. This lets us ingest\\nwhatever value the URL\\'s presence conveys without it taking up valuable space.\\n\\n### 3\\\\. Streaming ingestion pipeline\\n\\nLet\\'s dive into the streaming pipeline, starting from the top and working our\\nway to the bottom ‚Üì\\n\\n#### 3.1. The Bytewax flow\\n\\n**The Bytewax flow** transparently conveys all the steps of the streaming\\npipeline.\\n\\nThe first step is ingesting every LinkedIn post from our JSON files. In the\\nnext steps, every map operation has a single responsibility:\\n\\n  * validate the ingested data using a _RawPost pydantic model_\\n\\n  * clean the posts\\n\\n  * chunk the posts; because chunking will output a list of ChunkedPost objects, we use a flat_map operation to flatten them out\\n\\n  * embed the posts\\n\\n  * load the posts to a Qdrant vector DB\\n\\n    \\n    \\n    def build_flow():\\n        embedding_model = EmbeddingModelSingleton()\\n    \\n        flow = Dataflow(\"flow\")\\n    \\n        stream = op.input(\"input\", flow, JSONSource([\"data/paul.json\"]))\\n        stream = op.map(\"raw_post\", stream, RawPost.from_source)\\n        stream = op.map(\"cleaned_post\", stream, CleanedPost.from_raw_post)\\n        stream = op.flat_map(\\n            \"chunked_post\",\\n            stream,\\n            lambda cleaned_post: ChunkedPost.from_cleaned_post(\\n                cleaned_post, embedding_model=embedding_model\\n            ),\\n        )\\n        stream = op.map(\\n            \"embedded_chunked_post\",\\n            stream,\\n            lambda chunked_post: EmbeddedChunkedPost.from_chunked_post(\\n                chunked_post, embedding_model=embedding_model\\n            ),\\n        )\\n        op.inspect(\"inspect\", stream, print)\\n        op.output(\\n            \"output\", stream, QdrantVectorOutput(vector_size=model.embedding_size)\\n        )\\n        \\n        return flow\\n\\n#### 3.2. The processing steps\\n\\nEvery processing step is incorporated into a _pydantic model_. This way, we\\ncan easily validate the data at each step and reuse the code in the retrieval\\nmodule.\\n\\nWe isolate every step of an ingestion pipeline into its own class:\\n\\n  * cleaning\\n\\n  * chunking\\n\\n  * embedding \\n\\nDoing so, we follow the separation of concerns good SWE practice. Thus, every\\nclass has its own responsibility.\\n\\nNow the code is easy to read and understand. Also, it‚Äôs future-proof, as it‚Äôs\\nextremely easy to change or extend either of the 3 steps: cleaning, chunking\\nand embedding.\\n\\nHere is the interface of the _pydantic models_ :\\n\\n    \\n    \\n    class RawPost(BaseModel):\\n        post_id: str\\n        text: str\\n        image: Optional[str]\\n    \\n        @classmethod\\n        def from_source(cls, k_v: Tuple[str, dict]) -> \"RawPost\":\\n            ... # Mapping a dictionary to a RawPost validated pydantic model.\\n    \\n            return cls(...)\\n    \\n    class CleanedPost(BaseModel):\\n        post_id: str\\n        raw_text: str\\n        text: str\\n        image: Optional[str]\\n    \\n        @classmethod\\n        def from_raw_post(cls, raw_post: RawPost) -> \"CleanedPost\":\\n            ... # Cleaning the raw post\\n    \\n            return cls(...)\\n    \\n    \\n    class ChunkedPost(BaseModel):\\n        post_id: str\\n        chunk_id: str\\n        full_raw_text: str\\n        text: str\\n        image: Optional[str]\\n    \\n        @classmethod\\n        def from_cleaned_post(\\n            cls, cleaned_post: CleanedPost, embedding_model: EmbeddingModelSingleton\\n        ) -> list[\"ChunkedPost\"]:\\n            chunks = ... # Compute chunks\\n    \\n            return [cls(...) for chunk in chunks]\\n    \\n    \\n    class EmbeddedChunkedPost(BaseModel):\\n        post_id: str\\n        chunk_id: str\\n        full_raw_text: str\\n        text: str\\n        text_embedding: list\\n        image: Optional[str] = None\\n        score: Optional[float] = None\\n        rerank_score: Optional[float] = None\\n    \\n        @classmethod\\n        def from_chunked_post(\\n            cls, chunked_post: ChunkedPost, embedding_model: EmbeddingModelSingleton\\n        ) -> \"EmbeddedChunkedPost\":\\n            ... # Compute embedding.\\n    \\n            return cls(...)\\n    \\n\\nNow, the data at each step is validated and has a clear structure.\\n\\n**Note:** Providing different types when instantiating a _pydantic_ model will\\nthrow a validation error. For example, if the  _post_id_ is defined as a\\n_string_ , and we try to instantiate an  _EmbeddedChunkedPost_ with a  _None_\\nor  _int_  _post_id_ , it will throw an error.\\n\\n> Check out the full implementation on our üîó GitHub Articles Hub repository.\\n\\n#### 3.3. Load to Qdrant\\n\\nTo load the LinkedIn posts to Qdrant, you have to override Bytewax\\'s\\n_StatelessSinkPartition_ class (which acts as an **output** in a Bytewax\\nflow):\\n\\n    \\n    \\n    class QdrantVectorSink(StatelessSinkPartition):\\n        def __init__(\\n            self,\\n            client: QdrantClient,\\n            collection_name: str\\n        ):\\n            self._client = client\\n            self._collection_name = collection_name\\n    \\n        def write_batch(self, chunks: list[EmbeddedChunkedPost]):\\n            ... # Map chunks to ids, embeddings, and metadata.\\n    \\n            self._client.upsert(\\n                collection_name=self._collection_name,\\n                points=Batch(\\n                    ids=ids,\\n                    vectors=embeddings,\\n                    payloads=metadata,\\n                ),\\n            )\\n\\nWithin this class, you must overwrite the _write_batch()_ method, where we\\nwill serialize every _EmbeddedChunkedPost_ to a format expected by Qdrant and\\nload it to the vector DB.\\n\\n### 4\\\\. Retrieval client\\n\\nHere, we focus on preprocessing a user\\'s query, searching the vector DB, and\\npostprocessing the retrieved posts for maximum results.\\n\\nTo design the retrieval step, we implement a _QdrantVectorDBRetriever_ class\\nto expose all the necessary features for our retrieval client.\\n\\n    \\n    \\n    class QdrantVectorDBRetriever:\\n        def __init__(\\n            self,\\n            embedding_model: EmbeddingModelSingleton,\\n            vector_db_client: QdrantClient,\\n            cross_encoder_model: CrossEncoderModelSingleton\\n            vector_db_collection: str\\n        ):\\n            self._embedding_model = embedding_model\\n            self._vector_db_client = vector_db_client\\n            self._cross_encoder_model = cross_encoder_model\\n            self._vector_db_collection = vector_db_collection\\n    \\n        def search(\\n            self, query: str, limit: int = 3, return_all: bool = False\\n        ) -> Union[list[EmbeddedChunkedPost], dict[str, list]]:\\n            ... # Search the Qdrant vector DB based on the given query.\\n    \\n        def embed_query(self, query: str) -> list[list[float]]:\\n            ... # Embed the given query.\\n    \\n        def rerank(self, query: str, posts: list[EmbeddedChunkedPost]) -> list[EmbeddedChunkedPost]:\\n            ... # Rerank the posts relative to the given query.\\n    \\n        def render_as_html(self, post: EmbeddedChunkedPost) -> None:\\n            ... # Map the embedded post to HTML to display it.\\n\\n#### 4.1. Embed query\\n\\nWe must embed the query in precisely the same way we ingested our posts into\\nthe vector DB. Because the streaming pipeline is written in Python (thanks to\\nBytewax), and every preprocessing operation is modular, we can quickly\\nreplicate all the steps necessary to embed the query.\\n\\n    \\n    \\n    class QdrantVectorDBRetriever:\\n    \\n        ...\\n    \\n        def embed_query(self, query: str) -> list[list[float]]:\\n            cleaned_query = CleanedPost.clean(query)\\n            chunks = ChunkedPost.chunk(cleaned_query, self._embedding_model)\\n            embdedded_queries = [\\n                self._embedding_model(chunk, to_list=True) for chunk in chunks\\n            ]\\n    \\n            return embdedded_queries\\n\\n> Check out the full implementation on our üîó GitHub repository.\\n\\n#### 4.2. Plain retrieval\\n\\nLet‚Äôs try to retrieve a set of posts without using the rerank algorithm.\\n\\n    \\n    \\n    vector_db_retriever = QdrantVectorDBRetriever(\\n        embedding_model=EmbeddingModelSingleton(),\\n        vector_db_client=build_qdrant_client()\\n    )\\n    \\n    query = \"Posts about Qdrant\"\\n    retrieved_results = vector_db_retriever.search(query=query)\\n    for post in retrieved_results[\"posts\"]:\\n        vector_db_retriever.render_as_html(post)\\n\\nHere are the **top 2 retrieved results** sorted using the cosine similarity\\nscore ‚Üì\\n\\n**Result 1:**\\n\\nResult 1 for the \"Posts about Qdrant\" query (without using reranking) [Image\\nby the Author - in collaboration with VectorHub]\\n\\n**Result 2:**\\n\\nResult 2 for the \"Posts about Qdrant\" query (without using reranking) [Image\\nby the Author - in collaboration with VectorHub]\\n\\nYou can see from the results above, that starting from the second post the\\nresults are irrelevant. Even though it has a cosine similarly score of ~0.69\\nthe posts doesn‚Äôt contain any information about Qdrant or vector DBs.\\n\\n**Note:** We looked over the top 5 retrieved results. Nothing after the first\\npost was relevant. We haven‚Äôt added them here as the article is already too\\nlong.\\n\\n#### 4.3. Visualize retrieval\\n\\nTo visualize our retrieval, we implement a dedicated class that uses the UMAP\\ndimensionality reduction algorithm. We have picked UMAP as it preserves the\\ngeometric properties between points (e.g., the distance) in higher dimensions\\nwhen they are projected onto lower dimensions better than its peers (e.g.,\\nPCA, t-SNE).\\n\\nThe _RetrievalVisualizer_ computes the projected embeddings for the entire\\nvector space once. Afterwards, it uses the render() method to project only the\\ngiven query and retrieved posts, and plot them to a 2D graph.\\n\\n    \\n    \\n    class RetrievalVisualizer:\\n        def __init__(self, posts: list[EmbeddedChunkedPost]):\\n            self._posts = posts\\n    \\n            self._umap_transform = self._fit_model(self._posts)\\n            self._projected_post_embeddings = self.project_posts(self._posts)\\n    \\n        def _fit_model(self, posts: list[EmbeddedChunkedPost]) -> umap.UMAP:\\n            umap_transform = ... # Fit a UMAP model on the given posts.\\n    \\n            return umap_transform\\n    \\n        def project_posts(self, posts: list[EmbeddedChunkedPost]) -> np.ndarray:\\n            embeddings = np.array([post.text_embedding for post in posts])\\n    \\n            return self._project(embeddings=embeddings)\\n    \\n        def _project(self, embeddings: np.ndarray) -> np.ndarray:\\n            ... # Project the embeddings to 2D using UMAP.\\n    \\n            return umap_embeddings\\n    \\n        def render(\\n            self,\\n            embedded_queries: list[list[float]],\\n            retrieved_posts: list[EmbeddedChunkedPost],\\n        ) -> None:\\n          ... # Render the given queries & retrieved posts using matplotlib.\\n\\nLet\\'s take a look at the result to see how the _\" Posts about Qdrant\"_ query\\nlooks ‚Üì\\n\\nVisualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (without reranking)\\n[Image by the Author - in collaboration with VectorHub].\\n\\nOur results are not great. You can see how far the retrieved posts are from\\nour query in the vector space.\\n\\nCan we improve the quality of our retrieval system using the **rerank**\\nalgorithm?\\n\\n#### 4.4. Rerank\\n\\nWe use the _reranking_ algorithm to refine our retrieval for the initial\\nquery. Our initial retrieval step - because it used cosine similarity (or\\nsimilar distance metrics) to compute the distance between a query and post\\nembeddings - may have missed more complex (but essential) relationships\\nbetween the query and the documents in the vector space. Reranking leverages\\nthe power of transformer models that are capable of understanding more nuanced\\nsemantic relationships.\\n\\nWe use a **cross-encoder** model to implement the reranking step, so we can\\nscore the query relative to all retrieved posts individually. These scores\\ntake into consideration more complex relationships than cosine similarity can.\\nUnder the hood is a BERT classifier that outputs a number between 0 and 1\\naccording to how similar the 2 given sentences are. The BERT classifier\\noutputs 0 if they are entirely different and 1 if they are a perfect match.\\n\\nBi-Encoder vs. Cross-Encoder [Image by the Author - in collaboration with\\nVectorHub]\\n\\nBut, you might ask, \"_Why not use the**cross-encoder** model from the start if\\nit is that much better?\"_\\n\\nThe answer, in a word, is speed. Using a cross-encoder model to search your\\nwhole collection is much slower than using cosine similarity. To optimize your\\nretrieval, therefore, your reranking process should involve 2 steps:\\n\\n  1. an initial rough retrieval step using cosine similarity, which retrieves the top N items as potential candidates\\n\\n  2. filtering the rough search using the rerank strategy, which retrieves the top K items as your final results\\n\\nThe implementation is relatively straightforward. For each retrieved post, we\\ncreate a pair consisting of the (cleaned) query and the text of the post. We\\ndo this for all retrieved posts, resulting in a list of pairs.\\n\\nNext, we call a _cross-encoder/ms-marco-MiniLM-L-6-v2_ model (from sentence-\\ntransformers) to give the retrieved posts their rerank score. We then sort the\\nposts in descending order based on their rerank score.\\n\\n> Check out the rerank algorithm implementation on our üîó GitHub repository.\\n\\n#### 4.5. Visualize retrieval with rerank\\n\\nNow that we\\'ve added the rerank pattern to our retrieval system, let\\'s see if\\nit improves the results of our _\" Posts about Qdrant\"_ query ‚Üì\\n\\n**Result 1**\\n\\nResult 1 for the \"Posts about Qdrant\" query (using reranking) [Image by the\\nAuthor - in collaboration with VectorHub]\\n\\n**Result 2:**\\n\\nResult 2 for the \"Posts about Qdrant\" query (using reranking) [Image by the\\nAuthor - in collaboration with VectorHub]\\n\\nThe improvement is remarkable! All our results are about Qdrant and vector\\nDBs.\\n\\n**Note:** We looked over the top 5 retrieved results. The top 4 out of 5 posts\\nare relevant to our query, which is incredible.\\n\\nNow, let\\'s look at the UMAP visualization:\\n\\nVisualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (with reranking)\\n[Image by the Author - in collaboration with VectorHub].\\n\\nWhile the returned posts aren\\'t very close to the query, they are **a lot\\ncloser to the query compared to when we weren\\'t reranking the retrieved\\nposts**.\\n\\n* * *\\n\\n### 5\\\\. Conclusion\\n\\nIn this article, we learned how to adapt a RAG retrieval pattern to improve\\nLinkedIn post retrieval. To keep our database up to date with rapidly changing\\nsocial media data, we implemented a real-time streaming pipeline that uses CDC\\nto sync the raw LinkedIn posts data source with a vector DB. You also saw how\\nto use Bytewax to write - using only Python - a streaming pipeline that\\ncleans, chunks, and embeds LinkedIn posts.\\n\\nFinally, you learned how to implement a standard retrieval client for RAG and\\nsaw how to improve it using the rerank pattern. As retrieval is complex to\\nevaluate, you saw how to visualize the retrieval for a given query by\\nrendering all the posts, the query, and the retrieved posts in a 2D space\\nusing UMAP.\\n\\n> This **article** is a **summary** __ of **my contribution** from\\n> **VectorHub**. Check out the full article here to **dig** **into** the\\n> **details,** the**code** and **more experiments**.\\n\\n31\\n\\nShare this post\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n4\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| OlaMar 8Liked by Paul IusztinNice read, full of insights.Expand full\\ncommentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n| VenkataMar 23Liked by Paul IusztinExcellent article. Thanks a lot for\\nposting this.Expand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n2 more comments...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/a-real-time-retrieval-system-for?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('cb6e689e-e718-42c8-80b1-44db7d568c3b'), content={'Title': '4 key decoding strategies for LLMs that you must know', 'Subtitle': 'The only 6 prompt engineering techniques you need to know. One thing that I do that sets me apart from the crowd.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### 4 key decoding strategies for LLMs that you must know\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# 4 key decoding strategies for LLMs that you must know\\n\\n### The only 6 prompt engineering techniques you need to know. One thing that\\nI do that sets me apart from the crowd.\\n\\nPaul Iusztin\\n\\nFeb 15, 2024\\n\\n9\\n\\nShare this post\\n\\n#### 4 key decoding strategies for LLMs that you must know\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nHello everyone,\\n\\nI hope you enjoyed what Alex R. & Alex V. have prepared for you in their\\nprevious articles.\\n\\nI promised that the 3 of us would dig deeper into more exciting topics about\\nproduction-ready LLM and CV models.\\n\\n_‚Üí But this is just the beginning. Stay tuned for more production ML_ üî•\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * 4 key decoding strategies for LLMs that you must know\\n\\n  * The only 6 prompt engineering techniques you need to know\\n\\n  * One thing that I do that sets me apart from the crowd\\n\\n* * *\\n\\n> Want to build your first ùóüùóüùó† ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ but don\\'t know where to start?\\n\\nIf you want to **learn** in a **structured** **way** to **build** hands-on\\n**LLM systems** using good **LLMOps** principles‚Ä¶\\n\\nWe want to **announce** that we just **released** **8 Medium lessons** for the\\n**Hands-on LLMs** **course** that will put you on the right track ‚Üì\\n\\nWithin the **8 Medium lessons** , you will go step-by-step through the\\n**theory** , **system** **design** , and **code** to learn how to build a:\\n\\n  * **real-time streaming pipeline** (deployed on AWS) that uses Bytewax as the stream engine to listen to financial news, cleans & embeds the documents, and loads them to a vector DB\\n\\n  * **fine-tuning pipeline** (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\\n\\n  * **inference pipeline** built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news)\\n\\nWe will also show you how to **integrate** various **serverless tools** , such\\nas:  \\n  \\n‚Ä¢ Comet ML as your ML Platform;  \\n‚Ä¢ Qdrant as your vector DB;  \\n‚Ä¢ Beam as your infrastructure.\\n\\nThe architecture of the system you will learn to build during the **Hands-on\\nLLMs** course [Image by the Author].\\n\\n**Who is this for?**\\n\\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM\\nsystems using LLMOps good principles.\\n\\n**How will you learn?**\\n\\nThe series contains 4 hands-on video lessons and the open-source code you can\\naccess on GitHub.\\n\\n**Curious?** ‚Üì\\n\\nCheck out the 8 Medium lessons of the Hands-on LLMs course and start building\\nyour own LLMs system:\\n\\nüîó The Hands-on LLMs Medium Series\\n\\n* * *\\n\\n### 4 key decoding strategies for LLMs that you must know\\n\\nYou see, LLMs don\\'t just spit out text.  \\n  \\nThey calculate \"logits\", which are mapped to probabilities for every possible\\ntoken in their vocabulary.  \\n  \\nIt uses previous token IDs to predict the next most likely token (the auto-\\nregressive nature of decoder models).  \\n  \\nThe real magic happens in the decoding strategy you pick ‚Üì  \\n  \\n\\\\- Greedy Search  \\n\\\\- Beam Search  \\n\\\\- Top-K Sampling  \\n\\\\- Nucleus Sampling  \\n  \\n.  \\n  \\nùóöùóøùó≤ùó≤ùó±ùòÜ ùó¶ùó≤ùóÆùóøùó∞ùóµ  \\n  \\nIt only holds onto the most likely token at each stage. It\\'s fast and\\nefficient, but it is short-sighted.  \\n  \\nùóïùó≤ùóÆùó∫ ùó¶ùó≤ùóÆùóøùó∞ùóµ  \\n  \\nThis time, you are not looking at just the token with the highest probability.\\nBut you are considering the N most likely tokens.  \\n  \\nThis will create a tree-like structure, where each node will have N children.  \\n  \\nThe procedure repeats until you hit a maximum length or an end-of-sequence\\ntoken.  \\n  \\nUltimately, you pick the leaf with the biggest score and recursively pick its\\nparent until you hit the root node.  \\n  \\nFor example, in the graph below, we have \"ùò£ùò¶ùò¢ùòÆùò¥ = 2\" and \"ùò≠ùò¶ùòØùò®ùòµùò© = 3\".  \\n  \\nùóßùóºùóΩ-ùóû ùó¶ùóÆùó∫ùóΩùóπùó∂ùóªùó¥  \\n  \\nThis technique extends the Beam search strategy and adds a dash of randomness\\nto the generation process.  \\n  \\nInstead of just picking the most likely tokens, it\\'s selecting a token\\nrandomly from the top k most likely choices.  \\n  \\nThus, the tokens with the highest probability will appear more often, but\\nother tokens will be generated occasionally to add some randomness\\n(\"creativity\").  \\n  \\nùó°ùòÇùó∞ùóπùó≤ùòÇùòÄ ùó¶ùóÆùó∫ùóΩùóπùó∂ùóªùó¥  \\n  \\nIn this case, you\\'re not just picking the top k most probable tokens here.\\nYou\\'re picking a cutoff value _p_ and forming a \"nucleus\" of tokens.  \\n  \\nIn other words, rather than selecting the top k most probable tokens, nucleus\\nsampling chooses a cutoff value p such that the sum of the probabilities of\\nthe selected tokens exceeds p.  \\n  \\nThus, at every step, you will have a various number of possible tokens\\nincluded in the \"nucleus\" from which you sample. This introduces even more\\ndiversity and creativity into your output.  \\n  \\n.  \\n  \\nùó°ùóºùòÅùó≤: For ùòµùò∞ùò±-ùò¨ and ùòØùò∂ùò§ùò≠ùò¶ùò∂ùò¥ ùò¥ùò¢ùòÆùò±ùò≠ùò™ùòØùò®, you can also use the \"ùòµùò¶ùòÆùò±ùò¶ùò≥ùò¢ùòµùò¶\"\\nhyperparameter to tweak the output probabilities. It is a parameter that\\nranges from 0 to 1. A low temperature (e.g., 0.1) will decrease the entropy\\n(randomness), making the generation more stable.\\n\\n4 key decoding strategies for LLMs that you must know [Image by the Author].\\n\\nTo summarize...  \\n  \\nThere are 2 main decoding strategies for LLMs:  \\n\\\\- greedy search  \\n\\\\- beam search  \\n  \\nTo add more variability and creativity to beam search, you can use:  \\n\\\\- top-k sampling  \\n\\\\- nucleus sampling\\n\\n* * *\\n\\n### The only 6 prompt engineering techniques you need to know\\n\\nThe whole field of prompt engineering can be reduced to these 6 techniques I\\nuse almost daily when using ChatGPT (or other LLMs).  \\n  \\nHere they are ‚Üì  \\n  \\n#1. ùêÖùêûùê∞ ùê¨ùê°ùê®ùê≠ ùê©ùê´ùê®ùê¶ùê©ùê≠ùê¢ùêßùê†  \\n  \\nAdd in your prompt 2 or 3 high-quality demonstrations, each consisting of both\\ninput and desired output, on the target task.  \\n  \\nThe LLM will better understand your intention and what kind of answers you\\nexpect based on concrete examples.  \\n  \\n#2. ùêíùêûùê•ùêü-ùêúùê®ùêßùê¨ùê¢ùê¨ùê≠ùêûùêßùêúùê≤ ùê¨ùêöùê¶ùê©ùê•ùê¢ùêßùê†  \\n  \\nSample multiple outputs with \"temperature > 0\" and select the best one out of\\nthese candidates.  \\n  \\nHow to pick the best candidate?  \\n  \\nIt will vary from task to task, but here are 2 primary scenarios ‚Üì  \\n  \\n1\\\\. Some tasks are easy to validate, such as programming questions. In this\\ncase, you can write unit tests to verify the correctness of the generated\\ncode.  \\n  \\n2\\\\. For more complicated tasks, you can manually inspect them or use another\\nLLM (or another specialized model) to rank them.  \\n  \\n#3. ùêÇùê°ùêöùê¢ùêß-ùê®ùêü-ùêìùê°ùê®ùêÆùê†ùê°ùê≠ (ùêÇùê®ùêì)  \\n  \\nYou want to force the LLM to explain its thought process, which eventually\\nleads to the final answer, step by step.  \\n  \\nThis will help the LLM to reason complex tasks better.  \\n  \\nYou want to use CoT for complicated reasoning tasks + large models (e.g., with\\nmore than 50B parameters). Simple tasks only benefit slightly from CoT\\nprompting.  \\n  \\nHere are a few methods to achieve CoT:  \\n\\\\- provide a list of bullet points with all the steps you expect the LLM to\\ntake  \\n\\\\- use \"Few shot prompt\" to teach the LLM to think in steps  \\n  \\n... or my favorite: use sentences such as \"Let\\'s think step by step.\"  \\n  \\n#4. ùêÄùêÆùê†ùê¶ùêûùêßùê≠ùêûùêù ùêèùê´ùê®ùê¶ùê©ùê≠ùê¨  \\n  \\nThe LLM\\'s internal knowledge is limited to the data it was trained on. Also,\\noften, it forgets specific details of older training datasets.  \\n  \\nThe most common use case is Retrieval-Augmented Generation (RAG).  \\n  \\nThat is why using the LLM as a reasoning engine is beneficial to parse and\\nextract information from a reliable source of information given as context in\\nthe prompt.  \\n  \\nùòûùò©ùò∫?  \\n\\\\- avoid retraining the model on new data  \\n\\\\- avoid hallucinating  \\n\\\\- access to references on the source  \\n  \\n#5. ùêÄ ùê¨ùê¢ùêßùê†ùê•ùêû ùê´ùêûùê¨ùê©ùê®ùêßùê¨ùê¢ùêõùê¢ùê•ùê¢ùê≠ùê≤ ùê©ùêûùê´ ùê©ùê´ùê®ùê¶ùê©ùê≠  \\n  \\nQuite self-explanatory. It is similar to the DRY principle in SWE.  \\n  \\nHaving only x1 task/prompt is good practice to avoid confusing the LLM.  \\n  \\nIf you have more complex tasks, split them into granular ones and merge the\\nresults later in a different prompt.  \\n  \\n#6. ùêÅùêû ùêöùê¨ ùêûùê±ùê©ùê•ùê¢ùêúùê¢ùê≠ ùêöùê¨ ùê©ùê®ùê¨ùê¨ùê¢ùêõùê•ùêû  \\n  \\nThe LLM cannot read your mind. To maximize the probability of getting\\nprecisely what you want, you can imagine the LLM as a 7-year-old to whom you\\nmust explain everything step-by-step to be sure he understood.  \\n  \\nùòïùò∞ùòµùò¶: The level of detail in the prompt is inversely proportional to the size\\n& complexity of the model.\\n\\n[Image generated by DALL-E]\\n\\nThe truth is that prompt engineering is quite intuitive, and we don\\'t have to\\noverthink it too much.  \\n  \\nWhat would you add to this list?\\n\\n* * *\\n\\n### One thing that I do that sets me apart from the crowd\\n\\nHere is one thing that I do that sets me apart from the crowd:  \\n  \\n\"ùòê ùò¢ùòÆ ùò∞ùò¨ùò¢ùò∫ ùò∏ùò™ùòµùò© ùò£ùò¶ùò™ùòØùò® ùòµùò©ùò¶ ùò•ùò∂ùòÆùò± ùò∞ùòØùò¶ ùòµùò©ùò¢ùòµ ùò¢ùò¥ùò¨ùò¥ ùòÆùò¢ùòØùò∫ ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØùò¥.\"  \\n  \\nùêáùê¶ùê¶... ùêñùê°ùê≤?  \\n  \\nThe reality is that even the brightest minds cannot understand everything from\\nthe first shot.  \\n  \\nIt is not necessarily that you cannot understand the concepts.  \\n  \\nThere are other factors, such as:  \\n\\\\- you are tired  \\n\\\\- you haven\\'t paid enough attention  \\n\\\\- the concept wasn\\'t explained at your level  \\n\\\\- the presenter wasn\\'t clear enough, etc.  \\n  \\nAlso, the truth is that many of us don\\'t understand everything from the first\\nshot when presented with a new concept.  \\n  \\nBut because of our ego, we are afraid to come out and ask something because we\\nare worried that we will sound stupid.  \\n  \\nThe jokes are on you.  \\n  \\nMost people will be grateful you broke the ice and asked to explain the\\nconcept again.  \\n  \\nùêñùê°ùê≤?  \\n  \\nIt will help the team to learn the new concepts better.  \\n  \\nIt will start a discussion to dig deeper into the subject.  \\n  \\nIt will piss off or annoy the people you don\\'t like.  \\n  \\nIt will help other people ask questions next time.  \\n  \\nIt will open up new perspectives on the problem.\\n\\nTo conclude...  \\n  \\nIgnore your ego and what people think of you. Own your curiosity and ask\\nquestions when you feel like it.  \\n  \\nIt is ok not to know everything.  \\n  \\nIt is better to be stupid for 5 minutes than your entire life.\\n\\n* * *\\n\\nCongrats on learning something new today!\\n\\n**Don‚Äôt hesitate to share your thoughts - we would love to hear them.**\\n\\n_**‚Üí** Remember, when ML looks **encoded - we‚Äôll help you decode it.**_\\n\\nSee you next Thursday at 9:00 am CET.\\n\\nHave a fantastic weekend!\\n\\n9\\n\\nShare this post\\n\\n#### 4 key decoding strategies for LLMs that you must know\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/4-key-decoding-strategies-for-llms?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('50a5a621-5799-4214-990d-3387ecc704e1'), content={'Title': 'DML: New year, the new & improved Decoding ML - What to expect?', 'Subtitle': 'How we plan to grow, provide more qualitative & hands-on content, and real-world ML projects to expand your professional skills', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: New year, the new & improved Decoding ML - What to expect?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: New year, the new & improved Decoding ML - What to expect?\\n\\n### How we plan to grow, provide more qualitative & hands-on content, and\\nreal-world ML projects to expand your professional skills\\n\\nPaul Iusztin\\n\\n,\\n\\nAlex Razvant\\n\\n, and\\n\\nVesa Alexandru\\n\\nJan 11, 2024\\n\\n10\\n\\nShare this post\\n\\n#### DML: New year, the new & improved Decoding ML - What to expect?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\nThis newsletter will differ from the others as I want to share my plans for\\nthe Decoding ML newsletter with you.\\n\\n> From now on, it will cost $1000/month. **Joking.** It will still be free.\\n> It‚Äôs not about the money but about growth, better quality & added value.\\n\\nTo be 100% transparent with you, I started this newsletter as an experiment,\\nbut when I saw people who actually read it, the perfectionist in me screamed\\nthat I should improve it and move to the next step.\\n\\nThis is the next step. And I‚Äôm taking you with me.\\n\\nThe big news is that I will go all in, pouring more time and resources into\\ngrowing the Decoding ML newsletter. My main goals are to:\\n\\n  * push better-quality content every week\\n\\n  * bring more real-world projects to increase your hands-on skills\\n\\n  * increases the number of articles with code examples to make it practical so you can benefit from it even more at your job \\n\\n> As the world constantly changes, especially AI, MLE & MLOps, you cannot\\n> stagnate. Decoding ML‚Äôs growth is about providing you with all the MLE &\\n> MLOps necessary resources to grow with it and smash it at your projects and\\n> job.\\n\\n* * *\\n    \\n    \\n    _So.. How do I plan to grow the Decoding ML newsletter?_\\n\\n## Well, there are 3 main steps ‚Üì\\n\\n## #1. Rebranding\\n\\nFrom now on, my face will no longer be the ‚Äúlogo‚Äù of Decoding ML.\\n\\nThis will be the new logo of Decoding ML ‚Üì\\n\\nSo you don‚Äôt have to see my annoying face every Thursday morning in your email\\nü§£\\n\\n* * *\\n\\n## #2. Bringing in talent\\n\\nAs I wanted to push more content of higher quality, I had to bring in more\\ntalented people to write beside me.\\n\\nI was lucky enough to know Alex Razvant and Alex Vesa, who are 2 fantastic MLE\\n& MLOps engineers with 10 years of hands-on experience in the AI industry.\\n\\nFrom now on, they will start contributing to the Decoding ML newsletter and\\nteam along with me.\\n\\n> Maybe you know this famous saying: ‚Äú**If you want to go fast, go alone; if\\n> you want to go far, go together**.‚Äù ‚Ä¶and I want Decoding ML to go far.\\n\\nOur primary goal is to help you level up in MLE & MLOps by offering hands-on\\nexamples that you can use at your job.\\n\\nI plan to improve the quality of the articles by including more code and\\nconcrete examples besides the system design talks we have discussed so far.\\n\\n‚Ä¶and here enters the scene ‚ÄúThe Alex‚Äôs‚Äù\\n\\nI have worked with them, and I know they are talented experts with fantastic\\nhands-on MLE & MLOps skills and insights to share with you.\\n\\nStarting from now on, Decoding ML will no longer be a one-person brand but a\\nbrand by itself, hosted by the new Decoding ML team:\\n\\n  * myself\\n\\n  * Alex Vesa\\n\\n  * Alex Razvant\\n\\n### #2.1. Now, let the team introduce itself ‚Üì\\n\\n####  _**Alex Vesa**_\\n\\n _Main niche: ‚ÄúDeep Learning/Computer Vision | ML System Infrastructure | Startups | Business‚Äù_\\n\\n‚Ü≥ üîó LinkedIn  \\n\\nHello everyone,\\n\\n  \\nI‚Äôm very grateful for this opportunity. I consider creativity and inspiration\\nto flourish when there's a merger of minds from various individuals.\\n\\nMy professional journey began in 2015, initially focusing on software\\nengineering with a keen interest in Python and AI technologies. I quickly\\nprogressed, taking on challenging roles and AI projects. My experience in\\nvarious startups as a CTO focused on leading teams in developing innovative\\nsoftware solutions. I worked in multiple sectors, notably healthcare and\\nautomotive, where I've implemented AI-driven systems to enhance operational\\nefficiency.\\n\\nMy technical skills are broad, encompassing Python, Django, and AWS. I'm\\ndedicated to leveraging my AI and software development expertise to drive\\norganizational success in this dynamic field.\\n\\nI value knowledge-sharing among our community, and my objective is to bring\\nsolid expertise in practical, real-world AI/ML systems to help you in your\\nday-to-day work and enhance your creativity and vision in product development.\\n\\nUltimately, I want to share with you the endless capabilities you can possess\\nto evolve.\\n\\n#### _Alex Razvant_\\n\\n _Main niche: ‚ÄúML/CV Systems in Production | MLOps_ /_Edge ML Deployments‚Äù_\\n\\n‚Ü≥ üîó LinkedIn\\n\\nHey everyone,\\n\\nI‚Äôm really happy about this merger, as you‚Äôll get 3X more quality content in a\\nconcise, valuable, and actionable manner directly to your inbox!\\n\\nHere are a few words about who I am:\\n\\nI started my journey as a SWE in 2015, diving into full-stack web development.  \\nAfter a few internships, hackathons, and a few failed projects, the ML field\\ncaught my eye, and I haven‚Äôt looked back ever since.\\n\\nMy journey includes over **15+** successful freelance projects, earning a\\n**Top-Rated** ML Engineer badge on **UpWork** , collaborating with **BMW** on\\nAI for self-driving cars, authoring a paper for IEEE RAL 2020, and developing\\nscalable Computer Vision systems to analyze 1000+ hours of CCTV footage.\\n\\nI aim to bring solid expertise via **code tutorials, diagrams, and system\\ndesigns** to help you overcome challenges in building and deploying ML & CV\\nsystems in cloud or edge environments, following the best practices I‚Äôve\\nlearned in SWE, ML, and MLOps.\\n\\n> _Follow them & check them out on LinkedIn to see their incredible experience\\n> in AI._\\n\\n### #2.2. Will we start approaching different topics?\\n\\n_TL/DR: No!_\\n\\nI was meticulous in bringing in more people with the same vision.\\n\\nThus, Decoding ML will approach the same niche as it has done: _‚Äúproduction-\\nready MLE & MLOps topics.‚Äù_\\n\\nSo‚Ä¶ you don‚Äôt have to unsubscribe. We will keep talking about the same topics\\nyou chose to follow in our newsletter: _‚Äúhands-on MLE & MLOps topics‚Äù_\\n\\nHowever, the advantage of having more people with different backgrounds on the\\nteam is that we all come with different perspectives and domain knowledge.\\n\\nFor example:\\n\\n  * Alex Razvant worked a lot with Computer Vision, Deep Learning, and MLOps technologies in the world of retail\\n\\n  * Alex Vesa has a lot of experience with Deep Learning and infrastructure projects in the medical field\\n\\n  * I am passioned about generative AI, MLOps, and SWE\\n\\n‚Ä¶combining our knowledge will result in exciting production-ready MLE & MLOps\\narticles that will significantly benefit you.\\n\\n* * *\\n\\n## #3. Expanding to new distribution channels\\n\\nEvery person consumes content differently.\\n\\nSo, we'd like to give you the best fit to enjoy our content.\\n\\nWe already started a Decoding ML Medium publication, where we will start this\\nmonth to push a deep dive into the code of the Hands-on LLMs Course.\\n\\n‚Ä¶and slowly, we will expand to video format content on:\\n\\n  * Youtube\\n\\n  * Instagram\\n\\n  * TikTok\\n\\nAlso, we started planning a set of eBooks about MLE, MLOps and LLMOps and a\\nnew course about LLMs and LLMOps.\\n\\n* * *\\n\\n### So‚Ä¶ What happens next?\\n\\nI hope you are excited about the news. For sure, I am üî•\\n\\n>  _Next Thursday at 9:00 a.m. CET_ , **Alex Vesa** will make his **grand\\n> opening** by writing a step-by-step article on **how** you can **deploy an\\n> LLaMA2-7b LLM** using **Amazon SageMaker** and **HuggingFace**.\\n\\nTo conclude, you don‚Äôt have to do anything on your side.\\n\\n_Decoding ML follows its natural course by bringing in more people and\\nexpanding to other platforms to give you more value for your time and a more\\npersonalized way to enjoy our content._\\n\\nSee you next Thursday!\\n\\nHave a fantastic weekend! ‚úåüèª\\n\\nPaul\\n\\n10\\n\\nShare this post\\n\\n#### DML: New year, the new & improved Decoding ML - What to expect?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Ahmed BesbesThe Tech Buffet Jan 11Liked by Paul IusztinGreat things coming\\nahead Paul! Looking forward to it!Expand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-new-year-the-new-and-improved?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('e85a60a3-6667-45fe-81fd-9384322b7cea'), content={'Title': 'DML: 8 types of MLOps tools that must be in your toolbelt to be a successful MLOps engineer', 'Subtitle': 'How to successfully present MLOps ideas to upper management. How I generated PyDocs for 100 Python functions in <1 hour', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 8 types of MLOps tools that must be in your toolbelt to be a successful\\nMLOps engineer\\n\\n### How to successfully present MLOps ideas to upper management. How I\\ngenerated PyDocs for 100 Python functions in <1 hour\\n\\nPaul Iusztin\\n\\nJan 04, 2024\\n\\n18\\n\\nShare this post\\n\\n#### DML: 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\nThe last Hands-on LLM series finished last week. In case you are curious, here\\nare the top 3 out of 9 lessons of the series:\\n\\n  1. Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n  2. Lesson 7: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n\\n  3. Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n* * *\\n\\n#### **This week‚Äôs topics:**\\n\\n  1. 8 types of MLOps tools that must be in your toolbelt to be a successful MLOps engineer\\n\\n  2. How to successfully present MLOps ideas to upper management\\n\\n  3. How I generated PyDocs for 100 Python functions in <1 hour\\n\\n* * *\\n\\n‚Üí Before diving into the topics, I have one important thing to share with you.\\n\\n> We finally finished the code & video lessons for the**Hands-on LLMs** course\\n> üî•\\n\\nBy finishing the **Hands-On LLMs** free course, you will learn how to use the\\n3-pipeline architecture & LLMOps good practices to design, build, and deploy a\\nreal-time financial advisor powered by LLMs & vector DBs.  \\n  \\nWe will primarily focus on the engineering & MLOps aspects.  \\n  \\nThus, by the end of this series, you will know how to build & deploy a real ML\\nsystem, not some isolated code in Notebooks.  \\n  \\nùêåùê®ùê´ùêû ùê©ùê´ùêûùêúùê¢ùê¨ùêûùê•ùê≤, ùê≠ùê°ùêûùê¨ùêû ùêöùê´ùêû ùê≠ùê°ùêû 3 ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ùê¨ ùê≤ùê®ùêÆ ùê∞ùê¢ùê•ùê• ùê•ùêûùêöùê´ùêß ùê≠ùê® ùêõùêÆùê¢ùê•ùêù:  \\n  \\n1\\\\. a ùê´ùêûùêöùê•-ùê≠ùê¢ùê¶ùêû ùê¨ùê≠ùê´ùêûùêöùê¶ùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû (deployed on AWS) that listens to financial\\nnews, cleans & embeds the documents, and loads them to a vector DB  \\n  \\n2\\\\. a ùêüùê¢ùêßùêû-ùê≠ùêÆùêßùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû (deployed as a serverless continuous training) that\\nfine-tunes an LLM on financial data using QLoRA, monitors the experiments\\nusing an experiment tracker and saves the best model to a model registry  \\n  \\n3\\\\. an ùê¢ùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû built in LangChain (deployed as a serverless RESTful\\nAPI) that loads the fine-tuned LLM from the model registry and answers\\nfinancial questions using RAG (leveraging the vector DB populated with\\nfinancial news in real-time)  \\n  \\nWe will also show you how to integrate various serverless tools, such as:  \\n  \\n‚Ä¢ Comet ML as your ML Platform;  \\n‚Ä¢ Qdrant as your vector DB;  \\n‚Ä¢ Beam as your infrastructure.  \\n  \\nùêñùê°ùê® ùê¢ùê¨ ùê≠ùê°ùê¢ùê¨ ùêüùê®ùê´?  \\n  \\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM\\nsystems using LLMOps good principles.  \\n  \\nùêáùê®ùê∞ ùê∞ùê¢ùê•ùê• ùê≤ùê®ùêÆ ùê•ùêûùêöùê´ùêß?  \\n  \\nThe series contains 4 hands-on video lessons and the open-source code you can\\naccess on GitHub.  \\n  \\nùêÇùêÆùê´ùê¢ùê®ùêÆùê¨?  \\n  \\n‚Ü≥ üîó Check it out and support us with a ‚≠ê\\n\\nThe architecture of a financial bot powered by LLMs, vector DBs and MLOps\\n[Image by the Authors]\\n\\n* * *\\n\\n### #1. 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\nThese are the ùü¥ ùòÅùòÜùóΩùó≤ùòÄ of ùó†ùóüùó¢ùóΩùòÄ ùòÅùóºùóºùóπùòÄ that must be in your toolbelt to be a\\nùòÄùòÇùó∞ùó∞ùó≤ùòÄùòÄùó≥ùòÇùóπ ùó†ùóüùó¢ùóΩùòÄ ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóø ‚Üì  \\n  \\nIf you are into MLOps, you are aware of the 1000+ tools in the space and think\\nyou have to know.  \\n  \\nThe reality is that all of these tools can be boiled down to 8 main\\ncategories.  \\n  \\nIf you learn the fundamentals and master one tool from each category, you will\\nbe fine.  \\n  \\n.\\n\\nBa≈üak Tuƒü√ße Eskili\\n\\nand\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\nwrote an excellent summary highlighting these 8 categories:  \\n  \\n1\\\\. ùôëùôöùôßùô®ùôûùô§ùô£ ùôòùô§ùô£ùô©ùôßùô§ùô°: crucial for the traceability and reproducibility of an ML\\nmodel deployment or run. Without a version control system, it is difficult to\\nfind out what exact code version was responsible for specific runs or errors\\nyou might have in production. (üîß GitHub, GitLab, etc.)  \\n  \\n2\\\\. ùòæùôÑ/ùòæùòø: automated tests are triggered upon pull request creation &\\ndeployment to production should only occur through the CD pipeline (üîß GitHub\\nActions, GitLab CI/CD, Jenkins, etc.)  \\n  \\n3\\\\. ùôíùô§ùôßùô†ùôõùô°ùô§ùô¨ ùô§ùôßùôòùôùùôöùô®ùô©ùôßùôñùô©ùôûùô§ùô£: manage complex dependencies between different\\ntasks, such as data preprocessing, feature engineering, ML model training (üîß\\nAirflow, ZenML, AWS Step Functions, etc.)  \\n  \\n4\\\\. ùôàùô§ùôôùôöùô° ùôßùôöùôúùôûùô®ùô©ùôßùôÆ: store, version, and share trained ML model artifacts,\\ntogether with additional metadata (üîß Comet ML, W&B, MLFlow, etc.)  \\n  \\n5\\\\. ùòøùô§ùôòùô†ùôöùôß ùôßùôöùôúùôûùô®ùô©ùôßùôÆ: store, version, and share Docker images. Basically, all\\nyour code will be wrapped up in Docker images and shared through this registry\\n(üîß Docker Hub, ECR, etc.)  \\n  \\n6 & 7\\\\. ùôàùô§ùôôùôöùô° ùô©ùôßùôñùôûùô£ùôûùô£ùôú & ùô®ùôöùôßùô´ùôûùô£ùôú ùôûùô£ùôõùôßùôñùô®ùô©ùôßùô™ùôòùô©ùô™ùôßùôö: if on-premise, you will\\nlikely have to go with Kubernetes. There are multiple choices if you are on a\\ncloud provider: Azure ML on Azure, Sagemaker on AWS, and Vertex AI on GCP.  \\n  \\n8\\\\. ùôàùô§ùô£ùôûùô©ùô§ùôßùôûùô£ùôú: Monitoring in ML systems goes beyond what is needed for\\nmonitoring regular software applications. The distinction lies in that the\\nmodel predictions can fail even if all typical health metrics appear in good\\ncondition. (üîß SageMaker, NannyML, Arize, etc.)  \\n  \\nThe secret sauce in MLOps is knowing how to glue all these pieces together\\nwhile keeping things simple.  \\n\\n[Image from Marvelous MLOps]\\n\\n‚Ü≥üîó To read more about these components, check out the article on\\n\\nMarvelousMLOps\\n\\n.\\n\\n* * *\\n\\n### #2. How to successfully present MLOps ideas to upper management\\n\\nHave you ever presented your MLOps ideas to upper management just to get\\nghosted?  \\n  \\nIn that case...  \\n  \\n\\nRapha√´l Hoogvliets\\n\\n,\\n\\nBa≈üak Tuƒü√ße Eskili\\n\\n, and\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\npresented a great step-by-step strategy for pitching your MLOps ideas to your\\nupper management and getting attention and resources to implement them.  \\n  \\nHere are the 6 steps you have to know ‚Üì  \\n  \\n1\\\\. ùêÇùê®ùê•ùê•ùêûùêúùê≠ ùêöùê•ùê• ùê≠ùê°ùêû ùê©ùêöùê¢ùêß ùê©ùê®ùê¢ùêßùê≠ùê¨  \\nTalk to data scientists, product owners, and stakeholders in your organization\\nto gather issues such as:  \\n\\\\- time to deployment  \\n\\\\- poor quality deployment  \\n\\\\- non-existing monitoring  \\n\\\\- lack of collaboration  \\n\\\\- external parties  \\n  \\n2\\\\. ùêÑùêùùêÆùêúùêöùê≠ùêû ùê©ùêûùê®ùê©ùê•ùêû  \\nOrganize workshops, meetings, etc., to present what MLOps is and how it can\\nhelp.  \\n  \\nI think it\\'s critical to present it to your target audience. For example, an\\nengineer looks at the problem differently than the business stakeholders.  \\n  \\n3\\\\. ùêèùê´ùêûùê¨ùêûùêßùê≠ ùêõùêûùêüùê®ùê´ùêû ùêöùêßùêù ùêöùêüùê≠ùêûùê´ ùê¨ùêúùêûùêßùêöùê´ùê¢ùê®ùê¨  \\nShow how MLOps can solve the company\\'s challenges and deliver tangible\\nbenefits to the organization, such as:  \\n\\\\- less cost  \\n\\\\- fast deployment  \\n\\\\- better collaboration  \\n\\\\- less risk  \\n  \\n4\\\\. ùêèùê´ùê®ùêØùêû ùê¢ùê≠  \\nUse concrete examples to support your ideas, such as:  \\n\\\\- how a competitor or an organization in the same or related field benefited\\nfrom introducing MLOps  \\n\\\\- build a PoC within your organization  \\n  \\n5\\\\. ùêíùêûùê≠ ùêÆùê© ùê≤ùê®ùêÆùê´ ùê≠ùêûùêöùê¶  \\nChoose 2-3 experienced individuals (not juniors) to set up the foundations in\\nyour team/organization.  \\n  \\nWith an emphasis on starting with experienced engineers and only later\\nbringing more juniors to the party.  \\n  \\n6\\\\. ùêäùêûùêûùê© ùê®ùêß ùê§ùêûùêûùê©ùê¢ùêß\\' ùê®ùêß  \\nOnce you successfully apply MLOps to one use case, you can bring in more\\nresponsibility by growing your team and taking on more projects.  \\n  \\n.  \\n  \\nAll of these are great tips for integrating MLOps in your organization.  \\n  \\nI love their \"Present before and after scenarios\" approach.  \\n  \\nYou can extrapolate this strategy for any other new processes (not only\\nMLOps).  \\n  \\n.  \\n  \\n‚Ü≥üîó To learn the details, check out the full article on\\n\\nMarvelousMLOps\\n\\n.\\n\\n* * *\\n\\n### #3. How I generated PyDocs for 100 Python functions in <1 hour\\n\\nThe most boring programming part is to write PyDocs, so I usually write clean\\ncode and let it speak for itself.  \\n  \\nBut, for open-source projects where you have to generate robust documentation,\\nPyDocs are a must.  \\n  \\nThe good news is that now you can automate this process using Copilot.  \\n  \\nYou can see in the video below an example of how easy it is.  \\n  \\nI tested it on more complex functions/classes, and it works well. I chose this\\nexample because it fits nicely on one screen.  \\n  \\nOnce I tested Copilot\\'s experience, I will never go back.  \\n  \\nIt is true that, in some cases, you have to make some minor adjustments. But\\nthat is still 10000% more efficient than writing it from scratch.  \\n\\nIf you want more examples, check out our **Hands-on LLMs** course, where all\\nthe PyDocs are generated 99% using Copilot in <1 hour.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).  \\n\\n18\\n\\nShare this post\\n\\n#### DML: 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-8-types-of-mlops-tools-that-must?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('8ff6064c-9c09-494f-a42d-a60b0e80387c'), content={'Title': 'DML: This is what you need to build an inference pipeline for a financial assistant powered by LLMs, vector DBs and LLMOps', 'Subtitle': 'Lesson 9 | The Hands-on LLMs Series', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\n### Lesson 9 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 28, 2023\\n\\n15\\n\\nShare this post\\n\\n#### DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 9 | The Hands-on LLMs Series**\\n\\n> This is the **last lesson** within the **Hands-on LLMs** series... _But\\n> certainly not the last MLE & MLOps series. We are cooking some exciting\\n> stuff._ But I hope you had fun and learned much during this series.\\n\\nNow, let's see how to glue everything we have done so far under the inference\\npipeline. Enjoy! üßÅ\\n\\n#### **Table of Contents:**\\n\\n  1. Inference pipeline video lesson\\n\\n  2. What do you need to build an inference pipeline for a financial assistant powered by LLMs and vector DBs?\\n\\n  3. How can you build & deploy an inference pipeline for a real-time financial advisor while considering good LLMOps practices?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n  * Lesson 7: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n\\n  * Lesson 8: 7-steps on how to fine-tune an open-source LLM to create your real-time financial advisor\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Inference pipeline video lesson\\n\\nWe ùê´ùêûùê•ùêûùêöùê¨ùêûùêù the ùêüùê¢ùêßùêöùê• video ùê•ùêûùê¨ùê¨ùê®ùêß of the ùêáùêöùêßùêùùê¨-ùê®ùêß ùêãùêãùêåùê¨ FREE course that will\\nteach you how to ùêõùêÆùê¢ùê•ùêù & ùêùùêûùê©ùê•ùê®ùê≤ an ùê¢ùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû for a financial advisor\\nusing ùêãùêöùêßùê†ùêÇùê°ùêöùê¢ùêß, ùêãùêãùêåùêéùê©ùê¨, and ùêØùêûùêúùê≠ùê®ùê´ ùêÉùêÅùê¨.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò¢ùò≥ùò¶ ùòµùò©ùò¶ ùò¨ùò¶ùò∫ ùòµùò∞ùò±ùò™ùò§ùò¥ ùò§ùò∞ùò∑ùò¶ùò≥ùò¶ùò• ùò™ùòØ ùòµùò©ùò¶ ùò∑ùò™ùò•ùò¶ùò∞ ùò≠ùò¶ùò¥ùò¥ùò∞ùòØ made by Pau Labarta ùò¢ùòØùò• ùòê\\n‚Üì  \\n  \\n1\\\\. Overview of the architecture of the inference pipeline and how to apply\\nLLMOps good practices  \\n  \\n2\\\\. How to build from scratch a RAG agent using LangChain:\\nContextExtractorChain + FinancialBotQAChain  \\n  \\n3\\\\. How to attach a callback class to log input prompts and LLM answers to\\nComet LLMOps  \\n  \\n4\\\\. Setting up and running the code locally  \\n  \\n5\\\\. Deploying the inference pipeline to Beam as a RESTful API  \\n  \\n.  \\n  \\nùòäùò∂ùò≥ùò™ùò∞ùò∂ùò¥?\\n\\nCheck out the video lesson\\n\\nPau Labarta Bajo\\n\\nand I did ‚Üì\\n\\n* * *\\n\\n### #2. What do you need to build an inference pipeline for a financial\\nassistant powered by LLMs and vector DBs?\\n\\nHere are its ùü≥ ùó∏ùó≤ùòÜ ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅùòÄ ‚Üì  \\n  \\n1\\\\. ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï ùóΩùóºùóΩùòÇùóπùóÆùòÅùó≤ùó± ùòÑùó∂ùòÅùóµ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóªùó≤ùòÑùòÄ: This is the output of the feature\\npipeline. More concretely, a Qdrant vector DB populated with chunks of\\nfinancial news from Alpaca. During the inference pipeline, we will use it to\\nquery valuable chunks of information and do RAG.  \\n  \\n2\\\\. ùó≤ùó∫ùóØùó≤ùó±ùó±ùó∂ùóªùó¥ ùóπùóÆùóªùó¥ùòÇùóÆùó¥ùó≤ ùó∫ùóºùó±ùó≤ùóπ: To embed the user question and query the vector\\nDB, you need the same embedding model used in the feature pipeline, more\\nconcretely `ùò¢ùò≠ùò≠-ùòîùò™ùòØùò™ùòìùòî-ùòì6-ùò∑2` from `ùò¥ùò¶ùòØùòµùò¶ùòØùò§ùò¶-ùòµùò≥ùò¢ùòØùò¥ùòßùò∞ùò≥ùòÆùò¶ùò≥ùò¥`. Using the same\\nencoder-only model is crucial, as the query vector and vector DB index vectors\\nhave to be in the same space.  \\n  \\n3\\\\. ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ùó± ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†: The output of the training pipeline will be a\\nfine-tuned Falcon 7B on financial tasks.  \\n  \\n4\\\\. ùó∫ùóºùó±ùó≤ùóπ ùóøùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ: The fine-tuned model will be shared between the training &\\ninference pipeline through Comet‚Äôs model registry. By doing so, you decouple\\nentirely the 2 components, and the model can easily be shared under specific\\nenvironments (e.g., staging, prod) and versions (e.g., v1.0.1).  \\n  \\n5\\\\. ùóÆ ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ ùó≥ùóºùóø ùóüùóüùó† ùóÆùóΩùóΩùóπùó∂ùó∞ùóÆùòÅùó∂ùóºùóªùòÄ: You need LangChain, as your LLM\\nframework, to glue all the steps together, such as querying the vector DB,\\nstoring the history of the conversation, creating the prompt, and calling the\\nLLM. LangChain provides out-of-the-box solutions to chain all these steps\\ntogether quickly.  \\n  \\n6\\\\. ùó±ùó≤ùóΩùóπùóºùòÜ ùòÅùóµùó≤ ùóüùóüùó† ùóÆùóΩùóΩ ùóÆùòÄ ùóÆ ùó•ùóòùó¶ùóßùó≥ùòÇùóπ ùóîùó£ùóú: One of the final steps is to deploy\\nyour awesome LLM financial assistant under a RESTful API. You can quickly do\\nthis using Beam as your serverless infrastructure provider. Beam specializes\\nin DL. Thus, it offers quick ways to load your LLM application on GPU machines\\nand expose it under a RESTful API.  \\n  \\n7\\\\. ùóΩùóøùóºùó∫ùóΩùòÅ ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥: The last step is to add eyes on top of your system. You\\ncan do this using Comet‚Äôs LLMOps features that allow you to track & monitor\\nall the prompts & responses of the system.\\n\\n> ‚Ü≥üîó Check out how these components are working together in our Hands-on LLMs\\n> free course.\\n\\n* * *\\n\\n### #3. How can you build & deploy an inference pipeline for a real-time\\nfinancial advisor while considering good LLMOps practices?\\n\\nùêáùê®ùê∞ can you ùêõùêÆùê¢ùê•ùêù & ùêùùêûùê©ùê•ùê®ùê≤ an ùê¢ùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû for a real-time financial\\nadvisor with ùêãùêöùêßùê†ùêÇùê°ùêöùê¢ùêß powered by ùêãùêãùêåùê¨ & ùêØùêûùêúùê≠ùê®ùê´ ùêÉùêÅùê¨ while considering ùê†ùê®ùê®ùêù\\nùêãùêãùêåùêéùê©ùê¨ ùê©ùê´ùêöùêúùê≠ùê¢ùêúùêûùê¨?\\n\\n.\\n\\nAs a quick reminder from previous posts, here is what we already have:  \\n\\\\- a Qdrant vector DB populated with financial news (the output of the feature\\npipeline)  \\n\\\\- fine-tuned Falcon-7B LoRA weights stored in Comet‚Äôs model registry (the\\noutput of the training pipeline)\\n\\nThe Qdrant vectorDB is accessed through a Python client.\\n\\nA specific version of the Falcon-7B LoRA weights is downloaded from Comet‚Äôs\\nmodel registry and loaded in memory using QLoRA.\\n\\nThe goal of the inference pipeline is to use LangChain to glue the 2\\ncomponents into a single `**FinancialAssistant** ` entity.\\n\\n.\\n\\nThe `**FinancialAssistant** ` entity is deployed in a request-response fashion\\nunder a RESTful API. We used Beam to deploy it quickly under a serverless web\\nendpoint.\\n\\nTo deploy any model using Beam as a RESTful API is as easy as writing the\\nfollowing Python decorator:\\n\\n    \\n    \\n    @financial_bot. rest_api(keep_warm_seconds=300, loader=load_bot)def run(**inputs):\\n       ....\\n\\n  \\nùêçùê®ùê∞ ùê•ùêûùê≠‚Äôùê¨ ùêÆùêßùêùùêûùê´ùê¨ùê≠ùêöùêßùêù ùê≠ùê°ùêû ùêüùê•ùê®ùê∞ ùê®ùêü ùê≠ùê°ùêû `ùêÖùê¢ùêßùêöùêßùêúùê¢ùêöùê•ùêÄùê¨ùê¨ùê¢ùê¨ùê≠ùêöùêßùê≠` ùêúùê°ùêöùê¢ùêß‚Üì\\n\\n1\\\\. Clean the user‚Äôs input prompt and use a pre-trained ‚Äú**all-MiniLM-L6-v2**\\n‚Äù encoder-only model to embed it (the same LM used to populate the vector DB).\\n\\n2\\\\. Using the embedded user input, query the Qdrant vector DB and extract the\\ntop 3 most similar financial news based on the cosine similarly distance\\n\\n‚Üí These 2 steps were necessary to do RAG. If you don‚Äôt know how RAG works,\\ncheck out Lesson 3.\\n\\n3\\\\. Build the final prompt using a ‚Äú**PromptTemplate** ‚Äù class (the same one\\nused for training) that formats the following components:  \\n\\\\- a system prompt  \\n\\\\- the user‚Äôs input prompt  \\n\\\\- the financial news context  \\n\\\\- the chat history\\n\\n4\\\\. Now that our prompt contains all the necessary data, we pass it to the\\nfine-tuned Falcon-7B LLM for the final answer.\\n\\nThe input prompt and LLM answer will be logged and monitored by Comet LLMOps.\\n\\n5\\\\. You can get the answer in one shot or use the `TextIteratorStreamer` class\\n(from HuggingFace) to stream it token-by-token.\\n\\n6\\\\. Store the user‚Äôs input prompt and LLM answer in the chat history.\\n\\n7\\\\. Pass the final answer to the client.\\n\\n**Note:** You can use the `**TextIteratorStreamer** ` class & wrap the\\n`**FinancialAssistant** ` under a WebSocket (instead of the RESTful API) to\\nstream the answer of the bot token by token.\\n\\nSimilar to what you see in the interface of ChatGPT.\\n\\nHow | Inference pipeline: Build & deploy an inference pipeline using LangChain powered by LLMs & vector DBs [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nWith this, we concluded the **Hands-On LLMs** series. I hope you enjoyed it üî•\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n15\\n\\nShare this post\\n\\n#### DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-this-is-what-you-need-to-build?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('ceacd8d8-91dc-42a7-ad33-97964bf91387'), content={'Title': 'DML: 7-steps on how to fine-tune an open-source LLM to create your real-time financial advisor', 'Subtitle': 'Lesson 8 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 7-steps on how to fine-tune an open-source LLM to create your real-time\\nfinancial advisor\\n\\n### Lesson 8 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 21, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 8 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. What is Beam? How does serverless make deploying ML models easy?\\n\\n  2. 7 tips you must know to reduce your VRAM consumption of your LLMs during training\\n\\n  3. 7-steps on how to fine-tune an open-source LLM to create your real-time financial advisor\\n\\n#### Previous Lessons:\\n\\n  * Lesson 5: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n  * Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n  * Lesson 7: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. What is Beam? How does serverless make deploying ML models easy?\\n\\nùóóùó≤ùóΩùóπùóºùòÜùó∂ùóªùó¥ & ùó∫ùóÆùóªùóÆùó¥ùó∂ùóªùó¥ ML models is ùóµùóÆùóøùó±, especially when running your models on\\nGPUs.  \\n  \\nBut ùòÄùó≤ùóøùòÉùó≤ùóøùóπùó≤ùòÄùòÄ makes things ùó≤ùóÆùòÄùòÜ.  \\n  \\nUsing Beam as your serverless provider, deploying & managing ML models can be\\nas easy as ‚Üì  \\n  \\nùóóùó≤ùó≥ùó∂ùóªùó≤ ùòÜùóºùòÇùóø ùó∂ùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ & ùó±ùó≤ùóΩùó≤ùóªùó±ùó≤ùóªùó∞ùó∂ùó≤ùòÄ  \\n  \\nIn a few lines of code, you define the application that contains:  \\n  \\n\\\\- the requirements of your infrastructure, such as the CPU, RAM, and GPU  \\n\\\\- the dependencies of your application  \\n\\\\- the volumes from where you can load your data and store your artifacts  \\n  \\nùóóùó≤ùóΩùóπùóºùòÜ ùòÜùóºùòÇùóø ùó∑ùóºùóØùòÄ  \\n  \\nUsing the Beam application, you can quickly decore your Python functions to:  \\n  \\n\\\\- run them once on the given serverless application  \\n\\\\- put your task/job in a queue to be processed or even schedule it using a\\nCRON-based syntax  \\n\\\\- even deploy it as a RESTful API endpoint\\n\\nHow do you use Beam as your serverless provider? [Image by the Author]\\n\\nAs you can see in the image below, you can have one central function for\\ntraining or inference, and with minimal effort, you can switch from all these\\ndeployment methods.  \\n  \\nAlso, you don\\'t have to bother at all with managing the infrastructure on\\nwhich your jobs run. You specify what you need, and Beam takes care of the\\nrest.  \\n  \\nBy doing so, you can directly start to focus on your application and stop\\ncarrying about the infrastructure.  \\n  \\nThis is the power of serverless!  \\n  \\n‚Ü≥üîó Check out Beam to learn more\\n\\n* * *\\n\\n### #2. 7 tips you must know to reduce your VRAM consumption of your LLMs\\nduring training\\n\\nHere are ùü≥ ùòÅùó∂ùóΩùòÄ you must know to ùóøùó≤ùó±ùòÇùó∞ùó≤ your ùó©ùó•ùóîùó† ùó∞ùóºùóªùòÄùòÇùó∫ùóΩùòÅùó∂ùóºùóª of your ùóüùóüùó†ùòÄ\\nduring ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ so you can ùó≥ùó∂ùòÅ it on ùòÖùü≠ ùóöùó£ùó®.  \\n  \\nWhen training LLMs, one of the pain points is to have enough VRAM on your\\nsystem.  \\n  \\nThe good news is that the gods of DL are with us, and there are methods to\\nlower your VRAM consumption without a significant impact on your performance ‚Üì  \\n  \\nùü≠\\\\. ùó†ùó∂ùòÖùó≤ùó±-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: During training you use both FP32 and FP16 in the\\nfollowing way: \"FP32 weights\" -> \"FP16 weights\" -> \"FP16 gradients\" -> \"FP32\\ngradients\" -> \"Update weights\" -> \"FP32 weights\" (and repeat). As you can see,\\nthe forward & backward passes are done in FP16, and only the optimization step\\nis done in FP32, which reduces both the VRAM and runtime.  \\n  \\nùüÆ\\\\. ùóüùóºùòÑùó≤ùóø-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: All your computations are done in FP16 instead of FP32.\\nBut the key is using bfloat16 (\"Brain Floating Point\"), a numerical\\nrepresentation Google developed for deep learning. It allows you to represent\\nvery large and small numbers, avoiding overflowing or underflowing scenarios.  \\n  \\nùüØ\\\\. ùó•ùó≤ùó±ùòÇùó∞ùó∂ùóªùó¥ ùòÅùóµùó≤ ùóØùóÆùòÅùó∞ùóµ ùòÄùó∂ùòáùó≤: This one is straightforward. Fewer samples per\\ntraining iteration result in smaller VRAM requirements. The downside of this\\nmethod is that you can\\'t go too low with your batch size without impacting\\nyour model\\'s performance.  \\n  \\nùü∞\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ ùóÆùó∞ùó∞ùòÇùó∫ùòÇùóπùóÆùòÅùó∂ùóºùóª: It is a simple & powerful trick to increase your\\nbatch size virtually. You compute the gradients for \"micro\" batches (forward +\\nbackward passes). Once the accumulated gradients reach the given \"virtual\"\\ntarget, the model weights are updated with the accumulated gradients. For\\nexample, you have a batch size of 4 and a micro-batch size of 1. Then, the\\nforward & backward passes will be done using only x1 sample, and the\\noptimization step will be done using the aggregated gradient of the 4 samples.  \\n  \\nùü±\\\\. ùó®ùòÄùó≤ ùóÆ ùòÄùòÅùóÆùòÅùó≤ùóπùó≤ùòÄùòÄ ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùó≤ùóø: Adam is the most popular optimizer. It is one\\nof the most stable optimizers, but the downside is that it has 2 additional\\nparameters (a mean & variance) for every model parameter. If you use a\\nstateless optimizer, such as SGD, you can reduce the number of parameters by\\n2/3, which is significant for LLMs.  \\n  \\nùü≤\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ (ùóºùóø ùóÆùó∞ùòÅùó∂ùòÉùóÆùòÅùó∂ùóºùóª) ùó∞ùóµùó≤ùó∞ùó∏ùóΩùóºùó∂ùóªùòÅùó∂ùóªùó¥: It drops specific activations\\nduring the forward pass and recomputes them during the backward pass. Thus, it\\neliminates the need to hold all activations simultaneously in VRAM. This\\ntechnique reduces VRAM consumption but makes the training slower.  \\n  \\nùü≥\\\\. ùóñùó£ùó® ùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùóºùó≥ùó≥ùóπùóºùóÆùó±ùó∂ùóªùó¥: As the name suggests, the parameters that do not\\nfit on your GPU\\'s VRAM are loaded on the CPU. Intuitively, you can see it as a\\nmodel parallelism between your GPU & CPU.\\n\\nA happy dude going for a walk with his GPU [Image by DALL-E]\\n\\nMost of these methods are orthogonal, so you can combine them and drastically\\nreduce your VRAM requirements during training.\\n\\n* * *\\n\\n### #3. 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\nIn the past weeks, we covered ùòÑùóµùòÜ you have to fine-tune an LLM and ùòÑùóµùóÆùòÅ\\nresources & tools you need:  \\n\\\\- Q&A dataset  \\n\\\\- pre-trained LLM (Falcon 7B) & QLoRA  \\n\\\\- MLOps: experiment tracker, model registry, prompt monitoring (Comet ML)  \\n\\\\- compute platform (Beam)  \\n  \\n.  \\n  \\nNow, let\\'s see how you can hook all of these pieces together into a single\\nfine-tuning module ‚Üì  \\n  \\nùü≠\\\\. ùóüùóºùóÆùó± ùòÅùóµùó≤ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ  \\n  \\nOur Q&A samples have the following structure keys: \"about_me,\" \"user_context,\"\\n\"question,\" and \"answer.\"  \\n  \\nFor task-specific fine-tuning, you need only 100-1000 samples. Thus, you can\\ndirectly load the whole JSON in memory.  \\n  \\nAfter you map every sample to a list of Python ùò•ùò¢ùòµùò¢ùò§ùò≠ùò¢ùò¥ùò¥ùò¶ùò¥ to validate the\\nstructure & type of the ingested instances.  \\n  \\nùüÆ\\\\. ùó£ùóøùó≤ùóΩùóøùóºùó∞ùó≤ùòÄùòÄ ùòÅùóµùó≤ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ ùó∂ùóªùòÅùóº ùóΩùóøùóºùó∫ùóΩùòÅùòÄ  \\n  \\nThe first step is to use ùò∂ùòØùò¥ùòµùò≥ùò∂ùò§ùòµùò∂ùò≥ùò¶ùò• to clean every sample by removing\\nredundant characters.  \\n  \\nAfter, as every sample consists of multiple fields, you must map it to a\\nsingle piece of text, also known as the prompt.  \\n  \\nTo do so, you define a ùòóùò≥ùò∞ùòÆùò±ùòµùòõùò¶ùòÆùò±ùò≠ùò¢ùòµùò¶ class to manage all your prompts. You\\nwill use it to map all the sample keys to a prompt using a Python f-string.  \\n  \\nThe last step is to map the list of Python ùò•ùò¢ùòµùò¢ùò§ùò≠ùò¢ùò¥ùò¥ùò¶ùò¥ to a HuggingFace\\ndataset and map every sample to a prompt, as discussed above.  \\n  \\nùüØ\\\\. ùóüùóºùóÆùó± ùòÅùóµùó≤ ùóüùóüùó† ùòÇùòÄùó∂ùóªùó¥ ùó§ùóüùóºùó•ùóî  \\n  \\nLoad a pretrained Falcon 7B LLM by passing a ùò£ùò™ùòµùò¥ùò¢ùòØùò•ùò£ùò∫ùòµùò¶ùò¥ quantization\\nconfiguration that loads all the weights on 4 bits.  \\n  \\nAfter using LoRA, you freeze the weights of the original Falcon LLM and attach\\nto it a set of trainable adapters.  \\n  \\nùü∞\\\\. ùóôùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥  \\n  \\nThe ùòµùò≥ùò≠ Python package makes this step extremely simple.  \\n  \\nYou pass to the ùòöùòçùòõùòõùò≥ùò¢ùò™ùòØùò¶ùò≥ class the training arguments, the dataset and the\\nmodel and call the ùòµùò≥ùò¢ùò™ùòØ() method.  \\n  \\nOne crucial aspect is configuring an experiment tracker, such as Comet ML, to\\nlog the loss and other vital metrics & artifacts.  \\n  \\nùü±\\\\. ùó£ùòÇùòÄùóµ ùòÅùóµùó≤ ùóØùó≤ùòÄùòÅ ùó∫ùóºùó±ùó≤ùóπ ùòÅùóº ùòÅùóµùó≤ ùó∫ùóºùó±ùó≤ùóπ ùóøùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ  \\n  \\nOne of the final steps is to attach a callback to the ùòöùòçùòõùòõùò≥ùò¢ùò™ùòØùò¶ùò≥ class that\\nruns when the training ends to push the model with the lowest loss to the\\nmodel registry as the new production candidate.  \\n  \\nùü≤\\\\. ùóòùòÉùóÆùóπùòÇùóÆùòÅùó≤ ùòÅùóµùó≤ ùóªùó≤ùòÑ ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª ùó∞ùóÆùóªùó±ùó∂ùó±ùóÆùòÅùó≤  \\n  \\nEvaluating generative AI models can be pretty tricky.  \\n  \\nYou can run the LLM on the test set and log the prompts & answers to Comet\\nML\\'s monitoring system to check them manually.  \\n  \\nIf the provided answers are valid, using the model registry dashboard, you\\nwill manually release it to replace the old LLM.  \\n  \\nùü≥\\\\. ùóóùó≤ùóΩùóπùóºùòÜ ùòÅùóº ùóïùó≤ùóÆùó∫  \\n  \\nIt is as easy as wrapping the training & inference functions (or classes) with\\na Python \"@ùò¢ùò±ùò±.ùò≥ùò∂ùòØ()\" decorator.\\n\\nA step-by-step guide on fine-tuning an LLM to create a real-time financial\\nadvisor [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 9** ,**** the last lesson of the **Hands-\\nOn LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-7-steps-on-how-to-fine-tune-an?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('dffed5e0-c824-40db-9388-a26fa09f7b49'), content={'Title': 'DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?', 'Subtitle': 'Lesson 7 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\n### Lesson 7 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 14, 2023\\n\\n5\\n\\nShare this post\\n\\n#### DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 7 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. Real-time feature pipeline video lesson\\n\\n  2. How do you generate a synthetic domain-specific Q&A dataset in <30 minutes to fine-tune your open-source LLM?\\n\\n  3. My personal list of filtered resources about LLMs & vector DBs\\n\\n#### Previous Lessons:\\n\\n  * Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n  * Lesson 5: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n  * Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Real-time feature pipeline video lesson\\n\\nI know we are currently talking about the training pipeline and Q&A dataset\\ngeneration, but sometimes, mixing the information to remember and make new\\nconnections is healthy.\\n\\n‚Ä¶or maybe that is only an excuse to share the video lesson about the feature\\npipeline that wasn‚Äôt ready when I started this series.\\n\\nIt will teach you how to ùó∂ùóªùó¥ùó≤ùòÄùòÅ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóªùó≤ùòÑùòÄ in ùóøùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ from Alpaca, ùó∞ùóπùó≤ùóÆùóª\\n& ùó≤ùó∫ùóØùó≤ùó± the ùó±ùóºùó∞ùòÇùó∫ùó≤ùóªùòÅùòÄ, and ùóπùóºùóÆùó± them in a ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï.\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùóÆùóª ùóºùòÉùó≤ùóøùòÉùó∂ùó≤ùòÑ ùóºùó≥ ùòÅùóµùó≤ ùòÉùó∂ùó±ùó≤ùóº ‚Üì  \\n  \\n1\\\\. Step-by-step instructions on how to set up the streaming pipeline code & a\\nQdrant vector DB serverless cluster  \\n2\\\\. Why we used Bytewax to build the streaming pipeline  \\n3\\\\. How we used Bytewax to ingest financial news in real-time leveraging a\\nWebSocket, clean the documents, chunk them, embed them and ingest them in the\\nQdrant vector DB  \\n4\\\\. How we adapted the Bytewax streaming pipeline to also work in batch mode\\nto populate the vector DB with historical data  \\n5\\\\. How to run the code  \\n6\\\\. How to deploy the code to AWS\\n\\nHere it is ‚Üì Enjoy üëÄ\\n\\n* * *\\n\\n## #2. How do you generate a synthetic domain-specific Q&A dataset in <30\\nminutes to fine-tune your open-source LLM?\\n\\nThis method is also known as ùó≥ùó∂ùóªùó≤ùòÅùòÇùóªùó∂ùóªùó¥ ùòÑùó∂ùòÅùóµ ùó±ùó∂ùòÄùòÅùó∂ùóπùóπùóÆùòÅùó∂ùóºùóª. Here are its 3 ùòÆùò¢ùò™ùòØ\\nùò¥ùòµùò¶ùò±ùò¥ ‚Üì  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶, ùò≠ùò¶ùòµ\\'ùò¥ ùò®ùò¶ùòØùò¶ùò≥ùò¢ùòµùò¶ ùò¢ ùòò&ùòà ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùò•ùò¢ùòµùò¢ùò¥ùò¶ùòµ ùò∂ùò¥ùò¶ùò• ùòµùò∞ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò¶ ùò¢\\nùòßùò™ùòØùò¢ùòØùò§ùò™ùò¢ùò≠ ùò¢ùò•ùò∑ùò™ùò¥ùò∞ùò≥ ùòìùòìùòî.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: ùó†ùóÆùóªùòÇùóÆùóπùóπùòÜ ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóÆ ùó≥ùó≤ùòÑ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nGenerate a few input samples (~3) that have the following structure:  \\n\\\\- ùò∂ùò¥ùò¶ùò≥_ùò§ùò∞ùòØùòµùò¶ùòπùòµ: describe the type of investor (e.g., \"I am a 28-year-old\\nmarketing professional\")  \\n\\\\- ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØ: describe the user\\'s intention (e.g., \"Is Bitcoin a good\\ninvestment option?\")  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: ùóòùòÖùóΩùóÆùóªùó± ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ùóµùó≤ùóπùóΩ ùóºùó≥ ùóÆ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó†  \\n  \\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up\\nto +N similar input examples.  \\n  \\nWe generated 100 input examples in our use case, but you can generate more.  \\n  \\nYou will use the manually filled input examples to do few-shot prompting.  \\n  \\nThis will guide the LLM to give you domain-specific samples.  \\n  \\nùòõùò©ùò¶ ùò±ùò≥ùò∞ùòÆùò±ùòµ ùò∏ùò™ùò≠ùò≠ ùò≠ùò∞ùò∞ùò¨ ùò≠ùò™ùò¨ùò¶ ùòµùò©ùò™ùò¥:  \\n\"\"\"  \\n...  \\nGenerate 100 more examples with the following pattern:  \\n  \\n# USER CONTEXT 1  \\n...  \\n  \\n# QUESTION 1  \\n...  \\n  \\n# USER CONTEXT 2  \\n...  \\n\"\"\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: ùó®ùòÄùó≤ ùòÅùóµùó≤ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó† ùòÅùóº ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóºùòÇùòÅùóΩùòÇùòÅùòÄ ùó≥ùóºùóø ùóÆùóπùóπ ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nNow, you will have the same powerful LLM as a teacher, but this time, it will\\nanswer all your N input examples.  \\n  \\nBut first, to introduce more variance, we will use RAG to enrich the input\\nexamples with news context.  \\n  \\nAfterward, we will use the teacher LLM to answer all N input examples.  \\n  \\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual\\nwork.  \\n  \\n.  \\n  \\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a\\nniched task, such as financial advising.  \\n  \\nThis technique is known as finetuning with distillation because you use a\\npowerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data,\\nwhich will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as\\nthe student.  \\n  \\n‚úíÔ∏è ùòïùò∞ùòµùò¶: To ensure that the generated data is of high quality, you can hire a\\ndomain expert to check & refine it.\\n\\nHow do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n[Image by the Author].\\n\\n‚Ü≥ To learn more about this technique, check out ‚ÄúHow to generate a Q&A dataset\\nin less than 30 minutes‚Äù Pau Labarta\\'s article from\\n\\nReal-World Machine Learning\\n\\n.\\n\\n* * *\\n\\n### #3. My personal list of filtered resources about LLMs & vector DBs\\n\\nThe internet is full of ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ ùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ùòÄ about ùóüùóüùó†ùòÄ & ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóïùòÄ. But ùó∫ùóºùòÄùòÅ\\nùóºùó≥ ùó∂ùòÅ is ùòÅùóøùóÆùòÄùóµ.  \\n  \\nAfter ùü≤ ùó∫ùóºùóªùòÅùóµùòÄ of ùóøùó≤ùòÄùó≤ùóÆùóøùó∞ùóµùó∂ùóªùó¥ ùóüùóüùó†ùòÄ & ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóïùòÄ, here is a ùóπùó∂ùòÄùòÅ ùóºùó≥ ùó≥ùó∂ùóπùòÅùó≤ùóøùó≤ùó±\\nùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ùòÄ that I ùóΩùó≤ùóøùòÄùóºùóªùóÆùóπùóπùòÜ ùòÇùòÄùó≤ ‚Üì  \\n  \\nùòâùò≠ùò∞ùò®ùò¥:  \\n  \\n\\\\- philschmid  \\n\\\\- Chip Huyen  \\n\\\\- eugeneyan  \\n\\\\- LLM Learning Lab  \\n\\\\- Lil\\'Log  \\n\\\\- VectorHub by SuperLinked  \\n\\\\- Qdrant Blog  \\n  \\nùòàùò≥ùòµùò™ùò§ùò≠ùò¶ùò¥:  \\n  \\n\\\\- Patterns for Building LLM-based Systems & Products  \\n\\\\- RLHF: Reinforcement Learning from Human Feedback  \\n\\\\- Illustrating Reinforcement Learning from Human Feedback (RLHF)  \\n\\\\- Understanding Encoder And Decoder LLMs  \\n\\\\- Building LLM applications for production  \\n\\\\- Prompt Engineering  \\n\\\\- Transformers  \\n\\\\- Bidirectional Encoder Representations from Transformers (BERT)  \\n\\\\- Multimodality and Large Multimodal Models (LMMs) by Chip Huyen  \\n  \\nùòùùò™ùò•ùò¶ùò∞ùò¥:  \\n  \\n\\\\- Word Embedding and Word2Vec, Clearly Explained!!!  \\n\\\\- Let\\'s build GPT: from scratch, in code, spelled out  \\n\\\\- Transformer Neural Networks, ChatGPT\\'s foundation, Clearly Explained!!!  \\n\\\\- Large Language Models with Semantic Search  \\n\\\\- Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly\\nExplained!!!  \\n  \\nùòäùò∞ùò•ùò¶ ùòôùò¶ùò±ùò∞ùò¥ùò™ùòµùò∞ùò≥ùò™ùò¶ùò¥:  \\n  \\n\\\\- OpenAI Cookbook  \\n\\\\- generative-ai-for-beginners  \\n  \\nùòäùò∞ùò∂ùò≥ùò¥ùò¶ùò¥:  \\n  \\n\\\\- LangChain for LLM Application Development  \\n\\\\- Building Systems with the ChatGPT API  \\n\\\\- ChatGPT Prompt Engineering for Developers  \\n  \\n.  \\n  \\n...and hopefully, my üîó Hands-on LLMs course will soon appear along them.\\n\\nImage by DALL-E\\n\\nLet me know what you think of this list and have fun learning üî•\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 8** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n5\\n\\nShare this post\\n\\n#### DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-do-you-generate-a-q-and-a?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('15c3831b-67fd-4279-970a-a720aafefa67'), content={'Title': 'DML: What do you need to fine-tune an open-source LLM to create your financial advisor?', 'Subtitle': 'Lesson 6 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\n### Lesson 6 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 07, 2023\\n\\n4\\n\\nShare this post\\n\\n#### DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 6 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. The difference between encoders, decoders, and encoder-decoder LLMs.\\n\\n  2. You must know these 3 main stages of training an LLM to train your own LLM on your proprietary data.\\n\\n  3. What do you need to fine-tune an open-source LLM to create your own financial advisor?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 3: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?\\n\\n  * Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n  * Lesson 5: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. The difference between encoders, decoders, and encoder-decoder LLMs\\n\\nLet\\'s see when to use each architecture ‚Üì  \\n  \\nAs embeddings are everywhere, both encoders and decoders use self-attention\\nlayers to encode word tokens into embeddings.  \\n  \\nThe devil is in the details. Let\\'s clarify it ‚Üì  \\n  \\nùóßùóµùó≤ ùó¢ùóøùó∂ùó¥ùó∂ùóªùóÆùóπ ùóßùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùó≤ùóø  \\n  \\nIt is an encoder-decoder setup. The encoder processes the input text and hands\\noff its understanding as embeddings to the decoder, which will generate the\\nfinal output.  \\n  \\nThe key difference between an encoder & decoder is in how it processes its\\ninputs & outputs.  \\n  \\n=== ùóòùóªùó∞ùóºùó±ùó≤ùóøùòÄ ===  \\n  \\nThe role of an encoder is to extract relevant information from the whole input\\nand encode it into an embedding (e.g., BERT, RoBERTa).  \\n  \\nWithin the \"Multi-head attention\" of the transformer, all the tokens are\\nallowed to speak to each other.  \\n  \\nA token at position t can talk to all other previous tokens [0, t-1] and\\nfuture tokens [t+1, T]. This means that the attention mask is computed along\\nthe whole vector.  \\n  \\nThus, because the encoder processes the whole input, it is helpful for\\nclassification tasks (e.g., sentiment analysis) and creates embeddings for\\nclustering, recommender systems, vector DB indexes, etc.  \\n  \\n=== ùóóùó≤ùó∞ùóºùó±ùó≤ùóøùòÄ ===  \\n  \\nOn the flip side, if you want to generate text, use decoder-only models (e.g.,\\nGPT family).  \\n  \\nOnly the current and previous tokens (not the whole input) are used to predict\\nthe next token.  \\n  \\nWithin the \"Masked Multi-head attention,\" the future positions are masked to\\nmaintain the autoregressive property of the decoding process.  \\n  \\nFor example, within the \"Masked Multi-head attention,\" instead of all the\\ntokens talking to each other, a token at position t will have access only to\\nprevious tokens at positions t-1, t-2, t-3, ..., 0.  \\n  \\n=== ùóòùóªùó∞ùóºùó±ùó≤ùóø-ùó±ùó≤ùó∞ùóºùó±ùó≤ùóø ===  \\n  \\nThis technique is used when you have to understand the entire input sequence\\n(encoder) and the previously generated sequence (decoder -> autoregressive).  \\n  \\nTypical use cases are text translation & summarization (the original\\ntransformer was built for text translation), where the output heavily relies\\non the input.  \\n  \\nWhy? Because the decoding step always has to be conditioned by the encoded\\ninformation. Also known as cross-attention, the decoder queries the encoded\\ninformation for information to guide the decoding process.  \\n  \\nFor example, when translating English to Spanish, every Spanish token\\npredicted is conditioned by the previously predicted Spanish tokens & the\\nentire English sentence.\\n\\nEncoder vs. Decoder vs. Encoder-Decoder LLMs [Image by the Author].\\n\\nTo conclude...  \\n  \\n\\\\- a decoder takes as input previous tokens and predicts the next one (in an\\nautoregressive way)  \\n\\\\- by dropping the \"Masked\" logic from the \"Masked Multi-head attention,\" you\\nprocess the whole input, transforming the decoder into an encoder  \\n\\\\- if you hook the encoder to the decoder through a cross-attention layer, you\\nhave an encoder-decoder architecture\\n\\n* * *\\n\\n### #2. You must know these 3 main stages of training an LLM to train your own\\nLLM on your proprietary data\\n\\nYou must know these ùüØ ùó∫ùóÆùó∂ùóª ùòÄùòÅùóÆùó¥ùó≤ùòÄ of ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóÆùóª ùóüùóüùó† to train your own ùóüùóüùó† on\\nyour ùóΩùóøùóºùóΩùóøùó∂ùó≤ùòÅùóÆùóøùòÜ ùó±ùóÆùòÅùóÆ.  \\n  \\n# ùó¶ùòÅùóÆùó¥ùó≤ ùü≠: ùó£ùóøùó≤ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó≥ùóºùóø ùó∞ùóºùó∫ùóΩùóπùó≤ùòÅùó∂ùóºùóª  \\n  \\nYou start with a bear foot randomly initialized LLM.  \\n  \\nThis stage aims to teach the model to spit out tokens. More concretely, based\\non previous tokens, the model learns to predict the next token with the\\nhighest probability.  \\n  \\nFor example, your input to the model is \"The best programming language is\\n___\", and it will answer, \"The best programming language is Rust.\"  \\n  \\nIntuitively, at this stage, the LLM learns to speak.  \\n  \\nùòãùò¢ùòµùò¢: >1 trillion token (~= 15 million books). The data quality doesn\\'t have\\nto be great. Hence, you can scrape data from the internet.  \\n  \\n# ùó¶ùòÅùóÆùó¥ùó≤ ùüÆ: ùó¶ùòÇùóΩùó≤ùóøùòÉùó∂ùòÄùó≤ùó± ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ (ùó¶ùóôùóß) ùó≥ùóºùóø ùó±ùó∂ùóÆùóπùóºùó¥ùòÇùó≤  \\n  \\nYou start with the pretrained model from stage 1.  \\n  \\nThis stage aims to teach the model to respond to the user\\'s questions.  \\n  \\nFor example, without this step, when prompting: \"What is the best programming\\nlanguage?\", it has a high probability of creating a series of questions such\\nas: \"What is MLOps? What is MLE? etc.\"  \\n  \\nAs the model mimics the training data, you must fine-tune it on Q&A (questions\\n& answers) data to align the model to respond to questions instead of\\npredicting the following tokens.  \\n  \\nAfter the fine-tuning step, when prompted, \"What is the best programming\\nlanguage?\", it will respond, \"Rust\".  \\n  \\nùòãùò¢ùòµùò¢: 10K - 100K Q&A example  \\n  \\nùòïùò∞ùòµùò¶: After aligning the model to respond to questions, you can further\\nsingle-task fine-tune the model, on Q&A data, on a specific use case to\\nspecialize the LLM.  \\n  \\n# ùó¶ùòÅùóÆùó¥ùó≤ ùüØ: ùó•ùó≤ùó∂ùóªùó≥ùóºùóøùó∞ùó≤ùó∫ùó≤ùóªùòÅ ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ ùó≥ùóøùóºùó∫ ùóµùòÇùó∫ùóÆùóª ùó≥ùó≤ùó≤ùó±ùóØùóÆùó∞ùó∏ (ùó•ùóüùóõùóô)  \\n  \\nDemonstration data tells the model what kind of responses to give but doesn\\'t\\ntell the model how good or bad a response is.  \\n  \\nThe goal is to align your model with user feedback (what users liked or didn\\'t\\nlike) to increase the probability of generating answers that users find\\nhelpful.  \\n  \\nùòôùòìùòèùòç ùò™ùò¥ ùò¥ùò±ùò≠ùò™ùòµ ùò™ùòØ 2:  \\n  \\n1\\\\. Using the LLM from stage 2, train a reward model to act as a scoring\\nfunction using (prompt, winning_response, losing_response) samples (=\\ncomparison data). The model will learn to maximize the difference between\\nthese 2. After training, this model outputs rewards for (prompt, response)\\ntuples.  \\n  \\nùòãùò¢ùòµùò¢: 100K - 1M comparisons  \\n  \\n2\\\\. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here,\\nyou will use the reward model trained above to give a score for every:\\n(prompt, response). The RL algorithm will align the LLM to generate prompts\\nwith higher rewards, increasing the probability of generating responses that\\nusers liked.  \\n  \\nùòãùò¢ùòµùò¢: 10K - 100K prompts\\n\\nThe 3 main stages of training an LLM that you must know [Image by the Author].\\n\\n**Note:** Post inspired by Chip Huyen\\'s üîó RLHF: Reinforcement Learning from\\nHuman Feedback\" article.\\n\\n* * *\\n\\n### #3. What do you need to fine-tune an open-source LLM to create your own\\nfinancial advisor?\\n\\nThis is the ùóüùóüùó† ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùó∏ùó∂ùòÅ you must know ‚Üì  \\n  \\nùóóùóÆùòÅùóÆùòÄùó≤ùòÅ  \\n  \\nThe key component of any successful ML project is the data.  \\n  \\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial\\nscenarios.  \\n  \\nThe best approach is to hire a bunch of experts to create it manually.  \\n  \\nBut, for a PoC, that might get expensive & slow.  \\n  \\nThe good news is that a method called \"ùòçùò™ùòØùò¶ùòµùò∂ùòØùò™ùòØùò® ùò∏ùò™ùòµùò© ùò•ùò™ùò¥ùòµùò™ùò≠ùò≠ùò¢ùòµùò™ùò∞ùòØ\" exists.  \\n  \\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to\\ngenerate your fine-tuning data. After, use this data to fine-tune a smaller\\nmodel (e.g., Falcon 7B).\"  \\n  \\nFor specializing smaller LLMs on specific use cases (e.g., financial\\nadvisors), this is an excellent method to kick off your project.  \\n  \\nùó£ùóøùó≤-ùòÅùóøùóÆùó∂ùóªùó≤ùó± ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†  \\n  \\nYou never want to start training your LLM from scratch (or rarely).  \\n  \\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.  \\n  \\nYou want to fine-tune your LLM on your specific task.  \\n  \\nThe good news is that you can find a plethora of open-source LLMs on\\nHuggingFace (e.g., Falcon, LLaMa, etc.)  \\n  \\nùó£ùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùó≤ùó≥ùó≥ùó∂ùó∞ùó∂ùó≤ùóªùòÅ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥  \\n  \\nAs LLMs are big... duh...  \\n  \\n... they don\\'t fit on a single GPU.  \\n  \\nAs you want only to fine-tune the LLM, the community invented clever\\ntechniques that quantize the LLM (to fit on a single GPU) and fine-tune only a\\nset of smaller adapters.  \\n  \\nOne popular approach is QLoRA, which can be implemented using HF\\'s `ùò±ùò¶ùòßùòµ`\\nPython package.  \\n  \\nùó†ùóüùó¢ùóΩùòÄ  \\n  \\nAs you want your project to get to production, you have to integrate the\\nfollowing MLOps components:  \\n  \\n\\\\- experiment tracker to monitor & compare your experiments  \\n\\\\- model registry to version & share your models between the FTI pipelines  \\n\\\\- prompts monitoring to debug & track complex chains  \\n  \\n‚Ü≥üîó All of them are available on ML platforms, such as Comet ML  \\n  \\nùóñùóºùó∫ùóΩùòÇùòÅùó≤ ùóΩùóπùóÆùòÅùó≥ùóºùóøùó∫  \\n  \\nThe most common approach is to train your LLM on your on-prem Nivida GPUs\\ncluster or rent them on cloud providers such as AWS, Paperspace, etc.  \\n  \\nBut what if I told you that there is an easier way?  \\n  \\nThere is! It is called serverless.  \\n  \\nFor example, Beam is a GPU serverless provider that makes deploying your\\ntraining pipeline as easy as decorating your Python function with\\n`@ùò¢ùò±ùò±.ùò≥ùò∂ùòØ()`.  \\n  \\nAlong with ease of deployment, you can easily add your training code to your\\nCI/CD to add the final piece of the MLOps puzzle, called CT (continuous\\ntraining).  \\n  \\n‚Ü≥üîó Beam\\n\\nWhat | Training Pipeline [Image by the Author].\\n\\n> ‚Ü≥üîó To see all these components in action, check out our FREE ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ\\n> ùó∞ùóºùòÇùóøùòÄùó≤ & give it a ‚≠ê\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 7** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n4\\n\\nShare this post\\n\\n#### DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-what-do-you-need-to-fine-tune?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('174d6f07-42f4-4190-9150-bb4ad35f8413'), content={'Title': 'DML: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?', 'Subtitle': 'Lesson 5 | The Hands-on LLMs Series', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Why & when do you need to fine-tune open-source LLMs? What about\\nfine-tuning vs. prompt engineering?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Why & when do you need to fine-tune open-source LLMs? What about fine-\\ntuning vs. prompt engineering?\\n\\n### Lesson 5 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 30, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: Why & when do you need to fine-tune open-source LLMs? What about\\nfine-tuning vs. prompt engineering?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 5 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. Using this Python package, you can x10 your text preprocessing pipeline development.\\n\\n  2. Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n  3. Fine-tuning video lessons\\n\\n#### Previous Lessons:\\n\\n  * Lesson 2: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n  * Lesson 3: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?\\n\\n  * Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Using this Python package, you can x10 your text preprocessing\\npipeline development\\n\\nAny text preprocessing pipeline has to clean, partition, extract, or chunk\\ntext data to feed it into your LLMs.  \\n  \\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± offers a ùóøùó∂ùó∞ùóµ and ùó∞ùóπùó≤ùóÆùóª ùóîùó£ùóú that allows you to quickly:  \\n  \\n\\\\- ùò±ùò¢ùò≥ùòµùò™ùòµùò™ùò∞ùòØ your data into smaller segments from various data sources (e.g.,\\nHTML, CSV, PDFs, even images, etc.)  \\n\\\\- ùò§ùò≠ùò¶ùò¢ùòØùò™ùòØùò® the text of anomalies (e.g., wrong ASCII characters), any\\nirrelevant information (e.g., white spaces, bullets, etc.), and filling\\nmissing values  \\n\\\\- ùò¶ùòπùòµùò≥ùò¢ùò§ùòµùò™ùòØùò® information from pieces of text (e.g., datetimes, addresses, IP\\naddresses, etc.)  \\n\\\\- ùò§ùò©ùò∂ùòØùò¨ùò™ùòØùò® your text segments into pieces of text that can be inserted into\\nyour embedding model  \\n\\\\- ùò¶ùòÆùò£ùò¶ùò•ùò•ùò™ùòØùò® data (e.g., wrapper over OpenAIEmbeddingEncoder,\\nHuggingFaceEmbeddingEncoders, etc.)  \\n\\\\- ùò¥ùòµùò¢ùò®ùò¶ your data to be fed into various tools (e.g., Label Studio, Label\\nBox, etc.)\\n\\nUnstructured [Image by the Author].\\n\\nùóîùóπùóπ ùòÅùóµùó≤ùòÄùó≤ ùòÄùòÅùó≤ùóΩùòÄ ùóÆùóøùó≤ ùó≤ùòÄùòÄùó≤ùóªùòÅùó∂ùóÆùóπ ùó≥ùóºùóø:  \\n  \\n\\\\- feeding your data into your LLMs  \\n\\\\- embedding the data and ingesting it into a vector DB  \\n\\\\- doing RAG  \\n\\\\- labeling  \\n\\\\- recommender systems  \\n  \\n... basically for any LLM or multimodal applications  \\n  \\n.  \\n  \\nImplementing all these steps from scratch will take a lot of time.  \\n  \\nI know some Python packages already do this, but the functionality is\\nscattered across multiple packages.  \\n  \\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± packages everything together under a nice, clean API.  \\n  \\n‚Ü≥ Check it out.\\n\\n* * *\\n\\n### #2. Why & when do you need to fine-tune open-source LLMs? What about fine-\\ntuning vs. prompt engineering?\\n\\nFine-tuning is the process of taking a pre-trained model and further refining\\nit on a specific task.  \\n  \\nùóôùó∂ùóøùòÄùòÅ, ùóπùó≤ùòÅ'ùòÄ ùó∞ùóπùóÆùóøùó∂ùó≥ùòÜ ùòÑùóµùóÆùòÅ ùó∫ùó≤ùòÅùóµùóºùó±ùòÄ ùóºùó≥ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùóÆùóª ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó† ùó≤ùòÖùó∂ùòÄt ‚Üì  \\n  \\n\\\\- ùòäùò∞ùòØùòµùò™ùòØùò∂ùò¶ùò• ùò±ùò≥ùò¶-ùòµùò≥ùò¢ùò™ùòØùò™ùòØùò®: utilize domain-specific data to apply the same pre-\\ntraining process (next token prediction) on the pre-trained (base) model  \\n\\\\- ùòêùòØùò¥ùòµùò≥ùò∂ùò§ùòµùò™ùò∞ùòØ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò®: the pre-trained (base) model is fine-tuned on a\\nQ&A dataset to learn to answer questions  \\n\\\\- ùòöùò™ùòØùò®ùò≠ùò¶-ùòµùò¢ùò¥ùò¨ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò®: the pre-trained model is refined for a specific\\ntask, such as toxicity detection, coding, medicine advice, etc.  \\n\\\\- ùòôùòìùòèùòç: It requires collecting human preferences (e.g., pairwise\\ncomparisons), which are then used to train a reward model. The reward model is\\nused to fine-tune the LLM via RL techniques such as PPO.  \\n  \\nCommon approaches are to take a pre-trained LLM (next-word prediction) and\\napply instruction & single-task fine-tuning.  \\n  \\nùó™ùóµùòÜ ùó±ùóº ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùòÅùóº ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùòÅùóµùó≤ ùóüùóüùó†?  \\n  \\nYou do instruction fine-tuning to make the LLM learn to answer your questions.  \\n  \\nThe exciting part is when you want to fine-tune your LLM on a single task.  \\n  \\nHere is why ‚Üì  \\n  \\nùò±ùò¶ùò≥ùòßùò∞ùò≥ùòÆùò¢ùòØùò§ùò¶: it will improve your LLM performance on given use cases (e.g.,\\ncoding, extracting text, etc.). Mainly, the LLM will specialize in a given\\ntask (a specialist will always beat a generalist in its domain)  \\n  \\nùò§ùò∞ùòØùòµùò≥ùò∞ùò≠: you can refine how your model should behave on specific inputs and\\noutputs, resulting in a more robust product  \\n  \\nùòÆùò∞ùò•ùò∂ùò≠ùò¢ùò≥ùò™ùòªùò¢ùòµùò™ùò∞ùòØ: you can create an army of smaller models, where each is\\nspecialized on a particular task, increasing the overall system's performance.\\nUsually, when you fine-tune one task, it reduces the performance of the other\\ntasks (known as the  \\nalignment tax). Thus, having an expert system of multiple smaller models can\\nimprove the overall performance.  \\n  \\nùó™ùóµùóÆùòÅ ùóÆùóØùóºùòÇùòÅ ùóΩùóøùóºùó∫ùóΩùòÅ ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóøùó∂ùóªùó¥ ùòÉùòÄ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥?  \\n  \\nùò•ùò¢ùòµùò¢: use prompting when you don't have data available (~2 examples are\\nenough). Fine-tuning needs at least >=100 examples to work.  \\n  \\nùò§ùò∞ùò¥ùòµ: prompting forces you to write long & detailed prompts to achieve your\\nlevel of performance. You pay per token (API or compute-wise). Thus, when a\\nprompt gets bigger, your costs increase. But, when fine-tuning an LLM, you\\nincorporate all that knowledge inside the model. Hence, you can use smaller\\nprompts with similar performance.\\n\\nFine-tuning LLMs [Image by the Author].\\n\\nWhen you start a project, a good strategy is to write a wrapper over an API\\n(e.g., OpenAI's GPT-4, Anyscale, etc.) that defines a desired interface that\\ncan easily be swapped with your open-source implementation in future\\niterations.\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\n### #3. Fine-tuning video lessons  \\n\\nAs you might know,\\n\\nPau Labarta Bajo\\n\\nfrom\\n\\nReal-World Machine Learning\\n\\nand I are also working on a free Hands-on LLMs course that contains the open-\\nsource code + a set of video lessons.\\n\\nHere are the 2 video lessons about fine-tuning ‚Üì\\n\\n#### 01 Hands-on LLMS | Theoretical Part\\n\\nHere is a ùò¥ùò∂ùòÆùòÆùò¢ùò≥ùò∫ of the 1ùò¥ùòµ ùò∑ùò™ùò•ùò¶ùò∞ ùò≠ùò¶ùò¥ùò¥ùò∞ùòØ ‚Üì\\n\\nùó™ùóµùòÜ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùóπùóÆùóøùó¥ùó≤ ùóπùóÆùóªùó¥ùòÇùóÆùó¥ùó≤ ùó∫ùóºùó±ùó≤ùóπùòÄ?  \\n  \\n1\\\\. ùòóùò¶ùò≥ùòßùò∞ùò≥ùòÆùò¢ùòØùò§ùò¶: Fine-tuning a large language model (LLM) can improve\\nperformance, especially for specialized tasks.  \\n  \\n2\\\\. ùòåùò§ùò∞ùòØùò∞ùòÆùò™ùò§ùò¥: Fine-tuned models are smaller and thus cheaper to run. This is\\ncrucial, given that LLMs can have billions of parameters.  \\n  \\nùó™ùóµùóÆùòÅ ùó±ùóº ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùòÅùóº ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùóÆ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤?  \\n  \\n1\\\\. ùòãùò¢ùòµùò¢ùò¥ùò¶ùòµ: You need a dataset of input-output examples. This dataset can be\\ncreated manually or semi-automatically using existing LLMs like GPT-3.5.  \\n  \\n2\\\\. ùòâùò¢ùò¥ùò¶ ùòìùòìùòî: Choose an open-source LLM from repositories like Hugging Face's\\nModel Hub (e.g., Falcon 7B)  \\n  \\n3\\\\. ùòçùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùò¥ùò§ùò≥ùò™ùò±ùòµ: Data loader + Trainer  \\n  \\n4\\\\. ùòàùò•ùò∑ùò¢ùòØùò§ùò¶ùò• ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùòµùò¶ùò§ùò©ùòØùò™ùò≤ùò∂ùò¶ùò¥ ùòµùò∞ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò¶ ùòµùò©ùò¶ ùòÆùò∞ùò•ùò¶ùò≠ ùò∞ùòØ ùò§ùò©ùò¶ùò¢ùò± ùò©ùò¢ùò≥ùò•ùò∏ùò¢ùò≥ùò¶:\\nQLoRA  \\n  \\n5\\\\. ùòîùòìùòñùò±ùò¥: Experiment Tracker + Model Registry  \\n  \\n6\\\\. ùòêùòØùòßùò≥ùò¢ùò¥ùòµùò≥ùò∂ùò§ùòµùò∂ùò≥ùò¶: Comet \\\\+ Beam\\n\\n#### 02 Hands-on LLMS | Diving into the code\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùóÆ ùòÄùóµùóºùóøùòÅ ùòÑùóÆùóπùó∏ùòÅùóµùóøùóºùòÇùó¥ùóµ ùóºùó≥ ùòÅùóµùó≤ ùóπùó≤ùòÄùòÄùóºùóª ‚Üì  \\n  \\n1\\\\. How to set up the code and environment using Poetry  \\n2\\\\. How to configure Comet & Beam  \\n3\\\\. How to start the training pipeline locally (if you have a CUDA-enabled\\nGPU) or on Beam (for running your training pipeline on a serverless\\ninfrastructure -> doesn't matter what hardware you have).  \\n4\\\\. An overview of the code  \\n5\\\\. Clarifying why we integrated Poetry, a model registry and linting within\\nthe training pipeline.  \\n  \\n‚ùóThis video is critical for everyone who wants to replicate the training\\npipeline of our course on their system. The previous lesson focused on the\\ntheoretical parts of the training pipeline.\\n\\n> ‚Ü≥üîó To find out the code & all the videos, check out the **Hands-on LLMs**\\n> GitHub repository.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 6** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: Why & when do you need to fine-tune open-source LLMs? What about\\nfine-tuning vs. prompt engineering?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-why-and-when-do-you-need-to-fine?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('b6d86294-1bcc-4226-8218-3a63cab813a2'), content={'Title': 'DML: How to implement a streaming pipeline to populate a vector DB for real-time RAG?', 'Subtitle': 'Lesson 4 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How to implement a streaming pipeline to populate a vector DB for real-\\ntime RAG?\\n\\n### Lesson 4 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 23, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 4 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. What is Bytewax?\\n\\n  2. Why have vector DBs become so popular? Why are they so crucial for most ML applications?\\n\\n  3. How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 1: How to design an LLM system for a financial assistant using the 3-pipeline design\\n\\n  * Lesson 2: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n  * Lesson 3: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. What is Bytewax?\\n\\nAre you afraid of writing ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ùòÄ? Or do you think they are hard\\nto implement?  \\n  \\nI did until I discovered Bytewax üêù. Let me show you ‚Üì  \\n  \\nBytewax üêù is an ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùòÄùòÅùóøùó≤ùóÆùó∫ ùóΩùóøùóºùó∞ùó≤ùòÄùòÄùó∂ùóªùó¥ ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ that:  \\n\\\\- is built in Rust ‚öôÔ∏è for performance  \\n\\\\- has Python üêç binding for ease of use  \\n  \\n... so for all the Python fanatics out there, no more JVM headaches for you.  \\n  \\nJokes aside, here is why Bytewax üêù is so powerful ‚Üì  \\n  \\n\\\\- Bytewax local setup is plug-and-play  \\n\\\\- can quickly be integrated into any Python project (you can go wild -- even\\nuse it in Notebooks)  \\n\\\\- can easily be integrated with other Python packages (NumPy, PyTorch,\\nHuggingFace, OpenCV, SkLearn, you name it)  \\n\\\\- out-of-the-box connectors for Kafka, local files, or you can quickly\\nimplement your own  \\n\\\\- CLI tool to easily deploy it to K8s, AWS, or GCP.  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶ (ùò≠ùò∞ùò∞ùò¨ ùò¢ùòµ ùòµùò©ùò¶ ùò™ùòÆùò¢ùò®ùò¶ ùò£ùò¶ùò≠ùò∞ùò∏):  \\n1\\\\. We defined a streaming app in a few lines of code.  \\n2\\\\. We run the streaming app with one command.  \\n  \\n.  \\n  \\nThe thing is that I worked in Kafka Streams (in Kotlin) for one year.  \\n  \\nI loved & understood the power of building streaming applications. The only\\nthing that stood in my way was, well... Java.  \\n  \\nI don\\'t have something with Java; it is a powerful language. However, building\\nan ML application in Java + Python takes much time due to a more significant\\nresistance to integrating the two.  \\n  \\n...and that\\'s where Bytewax üêù kicks in.  \\n  \\nWe used Bytewax üêù for building the streaming pipeline for the ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ\\ncourse and loved it.\\n\\nWhat is Bytewax? [Iamge by the Author].\\n\\n* * *\\n\\n### #2. Why have vector DBs become so popular? Why are they so crucial for\\nmost ML applications?\\n\\nIn the world of ML, everything can be represented as an embedding.  \\n  \\nA vector DB is an intelligent way to use your data embeddings as an index and\\nperform fast and scalable searches between unstructured data points.  \\n  \\nSimply put, a vector DB allows you to find matches between anything and\\nanything (e.g., use an image as a query to find similar pieces of text, video,\\nother images, etc.).  \\n  \\n.  \\n  \\nùòêùòØ ùò¢ ùòØùò∂ùòµùò¥ùò©ùò¶ùò≠ùò≠, ùòµùò©ùò™ùò¥ ùò™ùò¥ ùò©ùò∞ùò∏ ùò∫ùò∞ùò∂ ùò§ùò¢ùòØ ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò¶ ùò¢ ùò∑ùò¶ùò§ùòµùò∞ùò≥ ùòãùòâ ùò™ùòØ ùò≥ùò¶ùò¢ùò≠-ùò∏ùò∞ùò≥ùò≠ùò•\\nùò¥ùò§ùò¶ùòØùò¢ùò≥ùò™ùò∞ùò¥ ‚Üì  \\n  \\nUsing various DL techniques, you can project your data points (images, videos,\\ntext, audio, user interactions) into the same vector space (aka the embeddings\\nof the data).  \\n  \\nYou will load the embeddings along a payload (e.g., a URL to the image, date\\nof creation, image description, properties, etc.) into the vector DB, where\\nthe data will be indexed along the:  \\n\\\\- vector  \\n\\\\- payload  \\n\\\\- text within the payload  \\n  \\nNow that the embedding indexes your data, you can query the vector DB by\\nembedding any data point.  \\n  \\nFor example, you can query the vector DB with an image of your cat and use a\\nfilter to retrieve only \"black\" cats.  \\n  \\nTo do so, you must embed the image using the same model you used to embed the\\ndata within your vector DB. After you query the database using a given\\ndistance (e.g., cosine distance between 2 vectors) to find similar embeddings.  \\n  \\nThese similar embeddings have attached to them their payload that contains\\nvaluable information such as the URL to an image, a URL to a site, an ID of a\\nuser, a chapter from a book about the cat of a witch, etc.  \\n  \\n.  \\n  \\nUsing this technique, I used Qdrant to implement RAG for a financial assistant\\npowered by LLMs.  \\n  \\nBut vector DBs go beyond LLMs & RAG.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò™ùò¥ ùò¢ ùò≠ùò™ùò¥ùòµ ùò∞ùòß ùò∏ùò©ùò¢ùòµ ùò∫ùò∞ùò∂ ùò§ùò¢ùòØ ùò£ùò∂ùò™ùò≠ùò• ùò∂ùò¥ùò™ùòØùò® ùò∑ùò¶ùò§ùòµùò∞ùò≥ ùòãùòâùò¥ (e.g., Qdrant ):  \\n  \\n\\\\- similar image search  \\n\\\\- semantic text search (instead of plain text search)  \\n\\\\- recommender systems  \\n\\\\- RAG for chatbots  \\n\\\\- anomalies detection  \\n  \\n‚Ü≥üîó ùòäùò©ùò¶ùò§ùò¨ ùò∞ùò∂ùòµ ùòòùò•ùò≥ùò¢ùòØùòµ\\'ùò¥ ùò®ùò∂ùò™ùò•ùò¶ùò¥ ùò¢ùòØùò• ùòµùò∂ùòµùò∞ùò≥ùò™ùò¢ùò≠ùò¥ ùòµùò∞ ùò≠ùò¶ùò¢ùò≥ùòØ ùòÆùò∞ùò≥ùò¶ ùò¢ùò£ùò∞ùò∂ùòµ ùò∑ùò¶ùò§ùòµùò∞ùò≥ ùòãùòâùò¥.\\n\\nQdrant‚Äôs Architecture [Image from Qdrant docs].\\n\\n* * *\\n\\n### #3. How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\nThis is ùóµùóºùòÑ you can ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ to populate a ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï to\\ndo ùó•ùóîùóö for a ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùòÄùòÄùó∂ùòÄùòÅùóÆùóªùòÅ powered by ùóüùóüùó†ùòÄ.  \\n  \\nIn a previous post, I covered ùòÑùóµùòÜ you need a streaming pipeline over a batch\\npipeline when implementing RAG.  \\n  \\nNow, we will focus on the ùóµùóºùòÑ, aka implementation details ‚Üì  \\n  \\nüêù All the following steps are wrapped in Bytewax functions and connected in a\\nsingle streaming pipeline.  \\n  \\nùóòùòÖùòÅùóøùóÆùó∞ùòÅ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóªùó≤ùòÑùòÄ ùó≥ùóøùóºùó∫ ùóîùóπùóΩùóÆùó∞ùóÆ  \\n  \\nYou need 2 types of inputs:  \\n  \\n1\\\\. A WebSocket API to listen to financial news in real-time. This will be\\nused to listen 24/7 for new data and ingest it as soon as it is available.  \\n  \\n2\\\\. A RESTful API to ingest historical data in batch mode. When you deploy a\\nfresh vector DB, you must populate it with data between a given range\\n[date_start; date_end].  \\n  \\nYou wrap the ingested HTML document and its metadata in a `pydantic`\\nNewsArticle model to validate its schema.  \\n  \\nRegardless of the input type, the ingested data is the same. Thus, the\\nfollowing steps are the same for both data inputs ‚Üì  \\n  \\nùó£ùóÆùóøùòÄùó≤ ùòÅùóµùó≤ ùóõùóßùó†ùóü ùó∞ùóºùóªùòÅùó≤ùóªùòÅ  \\n  \\nAs the ingested financial news is in HTML, you must extract the text from\\nparticular HTML tags.  \\n  \\n`unstructured` makes it as easy as calling `partition_html(document)`, which\\nwill recursively return the text within all essential HTML tags.  \\n  \\nThe parsed NewsArticle model is mapped into another `pydantic` model to\\nvalidate its new schema.  \\n  \\nThe elements of the news article are the headline, summary and full content.  \\n  \\nùóñùóπùó≤ùóÆùóª ùòÅùóµùó≤ ùòÅùó≤ùòÖùòÅ  \\n  \\nNow we have a bunch of text that has to be cleaned. Again, `unstructured`\\nmakes things easy. Calling a few functions we clean:  \\n\\\\- the dashes & bullets  \\n\\\\- extra whitespace & trailing punctuation  \\n\\\\- non ascii chars  \\n\\\\- invalid quotes  \\n  \\nFinally, we standardize everything to lowercase.  \\n  \\nùóñùóµùòÇùóªùó∏ ùòÅùóµùó≤ ùòÅùó≤ùòÖùòÅ  \\n  \\nAs the text can exceed the context window of the embedding model, we have to\\nchunk it.  \\n  \\nYet again, `unstructured` provides a valuable function that splits the text\\nbased on the tokenized text and expected input length of the embedding model.  \\n  \\nThis strategy is naive, as it doesn\\'t consider the text\\'s structure, such as\\nchapters, paragraphs, etc. As the news is short, this is not an issue, but\\nLangChain provides a `RecursiveCharacterTextSplitter` class that does that if\\nrequired.  \\n  \\nùóòùó∫ùóØùó≤ùó± ùòÅùóµùó≤ ùó∞ùóµùòÇùóªùó∏ùòÄ  \\n  \\nYou pass all the chunks through an encoder-only model.  \\n  \\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model\\nthat can run on a CPU and outputs a 384 embedding.  \\n  \\nBut based on the size and complexity of your data, you might need more complex\\nand bigger models.  \\n  \\nùóüùóºùóÆùó± ùòÅùóµùó≤ ùó±ùóÆùòÅùóÆ ùó∂ùóª ùòÅùóµùó≤ ùó§ùó±ùóøùóÆùóªùòÅ ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï  \\n  \\nFinally, you insert the embedded chunks and their metadata into the Qdrant\\nvector DB.  \\n  \\nThe metadata contains the embedded text, the source_url and the publish date.\\n\\nHow to implement a streaming pipeline to populate a vector DB for real-time\\nRAG [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 5** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-to-implement-a-streaming?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('b2296169-eed0-4b28-864a-08b061f5ee45'), content={'Title': 'DML: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?', 'Subtitle': 'Lesson 3 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\n### Lesson 3 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 16, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 3 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. RAG: What problems does it solve, and how it\\'s integrated into LLM-powered applications?\\n\\n  2. Why do you need a streaming pipeline instead of a batch pipeline when implementing RAG in your LLM applications?\\n\\n  3. What do you need to implement a streaming pipeline for a financial assistant?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 1: How to design an LLM system for a financial assistant using the 3-pipeline design\\n\\n  * Lesson 2: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. RAG: What problems does it solve, and how it\\'s integrated into LLM-\\npowered applications?\\n\\nLet\\'s find out ‚Üì  \\n  \\nRAG is a popular strategy when building LLMs to add external data to your\\nprompt.  \\n  \\n=== ùó£ùóøùóºùóØùóπùó≤ùó∫ ===  \\n  \\nWorking with LLMs has 3 main issues:  \\n  \\n1\\\\. The world moves fast  \\n  \\nAn LLM learns an internal knowledge base. However, the issue is that its\\nknowledge is limited to its training dataset.  \\n  \\nThe world moves fast. New data flows on the internet every second. Thus, the\\nmodel\\'s knowledge base can quickly become obsolete.  \\n  \\nOne solution is to fine-tune the model every minute or day...  \\n  \\nIf you have some billions to spend around, go for it.  \\n  \\n2\\\\. Hallucinations  \\n  \\nAn LLM is full of testosterone and likes to be blindly confident.  \\n  \\nEven if the answer looks 100% legit, you can never fully trust it.  \\n  \\n3\\\\. Lack of reference links  \\n  \\nIt is hard to trust the response of the LLM if we can\\'t see the source of its\\ndecisions.  \\n  \\nEspecially for important decisions (e.g., health, financials)  \\n  \\n=== ùó¶ùóºùóπùòÇùòÅùó∂ùóºùóª ===  \\n  \\n‚Üí Surprize! It is RAG.  \\n  \\n1\\\\. Avoid fine-tuning  \\n  \\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge\\nbase as the main memory (e.g., vector DB).  \\n  \\nThe memory is volatile, so you can quickly introduce or remove data.  \\n  \\n2\\\\. Avoid hallucinations  \\n  \\nBy forcing the LLM to answer solely based on the given context, the LLM will\\nprovide an answer as follows:  \\n\\\\- use the external data to respond to the user\\'s question if it contains the\\nnecessary insights  \\n\\\\- \"I don\\'t know\" if not  \\n  \\n3\\\\. Add reference links  \\n  \\nUsing RAG, you can easily track the source of the data and highlight it to the\\nuser.  \\n  \\n=== ùóõùóºùòÑ ùó±ùóºùó≤ùòÄ ùó•ùóîùóö ùòÑùóºùóøùó∏? ===  \\n  \\nLet\\'s say we want to use RAG to build a financial assistant.  \\n  \\nùòûùò©ùò¢ùòµ ùò•ùò∞ ùò∏ùò¶ ùòØùò¶ùò¶ùò•?  \\n  \\n\\\\- a data source with historical and real-time financial news (e.g. Alpaca)  \\n\\\\- a stream processing engine (e.g., Bytewax)  \\n\\\\- an encoder-only model for embedding the documents (e.g., pick one from\\n`sentence-transformers`)  \\n\\\\- a vector DB (e.g., Qdrant)  \\n  \\nùòèùò∞ùò∏ ùò•ùò∞ùò¶ùò¥ ùò™ùòµ ùò∏ùò∞ùò≥ùò¨?  \\n  \\n‚Ü≥ On the feature pipeline side:  \\n  \\n1\\\\. using Bytewax, you ingest the financial news and clean them  \\n2\\\\. you chunk the news documents and embed them  \\n3\\\\. you insert the embedding of the docs along with their metadata (e.g., the\\ninitial text, source_url, etc.) to Qdrant  \\n  \\n‚Ü≥ On the inference pipeline side:  \\n  \\n4\\\\. the user question is embedded (using the same embedding model)  \\n5\\\\. using this embedding, you extract the top K most similar news documents\\nfrom Qdrant  \\n6\\\\. along with the user question, you inject the necessary metadata from the\\nextracted top K documents into the prompt template (e.g., the text of\\ndocuments & its source_url)  \\n7\\\\. you pass the whole prompt to the LLM for the final answer\\n\\nWhat is Retrieval Augmented Generation (RAG)? [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\n### #2. Why do you need a streaming pipeline instead of a batch pipeline when\\nimplementing RAG in your LLM applications?\\n\\nThe quality of your RAG implementation is as good as the quality & freshness\\nof your data.  \\n  \\nThus, depending on your use case, you have to ask:  \\n\"How fresh does my data from the vector DB have to be to provide accurate\\nanswers?\"  \\n  \\nBut for the best user experience, the data has to be as fresh as possible, aka\\nreal-time data.  \\n  \\nFor example, when implementing a financial assistant, being aware of the\\nlatest financial news is critical. A new piece of information can completely\\nchange the course of your strategy.  \\n  \\nHence, when implementing RAG, one critical aspect is to have your vector DB\\nsynced with all your external data sources in real-time.  \\n  \\nA batch pipeline will work if your use case accepts a particular delay (e.g.,\\none hour, one day, etc.).  \\n  \\nBut with tools like Bytewax üêù, building streaming applications becomes much\\nmore accessible. So why not aim for the best?\\n\\nStreaming vs. batch pipelines when doing RAG [Image by the Author]\\n\\n* * *\\n\\n### #3. What do you need to implement a streaming pipeline for a financial\\nassistant?\\n\\n\\\\- A financial news data source exposed through a web socket (e.g., Alpaca)  \\n  \\n\\\\- A Python streaming processing framework. For example, Bytewax üêù is built in\\nRust for efficiency and exposes a Python interface for ease of use - you don\\'t\\nneed the Java ecosystem to implement real-time pipelines anymore.  \\n  \\n\\\\- A Python package to process, clean, and chunk documents. `unstructured`\\noffers a rich set of features that makes parsing HTML documents extremely\\nconvenient.  \\n  \\n\\\\- An encoder-only language model that maps your chunked documents into\\nembeddings. `setence-transformers` is well integrated with HuggingFace and has\\na huge list of models of various sizes.  \\n  \\n\\\\- A vector DB, where to insert your embeddings and their metadata (e.g., the\\nembedded text, the source_url, the creation date, etc.). For example, Qdrant\\nprovides a rich set of features and a seamless experience.  \\n  \\n\\\\- A way to deploy your streaming pipeline. Docker + AWS will never disappoint\\nyou.  \\n  \\n\\\\- A CI/CD pipeline for continuous tests & deployments. GitHub Actions is a\\ngreat serverless option with a rich ecosystem.  \\n  \\nThis is what you need to build & deploy a streaming pipeline solely in Python\\nüî•\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 4** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-why-and-what-do-you-need-a-streaming?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('032f3296-b891-484d-9e00-c2872bbb9bbe'), content={'Title': 'DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps', 'Subtitle': 'Lesson 2 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n### Lesson 2 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 09, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 2 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. Introduction video lessons \\n\\n  2. What is LLMOps? MLOps vs. LLMOps\\n\\n  3. Unwrapping step-by-step the 3-pipeline design of a financial assistant powered by LLMs\\n\\n#### Previous Lessons:\\n\\n  * Lesson 1: How to design an LLM system for a financial assistant using the 3-pipeline design\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Introduction video lessons\\n\\nWe started releasing the first video lessons of the course.\\n\\nThis is a recording of me, where I presented at a webinar hosted by Gathers, a\\n1.5-hour overview of the ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ course.\\n\\nCheck it out to get a gut feeling of the LLM system ‚Üì\\n\\nThis is the **1st official lesson** of the **Hands-on LLMs** course presented\\nby no other but\\n\\nPau Labarta Bajo\\n\\nfrom the **Real-World Machine Learning** newsletter (if you wonder, the course\\nis the result of our collaboration).\\n\\nPau is one of the best teachers I know. If you have some spare time, it is\\nworth it ‚Üì\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #2. What is LLMOps? MLOps vs. LLMOps\\n\\nLLMOps here, LLMOps there, but did you take the time to see how it differs\\nfrom MLOps?  \\n  \\nIf not, here is a 2-min LLMOps vs. MLOps summary ‚Üì  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóüùóüùó†ùó¢ùóΩùòÄ?  \\n  \\nWell, everything revolves around the idea that \"Size matters.\"  \\n  \\nLLMOps is about best practices for efficient deployment, monitoring and\\nmaintenance, but this time for large language models.  \\n  \\nLLMOps is a subset of MLOps, focusing on training & deploying large models\\ntrained on big data.  \\n  \\nIntuitive right?  \\n  \\nùóïùòÇùòÅ ùóµùó≤ùóøùó≤ ùóÆùóøùó≤ ùü± ùóüùóüùó†ùó¢ùóΩùòÄ ùòÇùóªùó∂ùóæùòÇùó≤ ùó≥ùóÆùó∞ùòÅùóºùóøùòÄ ùòÅùóµùóÆùòÅ ùòÄùó≤ùòÅ ùó∂ùòÅ ùóÆùóΩùóÆùóøùòÅ ùó≥ùóøùóºùó∫ ùó†ùóüùó¢ùóΩùòÄ ‚Üì  \\n  \\nùü≠\\\\. ùóñùóºùó∫ùóΩùòÇùòÅùóÆùòÅùó∂ùóºùóªùóÆùóπ ùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ùòÄ: training your models on CUDA-enabled GPUs is more\\ncritical than ever, along with knowing how to run your jobs on a cluster of\\nGPUs leveraging data & model parallelism using techniques such as ZeRO from\\nDeepSpeed. Also, the high cost of inference makes model compression techniques\\nessential for deployment.  \\n  \\nùüÆ\\\\. ùóßùóøùóÆùóªùòÄùó≥ùó≤ùóø ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥: training models from scratch is a thing of the past. In\\nmost use cases, you will fine-tune the model on specific tasks, leveraging\\ntechniques such as LLaMA-Adapters or QLora.  \\n  \\nùüØ\\\\. ùóõùòÇùó∫ùóÆùóª ùó≥ùó≤ùó≤ùó±ùóØùóÆùó∞ùó∏: reinforcement learning from human feedback (RLHF) showed\\nmuch potential in improving the quality of generated outputs. But to do RLHF,\\nyou have to introduce a feedback loop within your ML system that lets you\\nevaluate the generated results based on human feedback, which are even further\\nused to fine-tune your LLMs.  \\n  \\nùü∞\\\\. ùóöùòÇùóÆùóøùó±ùóøùóÆùó∂ùóπùòÄ: to create safe systems, you must protect your systems against\\nharmful or violent inputs and outputs. Also, when designing your prompt\\ntemplates, you must consider hallucinations and prompt hacking.  \\n  \\nùü±\\\\. ùó†ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ & ùóÆùóªùóÆùóπùòÜùòáùó∂ùóªùó¥ ùóΩùóøùóºùó∫ùóΩùòÅùòÄ: most ML platforms (e.g., Comet ML)\\nintroduced specialized logging tools to debug and monitor your LLMs to help\\nyou find better prompt templates and protect against hallucination and\\nhacking.\\n\\nWhat is LLMOps? LLMOps vs. MLOps [Image by the Author]\\n\\nTo conclude...  \\n  \\nLLMOps isn\\'t anything new for those familiar with MLOps and Deep Learning.  \\n  \\nFor example, training deep learning models on clusters of GPUs or fine-tuning\\nthem isn\\'t new, but now it is more important than ever to master these skills\\nas models get bigger and bigger.  \\n  \\nBut it indeed introduced novel techniques to fine-tune models (e.g., QLora),\\nto merge the fields of RL and DL, and a plethora of tools around prompt\\nmanipulation & storing, such as:  \\n\\\\- vector DBs (e.g., Qdrant)  \\n\\\\- prompt chaining (e.g., LangChain)  \\n\\\\- prompt logging & analytics (e.g., Comet LLMOps)  \\n  \\n.  \\n  \\nBut with the new multi-modal large models trend, these tips & tricks will\\nconverge towards all deep learning models (e.g., computer vision), and soon,\\nwe will change the name of LLMOps to DLOps or LMOps.  \\n  \\nWhat do you think? Is the term of LLMOps going to stick around?\\n\\n* * *\\n\\n### #3. Unwrapping step-by-step the 3-pipeline design of a financial assistant\\npowered by LLMs\\n\\nHere is a step-by-step guide on designing the architecture of a financial\\nassistant powered by LLMs, vector DBs and MLOps.  \\n  \\nThe 3-pipeline design, also known as the FTI architecture, makes things simple\\n‚Üì  \\n  \\n=== ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWe want to build a streaming pipeline that listens to real-time financial\\nnews, embeds the news, and loads everything in a vector DB. The goal is to add\\nup-to-date news to the user\\'s questions using RAG to avoid retraining.  \\n  \\n1\\\\. We listen 24/7 to financial news from Alpaca through a WebSocket wrapped\\nover a Bytewax connector  \\n2\\\\. Once any financial news is received, these are passed to the Bytewax flow\\nthat:  \\n\\\\- extracts & cleans the necessary information from the news HTML document  \\n\\\\- chunks the text based on the LLM\\'s max context window  \\n\\\\- embeds all the chunks using the \"all-MiniLM-L6-v2\" encoder-only model from\\nsentence-transformers  \\n\\\\- inserts all the embeddings along their metadata to Qdrant  \\n3\\\\. The streaming pipeline is deployed to an EC2 machine that runs multiple\\nBytewax processes. It can be deployed to K8s into a multi-node setup to scale\\nup.  \\n  \\n=== ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWe want to fine-tune a pretrained LLM to specialize the model to answer\\nfinancial-based questions.  \\n  \\n1\\\\. Manually fill ~100 financial questions.  \\n2\\\\. Use RAG to enrich the questions using the financial news from the Qdrant\\nvector DB.  \\n3\\\\. Use a powerful model such as GPT-4 to answer them, or hire an expert if\\nyou have more time and resources.  \\n4\\\\. Load Falcon from HuggingFace using QLoRA to fit on a single GPU.  \\n5\\\\. Preprocess the Q&A dataset into prompts.  \\n6\\\\. Fine-tune the LLM and log all the artifacts to Comet\\'s experiment tracker\\n(loss, model weights, etc.)  \\n7\\\\. For every epoch, run the LLM on your test set, log the prompts to Comet\\'s\\nprompt logging feature and compute the metrics.  \\n8\\\\. Send the best LoRA weights to the model registry as the next production\\ncandidate.  \\n9\\\\. Deploy steps 4-8 to Beam to run the training on an A10G or A100 Nvidia\\nGPU.  \\n  \\n=== ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWe want to hook the financial news stored in the Qdrant Vector DB and the\\nFalcon fine-tuned model into a single entity exposed under a RESTful API.  \\n  \\nSteps 1-7 are all chained together using LangChain.  \\n  \\n1\\\\. Use the \"all-MiniLM-L6-v2\" encoder-only model to embed the user\\'s\\nquestion.  \\n2\\\\. Using the question embedding, query the Qdrant vector DB to find the top 3\\nrelated financial news.  \\n3\\\\. Attach the text (stored as metadata along the embeddings) of the news to\\nthe prompt (aka RAG).  \\n4\\\\. Download Falcon\\'s pretrained weights from HF & LoRA\\'s fine-tuned weights\\nfrom Comet\\'s model registry.  \\n5\\\\. Load the LLM and pass the prompt (= the user\\'s question, financial news,\\nhistory) to it.  \\n6\\\\. Store the conversation in LangChain\\'s memory.  \\n7\\\\. Deploy steps 1-7 under a RESTful API using Beam.\\n\\n3-pipeline architecture [Image by the Author]\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 3** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-unwrapping-the-3-pipeline-design?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('21c92489-204c-4791-b4dd-f0c2487f7e82'), content={'Title': 'DML: How to design an LLM system for a financial assistant using the 3-pipeline design', 'Subtitle': 'Lesson 1 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\n### Lesson 1 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 02, 2023\\n\\n5\\n\\nShare this post\\n\\n#### DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n> As promised, starting this week, we will **begin** the **series** based on\\n> the **Hands-on LLMs FREE course**.\\n\\nNote that this is not the course itself. It is an overview for all the busy\\npeople who will focus on the key aspects.\\n\\nThe entire course will soon be available on üîó GitHub.\\n\\n* * *\\n\\n### **Lesson 1 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. What is the 3-pipeline design\\n\\n  2. How to apply the 3-pipeline design in architecting a financial assistant powered by LLMs\\n\\n  3. The tech stack used to build an end-to-end LLM system for a financial assistant \\n\\n* * *\\n\\nAs the Hands-on LLMs course is still a ùòÑùóºùóøùó∏ ùó∂ùóª ùóΩùóøùóºùó¥ùóøùó≤ùòÄùòÄ, we want to ùó∏ùó≤ùó≤ùóΩ ùòÜùóºùòÇ\\nùòÇùóΩùó±ùóÆùòÅùó≤ùó± on our progress ‚Üì  \\n\\n> ‚Ü≥ Thus, we opened up the ùó±ùó∂ùòÄùó∞ùòÇùòÄùòÄùó∂ùóºùóª ùòÅùóÆùóØ under the course\\'s GitHub\\n> Repository, where we will ùó∏ùó≤ùó≤ùóΩ ùòÜùóºùòÇ ùòÇùóΩùó±ùóÆùòÅùó≤ùó± with everything is happening.\\n\\n  \\nAlso, if you have any ùó∂ùó±ùó≤ùóÆùòÄ, ùòÄùòÇùó¥ùó¥ùó≤ùòÄùòÅùó∂ùóºùóªùòÄ, ùóæùòÇùó≤ùòÄùòÅùó∂ùóºùóªùòÄ or want to ùó∞ùóµùóÆùòÅ, we\\nencourage you to ùó∞ùóøùó≤ùóÆùòÅùó≤ ùóÆ \"ùóªùó≤ùòÑ ùó±ùó∂ùòÄùó∞ùòÇùòÄùòÄùó∂ùóºùóª\".  \\n  \\n‚Üì We want the course to fill your real needs ‚Üì  \\n  \\n‚Ü≥ Hence, if your suggestion fits well with our hands-on course direction, we\\nwill consider implementing it.\\n\\nHands-on LLMs course discussions section [Image by the Author].\\n\\nCheck it out and leave a ‚≠ê if you like what you see:  \\n‚Ü≥üîó Hands-on LLMs course\\n\\n* * *\\n\\n### #1. What is the 3-pipeline design\\n\\nWe all know how ùó∫ùó≤ùòÄùòÄùòÜ ùó†ùóü ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ can get. That is where the ùüØ-ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\nùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤ ùó∏ùó∂ùó∞ùó∏ùòÄ ùó∂ùóª.  \\n  \\nThe 3-pipeline design is a way to bring structure & modularity to your ML\\nsystem and improve your MLOps processes.  \\n  \\nThis is how ‚Üì  \\n  \\n=== ùó£ùóøùóºùóØùóπùó≤ùó∫ ===  \\n  \\nDespite advances in MLOps tooling, transitioning from prototype to production\\nremains challenging.  \\n  \\nIn 2022, only 54% of the models get into production. Auch.  \\n  \\nSo what happens?  \\n  \\nSometimes the model is not mature enough, sometimes there are some security\\nrisks, but most of the time...  \\n  \\n...the architecture of the ML system is built with research in mind, or the ML\\nsystem becomes a massive monolith that is extremely hard to refactor from\\noffline to online.  \\n  \\nSo, good processes and a well-defined architecture are as crucial as good\\ntools and models.  \\n  \\n  \\n=== ùó¶ùóºùóπùòÇùòÅùó∂ùóºùóª ===  \\n  \\nùòõùò©ùò¶ 3-ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùò¢ùò≥ùò§ùò©ùò™ùòµùò¶ùò§ùòµùò∂ùò≥ùò¶.  \\n  \\nFirst, let\\'s understand what the 3-pipeline design is.  \\n  \\nIt is a mental map that helps you simplify the development process and split\\nyour monolithic ML pipeline into 3 components:  \\n1\\\\. the feature pipeline  \\n2\\\\. the training pipeline  \\n3\\\\. the inference pipeline  \\n  \\n...also known as the Feature/Training/Inference (FTI) architecture.  \\n  \\n.  \\n  \\n#ùü≠. The feature pipeline transforms your data into features & labels, which\\nare stored and versioned in a feature store.  \\n  \\n#ùüÆ. The training pipeline ingests a specific version of the features & labels\\nfrom the feature store and outputs the trained models, which are stored and\\nversioned inside a model registry.  \\n  \\n#ùüØ. The inference pipeline takes a given version of the features and trained\\nmodels and outputs the predictions to a client.  \\n  \\n.  \\n  \\nThis is why the 3-pipeline design is so beautiful:  \\n  \\n\\\\- it is intuitive  \\n\\\\- it brings structure, as on a higher level, all ML systems can be reduced to\\nthese 3 components  \\n\\\\- it defines a transparent interface between the 3 components, making it\\neasier for multiple teams to collaborate  \\n\\\\- the ML system has been built with modularity in mind since the beginning  \\n\\\\- the 3 components can easily be divided between multiple teams (if\\nnecessary)  \\n\\\\- every component can use the best stack of technologies available for the\\njob  \\n\\\\- every component can be deployed, scaled, and monitored independently  \\n\\\\- the feature pipeline can easily be either batch, streaming or both  \\n  \\nBut the most important benefit is that...  \\n  \\n...by following this pattern, you know 100% that your ML model will move out\\nof your Notebooks into production.\\n\\nWhat is the 3-pipeline design & Why should you adopt it in your ML systems?\\n[Image by the Author].\\n\\nWhat do you think about the 3-pipeline architecture? Have you used it?  \\n  \\nIf you want to know more about the 3-pipeline design, I recommend this awesome\\narticle from Hopsworks ‚Üì  \\n‚Ü≥üîó From MLOps to ML Systems with Feature/Training/Inference Pipelines\\n\\n* * *\\n\\n### #2. How to apply the 3-pipeline design in architecting a financial\\nassistant powered by LLMs\\n\\nBuilding ML systems is hard, right? Wrong.  \\n  \\nHere is how the ùüØ-ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ùó±ùó≤ùòÄùó∂ùó¥ùóª can make ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùó∂ùóªùó¥ the ùó†ùóü ùòÄùòÜùòÄùòÅùó≤ùó∫ for a\\nùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùòÄùòÄùó∂ùòÄùòÅùóÆùóªùòÅ ùó≤ùóÆùòÄùòÜ ‚Üì  \\n  \\n.  \\n  \\nI already covered the concepts of the 3-pipeline design in my previous post,\\nbut here is a quick recap:  \\n  \\n\"\"\"  \\nIt is a mental map that helps you simplify the development process and split\\nyour monolithic ML pipeline into 3 components:  \\n1\\\\. the feature pipeline  \\n2\\\\. the training pipeline  \\n3\\\\. the inference pipeline  \\n...also known as the Feature/Training/Inference (FTI) architecture.  \\n\"\"\"  \\n  \\n.  \\n  \\nNow, let\\'s see how you can use the FTI architecture to build a financial\\nassistant powered by LLMs ‚Üì  \\n  \\n#ùü≠. ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\nThe feature pipeline is designed as a streaming pipeline that extracts real-\\ntime financial news from Alpaca and:  \\n  \\n\\\\- cleans and chunks the news documents  \\n\\\\- embeds the chunks using an encoder-only LM  \\n\\\\- loads the embeddings + their metadata in a vector DB  \\n\\\\- deploys it to AWS  \\n  \\nIn this architecture, the vector DB acts as the feature store.  \\n  \\nThe vector DB will stay in sync with the latest news to attach real-time\\ncontext to the LLM using RAG.  \\n  \\n#ùüÆ. ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\nThe training pipeline is split into 2 main steps:  \\n  \\n‚Ü≥ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ ùòÄùó≤ùó∫ùó∂-ùóÆùòÇùòÅùóºùó∫ùóÆùòÅùó≤ùó± ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóºùóª ùòÄùòÅùó≤ùóΩ  \\n  \\nIt takes the vector DB (feature store) and a set of predefined questions\\n(manually written) as input.  \\n  \\nAfter, you:  \\n  \\n\\\\- use RAG to inject the context along the predefined questions  \\n\\\\- use a large & powerful model, such as GPT-4, to generate the answers  \\n\\\\- save the generated dataset under a new version  \\n  \\n‚Ü≥ ùóôùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùòÄùòÅùó≤ùóΩ  \\n  \\n\\\\- download a pre-trained LLM from Huggingface  \\n\\\\- load the LLM using QLoRA  \\n\\\\- preprocesses the generated Q&A dataset into a format expected by the LLM  \\n\\\\- fine-tune the LLM  \\n\\\\- push the best QLoRA weights (model) to a model registry  \\n\\\\- deploy it using a serverless solution as a continuous training pipeline  \\n  \\n#ùüØ. ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\nThe inference pipeline is the financial assistant that the clients actively\\nuse.  \\n  \\nIt uses the vector DB (feature store) and QLoRA weights (model) from the model\\nregistry in the following way:  \\n  \\n\\\\- download the pre-trained LLM from Huggingface  \\n\\\\- load the LLM using the pretrained QLoRA weights  \\n\\\\- connect the LLM and vector DB into a chain  \\n\\\\- use RAG to add relevant financial news from the vector DB  \\n\\\\- deploy it using a serverless solution under a RESTful API\\n\\nThe architecture of a financial assistant using the 3 pipeline design [Image\\nby the Author].\\n\\nHere are the main benefits of using the FTI architecture:  \\n\\\\- it defines a transparent interface between the 3 modules  \\n\\\\- every component can use different technologies to implement and deploy the\\npipeline  \\n\\\\- the 3 pipelines are loosely coupled through the feature store & model\\nregistry  \\n\\\\- every component can be scaled independently\\n\\n> See this architecture in action in my üîó ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ FREE course.\\n\\n* * *\\n\\n### #3. The tech stack used to build an end-to-end LLM system for a financial\\nassistant\\n\\nThe tools are divided based on the ùüØ-ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ (aka ùóôùóßùóú) ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤:  \\n  \\n=== ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWhat do you need to build a streaming pipeline?  \\n  \\n‚Üí streaming processing framework: Bytewax (brings the speed of Rust into our\\nbeloved Python ecosystem)  \\n  \\n‚Üí parse, clean, and chunk documents: unstructured  \\n  \\n‚Üí validate document structure: pydantic  \\n  \\n‚Üí encoder-only language model: HuggingFace sentence-transformers, PyTorch  \\n  \\n‚Üí vector DB: Qdrant  \\n  \\n‚Üídeploy: Docker, AWS  \\n  \\n‚Üí CI/CD: GitHub Actions  \\n  \\n  \\n=== ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWhat do you need to build a fine-tuning pipeline?  \\n  \\n‚Üí pretrained LLM: HuggingFace Hub  \\n  \\n‚Üí parameter efficient tuning method: peft (= LoRA)  \\n  \\n‚Üí quantization: bitsandbytes (= QLoRA)  \\n  \\n‚Üí training: HuggingFace transformers, PyTorch, trl  \\n  \\n‚Üí distributed training: accelerate  \\n  \\n‚Üí experiment tracking: Comet ML  \\n  \\n‚Üí model registry: Comet ML  \\n  \\n‚Üí prompt monitoring: Comet ML  \\n  \\n‚Üí continuous training serverless deployment: Beam  \\n  \\n  \\n=== ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWhat do you need to build a financial assistant?  \\n  \\n‚Üí framework for developing applications powered by language models: LangChain  \\n  \\n‚Üí model registry: Comet ML  \\n  \\n‚Üí inference: HuggingFace transformers, PyTorch, peft (to load the LoRA\\nweights)  \\n  \\n‚Üí quantization: bitsandbytes  \\n  \\n‚Üí distributed inference: accelerate  \\n  \\n‚Üí encoder-only language model: HuggingFace sentence-transformers  \\n  \\n‚Üí vector DB: Qdrant  \\n  \\n‚Üí prompt monitoring: Comet ML  \\n  \\n‚Üí RESTful API serverless service: Beam  \\n  \\n.  \\n  \\nAs you can see, some tools overlap between the FTI pipelines, but not all.  \\n  \\nThis is the beauty of the 3-pipeline design, as every component represents a\\ndifferent entity for which you can pick the best stack to build, deploy, and\\nmonitor.  \\n  \\nYou can go wild and use Tensorflow in one of the components if you want your\\ncolleges to hate you üòÇ\\n\\n> See the tools in action in my üîó ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ FREE course.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 2** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n5\\n\\nShare this post\\n\\n#### DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-to-design-an-llm-system-for?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('007833f1-fb36-470f-adad-78143f817fee'), content={'Title': 'DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG in Your LLM Applications', 'Subtitle': 'Hello there, I am Paul Iusztin üëãüèº', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG\\nin Your LLM Applications\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG in\\nYour LLM Applications\\n\\nPaul Iusztin\\n\\nOct 26, 2023\\n\\n4\\n\\nShare this post\\n\\n#### DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG\\nin Your LLM Applications\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time Rag in Your LLM Applications\\n\\n> **Story:** If anyone told you that ML or MLOps is easy, they were right. A\\n> simple trick I learned the hard way.\\n\\n* * *\\n\\nThis week‚Äôs newsletter is shorter than usual, but I have some great news üî•\\n\\n> Next week, within the Decoding ML newsletter, I will start a step-by-step\\n> series based on the Hands-On LLMs course I am developing.\\n>\\n> By the end of this series, you will know how to design, build, and deploy a\\n> financial assistant powered by LLMs.\\n>\\n> ‚Ä¶all of this for FREE inside the Decoding ML newsletter\\n\\n‚Ü≥üîó Check out the Hands-On LLMs course GitHub page and give it a star to stay\\nupdated with our progress.\\n\\n* * *\\n\\n### #1. Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time Rag\\nin Your LLM Applications\\n\\nTo successfully use ùó•ùóîùóö in your ùóüùóüùó† ùóÆùóΩùóΩùóπùó∂ùó∞ùóÆùòÅùó∂ùóºùóªùòÄ, your ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï must\\nconstantly be updated with the latest data.  \\n  \\nHere is how you can implement a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ to keep your vector DB in\\nsync with your datasets ‚Üì  \\n  \\n.  \\n  \\nùó•ùóîùóö is a popular strategy when building LLMs to add context to your prompt\\nabout your private datasets.  \\n  \\nLeveraging your domain data using RAG provides 2 significant benefits:  \\n\\\\- you don\\'t need to fine-tune your model as often (or at all)  \\n\\\\- avoid hallucinations  \\n  \\n.  \\n  \\nOn the ùóØùóºùòÅ ùòÄùó∂ùó±ùó≤, to implement RAG, you have to:  \\n  \\n3\\\\. Embed the user\\'s question using an embedding model (e.g., BERT). Use the\\nembedding to query your vector DB and find the most similar vectors using a\\ndistance function (e.g., cos similarity).  \\n4\\\\. Get the top N closest vectors and their metadata.  \\n5\\\\. Attach the extracted top N vectors metadata + the chat history to the\\ninput prompt.  \\n6\\\\. Pass the prompt to the LLM.  \\n7\\\\. Insert the user question + assistant answer to the chat history.  \\n  \\n.  \\n  \\nBut the question is, ùóµùóºùòÑ do you ùó∏ùó≤ùó≤ùóΩ ùòÜùóºùòÇùóø ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï ùòÇùóΩ ùòÅùóº ùó±ùóÆùòÅùó≤ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ùóπùóÆùòÅùó≤ùòÄùòÅ\\nùó±ùóÆùòÅùóÆ?  \\n  \\n‚Ü≥ You need a real-time streaming pipeline.  \\n  \\nHow do you implement it?  \\n  \\nYou need 2 components:  \\n  \\n‚Ü≥ A streaming processing framework. For example, Bytewax is built in Rust for\\nefficiency and exposes a Python interface for ease of use - you don\\'t need\\nJava to implement real-time pipelines anymore.  \\n  \\nüîó Bytewax  \\n  \\n‚Ü≥ A vector DB. For example, Qdrant provides a rich set of features and a\\nseamless experience.  \\n  \\nüîó Qdrant  \\n  \\n.  \\n  \\nHere is an example of how to implement a streaming pipeline for financial news\\n‚Üì  \\n  \\n#ùü≠. Financial news data source (e.g., Alpaca):  \\n  \\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to\\nadd data to your vector DB in batch mode between a desired [start_date,\\nend_date] range. You can tweak the number of workers to parallelize this step\\nas much as possible.  \\n‚Üí You run this once in the beginning.  \\n  \\nYou need the data exposed under a web socket to ingest news in real time. So,\\nyou\\'ll be able to listen to the news and ingest it in your vector DB as soon\\nas they are available.  \\n‚Üí Listens 24/7 for financial news.  \\n  \\n#ùüÆ. Build the streaming pipeline using Bytewax:  \\n  \\nImplement 2 input connectors for the 2 different types of APIs: RESTful API &\\nweb socket.  \\n  \\nThe rest of the steps can be shared between both connectors ‚Üì  \\n  \\n\\\\- Clean financial news documents.  \\n\\\\- Chunk the documents.  \\n\\\\- Embed the documents (e.g., using Bert).  \\n\\\\- Insert the embedded documents + their metadata to the vector DB (e.g.,\\nQdrant).  \\n  \\n#ùüØ-ùü≥. When the users ask a financial question, you can leverage RAG with an\\nup-to-date vector DB to search for the latest news in the industry.\\n\\nSynced Vector DBs - A Guide to Streaming Pipelines for Real-Time Rag in Your\\nLLM Applications [Image by the Author]\\n\\n* * *\\n\\n### #Story. If anyone told you that ML or MLOps is easy, they were right. A\\nsimple trick I learned the hard way.\\n\\nIf anyone told you that ùó†ùóü or ùó†ùóüùó¢ùóΩùòÄ is ùó≤ùóÆùòÄùòÜ, they were ùóøùó∂ùó¥ùóµùòÅ.  \\n  \\nHere is a simple trick that I learned the hard way ‚Üì  \\n  \\nIf you are in this domain, you already know that everything changes fast:  \\n  \\n\\\\- a new tool every month  \\n\\\\- a new model every week  \\n\\\\- a new project every day  \\n  \\nYou know what I did? I stopped caring about all these changes and switched my\\nattention to the real gold.  \\n  \\nWhich is ‚Üí \"ùóôùóºùó∞ùòÇùòÄ ùóºùóª ùòÅùóµùó≤ ùó≥ùòÇùóªùó±ùóÆùó∫ùó≤ùóªùòÅùóÆùóπùòÄ.\"  \\n  \\n.  \\n  \\nLet me explain ‚Üì  \\n  \\nWhen you constantly chase the latest models (aka FOMO), you will only have a\\nshallow understanding of that new information (except if you are a genius or\\nalready deep into that niche).  \\n  \\nBut the joke\\'s on you. In reality, most of what you think you need to know,\\nyou don\\'t.  \\n  \\nSo you won\\'t use what you learned and forget most of it after 1-2 months.  \\n  \\nWhat a waste of time, right?  \\n  \\n.  \\n  \\nBut...  \\n  \\nIf you master the fundamentals of the topic, you want to learn.  \\n  \\nFor example, for deep learning, you have to know:  \\n  \\n\\\\- how models are built  \\n\\\\- how they are trained  \\n\\\\- groundbreaking architectures (Resnet, UNet, Transformers, etc.)  \\n\\\\- parallel training  \\n\\\\- deploying a model, etc.  \\n  \\n...when in need (e.g., you just moved on to a new project), you can easily\\npick up the latest research.  \\n  \\nThus, after you have laid the foundation, it is straightforward to learn SoTA\\napproaches when needed (if needed).  \\n  \\nMost importantly, what you learn will stick with you, and you will have the\\nflexibility to jump from one project to another quickly.  \\n  \\n.  \\n  \\nI am also guilty. I used to FOMO into all kinds of topics until I was honest\\nwith myself and admitted I am no Leonardo Da Vinci.  \\n  \\nBut here is what I did and worked well:  \\n  \\n\\\\- building projects  \\n\\\\- replicating the implementations of famous papers  \\n\\\\- teaching the subject I want to learn  \\n... and most importantly, take my time to relax and internalize the\\ninformation.\\n\\nTo conclude:  \\n  \\n\\\\- learn ahead only the fundamentals  \\n\\\\- learn the latest trend only when needed\\n\\n[Image by the Author]\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for the beginning of the Hands-On LLMs series üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n4\\n\\nShare this post\\n\\n#### DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG\\nin Your LLM Applications\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-synced-vector-dbs-a-guide-to?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('e9353901-9ba9-483c-8c59-2de649c9743a'), content={'Title': 'DML: What is the difference between your ML development and continuous training environments?', 'Subtitle': '3 techniques you must know to evaluate your LLMs quickly. Experimentation vs. continuous training environments.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\n### 3 techniques you must know to evaluate your LLMs quickly. Experimentation\\nvs. continuous training environments.\\n\\nPaul Iusztin\\n\\nOct 19, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. 3 techniques you must know to evaluate your LLMs quickly\\n\\n  2. What is the difference between your ML development and continuous training environments?\\n\\n> **Story:** Job roles tell you there is just one type of MLE, but there are\\n> actually 3.\\n\\n* * *\\n\\n> But first, I want to let you know that after 1 year of making content, I\\n> finally decided to share my content on **Twitter/X**.\\n\\nI took this decision because everybody has a different way of reading and\\ninteracting with their socials.  \\n  \\n...and I want everyone to enjoy my content on their favorite platform.\\n\\nI even bought that stu*** blue ticker to see that I am serious about this üòÇ\\n\\nSo...  \\n\\n> If **you like my content** and you are a **Twitter/X** **person** ‚Üì\\n>\\n> ‚Ü≥üîó **follow** at @ùê¢ùêÆùê¨ùê≥ùê≠ùê¢ùêßùê©ùêöùêÆùê•\\n\\n* * *\\n\\n###  #1. 3 techniques you must know to evaluate your LLMs quickly\\n\\nManually testing the output of your LLMs is a tedious and painful process ‚Üí\\nyou need to automate it.  \\n  \\nIn generative AI, most of the time, you cannot leverage standard metrics.  \\n  \\nThus, the real question is, how do you evaluate the outputs of an LLM?  \\n  \\nDepending on your problem, here is what you can do ‚Üì  \\n  \\n#ùü≠. ùó¶ùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùóÆùóªùòÄùòÑùó≤ùóøùòÄ - ùòÜùóºùòÇ ùó∏ùóªùóºùòÑ ùó≤ùòÖùóÆùó∞ùòÅùóπùòÜ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùòÑùóÆùóªùòÅ ùòÅùóº ùó¥ùó≤ùòÅ  \\n  \\nEven if you use an LLM to generate text, you can ask it to generate a response\\nin a structured format (e.g., JSON) that can be parsed.  \\n  \\nYou know exactly what you want (e.g., a list of products extracted from the\\nuser\\'s question).  \\n  \\nThus, you can easily compare the generated and ideal answers using classic\\napproaches.  \\n  \\nFor example, when extracting the list of products from the user\\'s input, you\\ncan do the following:  \\n\\\\- check if the LLM outputs a valid JSON structure  \\n\\\\- use a classic method to compare the generated and real answers  \\n  \\n#ùüÆ. ùó°ùóº \"ùóøùó∂ùó¥ùóµùòÅ\" ùóÆùóªùòÄùòÑùó≤ùóø (ùó≤.ùó¥., ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ ùó±ùó≤ùòÄùó∞ùóøùó∂ùóΩùòÅùó∂ùóºùóªùòÄ, ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùó≤ùòÄ, ùó≤ùòÅùó∞.)  \\n  \\nWhen generating sentences, the LLM can use different styles, words, etc. Thus,\\ntraditional metrics (e.g., BLUE score) are too rigid to be useful.  \\n  \\nYou can leverage another LLM to test the output of our initial LLM. The trick\\nis in what questions to ask.  \\n  \\nWhen testing LLMs, you won\\'t have a big testing split size as you are used to.\\nA set of 10-100 tricky examples usually do the job (it won\\'t be costly).  \\n  \\nHere, we have another 2 sub scenarios:  \\n  \\n‚Ü≥ ùüÆ.ùü≠ ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº (ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ\\nùóµùóÆùòÉùó≤ ùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nYou don\\'t have access to an expert to write an ideal answer for a given\\nquestion to compare it to.  \\n  \\nBased on the initial prompt and generated answer, you can compile a set of\\nquestions and pass them to an LLM. Usually, these are Y/N questions that you\\ncan easily quantify and check the validity of the generated answer.  \\n  \\nThis is known as \"Rubric Evaluation\"  \\n  \\nFor example:  \\n\"\"\"  \\n\\\\- Is there any disagreement between the response and the context? (Y or N)  \\n\\\\- Count how many questions the user asked. (output a number)  \\n...  \\n\"\"\"  \\n  \\nThis strategy is intuitive, as you can ask the LLM any question you are\\ninterested in as long it can output a quantifiable answer (Y/N or a number).  \\n  \\n‚Ü≥ ùüÆ.ùüÆ. ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóº ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóøùó≤ùòÄùóΩùóºùóªùòÄùó≤ ùòÅùóº (ùòÜùóºùòÇ ùóµùóÆùòÉùó≤\\nùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nWhen you can access an answer manually created by a group of experts, things\\nare easier.  \\n  \\nYou will use an LLM to compare the generated and ideal answers based on\\nsemantics, not structure.  \\n  \\nFor example:  \\n\"\"\"  \\n(A) The submitted answer is a subset of the expert answer and entirely\\nconsistent.  \\n...  \\n(E) The answers differ, but these differences don\\'t matter.  \\n\"\"\"\\n\\n3 techniques you must know to evaluate your LLMs quickly [Image by the\\nAuthor].\\n\\n* * *\\n\\n### #2. What is the difference between your ML development and continuous\\ntraining environments?\\n\\nThey might do the same thing, but their design is entirely different ‚Üì  \\n  \\nùó†ùóü ùóóùó≤ùòÉùó≤ùóπùóºùóΩùó∫ùó≤ùóªùòÅ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nAt this point, your main goal is to ingest the raw and preprocessed data\\nthrough versioned artifacts (or a feature store), analyze it & generate as\\nmany experiments as possible to find the best:  \\n\\\\- model  \\n\\\\- hyperparameters  \\n\\\\- augmentations  \\n  \\nBased on your business requirements, you must maximize some specific metrics,\\nfind the best latency-accuracy trade-offs, etc.  \\n  \\nYou will use an experiment tracker to compare all these experiments.  \\n  \\nAfter you settle on the best one, the output of your ML development\\nenvironment will be:  \\n\\\\- a new version of the code  \\n\\\\- a new version of the configuration artifact  \\n  \\nHere is where the research happens. Thus, you need flexibility.  \\n  \\nThat is why we decouple it from the rest of the ML systems through artifacts\\n(data, config, & code artifacts).  \\n  \\nùóñùóºùóªùòÅùó∂ùóªùòÇùóºùòÇùòÄ ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nHere is where you want to take the data, code, and config artifacts and:  \\n  \\n\\\\- train the model on all the required data  \\n\\\\- output a staging versioned model artifact  \\n\\\\- test the staging model artifact  \\n\\\\- if the test passes, label it as the new production model artifact  \\n\\\\- deploy it to the inference services  \\n  \\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub\\nActions):  \\n  \\n\\\\- builds a docker image from the code artifact (e.g., triggered manually or\\nwhen a new artifact version is created)  \\n\\\\- start the training pipeline inside the docker container that pulls the\\nfeature and config artifacts and outputs the staging model artifact  \\n\\\\- manually look over the training report -> If everything went fine, manually\\ntrigger the testing pipeline  \\n\\\\- manually look over the testing report -> if everything worked fine (e.g.,\\nthe model is better than the previous one), manually trigger the CD pipeline\\nthat deploys the new model to your inference services  \\n  \\nNote how the model registry quickly helps you to decouple all the components.  \\n  \\nAlso, because training and testing metrics are not always black & white, it is\\ntough to 100% automate the CI/CD pipeline.  \\n  \\nThus, you need a human in the loop when deploying ML models.\\n\\n. What is the difference between your ML development and continuous training\\nenvironments [Image by the Author]\\n\\nTo conclude...  \\n  \\nThe ML development environment is where you do your research to find better\\nmodels:  \\n\\\\- ùò™ùòØùò±ùò∂ùòµ: data artifact  \\n\\\\- ùò∞ùò∂ùòµùò±ùò∂ùòµ: code & config artifacts  \\n  \\nThe continuous training environment is used to train & test the production\\nmodel at scale:  \\n\\\\- ùò™ùòØùò±ùò∂ùòµ: data, code, config artifacts  \\n\\\\- ùò∞ùò∂ùòµùò±ùò∂ùòµ: model artifact\\n\\n> This is not a fixed solution, as ML systems are still an open question.\\n>\\n> But if you want to see this strategy in action ‚Üì  \\n>  \\n> ‚Ü≥üîó Check out my **The Full Stack 7-Steps MLOps Framework** FREE Course.\\n\\n* * *\\n\\n### Story: Job roles tell you there is just one type of MLE, but there are\\nactually 3\\n\\nHere they are ‚Üì  \\n  \\nThese are the 3 ML engineering personas I found while working with different\\nteams in the industry:  \\n  \\n#ùü≠. ùó•ùó≤ùòÄùó≤ùóÆùóøùó∞ùóµùó≤ùóøùòÄ ùòÇùóªùó±ùó≤ùóøùó∞ùóºùòÉùó≤ùóø  \\n  \\nThey like to stay in touch with the latest papers, understand the architecture\\nof models, optimize them, run experiments, etc.  \\n  \\nThey are great at picking the best models but not that great at writing clean\\ncode and scaling the solution.  \\n  \\n#ùüÆ. ùó¶ùó™ùóò ùòÇùóªùó±ùó≤ùóøùó∞ùóºùòÉùó≤ùóø  \\n  \\nThey pretend they read papers but don\\'t (maybe only when they have to). They\\nare more concerned with writing modular code and data quality than the latest\\nhot models. Usually, these are the \"data-centric\" people.  \\n  \\nThey are great at writing clean code & processing data at scale but lack deep\\nmathematical skills to develop complex DL solutions.  \\n  \\n#ùüØ. ùó†ùóüùó¢ùóΩùòÄ ùó≥ùóøùó≤ùóÆùó∏ùòÄ  \\n  \\nThey ultimately don\\'t care about the latest research & hot models. They are\\nmore into the latest MLOps tools and building ML systems. They love to\\nautomate everything and use as many tools as possible.  \\n  \\nGreat at scaling the solution and building ML pipelines, but not great at\\nrunning experiments & tweaking ML models. They love to treat the ML model as a\\nblack box.\\n\\nImage by the Author.\\n\\nI started as #1. , until I realized I hated it - now I am a mix of:  \\n  \\n‚Üí #ùü≠. 20%  \\n‚Üí #ùüÆ. 40%  \\n‚Üí #ùüØ. 40%  \\n  \\nBut that doesn\\'t mean one is better - these types are complementary.  \\n  \\nA great ML team should have at least one of each persona.  \\n  \\nWhat do you think? Did I get it right?\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-what-is-the-difference-between?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('aa199018-9dcc-4768-9e99-1b2356af2c21'), content={'Title': 'DML: 7-steps to build a production-ready financial assistant using LLMs ', 'Subtitle': 'How to fine-tune any LLM at scale in under 5 minutes. 7 steps to build a production-ready financial assistant using LLMs.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\n### How to fine-tune any LLM at scale in under 5 minutes. 7 steps to build a\\nproduction-ready financial assistant using LLMs.\\n\\nPaul Iusztin\\n\\nOct 12, 2023\\n\\n5\\n\\nShare this post\\n\\n#### DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Writing your own ML models is history. How to fine-tune any LLM at scale in under 5 minutes.\\n\\n  2. 7 steps to chain your prompts to build a production-ready financial assistant using LLMs.\\n\\n> **Extra:** 3 key resources on how to monitor your ML models\\n\\n* * *\\n\\n### #1. Writing your own ML models is history. How to fine-tune any LLM at\\nscale in under 5 minutes.\\n\\nWriting your own ML models is history.  \\n  \\nThe true value is in your data, how you prepare it, and your computer power.  \\n  \\nTo demonstrate my statement. Here is how you can write a Python script to\\ntrain your LLM at scale in under 5 minutes ‚Üì  \\n  \\n#ùü≠. Load your data in JSON format and convert it into a Hugging Dataset  \\n  \\n#ùüÆ. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with\\nthe tokenizer and training & evaluation datasets.  \\n  \\n#ùüØ. Wrap your training script with a serverless solution, such as Beam, which\\nquickly lets you access a cluster of GPUs to train large models.  \\n  \\nüö® As you can see, the secret ingredients are not the LLM but:  \\n\\\\- the amount of data  \\n\\\\- the quality of data  \\n\\\\- how you process the data  \\n\\\\- $$$ for compute power  \\n\\\\- the ability to scale the system\\n\\n3-steps to write a Python script to train your LLMs at scale [Image by the\\nAuthor].\\n\\nüí° My advice  \\n  \\n‚Ü≥ If you don\\'t plan to become an ML researcher, shift your focus from the\\nlatest models to your data and infrastructure.  \\n  \\n.  \\n  \\nùó°ùóºùòÅùó≤: Integrating serverless services, such as Beam, makes the deployment of\\nyour training pipeline fast & seamless, leaving you to focus only on the last\\npiece of the puzzle: your data.\\n\\n  \\n‚Ü≥üîó Check out Beam\\'s docs to find out more.\\n\\n* * *\\n\\n### #2. 7 steps to chain your prompts to build a production-ready financial\\nassistant using LLMs.\\n\\nùü≥ ùòÄùòÅùó≤ùóΩùòÄ on how to ùó∞ùóµùóÆùó∂ùóª your ùóΩùóøùóºùó∫ùóΩùòÅùòÄ to build a production-ready ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ\\nùóÆùòÄùòÄùó∂ùòÄùòÅùóÆùóªùòÅ using ùóüùóüùó†ùòÄ ‚Üì  \\n  \\nWhen building LLM applications, you frequently have to divide your application\\ninto multiple steps & prompts, which are known as \"chaining prompts\".  \\n  \\nHere are 7 standard steps when building a financial assistant using LLMs (or\\nany other assistant) ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: Check if the user\\'s question is safe using OpenAI\\'s Moderation API  \\n  \\nIf the user\\'s query is safe, move to ùó¶ùòÅùó≤ùóΩ ùüÆ ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: Query your proprietary data (e.g., financial news) to enrich the\\nprompt with fresh data & additional context.  \\n  \\nTo do so, you have to:  \\n\\\\- use an LM to embed the user\\'s input  \\n\\\\- use the embedding to query your proprietary data stored in a vector DB  \\n  \\nùòïùò∞ùòµùò¶: You must use the same LM model to embed:  \\n\\\\- the data that will be stored in the vector DB  \\n\\\\- the user\\'s question used to query the vector DB  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: Build the prompt using:  \\n\\\\- a predefined template  \\n\\\\- the user\\'s question  \\n\\\\- extracted financial news as context  \\n\\\\- your conversation history as context  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü∞: Call the LLM  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü±: Check if the assistant\\'s answer is safe using the OpenAI\\'s Moderation\\nAPI.  \\n  \\nIf the assistant\\'s answer is safe, move to ùó¶ùòÅùó≤ùóΩ ùü± ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≤: Use an LLM to check if the final answer is satisfactory.  \\n  \\nTo do so, you build a prompt using the following:  \\n\\\\- a validation predefined template  \\n\\\\- the user\\'s initial question  \\n\\\\- the assistants answer  \\n  \\nThe LLM has to give a \"yes\" or \"no\" answer.  \\n  \\nThus, if it answers \"yes,\" we show the final answer to the user. Otherwise, we\\nwill return a predefined response, such as:  \\n\"Sorry, we couldn\\'t answer your question because we don\\'t have enough\\ninformation.\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≥: Add the user\\'s question and assistant\\'s answer to a history cache.\\nWhich will be used to enrich the following prompts with the current\\nconversation.  \\n  \\nJust to remind you, the assistant should support a conversation. Thus, it\\nneeds to know what happened in the previous questions.  \\n  \\n‚Üí In practice, you usually keep only the latest N (question, answer) tuples or\\na conversation summary to keep your context length under control.\\n\\n7 Steps to Build a Production-Ready Financial Assistant Using LLMs [Image by\\nthe Author]\\n\\n‚Ü≥ If you want to see this strategy in action, check out our new FREE Hands-on\\nLLMs course (work in progress) & give it a ‚≠ê on GitHub to stay updated with\\nits latest progress.\\n\\n* * *\\n\\n### Extra: 3 key resources on how to monitor your ML models\\n\\nIn the last month, I read 100+ ML monitoring articles.  \\n  \\nI trimmed them for you to 3 key resources:  \\n  \\n1\\\\. A series of excellent articles made by Arize AI that will make you\\nunderstand what ML monitoring is all about.  \\n  \\n‚Ü≥üîó Arize Articles  \\n  \\n2\\\\. The Evidently AI Blog, where you can find answers to all your questions\\nregarding ML monitoring.  \\n  \\n‚Ü≥üîó Evidently Blog  \\n  \\n3\\\\. The monitoring hands-on examples hosted by DataTalksClub will teach you\\nhow to implement an ML monitoring system.  \\n  \\n‚Ü≥üîó DataTalks Course  \\n  \\nAfter wasting a lot of time reading other resources...  \\n  \\nUsing these 3 resources is a solid start for learning about monitoring ML\\nsystems.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n5\\n\\nShare this post\\n\\n#### DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('de3f1dc2-70e9-4621-825b-56dd9a8f99be'), content={'Title': 'DML: Chain of Thought Reasoning: Write robust & explainable prompts for your LLM', 'Subtitle': 'Everything you need to know about chaining prompts: increase your LLMs accuracy & debug and explain your LLM.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Chain of Thought Reasoning: Write robust & explainable prompts for your\\nLLM\\n\\n### Everything you need to know about chaining prompts: increase your LLMs\\naccuracy & debug and explain your LLM.\\n\\nPaul Iusztin\\n\\nOct 05, 2023\\n\\n1\\n\\nShare this post\\n\\n#### DML: Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Chaining Prompts to Reduce Costs, Increase Accuracy & Easily Debug Your LLMs\\n\\n  2. Chain of Thought Reasoning: Write robust & explainable prompts for your LLM\\n\\n> **Extra:** Why**** any ML system should use an ML platform as its central\\n> nervous system\\n\\n* * *\\n\\nBut first, I want to share with you this quick 7-minute guide teaching you how\\nstable diffusion models are trained and generate new images.  \\n  \\nDiffusion models are the cornerstone of most modern computer vision generative\\nAI applications.  \\n  \\nThus, if you are into generative AI, it is essential to have an intuition of\\nhow a diffusion model works.  \\n  \\nCheck out my article to quickly understand:  \\n\\\\- the general picture of how diffusion models work  \\n\\\\- how diffusion models generate new images  \\n\\\\- how they are trained  \\n\\\\- how they are controlled by a given context (e.g., text)  \\n  \\n‚Ü≥üîó Busy? This Is Your Quick Guide to Opening the Diffusion Models Black Box\\n\\n* * *\\n\\n### #1. Chaining Prompts to Reduce Costs, Increase Accuracy & Easily Debug\\nYour LLMs\\n\\n> Here it is ‚Üì\\n\\nùóñùóµùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùóøùóºùó∫ùóΩùòÅùòÄ is an intuitive technique that states that you must split\\nyour prompts into multiple calls.  \\n  \\nùó™ùóµùòÜ? ùóüùó≤ùòÅ\\'ùòÄ ùòÇùóªùó±ùó≤ùóøùòÄùòÅùóÆùóªùó± ùòÅùóµùó∂ùòÄ ùòÑùó∂ùòÅùóµ ùòÄùóºùó∫ùó≤ ùóÆùóªùóÆùóπùóºùó¥ùó∂ùó≤ùòÄ.  \\n  \\nWhen cooking, you are following a recipe split into multiple steps. You want\\nto move to the next step only when you know what you have done so far is\\ncorrect.  \\n  \\n‚Ü≥ You want every prompt to be simple & focused.  \\n  \\nAnother analogy is between reading all the code in one monolith/god class and\\nusing DRY to separate the logic between multiple modules.  \\n  \\n‚Ü≥ You want to understand & debug every prompt easily.  \\n  \\n.  \\n  \\nChaining prompts is a ùóΩùóºùòÑùó≤ùóøùó≥ùòÇùóπ ùòÅùóºùóºùóπ ùó≥ùóºùóø ùóØùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùóÆ ùòÄùòÅùóÆùòÅùó≤ùó≥ùòÇùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫ where you\\nmust take different actions depending on the current state.  \\n  \\nIn other words, you control what happens between 2 chained prompts.  \\n  \\nùòâùò∫ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµùò¥ ùò∞ùòß ùò§ùò©ùò¢ùò™ùòØùò™ùòØùò® ùò±ùò≥ùò∞ùòÆùò±ùòµùò¥:  \\n  \\n\\\\- increase in accuracy  \\n\\\\- reduce the number of tokens -> lower costs (skips steps of the workflow\\nwhen not needed)  \\n\\\\- avoid context limitations  \\n\\\\- easier to include a human-in-the-loop -> easier to control, moderate, test\\n& debug  \\n\\\\- use external tools/plugins (web search, API, databases, calculator, etc.)  \\n  \\n.  \\n  \\nùóòùòÖùóÆùó∫ùóΩùóπùó≤  \\n  \\nYou want to build a virtual assistant to respond to customer service queries.  \\n  \\nInstead of adding in one single prompt the system message, all the available\\nproducts, and the user inquiry, you can split it into the following:  \\n1\\\\. Use a prompt to extract the products and categories of interest.  \\n2\\\\. Enrich the context only with the products of interest.  \\n3\\\\. Call the LLM for the final answer.  \\n  \\nYou can evolve this example by adding another prompt that classifies the\\nnature of the user inquiry. Based on that, redirect it to billing, technical\\nsupport, account management, or a general LLM (similar to the complex system\\nof GPT-4).\\n\\nChaining Prompts to Reduce Costs, Increase Accuracy & Easily Debug Your LLMs\\n[Image by the Author].\\n\\nùóßùóº ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùòáùó≤:  \\n  \\nInstead of writing a giant prompt that includes multiple steps:  \\n  \\nSplit the god prompt into multiple modular prompts that let you keep track of\\nthe state externally and orchestrate the program.  \\n  \\nIn other words, you want modular prompts that you can combine easily (same as\\nin writing standard functions/classes)  \\n  \\n.  \\n  \\nTo ùóÆùòÉùóºùó∂ùó± ùóºùòÉùó≤ùóøùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóøùó∂ùóªùó¥, use this technique when your prompt contains >=\\ninstruction.  \\n  \\nYou can leverage the DRY principle from software -> one prompt = one\\ninstruction.  \\n  \\n‚Ü≥üîó Tools to chain prompts: LangChain  \\n‚Ü≥üîó Tools to monitor and debug prompts: Comet LLMOps Tools\\n\\n* * *\\n\\n### #2. Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\nùóñùóµùóÆùó∂ùóª ùóºùó≥ ùóßùóµùóºùòÇùó¥ùóµùòÅ ùó•ùó≤ùóÆùòÄùóºùóªùó∂ùóªùó¥ is a ùóΩùóºùòÑùó≤ùóøùó≥ùòÇùóπ ùóΩùóøùóºùó∫ùóΩùòÅ ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóøùó∂ùóªùó¥ ùòÅùó≤ùó∞ùóµùóªùó∂ùóæùòÇùó≤ to\\nùó∂ùó∫ùóΩùóøùóºùòÉùó≤ ùòÜùóºùòÇùóø ùóüùóüùó†\\'ùòÄ ùóÆùó∞ùó∞ùòÇùóøùóÆùó∞ùòÜ ùóÆùóªùó± ùó≤ùòÖùóΩùóπùóÆùó∂ùóª ùó∂ùòÅùòÄ ùóÆùóªùòÄùòÑùó≤ùóø.  \\n\\n> Let me explain ‚Üì\\n\\n  \\nIt is a method to force the LLM to follow a set of predefined steps.  \\n  \\nüß† ùó™ùóµùòÜ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùóñùóµùóÆùó∂ùóª ùóºùó≥ ùóßùóµùóºùòÇùó¥ùóµùòÅ ùó•ùó≤ùóÆùòÄùóºùóªùó∂ùóªùó¥?  \\n  \\nIn complex scenarios, the LLM must thoroughly reason about a problem before\\nresponding to the question.  \\n  \\nOtherwise, the LLM might rush to an incorrect conclusion.  \\n  \\nBy forcing the model to follow a set of steps, we can guide the model to\\n\"think\" more methodically about the problem.  \\n  \\nAlso, it helps us explain and debug how the model reached a specific answer.  \\n  \\n.  \\n  \\nüí° ùóúùóªùóªùó≤ùóø ùó†ùóºùóªùóºùóπùóºùó¥ùòÇùó≤  \\n  \\nThe inner monologue is all the steps needed to reach the final answer.  \\n  \\nOften, we want to hide all the reasoning steps from the end user.  \\n  \\nIn fancy words, we want to mimic an \"inner monologue\" and output only the\\nfinal answer.  \\n  \\nEach reasoning step is structured into a parsable format.  \\n  \\nThus, we can quickly load it into a data structure and output only the desired\\nsteps to the user.  \\n  \\n.  \\n  \\n‚Ü≥ ùóüùó≤ùòÅ\\'ùòÄ ùóØùó≤ùòÅùòÅùó≤ùóø ùòÇùóªùó±ùó≤ùóøùòÄùòÅùóÆùóªùó± ùòÅùóµùó∂ùòÄ ùòÑùó∂ùòÅùóµ ùóÆùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤:  \\n  \\nThe input prompt to the LLM consists of a system message + the user\\'s\\nquestion.  \\n  \\nThe secret is in defining the system message as follows:  \\n  \\n\"\"\"  \\nYou are a virtual assistant helping clients...  \\n  \\nFollow the next steps to answer the customer queries.  \\n  \\nStep 1: Decide if it is a question about a product ...  \\nStep 2: Retrieve the product ...  \\nStep 3: Extract user assumptions ...  \\nStep 4: Validate user assumptions ...  \\nStep 5: Answer politely ...  \\n  \\nMake sure to answer in the following format:  \\nStep 1: <ùò¥ùòµùò¶ùò±_1_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\nStep 2: <ùò¥ùòµùò¶ùò±_2_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\nStep 3: <ùò¥ùòµùò¶ùò±_3_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\nStep 4: <ùò¥ùòµùò¶ùò±_4_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\n  \\nResponse to the user: <ùòßùò™ùòØùò¢ùò≠_ùò≥ùò¶ùò¥ùò±ùò∞ùòØùò¥ùò¶>  \\n\"\"\"  \\n  \\nEnforcing the LLM to follow a set of steps, we ensured it would answer the\\nright questions.  \\n  \\nUltimately, we will show the user only the <ùòßùò™ùòØùò¢ùò≠_ùò≥ùò¶ùò¥ùò±ùò∞ùòØùò¥ùò¶> subset of the\\nanswer.  \\n  \\nThe other steps (aka \"inner monologue\") help:  \\n\\\\- the model to reason  \\n\\\\- the developer to debug  \\n  \\nHave you used this technique when writing prompts?\\n\\nChain of Thought Reasoning: Write robust & explainable prompts for your LLM\\n[Image by the Author].\\n\\n* * *\\n\\n### Extra: Why**** any ML system should use an ML platform as its central\\nnervous system\\n\\nAny ML system should use an ML platform as its central nervous system.  \\n  \\nHere is why ‚Üì  \\n  \\nThe primary role of an ML Platform is to bring structure to your:  \\n\\\\- experiments  \\n\\\\- visualizations  \\n\\\\- models  \\n\\\\- datasets  \\n\\\\- documentation  \\n  \\nAlso, its role is to decouple your data preprocessing, experiment, training,\\nand inference pipelines.  \\n  \\n.  \\n  \\nAn ML platform helps you automate everything mentioned above using these 6\\nfeatures:  \\n  \\n1\\\\. experiment tracking: log & compare experiments  \\n2\\\\. metadata store: know how a model (aka experiment) was generated  \\n3\\\\. visualisations: a central hub for your visualizations  \\n4\\\\. reports: create documents out of your experiments  \\n5\\\\. artifacts: version & share your datasets  \\n6\\\\. model registry: version & share your models\\n\\nWhy**** any ML system should use an ML platform as its central nervous system\\n[GIF by the Author].\\n\\nI have used many ML Platforms before, but lately, I started using Comet, and I\\nlove it.\\n\\n‚Ü≥üîó Comet ML  \\n  \\nWhat is your favorite ML Platform?\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n1\\n\\nShare this post\\n\\n#### DML: Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-chain-of-thought-reasoning-write?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('3d7e4ad6-60d2-4e20-bf42-e158930d168c'), content={'Title': 'DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs', 'Subtitle': 'Stop Manually Creating Your ML AWS Infrastructure - use Terraform! Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\n### Stop Manually Creating Your ML AWS Infrastructure - use Terraform! Build &\\nServe a Production-Ready Classifier in 1 Hour Using LLMs.\\n\\nPaul Iusztin\\n\\nSep 21, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Stop Manually Creating Your ML AWS Infrastructure. Use Terraform!\\n\\n  2. Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs.\\n\\n* * *\\n\\n> Before going into our subject of the day, I have some news to share with you\\n> üëÄ\\n\\nIf you want to ùóæùòÇùó∂ùó∞ùó∏ùóπùòÜ ùóπùó≤ùóÆùóøùóª in a ùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùòÑùóÆùòÜ how to ùóØùòÇùó∂ùóπùó± ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó†ùóü\\nùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ ùòÇùòÄùó∂ùóªùó¥ ùóüùóüùó†ùòÄ, emphasizing ùóøùó≤ùóÆùóπ-ùòÑùóºùóøùóπùó± ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ?\\n\\nI want to let you know that ‚Üì\\n\\nI am invited on ùó¶ùó≤ùóΩùòÅùó≤ùó∫ùóØùó≤ùóø ùüÆùü¥ùòÅùóµ to a ùòÑùó≤ùóØùó∂ùóªùóÆùóø to present an overview of the\\nùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ course I am creating.\\n\\nI will show you a ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ of how to ùóØùòÇùó∂ùóπùó± ùóÆ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóØùóºùòÅ ùòÇùòÄùó∂ùóªùó¥ ùóüùóüùó†ùòÄ.\\nHere is what I will cover ‚Üì\\n\\n  * creating your Q&A dataset in a semi-automated way (OpenAI GPT) \\n\\n  * fine-tuning an LLM on your new dataset using QLoRA (HuggingFace, Peft, Comet ML, Beam)\\n\\n  * build a streaming pipeline to ingest news in real time into a vector DB (Bytewax, Qdrant, AWS)\\n\\n  * build a financial bot based on the fine-tuned model and real-time financial news (LangChain, Comet ML, Beam) \\n\\n  * build a simple UI to interact with the financial bot \\n\\n‚ùóNo Notebooks or fragmented examples.\\n\\n‚úÖ I want to show you how to build a real product.\\n\\n‚Üí More precisely, I will focus on the engineering and system design, showing\\nyou how the components described above work together.\\n\\n.\\n\\nIf this is something you want to learn, be sure to register using the link\\nbelow ‚Üì\\n\\n‚Ü≥üîó Engineering an End-to-End ML System for a Financial Assistant Using LLMs\\n(September 28th).\\n\\nSee you there üëÄ\\n\\n> Now back to business üî•\\n\\n* * *\\n\\n### #1. Stop Manually Creating Your ML AWS Infrastructure. Use Terraform!\\n\\nI was uselessly spending 1000$ dollars every month on cloud machines until I\\nstarted using this tool üëá  \\n  \\nTerraform!  \\n  \\n.  \\n  \\nùêÖùê¢ùê´ùê¨ùê≠, ùê•ùêûùê≠\\'ùê¨ ùêÆùêßùêùùêûùê´ùê¨ùê≠ùêöùêßùêù ùê∞ùê°ùê≤ ùê∞ùêû ùêßùêûùêûùêù ùêìùêûùê´ùê´ùêöùêüùê®ùê´ùê¶.  \\n  \\nWhen you want to deploy a software application, there are two main steps:  \\n1\\\\. Provisioning infrastructure  \\n2\\\\. Deploying applications  \\n  \\nA regular workflow would be that before deploying your applications or\\nbuilding your CI/CD pipelines, you manually go and spin up your, let\\'s say,\\nAWS machines.  \\n  \\nInitially, this workflow should be just fine, but there are two scenarios when\\nit could get problematic.  \\n  \\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome\\nand might yield bugs in manually replicating it.  \\n  \\n#2. In the world of AI, there are many cases when you want to spin up a GPU\\nmachine to train your models, and afterward, you don\\'t need it anymore. Thus,\\nif you forget to close it, you will end up uselessly paying a lot of $$$.  \\n  \\nWith Terraform, you can solve both of these issues.  \\n  \\n.  \\n  \\nSo...  \\n  \\nùêñùê°ùêöùê≠ ùê¢ùê¨ ùêìùêûùê´ùê´ùêöùêüùê®ùê´ùê¶?  \\n  \\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as\\ncode\" tool that:  \\n  \\n\\\\- is declarative (you focus on the WHAT, not on the HOW)  \\n\\\\- automates and manages your infrastructure  \\n\\\\- is open source  \\n  \\nYeah... yeah... that sounds fancy. But ùê∞ùê°ùêöùê≠ ùêúùêöùêß ùêà ùêùùê® ùê∞ùê¢ùê≠ùê° ùê¢ùê≠?  \\n  \\nLet\\'s take AWS as an example, where you have to:  \\n\\\\- create a VPC  \\n\\\\- create AWS users and permissions  \\n\\\\- spin up EC2 machines  \\n\\\\- install programs (e.g., Docker)  \\n\\\\- create a K8s cluster  \\n  \\nUsing Terraform...  \\n  \\nYou can do all that just by providing a configuration file that reflects the\\nstate of your infrastructure.  \\n  \\nBasically, it helps you create all the infrastructure you need\\nprogrammatically. Isn\\'t that awesome?\\n\\nTerraform [Image by the Author].\\n\\nIf you want to quickly understand Terraform enough to start using it in your\\nown projects:  \\n  \\n‚Ü≥ check out my 7-minute read article: üîó Stop Manually Creating Your AWS\\nInfrastructure. Use Terraform!\\n\\n* * *\\n\\n### #2. Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\nùòìùòìùòîùò¥ ùò¢ùò≥ùò¶ ùò¢ ùò≠ùò∞ùòµ ùòÆùò∞ùò≥ùò¶ ùòµùò©ùò¢ùòØ ùò§ùò©ùò¢ùòµùò£ùò∞ùòµùò¥. ùòõùò©ùò¶ùò¥ùò¶ ùòÆùò∞ùò•ùò¶ùò≠ùò¥ ùò¢ùò≥ùò¶ ùò≥ùò¶ùò∑ùò∞ùò≠ùò∂ùòµùò™ùò∞ùòØùò™ùòªùò™ùòØùò® ùò©ùò∞ùò∏ ùòîùòì\\nùò¥ùò∫ùò¥ùòµùò¶ùòÆùò¥ ùò¢ùò≥ùò¶ ùò£ùò∂ùò™ùò≠ùòµ.  \\n  \\n.  \\n  \\nUsing the standard approach when building an end-to-end ML application, you\\nhad to:  \\n\\\\- get labeled data: 1 month  \\n\\\\- train the model: 2 months  \\n\\\\- serve de model: 3 months  \\n  \\nThese 3 steps might take ~6 months to implement.  \\n  \\nSo far, it worked great.  \\n  \\nBut here is the catch ‚Üì  \\n  \\n.  \\n  \\nùò†ùò∞ùò∂ ùò§ùò¢ùòØ ùò≥ùò¶ùò¢ùò§ùò© ùò¢ùò≠ùòÆùò∞ùò¥ùòµ ùòµùò©ùò¶ ùò¥ùò¢ùòÆùò¶ ùò≥ùò¶ùò¥ùò∂ùò≠ùòµ ùò™ùòØ ùò¢ ùòßùò¶ùò∏ ùò©ùò∞ùò∂ùò≥ùò¥ ùò∞ùò≥ ùò•ùò¢ùò∫ùò¥ ùò∂ùò¥ùò™ùòØùò® ùò¢ ùò±ùò≥ùò∞ùòÆùò±ùòµ-\\nùò£ùò¢ùò¥ùò¶ùò• ùò≠ùò¶ùò¢ùò≥ùòØùò™ùòØùò® ùò¢ùò±ùò±ùò≥ùò∞ùò¢ùò§ùò©.  \\n  \\nLet\\'s take a classification task as an example ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: You write a system prompt explaining the model and what types of\\ninputs and outputs it will get.  \\n  \\n\"  \\nYou will be provided with customer service queries.  \\n  \\nClassify each query into the following categories:  \\n\\\\- Billing  \\n\\\\- Account Management  \\n\\\\- General Inquiry  \\n\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: You can give the model an example to make sure it understands the task\\n(known as one-shot learning):  \\n  \\n\"  \\nUser: I want to know the price of the pro subscription plan.  \\nAssistant: Billing  \\n\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: Attach the user prompt and create the input prompt, which now consists\\nof the following:  \\n\\\\- system  \\n\\\\- example  \\n\\\\- user  \\n...prompts  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü∞: Call the LLM\\'s API... and boom, you built a classifier in under one\\nhour.  \\n  \\nCool, right? üî•  \\n  \\nUsing this approach, the only time-consuming step is to tweak the prompt until\\nit reaches the desired result.\\n\\nHow to quickly build a classifier using LLMs [GIF by the Author].\\n\\nTo conclude...  \\n  \\nIn today\\'s LLMs world, to build a classifier, you have to write:  \\n\\\\- a system prompt  \\n\\\\- an example  \\n\\\\- attach the user prompt  \\n\\\\- pass the input prompt to the LLM API\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-build-and-serve-a-production?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('49e2912f-313d-439d-8de6-522dc8379cb2'), content={'Title': 'DML: 4 key ideas you must know to train an LLM successfully', 'Subtitle': 'My time series forecasting Python code was a disaster until I started using this package. 4 key ideas you must know to train an LLM successfully.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 4 key ideas you must know to train an LLM successfully\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 4 key ideas you must know to train an LLM successfully\\n\\n### My time series forecasting Python code was a disaster until I started\\nusing this package. 4 key ideas you must know to train an LLM successfully.\\n\\nPaul Iusztin\\n\\nSep 14, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: 4 key ideas you must know to train an LLM successfully\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. My time series forecasting Python code was a disaster until I started using this package\\n\\n  2. 4 key ideas you must know to train an LLM successfully\\n\\n> **Extra** : My favorite ML & MLOps newsletter\\n\\n* * *\\n\\n### #1. My time series forecasting Python code was a disaster until I started\\nusing this package\\n\\nDoes building time series models sound more complicated than modeling standard\\ntabular datasets?  \\n  \\nWell... maybe it is... but that is precisely why you need to learn more about\\nùòÄùó∏ùòÅùó∂ùó∫ùó≤!  \\n  \\nWhen I first built forecasting models, I manually coded the required\\npreprocessing and postprocessing steps. What a newbie I was...  \\n  \\nHow easy would my life have been if I had started from the beginning to use\\nùòÄùó∏ùòÅùó∂ùó∫ùó≤?  \\n  \\n.  \\n  \\nùêñùê°ùêöùê≠ ùê¢ùê¨ ùê¨ùê§ùê≠ùê¢ùê¶ùêû?  \\n  \\nùòÄùó∏ùòÅùó∂ùó∫ùó≤ is a Python package that adds time-series functionality over well-known\\npackages such as statsmodels, fbprophet, scikit-learn, autoarima, xgboost,\\netc.  \\n  \\nThus, all of a sudden, all your beloved packages will support time series\\nfeatures such as:  \\n\\\\- easily swap between different models (e.g., xgboost, lightgbm, decision\\ntrees, etc.)  \\n\\\\- out-of-the-box windowing transformations & aggregations  \\n\\\\- functionality for multivariate, panel, and hierarchical learning  \\n\\\\- cross-validation adapted to time-series  \\n\\\\- cool visualizations  \\nand more...\\n\\nSktime example [Image by the Author].\\n\\n‚Ü≥ If you want to see ùòÄùó∏ùòÅùó∂ùó∫ùó≤ in action, check out my article: üîó A Guide to\\nBuilding Effective Training Pipelines for Maximum Results\\n\\n* * *\\n\\n### #2. 4 key ideas you must know to train an LLM successfully\\n\\nThese are 4 key ideas you must know to train an LLM successfully  \\n  \\nüìñ ùóõùóºùòÑ ùó∂ùòÄ ùòÅùóµùó≤ ùó∫ùóºùó±ùó≤ùóπ ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥?  \\n  \\nLLMs still leverage supervised learning.  \\n  \\nA standard NLP task is to build a classifier.  \\nFor example, you have a sequence of tokens as inputs and, as output, a set of\\nclasses (e.g., negative and positive).  \\n  \\nWhen training an LLM for text generation, you have as input a sequence of\\ntokens, and its task is to predict the next token:  \\n\\\\- Input: JavaScript is all you [...]  \\n\\\\- Output: Need  \\n  \\nThis is known as an autoregressive process.  \\n  \\n‚öîÔ∏è ùòÑùóºùóøùó±ùòÄ != ùòÅùóºùó∏ùó≤ùóªùòÄ  \\n  \\nTokens are created based on the frequency of sequences of characters.  \\n  \\nFor example:  \\n\\\\- In the sentence: \"Learning new things is fun!\" every work is a different\\ntoken as each is frequently used.  \\n\\\\- In the sentence: \"Prompting is a ...\" the word \\'prompting\\' is divided into\\n3 tokens: \\'prom\\', \\'pt\\', and \\'ing\\'  \\n  \\nThis is important because different LLMs have different limits for the input\\nnumber of tokens.\\n\\nHow to train an LLM cheatsheet [Image by the Author].\\n\\nüß† ùóßùòÜùóΩùó≤ùòÄ ùóºùó≥ ùóüùóüùó†ùòÄ  \\n  \\nThere are 3 primary types of LLMs:  \\n\\\\- base LLM  \\n\\\\- instruction tuned LLM  \\n\\\\- RLHF tuned LLM  \\n  \\nùòöùòµùò¶ùò±ùò¥ ùòµùò∞ ùò®ùò¶ùòµ ùòßùò≥ùò∞ùòÆ ùò¢ ùò£ùò¢ùò¥ùò¶ ùòµùò∞ ùò¢ùòØ ùò™ùòØùò¥ùòµùò≥ùò∂ùò§ùòµùò™ùò∞ùòØ-ùòµùò∂ùòØùò¶ùò• ùòìùòìùòî:  \\n  \\n1\\\\. Train the Base LLM on a lot of data (trillions of tokens) - trained for\\nmonths on massive GPU clusters  \\n  \\n2\\\\. Fine-tune the Base LLM on a Q&A dataset (millions of tokens) - trained for\\nhours or days on modest-size computational resources  \\n  \\n3\\\\. [Optional] Fine-tune the LLM further on human ratings reflecting the\\nquality of different LLM outputs, on criteria such as if the answer is\\nhelpful, honest and harmless using RLHF. This will increase the probability of\\ngenerating a more highly rated output.  \\n  \\nüèóÔ∏è ùóõùóºùòÑ ùòÅùóº ùóØùòÇùó∂ùóπùó± ùòÅùóµùó≤ ùóΩùóøùóºùó∫ùóΩùòÅ ùòÅùóº ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùòÅùóµùó≤ ùóüùóüùó† ùóºùóª ùóÆ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ  \\n  \\nThe most common approach consists of 4 steps:  \\n1\\\\. A system message that sets the general tone & behavior.  \\n2\\\\. The context that adds more information to help the model to answer\\n(Optional).  \\n3\\\\. The user\\'s question.  \\n4\\\\. The answer to the question.  \\n  \\nNote that you need to know the answer to the question during training. You can\\nintuitively see it as your label.\\n\\n* * *\\n\\n### Extra: My favorite ML & MLOps newsletter\\n\\nDo you want to learn ML & MLOps from real-world experience?  \\n  \\nThen I suggest you join Pau Labarta Bajo\\'s Real-World Machine Learning  \\nweekly newsletter, along with another 8k+ ML developers.  \\n  \\nPau Labarta Bajo inspired me to start my weekly newsletter and is a great\\nteacher who makes learning seamless ‚úå\\n\\n> üîó **Real-World Machine Learning -**Every Saturday Morning\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: 4 key ideas you must know to train an LLM successfully\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Pau Labarta BajoReal-World Machine Learning Sep 14, 2023Liked by Paul\\nIusztinThanks for the shout out Paul. I love the content you shareExpand full\\ncommentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-4-key-ideas-you-must-know-to?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('0b152bfd-0a90-4220-a1b8-77709ecb06d0'), content={'Title': 'DML: How to add real-time monitoring & metrics to your ML System', 'Subtitle': 'How to easily add retry policies to your Python code. How to add real-time monitoring & metrics to your ML System.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How to add real-time monitoring & metrics to your ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How to add real-time monitoring & metrics to your ML System\\n\\n### How to easily add retry policies to your Python code. How to add real-time\\nmonitoring & metrics to your ML System.\\n\\nPaul Iusztin\\n\\nSep 07, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: How to add real-time monitoring & metrics to your ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n _This week‚Äôs ML & MLOps topics:_\\n\\n  1. How to add real-time monitoring & metrics to your ML System\\n\\n  2. How to easily add retry policies to your Python code\\n\\n _Storytime:_ How am I writing code in 2023? ùóú ùó±ùóºùóª\\'ùòÅ.\\n\\n* * *\\n\\n> But first, I have some big news to share with you üéâ\\n\\n‚Äî> Want to learn how to ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùóÆùóª ùóüùóüùó†, build a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤, use a\\nùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï, build a ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóØùóºùòÅ and ùó±ùó≤ùóΩùóπùóºùòÜ ùó≤ùòÉùó≤ùóøùòÜùòÅùóµùó∂ùóªùó¥ using a serverless\\nsolution?\\n\\nThen you will enjoy looking at this new free course that me and\\n\\nPau Labarta Bajo\\n\\n(from the RWML newsletter) are cooking.\\n\\n  \\n‚Ü≥ The course will teach you how to build an end-to-end LLM solution.  \\n  \\nIt is structured into 4 modules ‚Üì  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùü≠: Learn how to generate a financial Q&A dataset in a semi-automated\\nway using the OpenAI API.  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùüÆ: Fine-tune the LLM (e.g., Falcon, Llama2, etc.) using HuggingFace &\\nPeft. Also, we will show you how to integrate an experiment tracker, model\\nregistry, and monitor the prompts using Comet.  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùüØ: Build a streaming pipeline using Bytewax that listens to financial\\nnews through a web socket, cleans it, embeds it, and loads it to a vector\\ndatabase using Qdrant.  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùü∞: Wrap the fine-tuned model and vector DB into a financial bot using\\nLangChain and deploy it under a RESTful API.  \\n  \\n‚ùóÔ∏è But all of this is useless if it isn\\'t deployed.  \\n  \\n‚Üí We will use Beam to deploy everything quickly - Beam is a serverless\\nsolution that lets you focus on your problem and quickly serve all your ML\\ncomponents. Say bye-bye to access policies and network configuration.  \\n  \\nùó°ùóºùòÅùó≤: This is still a work in progress, but the first 3 modules are almost\\ndone.\\n\\nArchitecture built during the **Hands-On LLMs Course** [GIF by the Author].\\n\\nCurious?\\n\\nThen, check out the repository and give it a ‚≠ê ‚Üì\\n\\n‚Ü≥ üîó Course GitHub Repository\\n\\n* * *\\n\\n### #1. How to add real-time monitoring & metrics to your ML System\\n\\nYour model is exposed to performance degradation after it is deployed to\\nproduction.  \\n  \\nThat is why you need to monitor it constantly.  \\n  \\nThe most common way to monitor an ML model is to compute its metrics.  \\n  \\nBut for that, you need the ground truth.  \\n  \\nùóúùóª ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª, ùòÜùóºùòÇ ùó∞ùóÆùóª ùóÆùòÇùòÅùóºùó∫ùóÆùòÅùó∂ùó∞ùóÆùóπùóπùòÜ ùóÆùó∞ùó∞ùó≤ùòÄùòÄ ùòÅùóµùó≤ ùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ ùó∂ùóª ùüØ ùó∫ùóÆùó∂ùóª\\nùòÄùó∞ùó≤ùóªùóÆùóøùó∂ùóºùòÄ:  \\n1\\\\. near real-time: you can access it quite quickly  \\n2\\\\. delayed: you can access it after a considerable amount of time (e.g., one\\nmonth)  \\n3\\\\. never: you have to label the data manually  \\n  \\n.  \\n  \\nùóôùóºùóø ùòÇùòÄùó≤ ùó∞ùóÆùòÄùó≤ùòÄ ùüÆ. ùóÆùóªùó± ùüØ. ùòÜùóºùòÇ ùó∞ùóÆùóª ùóæùòÇùó∂ùó∞ùó∏ùóπùòÜ ùó∞ùóºùó∫ùóΩùòÇùòÅùó≤ ùòÜùóºùòÇùóø ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ùó∂ùóª\\nùòÅùóµùó≤ ùó≥ùóºùóπùóπùóºùòÑùó∂ùóªùó¥ ùòÑùóÆùòÜ:  \\n  \\n\\\\- store the model predictions and GT as soon as they are available (these 2\\nwill be out of sync -> you can\\'t compute the metrics right away)  \\n  \\n\\\\- build a DAG (e.g., using Airflow) that extracts the predictions & GT\\ncomputes the metrics in batch mode and loads them into another storage (e.g.,\\nGCS)  \\n  \\n\\\\- use an orchestration tool to run the DAG in the following scenarios:  \\n1\\\\. scheduled: if the GT is available in near real-time (e.g., hourly), then\\nit makes sense to run your monitoring pipeline based on the known frequency  \\n2\\\\. triggered: if the GT is delayed and you don\\'t know when it may come up,\\nthen you can implement a webhook to trigger your monitoring pipeline  \\n  \\n\\\\- attach a consumer to your storage to use and display the metrics (e.g.,\\ntrigger alarms and display them in a dashboard)\\n\\nHow to add real-time monitoring & metrics to your ML system [Image by the\\nAuthor].\\n\\nIf you want to see how to implement a near real-time monitoring pipeline using\\nAirflow and GCS, check out my article ‚Üì\\n\\n‚Ü≥ üîó Ensuring Trustworthy ML Systems With Data Validation and Real-Time\\nMonitoring\\n\\n* * *\\n\\n### #2. How to easily add retry policies to your Python code\\n\\nOne strategy that makes the ùó±ùó∂ùó≥ùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóØùó≤ùòÅùòÑùó≤ùó≤ùóª ùó¥ùóºùóºùó± ùó∞ùóºùó±ùó≤ ùóÆùóªùó± ùó¥ùóøùó≤ùóÆùòÅ ùó∞ùóºùó±ùó≤ is\\nadding ùóøùó≤ùòÅùóøùòÜ ùóΩùóºùóπùó∂ùó∞ùó∂ùó≤ùòÄ.  \\n  \\nTo manually implement them can get tedious and complicated.  \\n  \\nRetry policies are a must when you:  \\n\\\\- make calls to an external API  \\n\\\\- read from a queue, etc.  \\n  \\n.  \\n  \\nùó®ùòÄùó∂ùóªùó¥ ùòÅùóµùó≤ ùóßùó≤ùóªùóÆùó∞ùó∂ùòÅùòÜ ùó£ùòÜùòÅùóµùóºùóª ùóΩùóÆùó∞ùó∏ùóÆùó¥ùó≤...  \\n  \\nùò†ùò∞ùò∂ ùò§ùò¢ùòØ ùò≤ùò∂ùò™ùò§ùò¨ùò≠ùò∫ ùò•ùò¶ùò§ùò∞ùò≥ùò¢ùòµùò¶ ùò∫ùò∞ùò∂ùò≥ ùòßùò∂ùòØùò§ùòµùò™ùò∞ùòØùò¥ ùò¢ùòØùò• ùò¢ùò•ùò• ùò§ùò∂ùò¥ùòµùò∞ùòÆùò™ùòªùò¢ùò£ùò≠ùò¶ ùò≥ùò¶ùòµùò≥ùò∫ ùò±ùò∞ùò≠ùò™ùò§ùò™ùò¶ùò¥,\\nùò¥ùò∂ùò§ùò© ùò¢ùò¥:  \\n  \\n1\\\\. Add fixed and random wait times between multiple retries.  \\n  \\n2\\\\. Add a maximum number of attempts or computation time.  \\n  \\n3\\\\. Retry only when specific errors are thrown (or not thrown).  \\n  \\n... as you can see, you easily compose these policies between them.  \\n  \\nThe cherry on top is that you can access the statistics of the retries of a\\nspecific function:  \\n\"  \\nprint(raise_my_exception.retry.statistics)  \\n\"\\n\\nExamples of the retry policies using tenacity [Image by the Author].\\n\\n‚Ü≥ üîó tenacity repository\\n\\n* * *\\n\\n###  _Storytime:_ How am I writing code in 2023? I don‚Äôt\\n\\nAs an engineer, you are paid to think and solve problems. How you do that, it\\ndoesn\\'t matter. Let me explain ‚Üì  \\n  \\n.  \\n  \\nThe truth is that I am lazy.  \\n  \\nThat is why I am a good engineer.  \\n  \\nWith the rise of LLMs, my laziness hit all times highs.  \\n  \\n.  \\n  \\nùóßùóµùòÇùòÄ, ùòÅùóµùó∂ùòÄ ùó∂ùòÄ ùóµùóºùòÑ ùóú ùòÑùóøùó∂ùòÅùó≤ ùó∫ùòÜ ùó∞ùóºùó±ùó≤ ùòÅùóµùó≤ùòÄùó≤ ùó±ùóÆùòÜùòÄ ‚Üì  \\n  \\n\\\\- 50% Copilot (tab is the new CTRL-C + CTRL-V)  \\n\\\\- 30% ChatGPT/Bard  \\n\\\\- 10% Stackoverflow (call me insane, but I still use StackOverflow from time\\nto time)  \\n\\\\- 10% Writing my own code  \\n  \\nThe thing is that I am more productive than ever.  \\n  \\n... and that 10% of \"writing my own code\" is the final step that connects all\\nthe dots and brings real value to the table.  \\n  \\n.  \\n  \\nùóúùóª ùóøùó≤ùóÆùóπùó∂ùòÅùòÜ, ùóÆùòÄ ùóÆùóª ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóø, ùòÜùóºùòÇ ùó∫ùóºùòÄùòÅùóπùòÜ ùóµùóÆùòÉùó≤ ùòÅùóº:  \\n  \\n\\\\- ask the right questions  \\n\\\\- understand & improve the architecture of the system  \\n\\\\- debug code  \\n\\\\- understand business requirements  \\n\\\\- communicate with other teams  \\n  \\n...not to write code.\\n\\n[Image by the Author]\\n\\nWriting code as we know it most probably will disappear with the rise of AI\\n(it kind of already did).  \\n  \\n.  \\n  \\nWhat do you think? How do you write code these days?\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 am CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: here, I approach in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where I will constantly aggregate all my work (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: How to add real-time monitoring & metrics to your ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-to-add-real-time-monitoring?r=1ttoeh'),\n",
       "  ArticleDocument(id=UUID('a520fdac-65b4-4340-9ee2-d16a1390b838'), content={'Title': 'DML: Top 6 ML Platform Features You Must Know to Build an ML System', 'Subtitle': 'Why serving an ML model using a batch architecture is so powerful? Top 6 ML platform features you must know.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\n### Why serving an ML model using a batch architecture is so powerful? Top 6\\nML platform features you must know.\\n\\nPaul Iusztin\\n\\nAug 31, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\nThis week we will cover:\\n\\n  1. Top 6 ML platform features you must know to build an ML system\\n\\n  2. Why serving an ML model using a batch architecture is so powerful?\\n\\n_Story:_ ‚ÄúI never forget anything‚Äù - said no one but your second brain.\\n\\n* * *\\n\\nThis week, no shameless promotion üëÄ\\n\\n* * *\\n\\n### #1. Top 6 ML platform features you must know to build an ML system\\n\\nHere they are ‚Üì  \\n  \\n#ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥  \\n  \\nIn your ML development phase, you generate lots of experiments.  \\n  \\nTracking and comparing the metrics between them is crucial in finding the\\noptimal model.  \\n  \\n#ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤  \\n  \\nIts primary purpose is reproducibility.  \\n  \\nTo know how a model was generated, you need to know:  \\n\\\\- the version of the code  \\n\\\\- the version of the packages  \\n\\\\- hyperparameters/config  \\n\\\\- total compute  \\n\\\\- version of the dataset  \\n... and more  \\n  \\n#ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nMost of the time, along with the metrics, you must log a set of visualizations\\nfor your experiment.  \\n  \\nSuch as:  \\n\\\\- images  \\n\\\\- videos  \\n\\\\- prompts  \\n\\\\- t-SNE graphs  \\n\\\\- 3D point clouds  \\n... and more  \\n  \\n#ùü∞. ùó•ùó≤ùóΩùóºùóøùòÅùòÄ  \\n  \\nYou don\\'t work in a vacuum.  \\n  \\nYou have to present your work to other colleges or clients.  \\n  \\nA report lets you take the metadata and visualizations from your experiment...  \\n  \\n...and create, deliver and share a targeted presentation for your clients or\\npeers.  \\n  \\n#ùü±. ùóîùóøùòÅùó∂ùó≥ùóÆùó∞ùòÅùòÄ  \\n  \\nThe most powerful feature out of them all.  \\n  \\nAn artifact is a versioned object that is an input or output for your task.  \\n  \\nEverything can be an artifact, but the most common cases are:  \\n\\\\- data  \\n\\\\- model  \\n\\\\- code  \\n  \\nWrapping your assets around an artifact ensures reproducibility.  \\n  \\nFor example, you wrap your features into an artifact (e.g., features:3.1.2),\\nwhich you can consume into your ML development step.  \\n  \\nThe ML development step will generate config (e.g., config:1.2.4) and code\\n(e.g., code:1.0.2) artifacts used in the continuous training pipeline.  \\n  \\nDoing so lets you quickly respond to questions such as \"What I used to\\ngenerate the model?\" and \"What Version?\"  \\n  \\n#ùü≤. ùó†ùóºùó±ùó≤ùóπ ùó•ùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ  \\n  \\nThe model registry is the ultimate way to make your model accessible to your\\nproduction ecosystem.  \\n  \\nFor example, in your continuous training pipeline, after the model is trained,\\nyou load the weights as an artifact into the model registry (e.g.,\\nmodel:1.2.4).  \\n  \\nYou label this model as \"staging\" under a new version and prepare it for\\ntesting. If the tests pass, mark it as \"production\" under a new version and\\nprepare it for deployment (e.g., model:2.1.5).\\n\\nTop 6 ML platform features you must know [Image by the Author].\\n\\n.  \\n  \\nAll of these features are used in a mature ML system. What is your favorite\\none?  \\n  \\n‚Ü≥ You can see all these features in action in my: üîó **The Full Stack 7-Steps\\nMLOps Framework** FREE course.\\n\\n* * *\\n\\n### #2. Why serving an ML model using a batch architecture is so powerful?\\n\\nWhen you first start deploying your ML model, you want an initial end-to-end\\nflow as fast as possible.  \\n  \\nDoing so lets you quickly provide value, get feedback, and even collect data.  \\n  \\n.  \\n  \\nBut here is the catch...  \\n  \\nSuccessfully serving an ML model is tricky as you need many iterations to\\noptimize your model to work in real-time:  \\n\\\\- low latency  \\n\\\\- high throughput  \\n  \\nInitially, serving your model in batch mode is like a hack.  \\n  \\nBy storing the model\\'s predictions in dedicated storage, you automatically\\nmove your model from offline mode to a real-time online model.  \\n  \\nThus, you no longer have to care for your model\\'s latency and throughput. The\\nconsumer will directly load the predictions from the given storage.  \\n  \\nùêìùê°ùêûùê¨ùêû ùêöùê´ùêû ùê≠ùê°ùêû ùê¶ùêöùê¢ùêß ùê¨ùê≠ùêûùê©ùê¨ ùê®ùêü ùêö ùêõùêöùê≠ùêúùê° ùêöùê´ùêúùê°ùê¢ùê≠ùêûùêúùê≠ùêÆùê´ùêû:  \\n\\\\- extracts raw data from a real data source  \\n\\\\- clean, validate, and aggregate the raw data within a feature pipeline  \\n\\\\- load the cleaned data into a feature store  \\n\\\\- experiment to find the best model + transformations using the data from the\\nfeature store  \\n\\\\- upload the best model from the training pipeline into the model registry  \\n\\\\- inside a batch prediction pipeline, use the best model from the model\\nregistry to compute the predictions  \\n\\\\- store the predictions in some storage  \\n\\\\- the consumer will download the predictions from the storage  \\n\\\\- repeat the whole process hourly, daily, weekly, etc. (it depends on your\\ncontext)  \\n.  \\n  \\nùòõùò©ùò¶ ùòÆùò¢ùò™ùòØ ùò•ùò∞ùò∏ùòØùò¥ùò™ùò•ùò¶ of deploying your model in batch mode is that the\\npredictions will have a level of lag.  \\n  \\nFor example, in a recommender system, if you make your predictions daily, it\\nwon\\'t capture a user\\'s behavior in real-time, and it will update the\\npredictions only at the end of the day.  \\n  \\nMoving to other architectures, such as request-response or streaming, will be\\nnatural after your system matures in batch mode.\\n\\nML Batch Architecture Design [Image by the Author].\\n\\nSo remember, when you initially deploy your model, using a batch mode\\narchitecture will be your best shot for a good user experience.\\n\\n* * *\\n\\n### _Story:_ ‚ÄúI never forget anything‚Äù - said no one but your second brain.\\n\\nAfter 6+ months of refinement, this is my second brain strategy üëá  \\n  \\nTiago\\'s Forte book inspired me, but I adapted his system to my needs.  \\n  \\n.  \\n  \\n#ùü¨. ùóñùóºùóπùóπùó≤ùó∞ùòÅ  \\n  \\nThis is where you are bombarded with information from all over the place.  \\n  \\n#ùü≠. ùóßùóµùó≤ ùóöùóøùóÆùòÉùó≤ùòÜùóÆùóøùó±  \\n  \\nThis is where I save everything that looks interesting.  \\n  \\nI won\\'t use 90% of what is here, but it satisfied my urge to save that \"cool\\narticle\" I saw on LinkedIn.  \\n  \\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists,\\netc.  \\n  \\n#ùüÆ. ùóßùóµùó≤ ùóïùóºùóÆùóøùó±  \\n  \\nHere, I start converging the information and planning what to do next.  \\n  \\nTools: Notion  \\n  \\n#ùüØ. ùóßùóµùó≤ ùóôùó∂ùó≤ùóπùó±  \\n  \\nHere is where I express myself through learning, coding, writing, etc.  \\n  \\nTools: whatever you need to express yourself.  \\n  \\n2 & 3 are iterative processes. Thus I often bounce between them until the\\ninformation is distilled.  \\n  \\n#ùü∞. ùóßùóµùó≤ ùó™ùóÆùóøùó≤ùóµùóºùòÇùòÄùó≤  \\n  \\nHere is where I take the distilled information and write it down for cold\\nstorage.  \\n  \\nTools: Notion, Google Drive  \\n  \\n.  \\n  \\nWhen I want to search for a piece of information, I start from the Warehouse\\nand go backward until I find what I need.  \\n  \\nAs a minimalist, I kept my tools to a minimum. I primarily use only: Brave,\\nNotion, and Google Drive.  \\n  \\nYou don\\'t need 100+ tools to be productive. They just want to take your money\\nfrom you.\\n\\nMy second brain strategy [Image by the Author].\\n\\nSo remember...  \\n  \\nYou have to:  \\n\\\\- collect  \\n\\\\- link  \\n\\\\- plan  \\n\\\\- distill  \\n\\\\- store\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 am CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: here, I approach in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where I will constantly aggregate all my work (courses, articles, webinars, podcasts, etc.),\\n\\n3\\n\\nShare this post\\n\\n#### DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Ahmed BesbesThe Tech Buffet Aug 31, 2023Liked by Paul IusztinHello Paul!\\nGreat newsletter. It\\'d be even more useful to suggest tools for each of these\\nfeatures (e.g. the model registry, the feature store, etc)Expand full\\ncommentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-top-6-ml-platform-features-you?r=1ttoeh')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    future_to_query = {\n",
    "        executor.submit(__fetch_articles, user_id): \"articles\",\n",
    "        executor.submit(__fetch_posts, user_id): \"posts\",\n",
    "        executor.submit(__fetch_repositories, user_id): \"repositories\",\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for future in as_completed(future_to_query):\n",
    "        query_name = future_to_query[future]\n",
    "        print(f\"Query: '{query_name}'\")\n",
    "        try:\n",
    "            results[query_name] = future.result()\n",
    "        except Exception:\n",
    "            print(f\"Exception: '{query_name}' request failed.\")\n",
    "\n",
    "            results[query_name] = []\n",
    "            \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3c981e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[], [], [ArticleDocument(id=UUID('34978aea-e179-44b5-975c-7deb64456380'), content={'Title': 'An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin', 'Subtitle': 'From data gathering to productionizing LLMs using LLMOps good practices.', 'Content': \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.1K13ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLet‚Äôs start with Lesson 1 ‚Üì‚Üì‚ÜìLesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component‚Äôs code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask‚Ä¶When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ‚Üì‚Üì‚ÜìFirst, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: ‚ÄúWrite me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].‚ÄùExcited? Let‚Äôs get started üî•2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios‚Ä¶‚Ä¶the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution‚Üí The 3-pipeline architectureLet‚Äôs understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline‚Ä¶also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that‚Ä¶‚Ä¶by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.‚Ü≥ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet‚Äôs understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesn‚Äôt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let‚Äôs say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let‚Äôs zoom in on each component to understand how they work individually and interact with each other. ‚Üì‚Üì‚Üì3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.üîó 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people‚Äôs data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.üîó 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn‚Äôt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes ‚Äî store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren‚Äôt scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust‚Äôs impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won‚Äôt focus too much on any specific LLM.All the experiments will be logged into Comet ML‚Äôs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet‚Äôs experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet‚Äôs prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet‚Äôs prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èHave you enjoyed this article? Then‚Ä¶‚Üì‚Üì‚ÜìJoin 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferences[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps: Metadata storeIntroductionJul 17ListsAI Regulation6 stories¬∑593 savesNatural Language Processing1766 stories¬∑1367 savesPredictive Modeling w/ Python20 stories¬∑1607 savesPractical Guides to Machine Learning10 stories¬∑1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f'), ArticleDocument(id=UUID('d331f23e-88c6-4606-b397-52842c9a6295'), content={'Title': 'A Real-time Retrieval System for RAG on Social Media Data', 'Subtitle': 'Use a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.', 'Content': 'Real-time Retrieval for RAG on Social Media Data | Decoding MLOpen in appSign upSign inWriteSign upSign inA Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑12 min read¬∑Mar 30, 2024358ListenShareImage by DALL-EIn this article, you will learn how to build a real-time retrieval system for social media data. In our example, we will use only my LinkedIn posts, but our implementation can easily be extended to other platforms supporting written content, such as X, Instagram, or Medium.In this article, you will learn how to:build a streaming pipeline that ingests LinkedIn posts into a vector DB in real-timeclean, chunk, and embed LinkedIn postsbuild a retrieval client to query LinkedIn postsuse a rerank pattern to improve retrieval accuracyvisualize content retrieved for a given query in a 2D plot using UMAPOur implementation focuses on just the retrieval part of an RAG system. But you can quickly hook the retrieved LinkedIn posts to an LLM for post analysis or personalized content generation.Table of Contents:System DesignDataStreaming ingestion pipelineRetrieval clientConclusion1. System DesignThe retrieval system is based on 2 detached components:the streaming ingestion pipelinethe retrieval clientThe architecture of the retrieval system [Image by the Author ‚Äî in collaboration with VectorHub].The streaming ingestion pipeline runs 24/7 to keep the vector DB synced up with current raw LinkedIn posts data source, while the retrieval client is used in RAG applications to query the vector DB. These 2 components communicate with each other only through the vector DB.1.1. The streaming ingestion pipelineThe streaming ingestion pipeline implements the Change Data Capture (CDC) pattern between a data source containing the raw LinkedIn posts and the vector DB used for retrieval.In a real-world scenario, the streaming pipeline listens to a queue populated by all the changes made to the source database. But because we are focusing primarily on the retrieval system, we simulate the data within the queue with a couple of JSON files.The streaming pipeline is built in Python using Bytewax, and cleans, chunks, and embeds the LinkedIn posts before loading them into a Qdrant vector DB.Why do we need a stream engine?Because LinkedIn posts (or any other social media data) evolve frequently, your vector DB can quickly get out of sync. To handle this, you can build a batch pipeline that runs every minute. But to really minimize data lag, to make sure your vector DB stays current with new social media posts, you need to use a streaming pipeline that immediately takes every new item the moment it‚Äôs posted, preprocesses it, and loads it into the vector DB.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines the impressive speed and reliability of Rust with the ease of use and ecosystem of Python.1.2. The retrieval clientOur retrieval client is a standard Python module that preprocesses user queries and searches the vector DB for most similar results. Qdrant vector DB lets us decouple the retrieval client from the streaming ingestion pipeline.Using a semantic-based retrieval system lets us query our LinkedIn post collection very flexibly. For example, we can retrieve similar posts using a variety of query types ‚Äî e.g., posts, questions, sentences.Also, to improve the retrieval system‚Äôs accuracy, we use a rerank pattern.Lastly, to better understand and explain the retrieval process for particular queries, we visualize our results on a 2D plot using UMAP.2. DataWe will ingest 215 LinkedIn posts from my Linked profile ‚Äî Paul Iusztin. Though we simulate the post ingestion step using JSON files, the posts themselves are authentic.Before diving into the code, let‚Äôs take a look at an example LinkedIn post to familiarize ourselves with the challenges it will introduce ‚Üì[    {        \"text\": \"ùó™ùóµùóÆùòÅ do you need to ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ an open-source ùóüùóüùó† to create your own ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùó±ùòÉùó∂ùòÄùóºùóø?\\\\nThis is the ùóüùóüùó† ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùó∏ùó∂ùòÅ you must know ‚Üì\\\\nùóóùóÆùòÅùóÆùòÄùó≤ùòÅ\\\\nThe key component of any successful ML project is the data.\\\\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\\\\nThe best approach is to hire a bunch of experts to create it manually.\\\\nBut, for a PoC, that might get expensive & slow.\\\\nThe good news is that a method called \\\\\"ùòçùò™ùòØùò¶ùòµùò∂ùòØùò™ùòØùò® ùò∏ùò™ùòµùò© ùò•ùò™ùò¥ùòµùò™ùò≠ùò≠ùò¢ùòµùò™ùò∞ùòØ\\\\\" exists.\\\\n ...Along with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\\\\n‚Ü≥ Beam: üîó\\\\nhttps://lnkd.in/dedCaMDh\\\\n.\\\\n‚Ü≥ To see all these components in action, check out my FREE ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ ùó∞ùóºùòÇùóøùòÄùó≤ & give it a ‚≠ê:  üîó\\\\nhttps://lnkd.in/dZgqtf8f\\\\nhashtag\\\\n#\\\\nmachinelearning\\\\nhashtag\\\\n#\\\\nmlops\\\\nhashtag\\\\n#\\\\ndatascience\",        \"image\": \"https://media.licdn.com/dms/image/D4D10AQHWQzZcToQQ1Q/image-shrink_800/0/1698388219549?e=1705082400&v=beta&t=9mrDC_NooJgD7u7Qk0PmrTGGaZtuwDIFKh3bEqeBsm0\"    }]The following features of the above post are not compatible with embedding models. We‚Äôll need to find some way of handling them in our preprocessing step:emojisbold, italic textother non-ASCII charactersURLscontent that exceeds the context window limit of the embedding modelEmojis and bolded and italic text are represented by Unicode characters that are not available in the vocabulary of the embedding model. Thus, these items cannot be tokenized and passed to the model; we have to remove them or normalize them to something that can be parsed by the tokenizer. The same holds true for all other non-ASCII characters.URLs take up space in the context window without providing much semantic value. Still, knowing that there‚Äôs a URL in the sentence may add context. For this reason, we replace all URLs with a [URL] token. This lets us ingest whatever value the URL‚Äôs presence conveys without it taking up valuable space.3. Streaming ingestion pipelineLet‚Äôs dive into the streaming pipeline, starting from the top and working our way to the bottom ‚Üì3.1. The Bytewax flowThe Bytewax flow transparently conveys all the steps of the streaming pipeline.The first step is ingesting every LinkedIn post from our JSON files. In the next steps, every map operation has a single responsibility:validate the ingested data using a RawPost pydantic modelclean the postschunk the posts; because chunking will output a list of ChunkedPost objects, we use a flat_map operation to flatten them outembed the postsload the posts to a Qdrant vector DBdef build_flow():    embedding_model = EmbeddingModelSingleton()    flow = Dataflow(\"flow\")    stream = op.input(\"input\", flow, JSONSource([\"data/paul.json\"]))    stream = op.map(\"raw_post\", stream, RawPost.from_source)    stream = op.map(\"cleaned_post\", stream, CleanedPost.from_raw_post)    stream = op.flat_map(        \"chunked_post\",        stream,        lambda cleaned_post: ChunkedPost.from_cleaned_post(            cleaned_post, embedding_model=embedding_model        ),    )    stream = op.map(        \"embedded_chunked_post\",        stream,        lambda chunked_post: EmbeddedChunkedPost.from_chunked_post(            chunked_post, embedding_model=embedding_model        ),    )    op.inspect(\"inspect\", stream, print)    op.output(        \"output\", stream, QdrantVectorOutput(vector_size=model.embedding_size)    )        return flow3.2. The processing stepsEvery processing step is incorporated into a pydantic model. This way, we can easily validate the data at each step and reuse the code in the retrieval module.We isolate every step of an ingestion pipeline into its own class:cleaningchunkingembeddingDoing so, we follow the separation of concerns good SWE practice. Thus, every class has its own responsibility.Now the code is easy to read and understand. Also, it‚Äôs future-proof, as it‚Äôs extremely easy to change or extend either of the 3 steps: cleaning, chunking and embedding.Here is the interface of the pydantic models:class RawPost(BaseModel):    post_id: str    text: str    image: Optional[str]    @classmethod    def from_source(cls, k_v: Tuple[str, dict]) -> \"RawPost\":        ... # Mapping a dictionary to a RawPost validated pydantic model.        return cls(...)class CleanedPost(BaseModel):    post_id: str    raw_text: str    text: str    image: Optional[str]    @classmethod    def from_raw_post(cls, raw_post: RawPost) -> \"CleanedPost\":        ... # Cleaning the raw post        return cls(...)class ChunkedPost(BaseModel):    post_id: str    chunk_id: str    full_raw_text: str    text: str    image: Optional[str]    @classmethod    def from_cleaned_post(        cls, cleaned_post: CleanedPost, embedding_model: EmbeddingModelSingleton    ) -> list[\"ChunkedPost\"]:        chunks = ... # Compute chunks        return [cls(...) for chunk in chunks]class EmbeddedChunkedPost(BaseModel):    post_id: str    chunk_id: str    full_raw_text: str    text: str    text_embedding: list    image: Optional[str] = None    score: Optional[float] = None    rerank_score: Optional[float] = None    @classmethod    def from_chunked_post(        cls, chunked_post: ChunkedPost, embedding_model: EmbeddingModelSingleton    ) -> \"EmbeddedChunkedPost\":        ... # Compute embedding.        return cls(...)Now, the data at each step is validated and has a clear structure.Note: Providing different types when instantiating a pydantic model will throw a validation error. For example, if the post_id is defined as a string, and we try to instantiate an EmbeddedChunkedPost with a None or int post_id, it will throw an error.Check out the full implementation on our üîó GitHub Articles Hub repository.3.3. Load to QdrantTo load the LinkedIn posts to Qdrant, you have to override Bytewax‚Äôs StatelessSinkPartition class (which acts as an output in a Bytewax flow):class QdrantVectorSink(StatelessSinkPartition):    def __init__(        self,        client: QdrantClient,        collection_name: str    ):        self._client = client        self._collection_name = collection_name    def write_batch(self, chunks: list[EmbeddedChunkedPost]):        ... # Map chunks to ids, embeddings, and metadata.        self._client.upsert(            collection_name=self._collection_name,            points=Batch(                ids=ids,                vectors=embeddings,                payloads=metadata,            ),        )Within this class, you must overwrite the write_batch() method, where we will serialize every EmbeddedChunkedPost to a format expected by Qdrant and load it to the vector DB.4. Retrieval clientHere, we focus on preprocessing a user‚Äôs query, searching the vector DB, and postprocessing the retrieved posts for maximum results.To design the retrieval step, we implement a QdrantVectorDBRetriever class to expose all the necessary features for our retrieval client.class QdrantVectorDBRetriever:    def __init__(        self,        embedding_model: EmbeddingModelSingleton,        vector_db_client: QdrantClient,        cross_encoder_model: CrossEncoderModelSingleton        vector_db_collection: str    ):        self._embedding_model = embedding_model        self._vector_db_client = vector_db_client        self._cross_encoder_model = cross_encoder_model        self._vector_db_collection = vector_db_collection    def search(        self, query: str, limit: int = 3, return_all: bool = False    ) -> Union[list[EmbeddedChunkedPost], dict[str, list]]:        ... # Search the Qdrant vector DB based on the given query.    def embed_query(self, query: str) -> list[list[float]]:        ... # Embed the given query.    def rerank(self, query: str, posts: list[EmbeddedChunkedPost]) -> list[EmbeddedChunkedPost]:        ... # Rerank the posts relative to the given query.    def render_as_html(self, post: EmbeddedChunkedPost) -> None:        ... # Map the embedded post to HTML to display it.4.1. Embed queryWe must embed the query in precisely the same way we ingested our posts into the vector DB. Because the streaming pipeline is written in Python (thanks to Bytewax), and every preprocessing operation is modular, we can quickly replicate all the steps necessary to embed the query.class QdrantVectorDBRetriever:    ...    def embed_query(self, query: str) -> list[list[float]]:        cleaned_query = CleanedPost.clean(query)        chunks = ChunkedPost.chunk(cleaned_query, self._embedding_model)        embdedded_queries = [            self._embedding_model(chunk, to_list=True) for chunk in chunks        ]        return embdedded_queriesCheck out the full implementation on our üîó GitHub repository.4.2. Plain retrievalLet‚Äôs try to retrieve a set of posts without using the rerank algorithm.vector_db_retriever = QdrantVectorDBRetriever(    embedding_model=EmbeddingModelSingleton(),    vector_db_client=build_qdrant_client())query = \"Posts about Qdrant\"retrieved_results = vector_db_retriever.search(query=query)for post in retrieved_results[\"posts\"]:    vector_db_retriever.render_as_html(post)Here are the top 2 retrieved results sorted using the cosine similarity score ‚ÜìResult 1:Result 1 for the ‚ÄúPosts about Qdrant‚Äù query (without using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]Result 2:Result 2 for the ‚ÄúPosts about Qdrant‚Äù query (without using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]You can see from the results above, that starting from the second post the results are irrelevant. Even though it has a cosine similarly score of ~0.69 the posts doesn‚Äôt contain any information about Qdrant or vector DBs.Note: We looked over the top 5 retrieved results. Nothing after the first post was relevant. We haven‚Äôt added them here as the article is already too long.4.3. Visualize retrievalTo visualize our retrieval, we implement a dedicated class that uses the UMAP dimensionality reduction algorithm. We have picked UMAP as it preserves the geometric properties between points (e.g., the distance) in higher dimensions when they are projected onto lower dimensions better than its peers (e.g., PCA, t-SNE).The RetrievalVisualizer computes the projected embeddings for the entire vector space once. Afterwards, it uses the render() method to project only the given query and retrieved posts, and plot them to a 2D graph.class RetrievalVisualizer:    def __init__(self, posts: list[EmbeddedChunkedPost]):        self._posts = posts        self._umap_transform = self._fit_model(self._posts)        self._projected_post_embeddings = self.project_posts(self._posts)    def _fit_model(self, posts: list[EmbeddedChunkedPost]) -> umap.UMAP:        umap_transform = ... # Fit a UMAP model on the given posts.        return umap_transform    def project_posts(self, posts: list[EmbeddedChunkedPost]) -> np.ndarray:        embeddings = np.array([post.text_embedding for post in posts])        return self._project(embeddings=embeddings)    def _project(self, embeddings: np.ndarray) -> np.ndarray:        ... # Project the embeddings to 2D using UMAP.        return umap_embeddings    def render(        self,        embedded_queries: list[list[float]],        retrieved_posts: list[EmbeddedChunkedPost],    ) -> None:      ... # Render the given queries & retrieved posts using matplotlib.Let‚Äôs take a look at the result to see how the ‚ÄúPosts about Qdrant‚Äù query looks ‚ÜìVisualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (without reranking) [Image by the Author ‚Äî in collaboration with VectorHub].Our results are not great. You can see how far the retrieved posts are from our query in the vector space.Can we improve the quality of our retrieval system using the rerank algorithm?4.4. RerankWe use the reranking algorithm to refine our retrieval for the initial query. Our initial retrieval step ‚Äî because it used cosine similarity (or similar distance metrics) to compute the distance between a query and post embeddings ‚Äî may have missed more complex (but essential) relationships between the query and the documents in the vector space. Reranking leverages the power of transformer models that are capable of understanding more nuanced semantic relationships.We use a cross-encoder model to implement the reranking step, so we can score the query relative to all retrieved posts individually. These scores take into consideration more complex relationships than cosine similarity can. Under the hood is a BERT classifier that outputs a number between 0 and 1 according to how similar the 2 given sentences are. The BERT classifier outputs 0 if they are entirely different and 1 if they are a perfect match.Bi-Encoder vs. Cross-Encoder [Image by the Author ‚Äî in collaboration with VectorHub]Bi-Encoder vs. Cross-Encoder [Image by the Author ‚Äî in collaboration with VectorHub]But, you might ask, ‚ÄúWhy not use the cross-encoder model from the start if it is that much better?‚ÄùThe answer, in a word, is speed. Using a cross-encoder model to search your whole collection is much slower than using cosine similarity. To optimize your retrieval, therefore, your reranking process should involve 2 steps:an initial rough retrieval step using cosine similarity, which retrieves the top N items as potential candidatesfiltering the rough search using the rerank strategy, which retrieves the top K items as your final resultsThe implementation is relatively straightforward. For each retrieved post, we create a pair consisting of the (cleaned) query and the text of the post. We do this for all retrieved posts, resulting in a list of pairs.Next, we call a cross-encoder/ms-marco-MiniLM-L-6-v2 model (from sentence-transformers) to give the retrieved posts their rerank score. We then sort the posts in descending order based on their rerank score.Check out the rerank algorithm implementation on our üîó GitHub repository.4.5. Visualize retrieval with rerankNow that we‚Äôve added the rerank pattern to our retrieval system, let‚Äôs see if it improves the results of our ‚ÄúPosts about Qdrant‚Äù query ‚ÜìResult 1Result 1 for the ‚ÄúPosts about Qdrant‚Äù query (using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]Result 2:Result 2 for the ‚ÄúPosts about Qdrant‚Äù query (using reranking) [Image by the Author ‚Äî in collaboration with VectorHub]The improvement is remarkable! All our results are about Qdrant and vector DBs.Note: We looked over the top 5 retrieved results. The top 4 out of 5 posts are relevant to our query, which is incredible.Now, let‚Äôs look at the UMAP visualization:Visualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (with reranking) [Image by the Author ‚Äî in collaboration with VectorHub].While the returned posts aren‚Äôt very close to the query, they are a lot closer to the query compared to when we weren‚Äôt reranking the retrieved posts.5. ConclusionIn this article, we learned how to adapt a RAG retrieval pattern to improve LinkedIn post retrieval. To keep our database up to date with rapidly changing social media data, we implemented a real-time streaming pipeline that uses CDC to sync the raw LinkedIn posts data source with a vector DB. You also saw how to use Bytewax to write ‚Äî using only Python ‚Äî a streaming pipeline that cleans, chunks, and embeds LinkedIn posts.Finally, you learned how to implement a standard retrieval client for RAG and saw how to improve it using the rerank pattern. As retrieval is complex to evaluate, you saw how to visualize the retrieval for a given query by rendering all the posts, the query, and the retrieved posts in a 2D space using UMAP.This article is a summary of my contribution from VectorHub. Check out the full article here to dig into the details, the code and more experiments.‚Üí Join 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthMl System DesignArtificial IntelligenceMachine LearningStreaming PipelineData Science358358FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13See all from Paul IusztinSee all from Decoding MLRecommended from MediumMdabdullahalhasibinTowards AIA Complete Guide to Embedding For NLP & Generative AI/LLMUnderstand the concept of vector embedding, why it is needed, and implementation with LangChain.3d agoVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesChatGPT prompts 50 stories¬∑2121 savesTarun SinghinAI AdvancesAI-Powered OCR with Phi-3-Vision-128K: The Future of Document ProcessingIn the fast-evolving world of artificial intelligence, multimodal models are setting new standards for integrating visual and textual data‚Ä¶Oct 989916Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Kamal DhunganaImplementing Human-in-the-Loop with LangGraphStreamlit app\\u200a‚Äî\\u200aHIL (Agent Framework\\u200a‚Äî\\u200aLangGraph)Jul 16205Umair Ali KhaninTowards Data ScienceIntegrating Multimodal Data into a Large Language ModelDeveloping a context-retrieval, multimodal RAG using advanced parsing, semantic & keyword search, and re-ranking4d ago841See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on-social-media-data-9cc01d50a2a0'), ArticleDocument(id=UUID('c647c345-aeb5-46f7-8f16-8a6345344069'), content={'Title': 'SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!', 'Subtitle': 'Use a Python streaming engine to populate a feature store from 4+ data sources', 'Content': \"Streaming Pipelines for LLMs and RAG | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICASOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!Use a Python streaming engine to populate a feature store from 4+ data sourcesPaul Iusztin¬∑FollowPublished inDecoding ML¬∑19 min read¬∑Apr 20, 20248241ListenShare‚Üí the 4th out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineerüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG AppsTo better understand the course‚Äôs goal, technical details, and system design ‚Üí Check out Lesson 1Let‚Äôs start with Lesson 4 ‚Üì‚Üì‚ÜìLesson 4: Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!In the 4th lesson, we will focus on the feature pipeline.The feature pipeline is the first pipeline presented in the 3 pipeline architecture: feature, training and inference pipelines.A feature pipeline is responsible for taking raw data as input, processing it into features, and storing it in a feature store, from which the training & inference pipelines will use it.The component is completely isolated from the training and inference code. All the communication is done through the feature store.To avoid repeating myself, if you are unfamiliar with the 3 pipeline architecture, check out Lesson 1 for a refresher.By the end of this article, you will learn to design and build a production-ready feature pipeline that:uses Bytewax as a stream engine to process data in real-time;ingests data from a RabbitMQ queue;uses SWE practices to process multiple data types: posts, articles, code;cleans, chunks, and embeds data for LLM fine-tuning and RAG;loads the features to a Qdrant vector DB.Note: In our use case, the feature pipeline is also a streaming pipeline, as we use a Bytewax streaming engine. Thus, we will use these words interchangeably.We will wrap up Lesson 4 by showing you how to deploy the feature pipeline to AWS and integrate it with the components from previous lessons: data collection pipeline, MongoDB, and CDC.In the 5th lesson, we will go through the vector DB retrieval client, where we will teach you how to query the vector DB and improve the accuracy of the results using advanced retrieval techniques.Excited? Let‚Äôs get started!The architecture of the feature/streaming pipeline.Table of ContentsWhy are we doing this?System design of the feature pipelineThe Bytewax streaming flowPydantic data modelsLoad data to QdrantThe dispatcher layerPreprocessing steps: Clean, chunk, embedThe AWS infrastructureRun the code locallyDeploy the code to AWS & Run it from the cloudConclusionüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. Why are we doing this?A quick reminder from previous lessonsTo give you some context, in Lesson 2, we crawl data from LinkedIn, Medium, and GitHub, normalize it, and load it to MongoDB.In Lesson 3, we are using CDC to listen to changes to the MongoDB database and emit events in a RabbitMQ queue based on any CRUD operation done on MongoDB.‚Ä¶and here we are in Lesson 4, where we are building the feature pipeline that listens 24/7 to the RabbitMQ queue for new events to process and load them to a Qdrant vector DB.The problem we are solvingIn our LLM Twin use case, the feature pipeline constantly syncs the MongoDB warehouse with the Qdrant vector DB while processing the raw data into features.Important: In our use case, the Qdrant vector DB will be our feature store.Why we are solving itThe feature store will be the central point of access for all the features used within the training and inference pipelines.For consistency and simplicity, we will refer to different formats of our text data as ‚Äúfeatures.‚Äù‚Üí The training pipeline will use the feature store to create fine-tuning datasets for your LLM twin.‚Üí The inference pipeline will use the feature store for RAG.For reliable results (especially for RAG), the data from the vector DB must always be in sync with the data from the data warehouse.The question is, what is the best way to sync these 2?Other potential solutionsThe most common solution is probably to use a batch pipeline that constantly polls from the warehouse, computes a difference between the 2 databases, and updates the target database.The issue with this technique is that computing the difference between the 2 databases is extremely slow and costly.Another solution is to use a push technique using a webhook. Thus, on any CRUD change in the warehouse, you also update the source DB.The biggest issue here is that if the webhook fails, you have to implement complex recovery logic.Lesson 3 on CDC covers more of this.2. System design of the feature pipeline: our solutionOur solution is based on CDC, a queue, a streaming engine, and a vector DB:‚Üí CDC adds any change made to the Mongo DB to the queue (read more in Lesson 3).‚Üí the RabbitMQ queue stores all the events until they are processed.‚Üí The Bytewax streaming engine cleans, chunks, and embeds the data.‚Üí A streaming engine works naturally with a queue-based system.‚Üí The data is uploaded to a Qdrant vector DB on the flyWhy is this powerful?Here are 4 core reasons:The data is processed in real-time.Out-of-the-box recovery system: If the streaming pipeline fails to process a message will be added back to the queueLightweight: No need for any diffs between databases or batching too many recordsNo I/O bottlenecks on the source database‚Üí It solves all our problems!The architecture of the feature/streaming pipeline.How is the data stored?We store 2 snapshots of our data in the feature store. Here is why ‚ÜìRemember that we said that the training and inference pipeline will access the features only from the feature store, which, in our case, is the Qdrant vector DB?Well, if we had stored only the chunked & embedded version of the data, that would have been useful only for RAG but not for fine-tuning.Thus, we make an additional snapshot of the cleaned data, which will be used by the training pipeline.Afterward, we pass it down the streaming flow for chunking & embedding.How do we process multiple data types?How do you process multiple types of data in a single streaming pipeline without writing spaghetti code?Yes, that is for you, data scientists! Joking‚Ä¶am I?We have 3 data types: posts, articles, and code.Each data type (and its state) will be modeled using Pydantic models.To process them we will write a dispatcher layer, which will use a creational factory pattern [9] to instantiate a handler implemented for that specific data type (post, article, code) and operation (cleaning, chunking, embedding).The handler follows the strategy behavioral pattern [10].Intuitively, you can see the combination between the factory and strategy patterns as follows:Initially, we know we want to clean the data, but as we don‚Äôt know the data type, we can‚Äôt know how to do so.What we can do, is write the whole code around the cleaning code and abstract away the login under a Handler() interface (aka the strategy).When we get a data point, the factory class creates the right cleaning handler based on its type.Ultimately the handler is injected into the rest of the system and executed.By doing so, we can easily isolate the logic for a given data type & operation while leveraging polymorphism to avoid filling up the code with 1000x ‚Äúif else‚Äù statements.We will dig into the implementation in future sections.Streaming over batchYou may ask why we need a streaming engine instead of implementing a batch job that polls the messages at a given frequency.That is a valid question.The thing is that‚Ä¶Nowadays, using tools such as Bytewax makes implementing streaming pipelines a lot more frictionless than using their JVM alternatives.The key aspect of choosing a streaming vs. a batch design is real-time synchronization between your source and destination DBs.In our particular case, we will process social media data, which changes fast and irregularly.Also, for our digital twin, it is important to do RAG on up-to-date data. We don‚Äôt want to have any delay between what happens in the real world and what your LLM twin sees.That being said choosing a streaming architecture seemed natural in our use case.3. The Bytewax streaming flowThe Bytewax flow is the central point of the streaming pipeline. It defines all the required steps, following the next simplified pattern: ‚Äúinput -> processing -> output‚Äù.As I come from the AI world, I like to see it as the ‚Äúgraph‚Äù of the streaming pipeline, where you use the input(), map(), and output() Bytewax functions to define your graph, which in the Bytewax world is called a ‚Äúflow‚Äù.As you can see in the code snippet below, we ingest posts, articles or code messages from a RabbitMQ queue. After we clean, chunk and embed them. Ultimately, we load the cleaned and embedded data to a Qdrant vector DB, which in our LLM twin use case will represent the feature store of our system.To structure and validate the data, between each Bytewax step, we map and pass a different Pydantic model based on its current state: raw, cleaned, chunked, or embedded.Bytewax flow ‚Üí GitHub Code ‚É™‚ÜêWe have a single streaming pipeline that processes everything.As we ingest multiple data types (posts, articles, or code snapshots), we have to process them differently.To do this the right way, we implemented a dispatcher layer that knows how to apply data-specific operations based on the type of message.More on this in the next sections ‚ÜìWhy Bytewax?Bytewax is an open-source streaming processing framework that:- is built in Rust ‚öôÔ∏è for performance- has Python üêç bindings for leveraging its powerful ML ecosystem‚Ä¶ so, for all the Python fanatics out there, no more JVM headaches for you.Jokes aside, here is why Bytewax is so powerful ‚Üì- Bytewax local setup is plug-and-play- can quickly be integrated into any Python project (you can go wild ‚Äî even use it in Notebooks)- can easily be integrated with other Python packages (NumPy, PyTorch, HuggingFace, OpenCV, SkLearn, you name it)- out-of-the-box connectors for Kafka and local files, or you can quickly implement your ownWe used Bytewax to build the streaming pipeline for the LLM Twin course and loved it.To learn more about Bytewax, go and check them out. They are open source, so no strings attached ‚Üí Bytewax [2] ‚Üê4. Pydantic data modelsLet‚Äôs take a look at what our Pydantic models look like.First, we defined a set of base abstract models for using the same parent class across all our components.Pydantic base model structure ‚Üí GitHub Code ‚É™‚ÜêAfterward, we defined a hierarchy of Pydantic models for:all our data types: posts, articles, or codeall our states: raw, cleaned, chunked, and embeddedThis is how the set of classes for the posts will look like ‚ÜìPydantic posts model structure ‚Üí GitHub Code ‚É™‚ÜêWe repeated the same process for the articles and code model hierarchy.Check out the other data classes on our GitHub.Why is keeping our data in Pydantic models so powerful?There are 4 main criteria:every field has an enforced type: you are ensured the data types are going to be correctthe fields are automatically validated based on their type: for example, if the field is a string and you pass an int, it will through an errorthe data structure is clear and verbose: no more clandestine dicts that you never know what is in themyou make your data the first-class citizen of your program5. Load data to QdrantThe first step is to implement our custom Bytewax DynamicSink class ‚ÜìQdrant DynamicSink ‚Üí GitHub Code ‚É™‚ÜêNext, for every type of operation we need (output cleaned or embedded data ) we have to subclass the StatelessSinkPartition Bytewax class (they also provide a stateful option ‚Üí more in their docs)An instance of the class will run on every partition defined within the Bytewax deployment.In the course, we are using a single partition per worker. But, by adding more partitions (and workers), you can quickly scale your Bytewax pipeline horizontally.Qdrant worker partitions ‚Üí GitHub Code ‚É™‚ÜêNote that we used Qdrant‚Äôs Batch method to upload all the available points at once. By doing so, we reduce the latency on the network I/O side: more on that here [8] ‚ÜêThe RabbitMQ streaming input follows a similar pattern. Check it out here ‚Üê6. The dispatcher layerNow that we have the Bytewax flow and all our data models.How do we map a raw data model to a cleaned data model?‚Üí All our domain logic is modeled by a set of Handler() classes.For example, this is how the handler used to map a PostsRawModel to a PostCleanedModel looks like ‚ÜìHandler hierarchy of classes ‚Üí GitHub Code ‚É™‚ÜêCheck out the other handlers on our GitHub:‚Üí ChunkingDataHandler and EmbeddingDataHandlerIn the next sections, we will explore the exact cleaning, chunking and embedding logic.Now, to build our dispatcher, we need 2 last components:a factory class: instantiates the right handler based on the type of the eventa dispatcher class: the glue code that calls the factory class and handlerHere is what the cleaning dispatcher and factory look like ‚ÜìThe dispatcher and factory classes ‚Üí GitHub Code ‚É™‚ÜêCheck out the other dispatchers on our GitHub.By repeating the same logic, we will end up with the following set of dispatchers:RawDispatcher (no factory class required as the data is not processed)CleaningDispatcher (with a ChunkingHandlerFactory class)ChunkingDispatcher (with a ChunkingHandlerFactory class)EmbeddingDispatcher (with an EmbeddingHandlerFactory class)7. Preprocessing steps: Clean, chunk, embedHere we will focus on the concrete logic used to clean, chunk, and embed a data point.Note that this logic is wrapped by our handler to be integrated into our dispatcher layer using the Strategy behavioral pattern [10].We already described that in the previous section. Thus, we will directly jump into the actual logic here, which can be found in the utils module of our GitHub repository.Note: These steps are experimental. Thus, what we present here is just the first iteration of the system. In a real-world scenario, you would experiment with different cleaning, chunking or model versions to improve it on your data.CleaningThis is the main utility function used to clean the text for our posts, articles, and code.Out of simplicity, we used the same logic for all the data types, but after more investigation, you would probably need to adapt it to your specific needs.For example, your posts might start containing some weird characters, and you don‚Äôt want to run the ‚Äúunbold_text()‚Äù or ‚Äúunitalic_text()‚Äù functions on your code data point as is completely redundant.Cleaning logic ‚Üí GitHub Code ‚É™‚ÜêMost of the functions above are from the unstructured [3] Python package. It is a great tool for quickly finding utilities to clean text data.üîó More examples of unstructured here [3] ‚ÜêOne key thing to notice is that at the cleaning step, we just want to remove all the weird, non-interpretable characters from the text.Also, we want to remove redundant data, such as extra whitespace or URLs, as they do not provide much value.These steps are critical for our tokenizer to understand and efficiently transform our string input into numbers that will be fed into the transformer models.Note that when using bigger models (transformers) + modern tokenization techniques, you don‚Äôt need to standardize your dataset too much.For example, it is redundant to apply lemmatization or stemming, as the tokenizer knows how to split your input into a commonly used sequence of characters efficiently, and the transformers can pick up the nuances of the words.üí° What is important at the cleaning step is to throw out the noise.ChunkingWe are using Langchain to chunk our text.We use a 2 step strategy using Langchain‚Äôs RecursiveCharacterTextSplitter [4] and SentenceTransformersTokenTextSplitter [5]. As seen below ‚ÜìChunking logic ‚Üí GitHub Code ‚É™‚ÜêOverlapping your chunks is a common pre-indexing RAG technique, which helps to cluster chunks from the same document semantically.Again, we are using the same chunking logic for all of our data types, but to get the most out of it, we would probably need to tweak the separators, chunk_size, and chunk_overlap parameters for our different use cases.But our dispatcher + handler architecture would easily allow us to configure the chunking step in future iterations.EmbeddingThe data preprocessing, aka the hard part is done.Now we just have to call an embedding model to create our vectors.Embedding logic ‚Üí GitHub Code ‚É™‚ÜêWe used the all-MiniLm-L6-v2 [6] from the sentence-transformers library to embed our articles and posts: a lightweight embedding model that can easily run in real-time on a 2 vCPU machine.As the code data points contain more complex relationships and specific jargon to embed, we used a more powerful embedding model: hkunlp/instructor-xl [7].This embedding model is unique as it can be customized on the fly with instructions based on your particular data. This allows the embedding model to specialize on your data without fine-tuning, which is handy for embedding pieces of code.8. The AWS infrastructureIn Lesson 2, we covered how to deploy the data collection pipeline that is triggered by a link to Medium, Substack, LinkedIn or GitHub ‚Üí crawls the given link ‚Üí saves the crawled information to a MongoDB.In Lesson 3, we explained how to deploy the CDC components that emit events to a RabbitMQ queue based on any CRUD operation done to MongoDB.What is left is to deploy the Bytewax streaming pipeline and Qdrant vector DB.We will use Qdrant‚Äôs self-hosted option, which is easy to set up and scale.To test things out, they offer a Free Tier plan for up to a 1GB cluster, which is more than enough for our course.‚Üí We explained in our GitHub repository how to configure Qdrant.AWS infrastructure of the feature/streaming pipeline.The last piece of the puzzle is the Bytewax streaming pipeline.As we don‚Äôt require a GPU and the streaming pipeline needs to run 24/7, we will deploy it to AWS Fargate, a cost-effective serverless solution from AWS.As a serverless solution, Fargate allows us to deploy our code quickly and scale it fast in case of high traffic.How do we deploy the streaming pipeline code to Fargate?Using GitHub Actions, we wrote a CD pipeline that builds a Docker image on every new commit made on the main branch.After, the Docker image is pushed to AWS ECR. Ultimately, Fargate pulls the latest version of the Docker image.This is a common CD pipeline to deploy your code to AWS services.Why not use lambda functions, as we did for the data pipeline?An AWS lambda function executes a function once and then closes down.This worked perfectly for the crawling logic, but it won't work for our streaming pipeline, which has to run 24/7.9. Run the code locallyTo quickly test things up, we wrote a docker-compose.yaml file to spin up the MongoDB, RabbitMQ queue and Qdrant vector db.You can spin up the Docker containers using our Makefile by running the following, which will start the CDC component and streaming pipeline:make local-startTo start the data collection pipeline, run the following:make local-test-githubThe documentation of our GitHub repository provides more details on how to run and set up everything.10. Deploy the code to AWS & Run it from the cloudThis article is already too long, so I won‚Äôt go into the details of how to deploy the AWS infrastructure described above and test it out here.But to give you some insights, we have used Pulumi as our infrastructure as a code (IaC) tool, which will allow you to spin it quickly with a few commands.Also, I won‚Äôt let you hang on to this one. We made a promise and‚Ä¶ ‚ÜìWe prepared step-by-step instructions in the README of our GitHub repository on how to use Pulumni to spin up the infrastructure and test it out.ConclusionNow you know how to write streaming pipelines like a PRO!In Lesson 4, you learned how to:design a feature pipeline using the 3-pipeline architecturewrite a streaming pipeline using Bytewax as a streaming engineuse a dispatcher layer to write a modular and flexible application to process multiple types of data (posts, articles, code)load the cleaned and embedded data to Qdrantdeploy the streaming pipeline to AWS‚Üí This is only the ingestion part used for fine-tuning LLMs and RAG.In Lesson 5, you will learn how to write a retrieval client for the 3 data types using good SWE practices and improve the retrieval accuracy using advanced retrieval & post-retrieval techniques. See you there!üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èEnjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE ‚ÜìDecoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Bytewax, Bytewax Landing Page[3] Unstructured Cleaning Examples, Unstructured Documentation[4] Recursively split by character, LangChain‚Äôs Documentation[5] Split by tokens, LangChain‚Äôs Documentation[6] sentence-transformers/all-MiniLM-L6-v2, HuggingFace[7] hkunlp/instructor-xl, HuggingFace[8] Qdrant, Qdrant Documentation[9] Abstract Factory Pattern, Refactoring Guru[10] Strategy Pattern, Refactoring GuruImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthMl System DesignMachine LearningArtificial IntelligenceData ScienceSoftware Engineering8248241FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13See all from Paul IusztinSee all from Decoding MLRecommended from MediumVipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Vishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesdata science and AI40 stories¬∑269 savesDerckData architecture for MLOps: Metadata storeIntroductionJul 17Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Tarun SinghinAI AdvancesMastering RAG Chunking Techniques for Enhanced Document ProcessingDividing large documents into smaller parts is a crucial yet intricate task that significantly impacts the performance of‚Ä¶Jun 182592Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87'), ArticleDocument(id=UUID('649bd7d7-aa0e-4ada-b5e2-1c50fe7c95e6'), content={'Title': 'The 4 Advanced RAG Algorithms You Must Know to Implement', 'Subtitle': 'Implement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithm', 'Content': '4 Advanced RAG Algorithms You Must Know | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICAThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmPaul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑May 4, 20241.8K12ListenShare‚Üí the 5th out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineerüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG AppsTo better understand the course‚Äôs goal, technical details, and system design ‚Üí Check out Lesson 1Let‚Äôs start with Lesson 5 ‚Üì‚Üì‚ÜìLesson 5: The 4 Advanced RAG Algorithms You Must Know to ImplementIn Lesson 5, we will focus on building an advanced retrieval module used for RAG.We will show you how to implement 4 retrieval and post-retrieval advanced optimization techniques to improve the accuracy of your RAG retrieval step.In this lesson, we will focus only on the retrieval part of the RAG system.In Lesson 4, we showed you how to clean, chunk, embed, and load social media data to a Qdrant vector DB (the ingestion part of RAG).In future lessons, we will integrate this retrieval module into the inference pipeline for a full-fledged RAG system.Retrieval Python Module ArchitectureWe assume you are already familiar with what a naive RAG looks like. If not, check out the following article from Decoding ML, where we present in a 2-minute read what a naive RAG looks like:Why you must choose streaming over batch pipelines when doing RAG in LLM applicationsLesson 2: RAG, streaming pipelines, vector DBs, text processingmedium.comTable of ContentsOverview of advanced RAG optimization techniquesAdvanced RAG techniques applied to the LLM twinRetrieval optimization (1): Query expansionRetrieval optimization (2): Self queryRetrieval optimization (3): Hybrid & filtered vector searchImplement the advanced retrieval Python classPost-retrieval optimization: Rerank using GPT-4How to use the retrievalConclusionüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. Overview of advanced RAG optimization techniquesA production RAG system is split into 3 main components:ingestion: clean, chunk, embed, and load your data to a vector DBretrieval: query your vector DB for contextgeneration: attach the retrieved context to your prompt and pass it to an LLMThe ingestion component sits in the feature pipeline, while the retrieval and generation components are implemented inside the inference pipeline.You can also use the retrieval and generation components in your training pipeline to fine-tune your LLM further on domain-specific prompts.You can apply advanced techniques to optimize your RAG system for ingestion, retrieval and generation.That being said, there are 3 main types of advanced RAG techniques:Pre-retrieval optimization [ingestion]: tweak how you create the chunksRetrieval optimization [retrieval]: improve the queries to your vector DBPost-retrieval optimization [retrieval]: process the retrieved chunks to filter out the noiseThe generation step can be improved through fine-tuning or prompt engineering, which will be explained in future lessons.The pre-retrieval optimization techniques are explained in Lesson 4.In this lesson, we will show you some popular retrieval and post-retrieval optimization techniques.2. Advanced RAG techniques applied to the LLM twinRetrieval optimizationWe will combine 3 techniques:Query ExpansionSelf QueryFiltered vector searchPost-retrieval optimizationWe will use the rerank pattern using GPT-4 and prompt engineering instead of Cohere or an open-source re-ranker cross-encoder [4].I don‚Äôt want to spend too much time on the theoretical aspects. There are plenty of articles on that.So, we will jump straight to implementing and integrating these techniques in our LLM twin system.But before seeing the code, let‚Äôs clarify a few things ‚ÜìAdvanced RAG architecture2.1 Important Note!We will show you a custom implementation of the advanced techniques and NOT use LangChain.Our primary goal is to build your intuition about how they work behind the scenes. However, we will attach LangChain‚Äôs equivalent so you can use them in your apps.Customizing LangChain can be a real headache. Thus, understanding what happens behind its utilities can help you build real-world applications.Also, it is critical to know that if you don‚Äôt ingest the data using LangChain, you cannot use their retrievals either, as they expect the data to be in a specific format.We haven‚Äôt used LangChain‚Äôs ingestion function in Lesson 4 either (the feature pipeline that loads data to Qdrant) as we want to do everything ‚Äúby hand‚Äù.2.2. Why Qdrant?There are many vector DBs out there, too many‚Ä¶But since we discovered Qdrant, we loved it.Why?It is built in Rust.Apache-2.0 license ‚Äî open-source üî•It has a great and intuitive Python SDK.It has a freemium self-hosted version to build PoCs for free.It supports unlimited document sizes, and vector dims of up to 645536.It is production-ready. Companies such as Disney, Mozilla, and Microsoft already use it.It is one of the most popular vector DBs out there.To put that in perspective, Pinecone, one of its biggest competitors, supports only documents with up to 40k tokens and vectors with up to 20k dimensions‚Ä¶. and a proprietary license.I could go on and on‚Ä¶‚Ä¶but if you are curious to find out more, check out Qdrant ‚Üê3. Retrieval optimization (1): Query expansionThe problemIn a typical retrieval step, you query your vector DB using a single point.The issue with that approach is that by using a single vector, you cover only a small area of your embedding space.Thus, if your embedding doesn\\'t contain all the required information, your retrieved context will not be relevant.What if we could query the vector DB with multiple data points that are semantically related?That is what the ‚ÄúQuery expansion‚Äù technique is doing!The solutionQuery expansion is quite intuitive.You use an LLM to generate multiple queries based on your initial query.These queries should contain multiple perspectives of the initial query.Thus, when embedded, they hit different areas of your embedding space that are still relevant to our initial question.You can do query expansion with a detailed zero-shot prompt.Here is our simple & custom solution ‚ÜìQuery expansion template ‚Üí GitHub Code ‚ÜêHere is LangChain‚Äôs MultiQueryRetriever class [5] (their equivalent).4. Retrieval optimization (2): Self queryThe problemWhen embedding your query, you cannot guarantee that all the aspects required by your use case are present in the embedding vector.For example, you want to be 100% sure that your retrieval relies on the tags provided in the query.The issue is that by embedding the query prompt, you can never be sure that the tags are represented in the embedding vector or have enough signal when computing the distance against other vectors.The solutionWhat if you could extract the tags within the query and use them along the embedded query?That is what self-query is all about!You use an LLM to extract various metadata fields that are critical for your business use case (e.g., tags, author ID, number of comments, likes, shares, etc.)In our custom solution, we are extracting just the author ID. Thus, a zero-shot prompt engineering technique will do the job.But, when extracting multiple metadata types, you should also use few-shot learning to optimize the extraction step.Self-queries work hand-in-hand with vector filter searches, which we will explain in the next section.Here is our solution ‚ÜìSelf-query template ‚Üí GitHub Code ‚ÜêHere is LangChain‚Äôs SelfQueryRetriever class [6] equivalent and this is an example using Qdrant [8].5. Retrieval optimization (3): Hybrid & filtered vector searchThe problemEmbeddings are great for capturing the general semantics of a specific chunk.But they are not that great for querying specific keywords.For example, if we want to retrieve article chunks about LLMs from our Qdrant vector DB, embeddings would be enough.However, if we want to query for a specific LLM type (e.g., LLama 3), using only similarities between embeddings won‚Äôt be enough.Thus, embeddings are not great for finding exact phrase matching for specific terms.The solutionCombine the vector search technique with one (or more) complementary search strategy, which works great for finding exact words.It is not defined which algorithms are combined, but the most standard strategy for hybrid search is to combine the traditional keyword-based search and modern vector search.How are these combined?The first method is to merge the similarity scores of the 2 techniques as follows:hybrid_score = (1 - alpha) * sparse_score + alpha * dense_scoreWhere alpha takes a value between [0, 1], with:alpha = 1: Vector Searchalpha = 0: Keyword searchAlso, the similarity scores are defined as follows:sparse_score: is the result of the keyword search that, behind the scenes, uses a BM25 algorithm [7] that sits on top of TF-IDF.dense_score: is the result of the vector search that most commonly uses a similarity metric such as cosine distanceThe second method uses the vector search technique as usual and applies a filter based on your keywords on top of the metadata of retrieved results.‚Üí This is also known as filtered vector search.In this use case, the similar score is not changed based on the provided keywords.It is just a fancy word for a simple filter applied to the metadata of your vectors.But it is essential to understand the difference between the first and second methods:the first method combines the similarity score between the keywords and vectors using the alpha parameter;the second method is a simple filter on top of your vector search.How does this fit into our architecture?Remember that during the self-query step, we extracted the author_id as an exact field that we have to match.Thus, we will search for the author_id using the keyword search algorithm and attach it to the 5 queries generated by the query expansion step.As we want the most relevant chunks from a given author, it makes the most sense to use a filter using the author_id as follows (filtered vector search) ‚Üìself._qdrant_client.search(      collection_name=\"vector_posts\",      query_filter=models.Filter(          must=[              models.FieldCondition(                  key=\"author_id\",                  match=models.MatchValue(                      value=metadata_filter_value,                  ),              )          ]      ),      query_vector=self._embedder.encode(generated_query).tolist(),      limit=k,)Note that we can easily extend this with multiple keywords (e.g., tags), making the combination of self-query and hybrid search a powerful retrieval duo.The only question you have to ask yourself is whether we want to use a simple vector search filter or the more complex hybrid search strategy.Note that LangChain‚Äôs SelfQueryRetriever class combines the self-query and hybrid search techniques behind the scenes, as can be seen in their Qdrant example [8]. That is why we wanted to build everything from scratch.6. Implement the advanced retrieval Python classNow that you‚Äôve understood the advanced retrieval optimization techniques we\\'re using, let‚Äôs combine them into a Python retrieval class.Here is what the main retriever function looks like ‚ÜìVectorRetriever: main retriever function ‚Üí GitHub ‚ÜêUsing a Python ThreadPoolExecutor is extremely powerful for addressing I/O bottlenecks, as these types of operations are not blocked by Python‚Äôs GIL limitations.Here is how we wrapped every advanced retrieval step into its own class ‚ÜìQuery expansion chains wrapper ‚Üí GitHub ‚ÜêThe SelfQuery class looks very similar ‚Äî üîó access it here [1] ‚Üê.Now the final step is to call Qdrant for each query generated by the query expansion step ‚ÜìVectorRetriever: main search function ‚Üí GitHub ‚ÜêNote that we have 3 types of data: posts, articles, and code repositories.Thus, we have to make a query for each collection and combine the results in the end.The most performant method is to use multi-indexing techniques, which allow you to query multiple types of data at once.But at the time I am writing this article, this is not a solved problem at the production level.Thus, we gathered data from each collection individually and kept the best-retrieved results using rerank.Which is the final step of the article.7. Post-retrieval optimization: Rerank using GPT-4We made a different search in the Qdrant vector DB for N prompts generated by the query expansion step.Each search returns K results.Thus, we end up with N x K chunks.In our particular case, N = 5 & K = 3. Thus, we end up with 15 chunks.Post-retrieval optimization: rerankThe problemThe retrieved context may contain irrelevant chunks that only:add noise: the retrieved context might be irrelevantmake the prompt bigger: results in higher costs & the LLM is usually biased in looking only at the first and last pieces of context. Thus, if you add a big context, there is a big chance it will miss the essence.unaligned with your question: the chunks are retrieved based on the query and chunk embedding similarity. The issue is that the embedding model is not tuned to your particular question, which might result in high similarity scores that are not 100% relevant to your question.The solutionWe will use rerank to order all the N x K chunks based on their relevance relative to the initial question, where the first one will be the most relevant and the last chunk the least.Ultimately, we will pick the TOP K most relevant chunks.Rerank works really well when combined with query expansion.A natural flow when using rerank is as follows:Search for >K chunks >>> Reorder using rerank >>> Take top KThus, when combined with query expansion, we gather potential useful context from multiple points in space rather than just looking for more than K samples in a single location.Now the flow looks like:Search for N x K chunks >>> Reoder using rerank >>> Take top KA typical re-ranking solution uses open-source Cross-Encoder models from sentence transformers [4].These solutions take both the question and context as input and return a score from 0 to 1.In this article, we want to take a different approach and use GPT-4 + prompt engineering as our reranker.If you want to see how to apply rerank using open-source algorithms, check out this hands-on article from Decoding ML:A Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.medium.comNow let‚Äôs see our implementation using GPT-4 & prompt engineering.Similar to what we did for the expansion and self-query chains, we define a template and a chain builder ‚ÜìRerank chain ‚Üí GitHub ‚ÜêHere is how we integrate the rerank chain into the retriever:Retriever: rerank step ‚Üí GitHub ‚Üê‚Ä¶and that‚Äôs it!Note that this is an experimental process. Thus, you can further tune your prompts for better results, but the primary idea is the same.8. How to use the retrievalThe last step is to run the whole thing.But there is a catch.As we said in the beginning the retriever will not be used as a standalone component in the LLM system.It will be used as a layer between the data and the Qdrant vector DB by the:training pipeline to retrieve raw data for fine-tuning (we haven‚Äôt shown that as it‚Äôs a straightforward search operation ‚Äî no RAG involved)inference pipeline to do RAG‚Üí That is why, for this lesson, there is no infrastructure involved!But, to test the retrieval, we wrote a simple script ‚ÜìRetriever testing entry point ‚Üí GitHub ‚ÜêLook at how easy it is to call the whole chain with our custom retriever‚Äîno fancy LangChain involved!Now, to call this script, run the following Make command:make local-test-retriever‚Ä¶and that‚Äôs it!In future lessons, we will learn to integrate it into the training & inference pipelines.‚Üí Check out the LLM Twin GitHub repository and try it yourself! ‚Ä¶ Of course, don‚Äôt forget to give it a ‚≠êÔ∏è to stay updated with the latest changes.ConclusionCongratulations!In Lesson 5, you learned to build an advanced RAG retrieval module optimized for searching posts, articles, and code repositories from a Qdrant vector DB.First, you learned about where the RAG pipeline can be optimized:pre-retrievalretrievalpost-retrievalAfter you learn how to build from scratch (without using LangChain‚Äôs utilities) the following advanced RAG retrieval & post-retrieval optimization techniques:query expansionself queryhybrid searchrerankUltimately, you understood where the retrieval component sits in an RAG production LLM system, where the code is shared between multiple microservices and doesn‚Äôt sit in a single Notebook.In Lesson 6, we will move to the training pipeline and show you how to automatically transform the data crawled from LinkedIn, Substack, Medium, and GitHub into an instruction dataset using GPT-4 to fine-tune your LLM Twin.See you there! ü§óüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èEnjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE ‚ÜìDecoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Bytewax, Bytewax Landing Page[3] Qdrant, Qdrant Documentation[4] Retrieve & Re-Rank, Sentence Transformers Documentation[5] MultiQueryRetriever, LangChain‚Äôs Documentation[6] Self-querying, LangChain‚Äôs Documentation[7] Okapi BM25, Wikipedia[8] Qdrant Self Query Example, LangChain‚Äôs DocumentationImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthData ScienceMachine LearningArtificial IntelligenceRagGenerative Ai1.8K1.8K12FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72Austin StarksinDataDrivenInvestorI used OpenAI‚Äôs o1 model to develop a trading strategy. It is DESTROYING the marketIt literally took one try. I was shocked.Sep 154.3K119ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesAI Regulation6 stories¬∑593 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18Louis-Fran√ßois BouchardinTowards AIThe Best RAG Stack to Date(exploring every component)Sep 1473911Necati DemirAdvanced RAG: Implementing Advanced Techniques to Enhance Retrieval-Augmented Generation SystemsMay 16481See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'), ArticleDocument(id=UUID('597ead2d-ae88-43f9-945d-d974630e858a'), content={'Title': 'Architect scalable and cost-effective LLM & RAG inference pipelines', 'Subtitle': 'Design, build and deploy RAG inference pipeline using LLMOps best practices.', 'Content': 'Architect LLM & RAG inference pipelines | Decoding MLOpen in appSign upSign inWriteSign upSign inLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICAArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑17 min read¬∑Jun 1, 20245601ListenShare‚Üí the 9th out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineerüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG AppsTo better understand the course‚Äôs goal, technical details, and system design ‚Üí Check out Lesson 1Let‚Äôs start with Lesson 9 ‚Üì‚Üì‚ÜìLesson 9: Architect scalable and cost-effective LLM & RAG inference pipelinesIn Lesson 9, we will focus on implementing and deploying the inference pipeline of the LLM twin system.First, we will design and implement a scalable LLM & RAG inference pipeline based on microservices, separating the ML and business logic into two layers.Secondly, we will use Comet ML to integrate a prompt monitoring service to capture all input prompts and LLM answers for further debugging and analysis.Ultimately, we will deploy the inference pipeline to Qwak and make the LLM twin service available worldwide.‚Üí Context from previous lessons. What you must know.This lesson is part of a more extensive series in which we learn to build an end-to-end LLM system using LLMOps best practices.In Lesson 4, we populated a Qdrant vector DB with cleaned, chunked, and embedded digital data (posts, articles, and code snippets).In Lesson 5, we implemented the advanced RAG retrieval module to query relevant digital data. Here, we will learn to integrate it into the final inference pipeline.In Lesson 7, we used Qwak to build a training pipeline to fine-tune an open-source LLM on our custom digital data. The LLM weights are available in a model registry.In Lesson 8, we evaluated the fine-tuned LLM to ensure the production candidate behaves accordingly.So‚Ä¶ What you must know from all of this?Don‚Äôt worry. If you don‚Äôt want to replicate the whole system, you can read this article independently from the previous lesson.Thus, the following assumptions are what you have to know. We have:a Qdrant vector DB populated with digital data (posts, articles, and code snippets)a vector DB retrieval module to do advanced RAGa fine-tuned open-source LLM available in a model registry from Comet ML‚Üí In this lesson, we will focus on gluing everything together into a scalable inference pipeline and deploying it to the cloud.Architect scalable and cost-effective LLM & RAG inference pipelinesTable of ContentsThe architecture of the inference pipelineThe training vs. the inference pipelineSettings Pydantic classThe RAG business moduleThe LLM microservicePrompt monitoringDeploying and running the inference pipelineConclusionüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. The architecture of the inference pipelineOur inference pipeline contains the following core elements:a fine-tuned LLMa RAG modulea monitoring serviceLet‚Äôs see how to hook these into a scalable and modular system.The interface of the inference pipelineAs we follow the feature/training/inference (FTI) pipeline architecture, the communication between the 3 core components is clear.Our LLM inference pipeline needs 2 things:a fine-tuned LLM: pulled from the model registryfeatures for RAG: pulled from a vector DB (which we modeled as a logical feature store)This perfectly aligns with the FTI architecture.‚Üí If you are unfamiliar with the FTI pipeline architecture, we recommend you review Lesson 1‚Äôs section on the 3-pipeline architecture.Monolithic vs. microservice inference pipelinesUsually, the inference steps can be split into 2 big layers:the LLM service: where the actual inference is being donethe business service: domain-specific logicWe can design our inference pipeline in 2 ways.Option 1: Monolithic LLM & business serviceIn a monolithic scenario, we implement everything into a single service.Pros:easy to implementeasy to maintainCons:harder to scale horizontally based on the specific requirements of each componentharder to split the work between multiple teamsnot being able to use different tech stacks for the two servicesMonolithic vs. microservice inference pipelinesOption 2: Different LLM & business microservicesThe LLM and business services are implemented as two different components that communicate with each other through the network, using protocols such as REST or gRPC.Pros:each component can scale horizontally individuallyeach component can use the best tech stack at handCons:harder to deployharder to maintainLet‚Äôs focus on the ‚Äúeach component can scale individually‚Äù part, as this is the most significant benefit of the pattern. Usually, LLM and business services require different types of computing. For example, an LLM service depends heavily on GPUs, while the business layer can do the job only with a CPU.As the LLM inference takes longer, you will often need more LLM service replicas to meet the demand. But remember that GPU VMs are really expensive.By decoupling the 2 components, you will run only what is required on the GPU machine and not block the GPU VM with other computing that can quickly be done on a much cheaper machine.Thus, by decoupling the components, you can scale horizontally as required, with minimal costs, providing a cost-effective solution to your system‚Äôs needs.Microservice architecture of the LLM twin inference pipelineLet‚Äôs understand how we applied the microservice pattern to our concrete LLM twin inference pipeline.As explained in the sections above, we have the following components:A business microserviceAn LLM microserviceA prompt monitoring microserviceThe business microservice is implemented as a Python module that:contains the advanced RAG logic, which calls the vector DB and GPT-4 API for advanced RAG operations;calls the LLM microservice through a REST API using the prompt computed utilizing the user‚Äôs query and retrieved contextsends the prompt and the answer generated by the LLM to the prompt monitoring microservice.As you can see, the business microservice is light. It glues all the domain steps together and delegates the computation to other services.The end goal of the business layer is to act as an interface for the end client. In our case, as we will ship the business layer as a Python module, the client will be a Streamlit application.However, you can quickly wrap the Python module with FastAPI and expose it as a REST API to make it accessible from the cloud.Microservice architecture of the LLM twin inference pipelineThe LLM microservice is deployed on Qwak. This component is wholly niched on hosting and calling the LLM. It runs on powerful GPU-enabled machines.How does the LLM microservice work?It loads the fine-tuned LLM twin model from Comet‚Äôs model registry [2].It exposes a REST API that takes in prompts and outputs the generated answer.When the REST API endpoint is called, it tokenizes the prompt, passes it to the LLM, decodes the generated tokens to a string and returns the answer.That‚Äôs it!The prompt monitoring microservice is based on Comet ML‚Äôs LLM dashboard. Here, we log all the prompts and generated answers into a centralized dashboard that allows us to evaluate, debug, and analyze the accuracy of the LLM.Remember that a prompt can get quite complex. When building complex LLM apps, the prompt usually results from a chain containing other prompts, templates, variables, and metadata.Thus, a prompt monitoring service, such as the one provided by Comet ML, differs from a standard logging service. It allows you to quickly dissect the prompt and understand how it was created. Also, by attaching metadata to it, such as the latency of the generated answer and the cost to generate the answer, you can quickly analyze and optimize your prompts.2. The training vs. the inference pipelineBefore diving into the code, let‚Äôs quickly clarify what is the difference between the training and inference pipelines.Along with the apparent reason that the training pipeline takes care of training while the inference pipeline takes care of inference (Duh!), there are some critical differences you have to understand.The input of the pipeline & How the data is accessedDo you remember our logical feature store based on the Qdrant vector DB and Comet ML artifacts? If not, consider checking out Lesson 6 for a refresher.The core idea is that during training, the data is accessed from an offline data storage in batch mode, optimized for throughput and data lineage.Our LLM twin architecture uses Comet ML artifacts to access, version, and track all our data.The data is accessed in batches and fed to the training loop.During inference, you need an online database optimized for low latency. As we directly query the Qdrant vector DB for RAG, that fits like a glove.During inference, you don‚Äôt care about data versioning and lineage. You just want to access your features quickly for a good user experience.The data comes directly from the user and is sent to the inference logic.The training vs. the inference pipelineThe output of the pipelineThe training pipeline‚Äôs final output is the trained weights stored in Comet‚Äôs model registry.The inference pipeline‚Äôs final output is the predictions served directly to the user.The infrastructureThe training pipeline requires more powerful machines with as many GPUs as possible.Why? During training, you batch your data and have to hold in memory all the gradients required for the optimization steps. Because of the optimization algorithm, the training is more compute-hungry than the inference.Thus, more computing and VRAM result in bigger batches, which means less training time and more experiments.The inference pipeline can do the job with less computation. During inference, you often pass a single sample or smaller batches to the model.If you run a batch pipeline, you will still pass batches to the model but don‚Äôt perform any optimization steps.If you run a real-time pipeline, as we do in the LLM twin architecture, you pass a single sample to the model or do some dynamic batching to optimize your inference step.Are there any overlaps?Yes! This is where the training-serving skew comes in.During training and inference, you must carefully apply the same preprocessing and postprocessing steps.If the preprocessing and postprocessing functions or hyperparameters don‚Äôt match, you will end up with the training-serving skew problem.Enough with the theory. Let‚Äôs dig into the RAG business microservice ‚Üì3. Settings Pydantic classFirst, let‚Äôs understand how we defined the settings to configure the inference pipeline components.We used pydantic_settings and inherited its BaseSettings class.This approach lets us quickly define a set of default settings variables and load sensitive values such as the API KEY from a .env file.from pydantic_settings import BaseSettings, SettingsConfigDictclass AppSettings(BaseSettings):    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\"    ... # Settings.    # CometML config    COMET_API_KEY: str    COMET_WORKSPACE: str    COMET_PROJECT: str = \"llm-twin-course\"    ... # More settings.settings = AppSettings()All the variables called settings.* (e.g., settings.Comet_API_KEY) come from this class.4. The RAG business moduleWe will define the RAG business module under the LLMTwin class. The LLM twin logic is directly correlated with our business logic.We don‚Äôt have to introduce the word ‚Äúbusiness‚Äù in the naming convention of the classes. What we presented so far was used for a clear separation of concern between the LLM and business layers.Initially, within the LLMTwin class, we define all the clients we need for our business logic ‚ÜìInference pipeline business module: __init__() method ‚Üí GitHub ‚ÜêNow let‚Äôs dig into the generate() method, where we:call the RAG module;create the prompt using the prompt template, query and context;call the LLM microservice;log the prompt, prompt template, and answer to Comet ML‚Äôs prompt monitoring service.Inference pipeline business module: generate() method ‚Üí GitHub ‚ÜêNow, let‚Äôs look at the complete code of the generate() method. It‚Äôs the same thing as what we presented above, but with all the nitty-little details.class LLMTwin:    def __init__(self) -> None:        ...    def generate(        self,        query: str,        enable_rag: bool = True,        enable_monitoring: bool = True,    ) -> dict:        prompt_template = self.template.create_template(enable_rag=enable_rag)        prompt_template_variables = {            \"question\": query,        }        if enable_rag is True:            retriever = VectorRetriever(query=query)            hits = retriever.retrieve_top_k(                k=settings.TOP_K,                 to_expand_to_n_queries=settings.EXPAND_N_QUERY            )            context = retriever.rerank(                hits=hits,                 keep_top_k=settings.KEEP_TOP_K            )            prompt_template_variables[\"context\"] = context            prompt = prompt_template.format(question=query, context=context)        else:            prompt = prompt_template.format(question=query)        input_ = pd.DataFrame([{\"instruction\": prompt}]).to_json()        response: list[dict] = self.qwak_client.predict(input_)        answer = response[0][\"content\"][0]        if enable_monitoring is True:            self.prompt_monitoring_manager.log(                prompt=prompt,                prompt_template=prompt_template.template,                prompt_template_variables=prompt_template_variables,                output=answer,                metadata=metadata,            )        return {\"answer\": answer}Let‚Äôs look at how our LLM microservice is implemented using Qwak.5. The LLM microserviceAs the LLM microservice is deployed on Qwak, we must first inherit from the QwakModel class and implement some specific functions.initialize_model(): where we load the fine-tuned model from the model registry at serving timeschema(): where we define the input and output schemapredict(): where we implement the actual inference logicNote: The build() function contains all the training logic, such as loading the dataset, training the LLM, and pushing it to a Comet experiment. To see the full implementation, consider checking out Lesson 7, where we detailed the training pipeline.LLM microservice ‚Üí GitHub ‚ÜêLet‚Äôs zoom into the implementation and the life cycle of the Qwak model.The schema() method is used to define how the input and output of the predict() method look like. This will automatically validate the structure and type of the predict() method. For example, the LLM microservice will throw an error if the variable instruction is a JSON instead of a string.The other Qwak-specific methods are called in the following order:__init__() ‚Üí when deploying the modelinitialize_model() ‚Üí when deploying the modelpredict() ‚Üí on every request to the LLM microservice>>> Note that these methods are called only during serving time (and not during training).Qwak exposes your model as a RESTful API, where the predict() method is called on each request.Inside the prediction method, we perform the following steps:map the input text to token IDs using the LLM-specific tokenizermove the token IDs to the provided device (GPU or CPU)pass the token IDs to the LLM and generate the answerextract only the generated tokens from the generated_ids variable by slicing it using the shape of the input_idsdecode the generated_ids back to textreturn the generated textHere is the complete code for the implementation of the Qwak LLM microservice:class CopywriterMistralModel(QwakModel):    def __init__(        self,        use_experiment_tracker: bool = True,        register_model_to_model_registry: bool = True,        model_type: str = \"mistralai/Mistral-7B-Instruct-v0.1\",        fine_tuned_llm_twin_model_type: str = settings.FINE_TUNED_LLM_TWIN_MODEL_TYPE,        dataset_artifact_name: str = settings.DATASET_ARTIFACT_NAME,        config_file: str = settings.CONFIG_FILE,        model_save_dir: str = settings.MODEL_SAVE_DIR,    ) -> None:        self.use_experiment_tracker = use_experiment_tracker        self.register_model_to_model_registry = register_model_to_model_registry        self.model_save_dir = model_save_dir        self.model_type = model_type        self.fine_tuned_llm_twin_model_type = fine_tuned_llm_twin_model_type        self.dataset_artifact_name = dataset_artifact_name        self.training_args_config_file = config_file  def build(self) -> None:      # Training logic      ...  def initialize_model(self) -> None:      self.model, self.tokenizer, _ = build_qlora_model(            pretrained_model_name_or_path=self.model_type,            peft_pretrained_model_name_or_path=self.fine_tuned_llm_twin_model_type,            bnb_config=self.nf4_config,            lora_config=self.qlora_config,            cache_dir=settings.CACHE_DIR,        )        self.model = self.model.to(self.device)      logging.info(f\"Successfully loaded model from {self.model_save_dir}\")  def schema(self) -> ModelSchema:      return ModelSchema(          inputs=[RequestInput(name=\"instruction\", type=str)],          outputs=[InferenceOutput(name=\"content\", type=str)],      )  @qwak.api(output_adapter=DefaultOutputAdapter())  def predict(self, df) -> pd.DataFrame:      input_text = list(df[\"instruction\"].values)      input_ids = self.tokenizer(          input_text, return_tensors=\"pt\", add_special_tokens=True      )      input_ids = input_ids.to(self.device)      generated_ids = self.model.generate(          **input_ids,          max_new_tokens=500,          do_sample=True,          pad_token_id=self.tokenizer.eos_token_id,      )      answer_start_idx = input_ids[\"input_ids\"].shape[1]      generated_answer_ids = generated_ids[:, answer_start_idx:]      decoded_output = self.tokenizer.batch_decode(generated_answer_ids)[0]      return pd.DataFrame([{\"content\": decoded_output}]) Where the settings used in the code above have the following values:class AppSettings(BaseSettings):    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")    ... # Other settings.        DATASET_ARTIFACT_NAME: str = \"posts-instruct-dataset\"    FINE_TUNED_LLM_TWIN_MODEL_TYPE: str = \"decodingml/llm-twin:1.0.0\"    CONFIG_FILE: str = \"./finetuning/config.yaml\"        MODEL_SAVE_DIR: str = \"./training_pipeline_output\"    CACHE_DIR: Path = Path(\"./.cache\")The most important one is the FINE_TUNED_LLM_TWIN_MODEL_TYPE setting, which reflects what model and version to load from the model registry.Access the code üîó here ‚ÜêThe final step is to look at Comet‚Äôs prompt monitoring service. ‚Üì6. Prompt monitoringComet makes prompt monitoring straightforward. There is just one API call where you connect to your project and workspace and send the following to a single function:the prompt and LLM outputthe prompt template and variables that created the final outputyour custom metadata specific to your use case ‚Äî here, you add information about the model, prompt token count, token generation costs, latency, etc.Prompt monitoring service ‚Üí GitHub ‚ÜêLet‚Äôs look at the logs in Comet ML‚ÄôsML‚Äôs LLMOps dashboard.Here is how you can quickly access them ‚Üìlog in to Comet (or create an account)go to your workspaceaccess the project with the ‚ÄúLLM‚Äù symbol attached to it. In our case, this is the ‚Äúllm-twin-course-monitoring‚Äù project.Note: Comet ML provides a free version which is enough to run these examples.Screenshot from Comet ML‚Äôs dashboardThis is how Comet ML‚Äôs prompt monitoring dashboard looks. Here, you can scroll through all the prompts that were ever sent to the LLM. ‚ÜìYou can click on any prompt and see everything we logged programmatically using the PromptMonitoringManager class.Screenshot from Comet ML‚Äôs dashboardBesides what we logged, adding various tags and the inference duration can be valuable.7. Deploying and running the inference pipelineQwak makes the deployment of the LLM microservice straightforward.During Lesson 7, we fine-tuned the LLM and built the Qwak model. As a quick refresher, we ran the following CLI command to build the Qwak model, where we used the build_config.yaml file with the build configuration:poetry run qwak models build -f build_config.yaml .After the build is finished, we can make various deployments based on the build. For example, we can deploy the LLM microservice using the following Qwak command:qwak models deploy realtime \\\\--model-id \"llm_twin\" \\\\--instance \"gpu.a10.2xl\" \\\\ --timeout 50000 \\\\ --replicas 2 \\\\--server-workers 2We deployed two replicas of the LLM twin. Each replica has access to a machine with x1 A10 GPU. Also, each replica has two workers running on it.üîó More on Qwak instance types ‚ÜêTwo replicas and two workers result in 4 microservices that run in parallel and can serve our users.You can scale the deployment to more replicas if you need to serve more clients. Qwak provides autoscaling mechanisms triggered by listening to the consumption of GPU, CPU or RAM.To conclude, you build the Qwak model once, and based on it, you can make multiple deployments with various strategies.You can quickly close the deployment by running the following:qwak models undeploy --model-id \"llm_twin\"We strongly recommend closing down the deployment when you are done, as GPU VMs are expensive.To run the LLM system with a predefined prompt example, you have to run the following Python file:poetry run python main.pyWithin the main.py file, we call the LLMTwin class, which calls the other services as explained during this lesson.Note: The ‚Üí complete installation & usage instructions ‚Üê are available in the README of the GitHub repository.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èConclusionCongratulations! You are close to the end of the LLM twin series.In Lesson 9 of the LLM twin course, you learned to build a scalable inference pipeline for serving LLMs and RAG systems.First, you learned how to architect an inference pipeline by understanding the difference between monolithic and microservice architectures. We also highlighted the difference in designing the training and inference pipelines.Secondly, we walked you through implementing the RAG business module and LLM twin microservice. Also, we showed you how to log all the prompts, answers, and metadata for Comet‚Äôs prompt monitoring service.Ultimately, we showed you how to deploy and run the LLM twin inference pipeline on the Qwak AI platform.In Lesson 10, we will show you how to evaluate the whole system by building an advanced RAG evaluation pipeline that analyzes the accuracy of the LLMs ‚Äô answers relative to the query and context.See you there! ü§óüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èEnjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE ‚ÜìDecoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Add your models to Model Registry (2024), Comet ML GuidesImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthMachine LearningProgrammingMl System DesignData ScienceArtificial Intelligence5605601FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Mar 162.1K13See all from Paul IusztinSee all from Decoding MLRecommended from MediumVipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Vishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72ListsPredictive Modeling w/ Python20 stories¬∑1607 savesNatural Language Processing1766 stories¬∑1367 savesPractical Guides to Machine Learning10 stories¬∑1961 savesChatGPT21 stories¬∑846 savesDerckData architecture for MLOps: Metadata storeIntroductionJul 17Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922MdabdullahalhasibinTowards AIA Complete Guide to Embedding For NLP & Generative AI/LLMUnderstand the concept of vector embedding, why it is needed, and implementation with LangChain.3d agoNecati DemirAdvanced RAG: Implementing Advanced Techniques to Enhance Retrieval-Augmented Generation SystemsMay 16481See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99'), ArticleDocument(id=UUID('d39ca560-21bf-4a6c-a080-064b1ad7996a'), content={'Title': 'Real-time feature pipelines for RAG - by Paul Iusztin', 'Subtitle': 'RAG hybrid search with transformers-based sparse vectors. CDC tech stack for event-driven architectures.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Real-time feature pipelines for RAG\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Real-time feature pipelines for RAG\\n\\n### RAG hybrid search with transformers-based sparse vectors. CDC tech stack\\nfor event-driven architectures.\\n\\nPaul Iusztin\\n\\nAug 17, 2024\\n\\n14\\n\\nShare this post\\n\\n#### Real-time feature pipelines for RAG\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n### **This week‚Äôs topics:**\\n\\n  * CDC tech stack for event-driven architectures\\n\\n  * Real-time feature pipelines with CDC\\n\\n  * RAG hybrid search with transformers-based sparse vectors\\n\\n* * *\\n\\n### CDC tech stack for event-driven architectures\\n\\nHere is the ùòÅùó≤ùó∞ùóµ ùòÄùòÅùóÆùó∞ùó∏ used to ùóØùòÇùó∂ùóπùó± a ùóñùóµùóÆùóªùó¥ùó≤ ùóóùóÆùòÅùóÆ ùóñùóÆùóΩùòÅùòÇùóøùó≤ (ùóñùóóùóñ) ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅ for\\nimplementing an ùó≤ùòÉùó≤ùóªùòÅ-ùó±ùóøùó∂ùòÉùó≤ùóª ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤ in our ùóüùóüùó† ùóßùòÑùó∂ùóª ùó∞ùóºùòÇùóøùòÄùó≤  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóñùóµùóÆùóªùó¥ùó≤ ùóóùóÆùòÅùóÆ ùóñùóÆùóΩùòÅùòÇùóøùó≤ (ùóñùóóùóñ)?  \\n  \\nThe purpose of CDC is to capture insertions, updates, and deletions applied to\\na database and to make this change data available in a format easily\\nconsumable by downstream applications.  \\n  \\nùó™ùóµùòÜ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùóñùóóùóñ ùóΩùóÆùòÅùòÅùó≤ùóøùóª?  \\n  \\n\\\\- Real-time Data Syncing  \\n\\\\- Efficient Data Pipelines  \\n\\\\- Minimized System Impact  \\n\\\\- Event-Driven Architectures  \\n  \\nùó™ùóµùóÆùòÅ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùó≥ùóºùóø ùóÆùóª ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅùóÆùòÅùó∂ùóºùóª ùóºùó≥ ùóñùóóùóñ?  \\n  \\nWe will take the tech stack used in our LLM Twin course as an example,\\nwhere...  \\n  \\n... we built a feature pipeline to gather cleaned data for fine-tuning and\\nchunked & embedded data for RAG  \\n  \\nùóòùòÉùó≤ùóøùòÜùòÅùóµùó∂ùóªùó¥ ùòÑùó∂ùóπùóπ ùóØùó≤ ùó±ùóºùóªùó≤ ùóºùóªùóπùòÜ ùó∂ùóª ùó£ùòÜùòÅùóµùóºùóª!  \\n  \\nùòèùò¶ùò≥ùò¶ ùòµùò©ùò¶ùò∫ ùò¢ùò≥ùò¶  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\n1\\\\. ùóßùóµùó≤ ùòÄùóºùòÇùóøùó∞ùó≤ ùó±ùóÆùòÅùóÆùóØùóÆùòÄùó≤: MongoDB (it (also works for most databases such as\\nMySQL, PostgreSQL, Oracle, etc.)  \\n  \\n2\\\\. ùóî ùòÅùóºùóºùóπ ùòÅùóº ùó∫ùóºùóªùó∂ùòÅùóºùóø ùòÅùóµùó≤ ùòÅùóøùóÆùóªùòÄùóÆùó∞ùòÅùó∂ùóºùóª ùóπùóºùó¥: MongoDB Watcher (also Debezium is a\\npopular & scalable solution)  \\n  \\n3\\\\. ùóî ùó±ùó∂ùòÄùòÅùóøùó∂ùóØùòÇùòÅùó≤ùó± ùóæùòÇùó≤ùòÇùó≤: RabbitMQ (another popular option is to use Kafka, but\\nit was overkill in our use case)  \\n  \\n4\\\\. ùóî ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó≤ùóªùó¥ùó∂ùóªùó≤: Bytewax (great streaming engine for the Python\\necosystem)  \\n  \\n5\\\\. ùóî ùòÄùóºùòÇùóøùó∞ùó≤ ùó±ùóÆùòÅùóÆùóØùóÆùòÄùó≤: Qdrant (this works with any other database, but we\\nneeded a vector DB to store our data for fine-tuning and RAG)\\n\\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶, ùò©ùò¶ùò≥ùò¶ ùò™ùò¥ ùò©ùò∞ùò∏ ùò¢ ùòûùòôùòêùòõùòå ùò∞ùò±ùò¶ùò≥ùò¢ùòµùò™ùò∞ùòØ ùò∏ùò™ùò≠ùò≠ ùò£ùò¶ ùò±ùò≥ùò∞ùò§ùò¶ùò¥ùò¥ùò¶ùò•:  \\n  \\n1\\\\. Write a post to the MongoDB warehouse  \\n2\\\\. A \"ùò§ùò≥ùò¶ùò¢ùòµùò¶\" operation is logged in the transaction log of Mongo  \\n3\\\\. The MongoDB watcher captures this and emits it to the RabbitMQ queue  \\n4\\\\. The Bytewax streaming pipelines read the event from the queue  \\n5\\\\. It cleans, chunks, and embeds it right away - in real time!  \\n6\\\\. The cleaned & embedded version of the post is written to Qdrant\\n\\n* * *\\n\\n### Real-time feature pipelines with CDC\\n\\nùóõùóºùòÑ to ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùóñùóóùóñ to ùòÄùòÜùóªùó∞ your ùó±ùóÆùòÅùóÆ ùòÑùóÆùóøùó≤ùóµùóºùòÇùòÄùó≤ and ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùòÄùòÅùóºùóøùó≤ using a\\nRabbitMQ ùóæùòÇùó≤ùòÇùó≤ and a Bytewax ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó≤ùóªùó¥ùó∂ùóªùó≤ ‚Üì  \\n  \\nùóôùó∂ùóøùòÄùòÅ, ùóπùó≤ùòÅ\\'ùòÄ ùòÇùóªùó±ùó≤ùóøùòÄùòÅùóÆùóªùó± ùòÑùóµùó≤ùóøùó≤ ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùòÅùóº ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùòÅùóµùó≤ ùóñùóµùóÆùóªùó¥ùó≤ ùóóùóÆùòÅùóÆ ùóñùóÆùóΩùòÅùòÇùóøùó≤\\n(ùóñùóóùóñ) ùóΩùóÆùòÅùòÅùó≤ùóøùóª:  \\n  \\nùòäùòãùòä ùò™ùò¥ ùò∂ùò¥ùò¶ùò• ùò∏ùò©ùò¶ùòØ ùò∫ùò∞ùò∂ ùò∏ùò¢ùòØùòµ ùòµùò∞ ùò¥ùò∫ùòØùò§ 2 ùò•ùò¢ùòµùò¢ùò£ùò¢ùò¥ùò¶ùò¥.  \\n  \\nThe destination can be a complete replica of the source database (e.g., one\\nfor transactional and the other for analytical applications)  \\n  \\n...or you can process the data from the source database before loading it to\\nthe destination DB (e.g., retrieve various documents and chunk & embed them\\nfor RAG).  \\n  \\nùòõùò©ùò¢ùòµ\\'ùò¥ ùò∏ùò©ùò¢ùòµ ùòê ùò¢ùòÆ ùò®ùò∞ùò™ùòØùò® ùòµùò∞ ùò¥ùò©ùò∞ùò∏ ùò∫ùò∞ùò∂:  \\n  \\n**How** to **use CDC** to **sync** a **MongoDB** & **Qdrant vector DB** to\\nstreamline real-time documents that must be ready for fine-tuning LLMs and\\nRAG.  \\n  \\n**MongoDB** is our data warehouse.  \\n  \\n**Qdrant** is our logical feature store.  \\n  \\n.  \\n  \\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùòÅùóµùó≤ ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅùóÆùòÅùó∂ùóºùóª ùóºùó≥ ùòÅùóµùó≤ ùóñùóóùóñ ùóΩùóÆùòÅùòÅùó≤ùóøùóª:  \\n  \\n1\\\\. Use Mongo\\'s ùò∏ùò¢ùòµùò§ùò©() method to listen for CRUD transactions  \\n  \\n2\\\\. For example, on a CREATE operation, along with saving it to Mongo, the\\nùò∏ùò¢ùòµùò§ùò©() method will trigger a change and return a JSON with all the\\ninformation.  \\n  \\n3\\\\. We standardize the JSON in our desired structure.  \\n  \\n4\\\\. We stringify the JSON and publish it to the RabbitMQ queue  \\n  \\nùóõùóºùòÑ ùó±ùóº ùòÑùó≤ ùòÄùó∞ùóÆùóπùó≤?  \\n  \\n‚Üí You can use Debezium instead of Mongo\\'s ùò∏ùò¢ùòµùò§ùò©() method for scaling up the\\nsystem, but the idea remains the same.  \\n  \\n‚Üí You can swap RabbitMQ with Kafka, but RabbitMQ can get you far.  \\n  \\nùó°ùóºùòÑ, ùòÑùóµùóÆùòÅ ùóµùóÆùóΩùóΩùó≤ùóªùòÄ ùóºùóª ùòÅùóµùó≤ ùóºùòÅùóµùó≤ùóø ùòÄùó∂ùó±ùó≤ ùóºùó≥ ùòÅùóµùó≤ ùóæùòÇùó≤ùòÇùó≤?  \\n  \\nYou have a Bytewax streaming pipeline - 100% written in Python that:  \\n  \\n5\\\\. Listens in real-time to new messages from the RabbitMQ queue  \\n  \\n6\\\\. It cleans, chunks, and embeds the events on the fly  \\n  \\n7\\\\. It loads the data to Qdrant for LLM fine-tuning & RAG\\n\\nMongoDB CDC example\\n\\n> Do you ùòÑùóÆùóªùòÅ to check out the ùó≥ùòÇùóπùóπ ùó∞ùóºùó±ùó≤?  \\n>  \\n> ...or even an ùó≤ùóªùòÅùó∂ùóøùó≤ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ about ùóñùóóùóñ?  \\n>  \\n> The CDC component is part of the ùóüùóüùó† ùóßùòÑùó∂ùóª FREE ùó∞ùóºùòÇùóøùòÄùó≤, made by Decoding ML.  \\n>  \\n> ‚Üì‚Üì‚Üì  \\n>  \\n> üîó ùòìùò¶ùò¥ùò¥ùò∞ùòØ 3: ùòäùò©ùò¢ùòØùò®ùò¶ ùòãùò¢ùòµùò¢ ùòäùò¢ùò±ùòµùò∂ùò≥ùò¶: ùòåùòØùò¢ùò£ùò≠ùò™ùòØùò® ùòåùò∑ùò¶ùòØùòµ-ùòãùò≥ùò™ùò∑ùò¶ùòØ ùòàùò≥ùò§ùò©ùò™ùòµùò¶ùò§ùòµùò∂ùò≥ùò¶ùò¥  \\n>  \\n> üîó ùòéùò™ùòµùòèùò∂ùò£\\n\\n* * *\\n\\n### RAG hybrid search with transformers-based sparse vectors\\n\\nùóõùòÜùóØùóøùó∂ùó± ùòÄùó≤ùóÆùóøùó∞ùóµ is standard in ùóÆùó±ùòÉùóÆùóªùó∞ùó≤ùó± ùó•ùóîùóö ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ. The ùòÅùóøùó∂ùó∞ùó∏ is to ùó∞ùóºùó∫ùóΩùòÇùòÅùó≤ the\\nsuitable ùòÄùóΩùóÆùóøùòÄùó≤ ùòÉùó≤ùó∞ùòÅùóºùóøùòÄ for it. Here is an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ that shows ùóµùóºùòÑ to use\\nùó¶ùó£ùóüùóîùóóùóò to ùó∞ùóºùó∫ùóΩùòÇùòÅùó≤ ùòÄùóΩùóÆùóøùòÄùó≤ ùòÉùó≤ùó∞ùòÅùóºùóøùòÄ using ùòÅùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùó≤ùóøùòÄ and integrate them into a\\nùóµùòÜùóØùóøùó∂ùó± ùòÄùó≤ùóÆùóøùó∞ùóµ ùóÆùóπùó¥ùóºùóøùó∂ùòÅùóµùó∫ using Qdrant.  \\n  \\nùôíùôùùôÆ ùôóùô§ùô©ùôùùôöùôß ùô¨ùôûùô©ùôù ùô®ùô•ùôñùôßùô®ùôö ùô´ùôöùôòùô©ùô§ùôßùô® ùô¨ùôùùôöùô£ ùô¨ùôö ùôùùôñùô´ùôö ùôôùôöùô£ùô®ùôö ùô´ùôöùôòùô©ùô§ùôßùô® (ùôöùô¢ùôóùôöùôôùôôùôûùô£ùôúùô®)?  \\n  \\nSparse vectors represent data by highlighting only the most relevant features\\n(like keywords), significantly reducing memory usage compared to dense\\nvectors.  \\n  \\nAlso, sparse vectors work great in finding specific keywords, which is why\\nthey work fantastic in combination with dense vectors used for finding\\nsimilarities in semantics but not particular words.  \\n  \\nùóßùóµùó≤ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ ùóµùó∂ùó¥ùóµùóπùó∂ùó¥ùóµùòÅùòÄ:  \\n  \\n\\\\- ùòöùò±ùò¢ùò≥ùò¥ùò¶ ùò∑ùò¥. ùò•ùò¶ùòØùò¥ùò¶ ùò∑ùò¶ùò§ùòµùò∞ùò≥ùò¥  \\n  \\n\\\\- ùòèùò∞ùò∏ ùòöùòóùòìùòàùòãùòå ùò∏ùò∞ùò≥ùò¨ùò¥: The SPLADE model leverages sparse vectors to perform\\nbetter than traditional methods like BM25 by computing it using transformer\\narchitectures.  \\n  \\n\\\\- ùòûùò©ùò∫ ùòöùòóùòìùòàùòãùòå ùò∏ùò∞ùò≥ùò¨ùò¥: It expands terms based on context rather than just\\nfrequency, offering a nuanced understanding of content relevancy.  \\n  \\n\\\\- ùòèùò∞ùò∏ ùòµùò∞ ùò™ùòÆùò±ùò≠ùò¶ùòÆùò¶ùòØùòµ ùò©ùò∫ùò£ùò≥ùò™ùò• ùò¥ùò¶ùò¢ùò≥ùò§ùò© ùò∂ùò¥ùò™ùòØùò® ùòöùòóùòìùòàùòãùòå with Qdrant: step-by-step code\\n\\nSparse vectors using transformers\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùòÅùóµùó≤ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòöùò±ùò¢ùò≥ùò¥ùò¶ ùòùùò¶ùò§ùòµùò∞ùò≥ùò¥ ùò™ùòØ ùòòùò•ùò≥ùò¢ùòØùòµ: ùòóùò∂ùò≥ùò¶ ùòùùò¶ùò§ùòµùò∞ùò≥-ùò£ùò¢ùò¥ùò¶ùò• ùòèùò∫ùò£ùò≥ùò™ùò• ùòöùò¶ùò¢ùò≥ùò§ùò©\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n14\\n\\nShare this post\\n\\n#### Real-time feature pipelines for RAG\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/real-time-feature-pipelines-with?r=1ttoeh'), ArticleDocument(id=UUID('4271a54f-6239-4f50-97e6-b3fa3a9a2fbd'), content={'Title': 'Building ML System Using the FTI Architecture', 'Subtitle': 'Introduction to the feature/training/inference (FTI) design pattern to build scalable and modular ML systems using MLOps best practices.', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Building ML systems the right way using the FTI architecture\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Building ML systems the right way using the FTI architecture\\n\\n### The fundamentals of the FTI architecture that will help you build modular\\nand scalable ML systems using MLOps best practices.\\n\\nPaul Iusztin\\n\\nAug 10, 2024\\n\\n12\\n\\nShare this post\\n\\n#### Building ML systems the right way using the FTI architecture\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nThe feature/training/inference (FTI) architecture builds scalable and modular\\nML systems using MLOps best practices.\\n\\nWe will start by discussing the problems of naively building ML systems. Then,\\nwe will examine other potential solutions and their problems.\\n\\nUltimately, we will present the feature/training/inference (FTI) design\\npattern and its benefits. We will also understand the benefits of using a\\nfeature store and model registry when architecting your ML system.\\n\\n### The problem with building ML systems\\n\\nBuilding production-ready ML systems is much more than just training a model.\\nFrom an engineering point of view, training the model is the most\\nstraightforward step in most use cases.\\n\\nHowever, training a model becomes complex when deciding on the correct\\narchitecture and hyperparameters. That‚Äôs not an engineering problem but a\\nresearch problem.\\n\\nAt this point, we want to focus on how to design a production-ready\\narchitecture. Training a model with high accuracy is extremely valuable, but\\njust by training it on a static dataset, you are far from deploying it\\nrobustly. We have to consider how to:\\n\\n  * ingest, clean and validate fresh data\\n\\n  * training vs. inference setups\\n\\n  * compute and serve features in the right environment\\n\\n  * serve the model in a cost-effective way\\n\\n  * version, track and share the datasets and models\\n\\n  * monitor your infrastructure and models\\n\\n  * deploy the model on a scalable infrastructure\\n\\n  * automate the deployments and training\\n\\nThese are the types of problems an ML or MLOps engineer must consider, while\\nthe research or data science team is often responsible for training the model.\\n\\nFigure 1: Components of an ML system. Photo from the Google Cloud Architecture\\ndocuments\\n\\nFigure 1 shows all the components the Google Cloud team suggests that a mature\\nML and MLOps system requires. Along with the ML code, there are many moving\\npieces. The rest of the system comprises configuration, automation, data\\ncollection, data verification, testing and debugging, resource management,\\nmodel analysis, process and metadata management, serving infrastructure, and\\nmonitoring. The point is that there are many components we must consider when\\nproductionizing an ML model.\\n\\n_Thus, the**critical question** is: ‚ÄúHow do we connect all these components\\ninto a single homogenous system‚Äù?_\\n\\nWe must create a boilerplate for clearly designing ML systems to answer that\\nquestion.\\n\\nSimilar solutions exist for classic software. For example, if you zoom out,\\nmost software applications can be split between a database, business logic and\\nUI layer. Every layer can be as complex as needed, but at a high-level\\noverview, the architecture of standard software can be boiled down to these\\nthree components.\\n\\nDo we have something similar for ML applications? The first step is to examine\\nprevious solutions and why they are unsuitable for building scalable ML\\nsystems.\\n\\n* * *\\n\\n### **The issue with previous solutions**\\n\\nIn Figure 2, you can observe the typical architecture present in most ML\\napplications. It is based on a monolithic batch architecture that couples the\\nfeature creation, model training, and inference into the same component.\\n\\nBy taking this approach, you quickly solve one critical problem in the ML\\nworld: the training-serving skew. The training-serving skew happens when the\\nfeatures passed to the model are computed differently at training and\\ninference time. In this architecture, the features are created using the same\\ncode. Hence, the training-serving skew issue is solved by default.\\n\\nThis pattern works fine when working with small data. The pipeline runs on a\\nschedule in batch mode, and the predictions are consumed by a third-party\\napplication such as a dashboard.\\n\\nFigure 2: Monolithic batch pipeline architecture\\n\\nUnfortunately, building a monolithic batch system raises many other issues,\\nsuch as:\\n\\n  * features are not reusable (by your system or others)\\n\\n  * if the data increases, you have to refactor the whole code to support PySpark or Ray\\n\\n  * hard to rewrite the prediction module in a more efficient language such as C++, Java or Rust\\n\\n  * hard to share the work between multiple teams between the features, training, and prediction modules\\n\\n  * impossible to switch to a streaming technology for real-time training\\n\\nIn Figure 3, we can see a similar scenario for a real-time system. This use\\ncase introduces another issue in addition to what we listed before. To make\\nthe predictions, we have to transfer the whole state through the client\\nrequest so the features can be computed and passed to the model.\\n\\nConsider the scenario of computing movie recommendations for a user. Instead\\nof simply passing the user ID, we must transmit the entire user state,\\nincluding their name, age, gender, movie history, and more. This approach is\\nfraught with potential errors, as the client must understand how to access\\nthis state, and it‚Äôs tightly coupled with the model service.\\n\\nAnother example would be when implementing an LLM with RAG support. The\\ndocuments we add as context along the query represent our external state. If\\nwe didn‚Äôt store the records in a vector DB, we would have to pass them with\\nthe user query. To do so, the client must know how to query and retrieve the\\ndocuments, which is not feasible. It is an antipattern for the client\\napplication to know how to access or compute the features. If you don‚Äôt\\nunderstand how RAG works, we will explain it in future chapters.\\n\\nFigure 3: Stateless real-time architecture\\n\\nIn conclusion, our problem is accessing the features to make predictions\\nwithout passing them at the client‚Äôs request. For example, based on our first\\nuser movie recommendation example, how can we predict the recommendations\\nsolely based on the user‚Äôs ID?\\n\\nRemember these questions, as we will answer them shortly.\\n\\n### **The solution: the FTI architecture**\\n\\nThe solution is based on creating a clear and straightforward mind map that\\nany team or person can follow to compute the features, train the model, and\\nmake predictions.\\n\\nBased on these three critical steps that any ML system requires, the pattern\\nis known as the FTI (feature, training, inference) pipelines. So, how does\\nthis differ from what we presented before?\\n\\nThe pattern suggests that any ML system can be boiled down to these three\\npipelines: feature, training, and inference (similar to the database, business\\nlogic and UI layers from classic software).\\n\\nThis is powerful, as we can clearly define the scope and interface of each\\npipeline. Also, it‚Äôs easier to understand how the three components interact.\\n\\nAs shown in Figure 4, we have the feature, training and inference pipelines.\\nWe will zoom in on each of them and understand their scope and interface.\\n\\nBefore going into the details, it is essential to understand that each\\npipeline is a different component that can run on a different process or\\nhardware. Thus, each pipeline can be written using a different technology, by\\na different team, or scaled differently. The key idea is that the design is\\nvery flexible to the needs of your team. It acts as a mind map for structuring\\nyour architecture.\\n\\nFigure 4: Feature/Training/Inference (FTI) pipelines architecture\\n\\n#### The feature pipeline\\n\\nThe feature pipelines take as input data and output features & labels used to\\ntrain the model.\\n\\nInstead of directly passing them to the model, the features and labels are\\nstored inside a feature store. Its responsibility is to store, version, track,\\nand share the features.\\n\\nBy saving the features into a feature store, we always have a state of our\\nfeatures. Thus, we can easily send the features to the training and inference\\npipeline(s).\\n\\nAs the data is versioned, we can always ensure that the training and inference\\ntime features match. Thus, we avoid the training-serving skew problem.\\n\\n#### The training pipeline\\n\\nThe training pipeline takes the features and labels from the features store as\\ninput and outputs a train model or models.\\n\\nThe models are stored in a model registry. Its role is similar to that of\\nfeature stores, but this time, the model is the first-class citizen. Thus, the\\nmodel registry will store, version, track, and share the model with the\\ninference pipeline.\\n\\nAlso, most modern model registries support a metadata store that allows you to\\nspecify essential aspects of how the model was trained. The most important are\\nthe features, labels and their version used to train the model. Thus, we will\\nalways know what data the model was trained on.\\n\\n#### The inference pipeline\\n\\nThe inference pipeline takes as input the features & labels from the feature\\nstore and the trained model from the model registry. With these two,\\npredictions can be easily made in either batch or real-time mode.\\n\\nAs this is a versatile pattern, it is up to you to decide what you do with\\nyour predictions. If it‚Äôs a batch system, they will probably be stored in a\\ndatabase. If it‚Äôs a real-time system, the predictions will be served to the\\nclient who requested them.\\n\\nAs the features, labels, and model are versioned. We can easily upgrade or\\nroll back the deployment of the model. For example, we will always know that\\nmodel v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus,\\nwe can quickly change the connections between the model and features.\\n\\n### Benefits of the FTI architecture\\n\\nTo conclude, the most important thing you must remember about the FTI\\npipelines is their interface:\\n\\n¬∑ The feature pipeline takes in data and outputs features & labels saved to\\nthe feature store.\\n\\n¬∑ The training pipelines query the features store for features & labels and\\noutput a model to the model registry.\\n\\n¬∑ The inference pipeline uses the features from the feature store and the\\nmodel from the model registry to make predictions.\\n\\nIt doesn‚Äôt matter how complex your ML system gets. These interfaces will\\nremain the same.\\n\\nNow that we better understand how the pattern works, we want to highlight the\\nmain benefits of using this pattern:\\n\\n  * as you have just three components, it is intuitive to use and easy to understand;\\n\\n  * each component can be written into its tech stack, so we can quickly adapt them to specific needs, such as big or streaming data. Also, it allows us to pick the best tools for the job;\\n\\n  * as there is a transparent interface between the three components, each one can be developed by a different team (if necessary), making the development more manageable and scalable;\\n\\n  * every component can be deployed, scaled, and monitored independently.\\n\\nThe final thing you must understand about the FTI pattern is that the system\\ndoesn‚Äôt have to contain only three pipelines. In most cases, it will include\\nmore. For example, the feature pipeline can be composed of a service that\\ncomputes the features and one that validates the data. Also, the training\\npipeline can be composed of the training and evaluation components.\\n\\nThe FTI pipelines act as logical layers. Thus, it is perfectly fine for each\\nto be complex and contain multiple services. However, what is essential is to\\nstick to the same interface on how the FTI pipelines interact with each other\\nthrough the feature store and model registries. By doing so, each FTI\\ncomponent can evolve differently, without knowing the details of each other\\nand without breaking the system on new changes.\\n\\n### Conclusion\\n\\nIn this article, we understood the fundamental problems when naively building\\nML systems.\\n\\nWe also looked at potential solutions and their downsides.\\n\\nUltimately, we presented the FTI architecture, its benefits, and how to apply\\nit to modern ML systems.\\n\\n* * *\\n\\n> My _**latest book** , ‚ÄúLLM Engineer‚Äôs Handbook,‚Äù _inspired me to write this\\n> article.\\n\\nIf you liked this article, consider supporting me by buying my book and enjoy\\na lot more similar content compressed into a single book:\\n\\nLLM Engineer's Handbook\\n\\nLLM Engineer‚Äôs Handbook Cover\\n\\n* * *\\n\\n### References\\n\\n### Literature\\n\\n[1] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference\\nPipelines [2023], Hopsworks blog\\n\\n### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n12\\n\\nShare this post\\n\\n#### Building ML systems the right way using the FTI architecture\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/building-ml-systems-the-right-way?r=1ttoeh'), ArticleDocument(id=UUID('2ce3c5d1-730b-4258-88ab-07009eddaf33'), content={'Title': 'Reduce your PyTorch code latency by 82% - by Paul Iusztin', 'Subtitle': 'How not to optimize the inference of your DL models. Computer science is dead.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Reduce your PyTorch code latency by 82%\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Reduce your PyTorch code latency by 82%\\n\\n### How not to optimize the inference of your DL models. Computer science is\\ndead.\\n\\nPaul Iusztin\\n\\nAug 03, 2024\\n\\n9\\n\\nShare this post\\n\\n#### Reduce your PyTorch code latency by 82%\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Reduce the latency of your PyTorch code by 82%\\n\\n  * How I failed to optimize the inference of my DL models\\n\\n  * Computer science is dead\\n\\n* * *\\n\\n> ùó°ùó≤ùòÑ ùóØùóºùóºùó∏ on engineering end-to-end LLM systems, from data collection and\\n> fine-tuning to LLMOps (deployment, monitoring).\\n\\nI kept this one a secret, but in the past months, in collaboration with Packt\\n, Alex Vesa and Maxime Labonne , we started working on the ùòìùòìùòî ùòåùòØùò®ùò™ùòØùò¶ùò¶ùò≥\\'ùò¥\\nùòèùò¢ùòØùò•ùò£ùò∞ùò∞ùò¨.  \\n  \\nùóî ùóØùóºùóºùó∏ that will walk you through everything you know to build a production-\\nready LLM project.\\n\\nI am a big advocate of learning with hands-on examples while being anchored in\\nreal-world use cases.  \\n  \\nThat is why this is not the standard theoretical book.  \\n  \\nWhile reading the book, you will learn to build a complex LLM project: an LLM\\nTwin. In contrast, theoretical aspects will back everything to understand why\\nwe make certain decisions.  \\n  \\nHowever, our ultimate goal is to present a framework that can be applied to\\nmost LLM projects.  \\n  \\n.  \\n  \\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùóÆ ùòÄùóªùó≤ùóÆùó∏ ùóΩùó≤ùó≤ùó∏ ùóºùó≥ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùòÑùó∂ùóπùóπ ùóπùó≤ùóÆùóøùóª ùòÑùó∂ùòÅùóµùó∂ùóª ùòÅùóµùó≤ ùóüùóüùó† ùóòùóªùó¥ùó∂ùóªùó≤ùó≤ùóø\\'ùòÄ\\nùóõùóÆùóªùó±ùóØùóºùóºùó∏:  \\n  \\n\\\\- collect unstructured data  \\n\\\\- create instruction datasets from raw data to fine-tune LLMs  \\n\\\\- SFT techniques such as LoRA and QLoRA  \\n\\\\- LLM evaluation techniques  \\n\\\\- Preference alignment using DPO  \\n\\\\- inference optimization methods (key optimization, model parallelism,\\nquantization, attention mechanisms)  \\n\\\\- advanced RAG algorithms using LangChain as our LLM framework and Qdrant as\\nour vector DB  \\n  \\n\\\\- design LLM systems using the FTI architecture  \\n\\\\- use AWS SageMaker to fine-tune and deploy open-source LLMs  \\n\\\\- use ZenML to orchestrate all the pipelines and track the data as artifacts  \\n\\\\- LLMOps patterns such as CT/CI/CD pipelines, model registries and using\\nComet for experiment tracking and prompt monitoring  \\n  \\n.  \\n  \\nThe book is still a work in progress, but we are very excited about it!  \\n  \\nThank you, Packt, for making this possible and Maxime and Alex for this\\nremarkable collaboration.  \\n  \\nIf you are curious, you can currently pre-order it from Amazon. The whole book\\nshould be released by the end of September 2024.  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòìùòìùòî ùòåùòØùò®ùò™ùòØùò¶ùò¶ùò≥\\'ùò¥ ùòèùò¢ùòØùò•ùò£ùò∞ùò∞ùò¨: ùòîùò¢ùò¥ùòµùò¶ùò≥ ùòµùò©ùò¶ ùò¢ùò≥ùòµ ùò∞ùòß ùò¶ùòØùò®ùò™ùòØùò¶ùò¶ùò≥ùò™ùòØùò® ùòìùò¢ùò≥ùò®ùò¶ ùòìùò¢ùòØùò®ùò∂ùò¢ùò®ùò¶ ùòîùò∞ùò•ùò¶ùò≠ùò¥\\nùòßùò≥ùò∞ùòÆ ùò§ùò∞ùòØùò§ùò¶ùò±ùòµ ùòµùò∞ ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ\\n\\n* * *\\n\\n### Reduce the latency of your PyTorch code by 82%\\n\\nThis is how I ùóøùó≤ùó±ùòÇùó∞ùó≤ùó± the ùóπùóÆùòÅùó≤ùóªùó∞ùòÜ of my ùó£ùòÜùóßùóºùóøùó∞ùóµ ùó∞ùóºùó±ùó≤ by ùü¥ùüÆ% ùòÇùòÄùó∂ùóªùó¥ ùóºùóªùóπùòÜ ùó£ùòÜùòÅùóµùóºùóª\\n& ùó£ùòÜùóßùóºùóøùó∞ùóµ. ùó°ùó¢ ùó≥ùóÆùóªùó∞ùòÜ ùòÅùóºùóºùóπùòÄ ùó∂ùóªùòÉùóºùóπùòÉùó≤ùó±!  \\n  \\nùôèùôùùôö ùô•ùôßùô§ùôóùô°ùôöùô¢?  \\n  \\nDuring inference, I am using 5 DL at ~25k images at once.  \\n  \\nThe script took around ~4 hours to run.  \\n  \\nThe problem is that this isn\\'t a batch job that runs over the night...  \\n  \\nVarious people across the company required it to run in \"real-time\" multiple\\ntimes a day.\\n\\nùôèùôùùôö ùô®ùô§ùô°ùô™ùô©ùôûùô§ùô£?  \\n  \\nThe first thing that might come to your mind is to start using some fancy\\noptimizer (e.g., TensorRT).  \\n  \\nEven though that should be done at some point...  \\n  \\nFirst, you should ùóÆùòÄùó∏ ùòÜùóºùòÇùóøùòÄùó≤ùóπùó≥:  \\n  \\n\\\\- I/O bottlenecks: reading & writing images  \\n\\\\- preprocessing & postprocessing - can it be parallelized?  \\n\\\\- are the CUDA cores used at their maximum potential?  \\n\\\\- is the bandwidth between the CPU & GPU throttled?  \\n\\\\- can we move more computation to the GPU?  \\n  \\nThat being said...  \\n  \\nùóõùó≤ùóøùó≤ is what I did I ùó±ùó≤ùó∞ùóøùó≤ùóÆùòÄùó≤ùó± the ùóπùóÆùòÅùó≤ùóªùó∞ùòÜ of the script by ùü¥ùüÆ%  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nùü≠\\\\. ùóïùóÆùòÅùó∞ùóµùó≤ùó± ùòÅùóµùó≤ ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùòÄùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nBatching is not only valuable for training but also mighty in speeding up your\\ninference time.  \\n  \\nOtherwise, you waste your GPU CUDA cores.  \\n  \\nInstead of passing through the models one sample at a time, I now process 64.  \\n  \\nùüÆ\\\\. ùóüùó≤ùòÉùó≤ùóøùóÆùó¥ùó≤ùó± ùó£ùòÜùóßùóºùóøùó∞ùóµ\\'ùòÄ ùóóùóÆùòÅùóÆùóüùóºùóÆùó±ùó≤ùóø  \\n  \\nThis has 2 main advantages:  \\n  \\n\\\\- parallel data loading & preprocessing on multiple processes (NOT threads)  \\n\\\\- copying your input images directly into the pinned memory (avoid a CPU ->\\nCPU copy operation)  \\n  \\nùüØ\\\\. ùó†ùóºùòÉùó≤ùó± ùóÆùòÄ ùó∫ùòÇùó∞ùóµ ùóºùó≥ ùòÅùóµùó≤ ùóΩùóºùòÄùòÅùóΩùóøùóºùó∞ùó≤ùòÄùòÄùó∂ùóªùó¥ ùóºùóª ùòÅùóµùó≤ ùóöùó£ùó®  \\n  \\nI saw that the tensor was moved too early on the CPU and mapped to a NumPy\\narray.  \\n  \\nI refactored the code to keep it on the GPU as much as possible, which had 2\\nmain advantages:  \\n  \\n\\\\- tensors are processed faster on the GPU  \\n\\\\- at the end of the logic, I had smaller tensors, resulting in smaller\\ntransfers between the CPU & GPU  \\n  \\nùü∞\\\\. ùó†ùòÇùóπùòÅùó∂ùòÅùóµùóøùó≤ùóÆùó±ùó∂ùóªùó¥ ùó≥ùóºùóø ùóÆùóπùóπ ùó∫ùòÜ ùóú/ùó¢ ùòÑùóøùó∂ùòÅùó≤ ùóºùóΩùó≤ùóøùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nFor I/O bottlenecks, using Python threads is extremely powerful.  \\n  \\nI moved all my writes under a ùòõùò©ùò≥ùò¶ùò¢ùò•ùòóùò∞ùò∞ùò≠ùòåùòπùò¶ùò§ùò∂ùòµùò∞ùò≥, batching my write\\noperations.  \\n  \\n.  \\n  \\nNote that I used only good old Python & PyTorch code.  \\n  \\n‚Üí When the code is poorly written, no tool can save you  \\n  \\nOnly now is the time to add fancy tooling, such as TensorRT.\\n\\n.\\n\\nSo remember...  \\n  \\nTo optimize the PyTorch code by 82%:  \\n  \\n1\\\\. Batched the inference samples  \\n2\\\\. Leveraged PyTorch\\'s DataLoader  \\n3\\\\. Moved as much of the postprocessing on the GPU  \\n4\\\\. Multithreading for all my I/O write operations  \\n  \\nWhat other methods do you have in mind? Leave them in the comments ‚Üì\\n\\n* * *\\n\\n### How I failed to optimize the inference of my DL models\\n\\nThis is how I FAILED to ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùó≤ the ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ of my ùóóùóü ùó∫ùóºùó±ùó≤ùóπùòÄ when ùóøùòÇùóªùóªùó∂ùóªùó¥\\nùòÅùóµùó≤ùó∫ on a ùó°ùòÉùó∂ùó±ùó∂ùóÆ ùóöùó£ùó®. Let me tell you ùòÑùóµùóÆùòÅ ùòÅùóº ùóÆùòÉùóºùó∂ùó± ‚Üì  \\n  \\nI had a simple task. To reduce the latency of the DL models used in\\nproduction.  \\n  \\nWe had 4 DL models that were running on Nvidia GPUs.  \\n  \\nAfter a first look at the inference code, I saw that the inputs to the models\\nweren\\'t batched.  \\n  \\nWe were processing one sample at a time.  \\n  \\nI said to myself: \"Ahaa! That\\'s it. I cracked it. We just have to batch as\\nmany samples as possible, and we are done.\"  \\n  \\nSo, I did just that...  \\n  \\nAfter 2-3 days of work adding the extra batch dimension to the PyTorch\\npreprocessing & postprocessing code, ùóú ùóøùó≤ùóÆùóπùó∂ùòáùó≤ùó± ùóú ùó™ùóîùó¶ ùó™ùó•ùó¢ùó°ùóö.\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùòÑùóµùòÜ  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nWe were using Nvidia GPUs from the A family (A6000, A5000, etc.).  \\n  \\nAs these GPUs have a lot of memory (>40GB), I managed to max out the VRAM and\\nsquash a batch of 256 images on the GPU.  \\n  \\nRelative to using a \"ùò£ùò¢ùòµùò§ùò© = 1\" it was faster, but not A LOT FASTER, as I\\nexpected.  \\n  \\nThen I tried batches of 128, 64, 32, 16, and 8.  \\n  \\n...and realized that everything > batch = 16 was running slower than using a\\nbatch of 16.  \\n  \\n‚Üí ùóî ùóØùóÆùòÅùó∞ùóµ ùóºùó≥ ùü≠ùü≤ ùòÑùóÆùòÄ ùòÅùóµùó≤ ùòÄùòÑùó≤ùó≤ùòÅ ùòÄùóΩùóºùòÅ.  \\n  \\nBut that is not good, as I was using only ~10% of the VRAM...  \\n  \\nùó™ùóµùòÜ ùó∂ùòÄ ùòÅùóµùóÆùòÅ?  \\n  \\nThe Nvidia A family of GPUs are known to:  \\n  \\n\\\\- having a lot of VRAM  \\n\\\\- not being very fast (the memory transfer between the CPU & GPU + the number\\nof CUDA cores isn\\'t that great)  \\n  \\nThat being said, my program was throttled.  \\n  \\nEven if my GPU could handle much more memory-wise, the memory transfer &\\nprocessing speeds weren\\'t keeping up.  \\n  \\nIn the end, it was a good optimization: ~75% faster  \\n  \\nùóïùòÇùòÅ ùòÅùóµùó≤ ùóπùó≤ùòÄùòÄùóºùóª ùóºùó≥ ùòÅùóµùó∂ùòÄ ùòÄùòÅùóºùóøùòÜ ùó∂ùòÄ:  \\n  \\n‚Üí ALWAYS KNOW YOUR HARDWARE ‚Üê  \\n  \\nMost probably, running a bigger batch on an A100 or V100 wouldn\\'t have the\\nsame problem.  \\n  \\nI plan to try that.  \\n  \\nBut that is why...  \\n  \\n‚Üí ùôÆùô§ùô™ ùôñùô°ùô¨ùôñùôÆùô® ùôùùôñùô´ùôö ùô©ùô§ ùô§ùô•ùô©ùôûùô¢ùôûùôØùôö ùô©ùôùùôö ùô•ùôñùôßùôñùô¢ùôöùô©ùôöùôßùô® ùô§ùôõ ùôÆùô§ùô™ùôß ùô®ùôÆùô®ùô©ùôöùô¢ ùôóùôñùô®ùôöùôô ùô§ùô£ ùôÆùô§ùô™ùôß\\nùôùùôñùôßùôôùô¨ùôñùôßùôö!\\n\\nIn theory, I knew this, but it is completely different when you encounter it\\nin production.  \\n  \\nLet me know in the comments if you want more similar stories on \"DO NOTs\" from\\nmy experience.\\n\\n* * *\\n\\n### Computer science is dead\\n\\nùóñùóºùó∫ùóΩùòÇùòÅùó≤ùóø ùòÄùó∞ùó∂ùó≤ùóªùó∞ùó≤ ùó∂ùòÄ ùó±ùó≤ùóÆùó±. Do this instead.  \\n  \\nIn a recent talk, Jensen Huang, CEO of Nvidia, said that kids shouldn\\'t learn\\nprogramming anymore.  \\n  \\nHe said that until now, most of us thought that everyone should learn to\\nprogram at some point.  \\n  \\nBut the actual opposite is the truth.  \\n  \\nWith the rise of AI, nobody should have or need to learn to program anymore.  \\n  \\nHe highlights that with AI tools, the technology divide between non-\\nprogrammers and engineers is closing.  \\n  \\n.  \\n  \\nùóîùòÄ ùóÆùóª ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóø, ùó∫ùòÜ ùó≤ùó¥ùóº ùó∂ùòÄ ùóµùòÇùóøùòÅ; ùó∫ùòÜ ùó≥ùó∂ùóøùòÄùòÅ ùóøùó≤ùóÆùó∞ùòÅùó∂ùóºùóª ùó∂ùòÄ ùòÅùóº ùòÄùóÆùòÜ ùó∂ùòÅ ùó∂ùòÄ ùòÄùòÅùòÇùóΩùó∂ùó±.  \\n  \\nBut after thinking about it more thoroughly, I tend to agree with him.  \\n  \\nAfter all, even now, almost anybody can work with AI.  \\n  \\nThis probably won\\'t happen in the next 10 years, but at some point, 100% will\\ndo.  \\n  \\nAt some point, we will ask our AI companion to write a program that does X for\\nus or whatever.  \\n  \\nBut, I think this is a great thing, as it will give us more time & energy to\\nfocus on what matters, such as:  \\n  \\n\\\\- solving real-world problems (not just tech problems)  \\n\\\\- moving to the next level of technology (Bioengineering, interplanetary\\ncolonization, etc.)  \\n\\\\- think about the grand scheme of things  \\n\\\\- be more creative  \\n\\\\- more time to connect with our family  \\n\\\\- more time to take care of our  \\n  \\nI personally think it is a significant step for humanity.  \\n  \\n.  \\n  \\nWhat do you think?  \\n  \\nAs an engineer, do you see your job still present in the next 10+ years?  \\n  \\nHere is the full talk  \\n  \\n‚Üì‚Üì‚Üì\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n9\\n\\nShare this post\\n\\n#### Reduce your PyTorch code latency by 82%\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| SorinAug 3Liked by Paul IusztinExcellent article, except the part CS is dead\\nis invalidExpand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/reduce-your-pytorchs-code-latency?r=1ttoeh'), ArticleDocument(id=UUID('7a276ac3-5c78-42d3-9ecf-05ff7f76fe31'), content={'Title': 'LLM Agents Demystified  - by Li - Decoding ML Newsletter ', 'Subtitle': 'Hands-on ReAct Agent implementation with AdalFlow library', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### LLM Agents Demystified\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# LLM Agents Demystified\\n\\n### Hands-on ReAct Agent implementation with AdalFlow library\\n\\nLi\\n\\nJul 27, 2024\\n\\n14\\n\\nShare this post\\n\\n#### LLM Agents Demystified\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nHi, all! I‚Äôm Li Yin, Author of AdalFlow and ex AI researcher @ MetaAI\\n\\nFind me on LinkedIn\\n\\nHandy links:\\n\\n  * AdalFlow Github\\n\\n  * Open in Colab\\n\\n _AdalFlow is an LLM library that not only helps developers build but also\\noptimizes LLM task pipelines. Embracing a design pattern similar to PyTorch,\\nAdalFlow is light, modular, and robust, with a 100% readable codebase._\\n\\n_There are many tutorials that show users how to call high-level agent APIs,\\nbut none of them explain how it really works in depth. This is where the\\nAdalFlow library aims to make a difference._\\n\\n_In this blog, you will not only learn how to use the ReAct Agent but more\\nimportantly, also understand how it was implemented and how you can customize\\nor build your own agent with AdalFlow._\\n\\n_Let‚Äôs get started!_\\n\\n_Image source , credits to Growtika_\\n\\n## Introduction\\n\\n _‚ÄúAn autonomous agent is a system situated within and a part of an\\nenvironment that senses that environment and acts on it, over time, in pursuit\\nof its own agenda and so as to effect what it senses in the future.‚Äù_\\n\\n _‚Äî Franklin and Graesser (1997)_\\n\\nAlongside the well-known RAGs, agents [1] are another popular family of LLM\\napplications. What makes agents stand out is their ability to reason, plan,\\nand act via accessible tools. When it comes to implementation, AdalFlow has\\nsimplified it down to a generator that can use tools, taking multiple steps\\n(sequential or parallel) to complete a user query.\\n\\n* * *\\n\\n### Table of Contents:\\n\\n  1. What is ReAct Agent\\n\\n  2. Introduction on tools/function calls\\n\\n  3. ReAct Agent implementation\\n\\n  4. ReAct Agent in action\\n\\n* * *\\n\\n### 1\\\\. What is ReAct Agent\\n\\nReAct [2] is a general paradigm for building agents that sequentially\\ninterleaves thought, action, and observation steps.\\n\\n  * **Thought** : The reasoning behind taking an action.\\n\\n  * **Action** : The action to take from a predefined set of actions. In particular, these are the tools/functional tools we have introduced in tools.\\n\\n  * **Observation** : The simplest scenario is the execution result of the action in string format. To be more robust, this can be defined in any way that provides the right amount of execution information for the LLM to plan the next step.\\n\\n#### **Prompt and Data Models**\\n\\n _The prompt is the most straightforward way to understand any LLM\\napplication. Always read the prompt._\\n\\nAdalFlow uses jinja2 syntax for the prompt.\\n\\nDEFAULT_REACT_AGENT_SYSTEM_PROMPT is the default prompt for the React agent‚Äôs\\nLLM planner. We can categorize the prompt template into four parts:\\n\\n  1. **Task description**\\n\\nThis part is the overall role setup and task description for the agent.\\n\\n    \\n    \\n    task_desc = r\"\"\"You are a helpful assistant.Answer the user\\'s query using the tools provided below with minimal steps and maximum accuracy.Each step you will read the previous Thought, Action, and Observation(execution result of the action) and then provide the next Thought and Action.\"\"\"\\n\\n  2. **Tools, output format, and example**\\n\\nThis part of the template is exactly the same as how we were calling functions\\nin the tools. The `output_format_str` is generated by `FunctionExpression` via\\n`JsonOutputParser`. It includes the actual output format and examples of a\\nlist of `FunctionExpression` instances. We use `thought` and `action` fields\\nof the `FunctionExpression` as the agent‚Äôs response. _You will be easily\\nvisualize the whole pipeline later by simply_`print(react).`\\n\\n    \\n    \\n    tools = r\"\"\"{% if tools %}\\n    <TOOLS>\\n    {% for tool in tools %}\\n    {{ loop.index }}.\\n    {{tool}}\\n    ------------------------\\n    {% endfor %}\\n    </TOOLS>\\n    {% endif %}\\n    {{output_format_str}}\"\"\"\\n\\n  3. **Task specification to teach the planner how to ‚Äúthink‚Äù.**\\n\\nWe provide more detailed instruction to ensure the agent will always end with\\n‚Äòfinish‚Äô action to complete the task. Additionally, we teach it how to handle\\nsimple queries and complex queries.\\n\\n  * For simple queries, we instruct the agent to finish with as few steps as possible.\\n\\n  * For complex queries, we teach the agent a ‚Äòdivide-and-conquer‚Äô strategy to solve the query step by step.\\n\\n    \\n    \\n    task_spec = r\"\"\"<TASK_SPEC>\\n    - For simple queries: Directly call the ``finish`` action and provide the answer.\\n    - For complex queries:\\n       - Step 1: Read the user query and potentially divide it into subqueries. And get started with the first subquery.\\n       - Call one available tool at a time to solve each subquery/subquestion. \\\\\\n       - At step \\'finish\\', join all subqueries answers and finish the task.\\n    Remember:\\n    - Action must call one of the above tools with name. It can not be empty.\\n    - You will always end with \\'finish\\' action to finish the task. The answer can be the final answer or failure message.\\n    </TASK_SPEC>\"\"\"\\n\\nWe put all these three parts together to be within the `<SYS></SYS>` tag.\\n\\n  4. **Agent step history.**\\n\\nWe use `StepOutput` to record the agent‚Äôs step history, including:\\n\\n  * `action`: This will be the `FunctionExpression` instance predicted by the agent.\\n\\n  * `observation`: The execution result of the action.\\n\\nIn particular, we format the steps history after the user query as follows:\\n\\n    \\n    \\n    step_history = r\"\"\"User query:\\n    {{ input_str }}\\n    {# Step History #}\\n    {% if step_history %}\\n    <STEPS>\\n    {% for history in step_history %}\\n    Step {{ loop.index }}.\\n    \"Thought\": \"{{history.action.thought}}\",\\n    \"Action\": \"{{history.action.action}}\",\\n    \"Observation\": \"{{history.observation}}\"\\n    ------------------------\\n    {% endfor %}\\n    </STEPS>\\n    {% endif %}\\n    You:\"\"\"\\n\\n### 2\\\\. Introduction on tools/function calls\\n\\nIn addition to the tools provided by users, by default, we add a new tool\\nnamed `finish` to allow the agent to stop and return the final answer.\\n\\n    \\n    \\n    def finish(answer: str) -> str:\\n       \"\"\"Finish the task with answer.\"\"\"\\n       return answer\\n\\nSimply returning a string might not fit all scenarios, and we might consider\\nallowing users to define their own finish function in the future for more\\ncomplex cases.\\n\\nAdditionally, since the provided tools cannot always solve user queries, we\\nallow users to configure if an LLM model should be used to solve a subquery\\nvia the `add_llm_as_fallback` parameter. This LLM will use the same model\\nclient and model arguments as the agent‚Äôs planner. Here is our code to specify\\nthe fallback LLM tool:\\n\\n    \\n    \\n    _additional_llm_tool = (\\n       Generator(model_client=model_client, model_kwargs=model_kwargs)\\n       if self.add_llm_as_fallback\\n       else None\\n    )\\n    \\n    def llm_tool(input: str) -> str:\\n       \"\"\"I answer any input query with llm\\'s world knowledge. Use me as a fallback tool or when the query is simple.\"\"\"\\n       # use the generator to answer the query\\n       try:\\n             output: GeneratorOutput = _additional_llm_tool(\\n                prompt_kwargs={\"input_str\": input}\\n             )\\n             response = output.data if output else None\\n             return response\\n       except Exception as e:\\n             log.error(f\"Error using the generator: {e}\")\\n             print(f\"Error using the generator: {e}\")\\n       return None\\n\\n### 3\\\\. ReAct Agent implementation\\n\\nWe define the class ReActAgent to put everything together. It will orchestrate\\ntwo components:\\n\\n  * `planner`: A `Generator` that works with a `JsonOutputParser` to parse the output format and examples of the function calls using `FunctionExpression`.\\n\\n  * `ToolManager`: Manages a given list of tools, the finish function, and the LLM tool. It is responsible for parsing and executing the functions.\\n\\nAdditionally, it manages step_history as a list of `StepOutput` instances for\\nthe agent‚Äôs internal state.\\n\\nPrompt the agent with an input query and process the steps to generate a\\nresponse.\\n\\n### 4\\\\. ReAct Agent in action\\n\\nWe will set up two sets of models, llama3‚Äì70b-8192 by Groq and gpt-3.5-turbo\\nby OpenAI, to test two queries. For comparison, we will compare these with a\\nvanilla LLM response without using the agent. Here are the code snippets:\\n\\n    \\n    \\n    from lightrag.components.agent import ReActAgent\\n    from lightrag.core import Generator, ModelClientType, ModelClient\\n    from lightrag.utils import setup_env\\n    \\n    setup_env()\\n    \\n    # Define tools\\n    def multiply(a: int, b: int) -> int:\\n       \"\"\"\\n       Multiply two numbers.\\n       \"\"\"\\n       return a * b\\n    def add(a: int, b: int) -> int:\\n       \"\"\"\\n       Add two numbers.\\n       \"\"\"\\n       return a + b\\n    def divide(a: float, b: float) -> float:\\n       \"\"\"\\n       Divide two numbers.\\n       \"\"\"\\n       return float(a) / b\\n    llama3_model_kwargs = {\\n       \"model\": \"llama3-70b-8192\",  # llama3 70b works better than 8b here.\\n       \"temperature\": 0.0,\\n    }\\n    gpt_model_kwargs = {\\n       \"model\": \"gpt-3.5-turbo\",\\n       \"temperature\": 0.0,\\n    }\\n    \\n    def test_react_agent(model_client: ModelClient, model_kwargs: dict):\\n       tools = [multiply, add, divide]\\n       queries = [\\n          \"What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2?\",\\n          \"Give me 5 words rhyming with cool, and make a 4-sentence poem using them\",\\n       ]\\n       # define a generator without tools for comparison\\n       generator = Generator(\\n          model_client=model_client,\\n          model_kwargs=model_kwargs,\\n       )\\n       react = ReActAgent(\\n          max_steps=6,\\n          add_llm_as_fallback=True,\\n          tools=tools,\\n          model_client=model_client,\\n          model_kwargs=model_kwargs,\\n       )\\n       # print(react)\\n       for query in queries:\\n          print(f\"Query: {query}\")\\n          agent_response = react.call(query)\\n          llm_response = generator.call(prompt_kwargs={\"input_str\": query})\\n          print(f\"Agent response: {agent_response}\")\\n          print(f\"LLM response: {llm_response}\")\\n          print(\"\")\\n\\nThe structure of React using `print(react)`, including the initialization\\narguments and two major components: `tool_manager` and `planner`. You can\\nvisualize the structure from our colab.\\n\\nNow, let‚Äôs run the test function to see the agent in action.\\n\\n    \\n    \\n    test_react_agent(ModelClientType.GROQ(), llama3_model_kwargs)\\n    test_react_agent(ModelClientType.OPENAI(), gpt_model_kwargs)\\n\\nOur agent will show the core steps for developers via colored printout,\\nincluding input_query, steps, and the final answer. The printout of the first\\nquery with llama3 is shown below (without the color here):\\n\\n    \\n    \\n    2024-07-10 16:48:47 - [react.py:287:call] - input_query: What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2\\n    \\n    2024-07-10 16:48:48 - [react.py:266:_run_one_step] - Step 1:\\n    StepOutput(step=1, action=FunctionExpression(thought=\"Let\\'s break down the query into subqueries and start with the first one.\", action=\\'llm_tool(input=\"What is the capital of France?\")\\'), function=Function(thought=None, name=\\'llm_tool\\', args=[], kwargs={\\'input\\': \\'What is the capital of France?\\'}), observation=\\'The capital of France is Paris!\\')\\n    _______\\n    2024-07-10 16:48:49 - [react.py:266:_run_one_step] - Step 2:\\n    StepOutput(step=2, action=FunctionExpression(thought=\"Now, let\\'s move on to the second subquery.\", action=\\'multiply(a=465, b=321)\\'), function=Function(thought=None, name=\\'multiply\\', args=[], kwargs={\\'a\\': 465, \\'b\\': 321}), observation=149265)\\n    _______\\n    2024-07-10 16:48:49 - [react.py:266:_run_one_step] - Step 3:\\n    StepOutput(step=3, action=FunctionExpression(thought=\"Now, let\\'s add 95297 to the result.\", action=\\'add(a=149265, b=95297)\\'), function=Function(thought=None, name=\\'add\\', args=[], kwargs={\\'a\\': 149265, \\'b\\': 95297}), observation=244562)\\n    _______\\n    2024-07-10 16:48:50 - [react.py:266:_run_one_step] - Step 4:\\n    StepOutput(step=4, action=FunctionExpression(thought=\"Now, let\\'s divide the result by 13.2.\", action=\\'divide(a=244562, b=13.2)\\'), function=Function(thought=None, name=\\'divide\\', args=[], kwargs={\\'a\\': 244562, \\'b\\': 13.2}), observation=18527.424242424244)\\n    _______\\n    2024-07-10 16:48:50 - [react.py:266:_run_one_step] - Step 5:\\n    StepOutput(step=5, action=FunctionExpression(thought=\"Now, let\\'s combine the answers of both subqueries.\", action=\\'finish(answer=\"The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\")\\'), function=Function(thought=None, name=\\'finish\\', args=[], kwargs={\\'answer\\': \\'The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\'}), observation=\\'The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\')\\n    _______\\n    2024-07-10 16:48:50 - [react.py:301:call] - answer:\\n    The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\n\\nThe comparison between the agent and the vanilla LLM response is shown below:\\n\\n    \\n    \\n    Answer with agent: The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244.\\n    Answer without agent: GeneratorOutput(data=\"I\\'d be happy to help you with that!\\\\n\\\\nThe capital of France is Paris.\\\\n\\\\nNow, let\\'s tackle the math problem:\\\\n\\\\n1. 465 √ó 321 = 149,485\\\\n2. Add 95,297 to that result: 149,485 + 95,297 = 244,782\\\\n3. Divide the result by 13.2: 244,782 √∑ 13.2 = 18,544.09\\\\n\\\\nSo, the answer is 18,544.09!\", error=None, usage=None, raw_response=\"I\\'d be happy to help you with that!\\\\n\\\\nThe capital of France is Paris.\\\\n\\\\nNow, let\\'s tackle the math problem:\\\\n\\\\n1. 465 √ó 321 = 149,485\\\\n2. Add 95,297 to that result: 149,485 + 95,297 = 244,782\\\\n3. Divide the result by 13.2: 244,782 √∑ 13.2 = 18,544.09\\\\n\\\\nSo, the answer is 18,544.09!\", metadata=None)\\n\\nThe ReAct agent is particularly helpful for answering queries that require\\ncapabilities like computation or more complicated reasoning and planning.\\nHowever, using it on general queries might be an overkill, as it might take\\nmore steps than necessary to answer the query.\\n\\n### 5\\\\. [Optional] Customization\\n\\nPlease refer to our tutorial for how to customize ReAct to your use case.\\n\\n* * *\\n\\n## References\\n\\n[1] A survey on large language model based autonomous agents: Paitesanshi/LLM-\\nAgent-Survey\\n\\n[2]**** ReAct: https://arxiv.org/abs/2210.03629\\n\\n[3] Tool Tutorial: https://lightrag.sylph.ai/tutorials/tool_helper.html  \\n\\n## API References\\n\\n  * components.agent.react.ReActAgent\\n\\n  * core.types.StepOutput\\n\\n  * components.agent.react.DEFAULT_REACT_AGENT_SYSTEM_PROMPT\\n\\n14\\n\\nShare this post\\n\\n#### LLM Agents Demystified\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n| A guest post by| LiAuthor of AdalFlow, Founder at SylphAI, ex AI researcher\\nat MetaAI. Github: liyin2015| Subscribe to Li  \\n---|---  \\n  \\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/llm-agents-demystified?r=1ttoeh'), ArticleDocument(id=UUID('12ad5863-ba57-4f5c-9ab7-4600c7edbf5c'), content={'Title': 'Scalable RAG pipeline using 74.3% less code', 'Subtitle': 'Tutorial on building a scalable & modular advanced RAG feature pipeline to chunk, embed and ingest multiple data categories to a vector DB using Superlinked', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Scalable RAG ingestion pipeline using 74.3% less code\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Scalable RAG ingestion pipeline using 74.3% less code\\n\\n### End-to-end implementation for an advanced RAG feature pipeline\\n\\nPaul Iusztin\\n\\nJul 20, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Scalable RAG ingestion pipeline using 74.3% less code\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _‚Üí the 1st lesson of the Superlinked bonus series from**the LLM Twin** free\\ncourse_\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> _More**details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà_\\n\\n## Latest lessons of the LLM Twin course\\n\\n**Lesson 8:** Best practices when evaluating fine-tuned LLM models\\n\\n‚Üí Quantitative/Qualitative Evaluation Metrics, Human-in-the-Loop, LLM-Eval\\n\\n**Lesson 9:** Architect scalable and cost-effective LLM & RAG inference\\npipelines\\n\\n‚ÜíMonolithic vs. microservice, Qwak Deployment, RAG Pipeline Walkthrough\\n\\n**Lesson 10:** How to evaluate your RAG using RAGAs Framework\\n\\n‚Üí RAG evaluation best practic, RAGAs framework\\n\\n* * *\\n\\n## **Lesson 11: Build a scalable RAG ingestion pipeline using 74.3% less\\ncode**\\n\\n**Lessons 11** and **12** are part of a **bonus serie** s in which we will\\ntake the advanced RAG system from the **LLM Twin course** (written in\\nLangChain) and refactor it using Superlinked, a framework specialized in\\nvector computing for information retrieval.\\n\\nIn **Lesson 11** **(this article)** , we will learn to build a highly\\nscalable, real-time RAG feature pipeline that ingests multi-data categories\\ninto a Redis vector database.\\n\\nMore concretely we will take the ingestion pipeline implemented in Lesson 4\\nand swap the chunking, embedding, and vector DB logic with Superlinked.\\n\\n_You don‚Äôt have to readLesson 4 to read this article. We will give enough\\ncontext to make sense of it._\\n\\nIn the **12th lesson** , we will use Superlinked to implement a multi-index\\nquery strategy and further optimize the advanced RAG retrieval module\\n(initially built in Lesson 5).\\n\\n> _The value of this article lies in understanding how easy it is to build\\n> complex advanced RAG systems usingSuperlinked._\\n>\\n> _**Using Superlinked** , we **reduced** the number of RAG-related **lines of\\n> code** by **74.3%**. Powerful, right?_\\n\\nBy the **end of this article** , **you will learn** to build a production-\\nready feature pipeline built in Superlinked that:\\n\\n  * uses Bytewax as a stream engine to process data in real-time;\\n\\n  * ingests multiple data categories from a RabbitMQ queue;\\n\\n  * validates the data with Pydantic;\\n\\n  * chunks, and embeds data using Superlinked for doing RAG;\\n\\n  * loads the embedded vectors along their metadata to a Redis vector DB;\\n\\nUltimately, on the infrastructure side, we will show you how to deploy a\\nSuperlinked vector compute server.\\n\\n### **Quick intro in feature pipelines**\\n\\nThe **feature pipeline** is the **first** **pipeline** presented in the\\n**FTI** **pipeline architecture** : feature, training and inference pipelines.\\n\\nA **feature pipeline** takes raw data as input, processes it into features,\\nand stores it in a feature store, from which the training & inference\\npipelines will use it.\\n\\nThe component is completely isolated from the training and inference code. All\\nthe communication is done through the feature store.\\n\\n> _To avoid repeating myself, if you are**unfamiliar** with the **FTI**\\n> **pipeline architecture** , check out Lesson 1 for a refresher._\\n\\n* * *\\n\\n## **Table of Contents**\\n\\n  1. What is Superlinked?\\n\\n  2. The old architecture of the RAG feature pipeline\\n\\n  3. The new Superlinked architecture of the RAG feature pipeline\\n\\n  4. Understanding the streaming flow for real-time processing\\n\\n  5. Loading data to Superlinked\\n\\n  6. Exploring the RAG Superlinked server\\n\\n  7. Using Redis as a vector DB\\n\\n>  _üîó**Check out** the code on GitHub [1] and support us with a _\\n\\n* * *\\n\\n## **1\\\\. What is Superlinked?**\\n\\n_Superlinked is a computing framework for turning complex data into vectors._\\n\\nIt lets you quickly build multimodal vectors and define weights at query time,\\nso you don‚Äôt need a custom reranking algorithm to optimize results.\\n\\nIt‚Äôs focused on turning complex data into vector embeddings within your RAG,\\nSearch, RecSys and Analytics stack.\\n\\nI love how Daniel Svonava, the CEO of Superlinked, described the value of\\nvector compute and implicitly Superlinked:\\n\\n> _Daniel Svonava, CEO at Superlinked:_\\n>\\n> _‚ÄúVectors power most of what you already do online ‚Äî hailing a cab, finding\\n> a funny video, getting a date, scrolling through a feed or paying with a\\n> tap. And yet, building production systems powered by vectors is still too\\n> hard! Our goal is to help enterprises put vectors at the center of their\\n> data & compute infrastructure, to build smarter and more reliable\\n> software.‚Äù_\\n\\nTo conclude, Superlinked is a framework that puts the vectors in the center of\\ntheir universe and allows you to:\\n\\n  * chunk and embed embeddings;\\n\\n  * store multi-index vectors in a vector DB;\\n\\n  * do complex vector search queries on top of your data.\\n\\nScreenshot from Superlinked‚Äôs landing page\\n\\n* * *\\n\\n## **2\\\\. The old architecture of the RAG feature pipeline**\\n\\nHere is a quick recap of the critical aspects of the architecture of the RAG\\nfeature pipeline presented in the 4th lesson of the LLM Twin course.\\n\\n_We are working with**3 different data categories** :_\\n\\n  * posts (e.g., LinkedIn, Twitter)\\n\\n  * articles (e.g., Medium, Substack, or any other blog)\\n\\n  * repositories (e.g., GitHub, GitLab)\\n\\nEvery data category has to be preprocessed differently. For example, you want\\nto chunk the posts into smaller documents while keeping the articles in bigger\\nones.\\n\\n_The**solution** is based on **CDC** , a **queue,** a **streaming engine,**\\nand a **vector DB:**_\\n\\n-> The raw data is collected from multiple social platforms and is stored in MongoDB. (Lesson 2)\\n\\n‚Üí CDC adds any change made to the MongoDB to a RabbitMQ queue (Lesson 3).\\n\\n‚Üí the RabbitMQ queue stores all the events until they are processed.\\n\\n‚Üí The Bytewax streaming engine reads the messages from the RabbitMQ queue and\\ncleans, chunks, and embeds them.\\n\\n‚Üí The processed data is uploaded to a Qdrant vector DB.\\n\\nThe old feature/streaming pipeline architecture that was presented in Lesson\\n4.\\n\\n### **Why is this design robust?**\\n\\nHere are 4 core reasons:\\n\\n  1. The **data** is **processed** in **real-time**.\\n\\n  2. **Out-of-the-box recovery system:** If the streaming pipeline fails to process a message, it will be added back to the queue\\n\\n  3. **Lightweight:** No need for any diffs between databases or batching too many records\\n\\n  4. **No I/O bottlenecks** on the source database\\n\\n### **What is the issue with this design?**\\n\\nIn this architecture, we had to write custom logic to chunk, embed, and load\\nthe data to Qdrant.\\n\\nThe issue with this approach is that we had to leverage various libraries,\\nsuch as LangChain and unstructured, to get the job done.\\n\\nAlso, because we have 3 data categories, we had to write a dispatcher layer\\nthat calls the right function depending on its category, which resulted in\\ntons of boilerplate code.\\n\\nUltimately, as the chunking and embedding logic is implemented directly in the\\nstreaming pipeline, it is harder to scale horizontally. The embedding\\nalgorithm needs powerful GPU machines, while the rest of the operations\\nrequire a strong CPU.\\n\\nThis results in:\\n\\n  * more time spent on development;\\n\\n  * more code to maintain;\\n\\n  * the code can quickly become less readable;\\n\\n  * less freedom to scale.\\n\\nSuperlinked can speed up this process by providing a very intuitive and\\npowerful Python API that can speed up the development of our ingestion and\\nretrieval logic.\\n\\nThus, let‚Äôs see how to redesign the architecture using Superlinked ‚Üì\\n\\n## **3\\\\. The new Superlinked architecture of the RAG feature pipeline**\\n\\nThe core idea of the architecture will be the same. We still want to:\\n\\n  * use a Bytewax streaming engine for real-time processing;\\n\\n  * read new events from RabbitMQ;\\n\\n  * clean, chunk, and embed the new incoming raw data;\\n\\n  * load the processed data to a vector DB.\\n\\n**The question is** , how will we do this with Superlinked?\\n\\nAs you can see in the image below, Superlinked will replace the logic for the\\nfollowing operations:\\n\\n  * chunking;\\n\\n  * embedding;\\n\\n  * vector storage;\\n\\n  * queries.\\n\\nAlso, we have to swap Qdrant with a Redis vector DB because Superlinked didn‚Äôt\\nsupport Qdrant when I wrote this article. But they plan to add it in future\\nmonths (along with many other vector DBs).\\n\\nWhat will remain unchanged are the following:\\n\\n  * the Bytewax streaming layer;\\n\\n  * the RabbitMQ queue ingestion component;\\n\\n  * the cleaning logic.\\n\\n> _By seeing**what we must change** to the architecture to integrate\\n> Superlinked, we can **see** the **framework‚Äôs core features**._\\n\\nThe components that can be refactored into the Superlinked framework.\\n\\nNow, let‚Äôs take a deeper look at the new architecture.\\n\\nAll the Superlinked logic will sit on its own server, completely decoupling\\nthe vector compute component from the rest of the feature pipeline.\\n\\nWe can quickly scale the streaming pipeline or the Superlinked server\\nhorizontally based on our needs. Also, this makes it easier to run the\\nembedding models (from Superlinked) on a machine with a powerful GPU while\\nkeeping the streaming pipeline on a machine optimized for network I/O\\noperations.\\n\\nAll the communication to Superlinked (ingesting or query data) will be done\\nthrough a REST API, automatically generated based on the schemas and queries\\nyou define in your Superlinked application.\\n\\nThe **Bytewax streaming pipeline** will perform the following operations:\\n\\n  * will concurrently read messages from RabbitMQ;\\n\\n  * clean each message based on it‚Äôs data category;\\n\\n  * send the cleaned document to the Superlinked server through an HTTP request.\\n\\n**On the** **Superlinked server side** , we have defined an ingestion endpoint\\nfor each data category (article, post or code). Each endpoint will know how to\\nchunk embed and store every data point based on its category.\\n\\nAlso, we have a query endpoint (automatically generated) for each data\\ncategory that will take care of embedding the query and perform a vector\\nsemantic search operation to retrieve similar results.\\n\\nThe RAG feature pipeline architecture after refactoring.\\n\\nNow, let‚Äôs finally jump into the code ‚Üì\\n\\n* * *\\n\\n## **4\\\\. Understanding the streaming flow for real-time processing**\\n\\nThe **Bytewax flow** is the **central point** of the **streaming pipeline**.\\nIt defines all the required steps, following the next simplified pattern:\\n_‚Äúinput - > processing -> output‚Äù._\\n\\nHere is the Bytewax flow and its core steps ‚Üì\\n\\n    \\n    \\n    flow = Dataflow(\"Streaming RAG feature pipeline\")\\n    stream = op.input(\"input\", flow, RabbitMQSource())\\n    stream = op.map(\"raw\", stream, RawDispatcher.handle_mq_message)\\n    stream = op.map(\"clean\", stream, CleaningDispatcher.dispatch_cleaner)\\n    op.output(\\n        \"superlinked_output\",\\n        stream,\\n        SuperlinkedOutputSink(client=SuperlinkedClient()),\\n    )\\n\\n## **5\\\\. Loading data to Superlinked**\\n\\nBefore we explore the Superlinked application, let‚Äôs review our Bytewax\\n_SuperlinkedOutputSink()_ and _SuperlinkedClient() _classes.\\n\\nThe purpose of the _SuperlinkedOutputSink()_ class is to instantiate a new\\n_SuperlinkedSinkPartition()_ instance for each worker within the Bytewax\\ncluster. Thus, we can optimize the system for I/O operations by scaling our\\noutput workers horizontally.\\n\\n    \\n    \\n    class SuperlinkedOutputSink(DynamicSink):\\n        def __init__(self, client: SuperlinkedClient) -> None:\\n            self._client = client\\n    \\n        def build(self, worker_index: int, worker_count: int) -> StatelessSinkPartition:\\n            return SuperlinkedSinkPartition(client=self._client)\\n\\nThe _SuperlinkedSinkPartition()_ class inherits the _StatelessSinkPartition\\nBytewax base class_ used to create custom stateless partitions.\\n\\nThis class takes as input batches of items and sends them to Superlinked\\nthrough the _SuperlinkedClient()_.\\n\\n    \\n    \\n    class SuperlinkedSinkPartition(StatelessSinkPartition):\\n        def __init__(self, client: SuperlinkedClient):\\n            self._client = client\\n    \\n        def write_batch(self, items: list[Document]) -> None:\\n            for item in tqdm(items, desc=\"Sending items to Superlinked...\"):\\n                match item.type:\\n                    case \"repositories\":\\n                        self._client.ingest_repository(item)\\n                    case \"posts\":\\n                        self._client.ingest_post(item)\\n                    case \"articles\":\\n                        self._client.ingest_article(item)\\n                    case _:\\n                        logger.error(f\"Unknown item type: {item.type}\")\\n\\nThe _SuperlinkedClient() _is a basic wrapper that makes HTTP requests to the\\nSuperlinked server that contains all the RAG logic. We use _httpx_ to make __\\nPOST requests for ingesting or searching data.\\n\\n    \\n    \\n    class SuperlinkedClient:\\n        ...\\n    \\n        def ingest_repository(self, data: RepositoryDocument) -> None:\\n            self.__ingest(f\"{self.base_url}/api/v1/ingest/repository_schema\", data)\\n    \\n        def ingest_post(self, data: PostDocument) -> None:\\n            self.__ingest(f\"{self.base_url}/api/v1/ingest/post_schema\", data)\\n    \\n        def ingest_article(self, data: ArticleDocument) -> None:\\n            self.__ingest(f\"{self.base_url}/api/v1/ingest/article_schema\", data)\\n    \\n        def __ingest(self, url: str, data: T) -> None:\\n            ...\\n    \\n        def search_repository(\\n            self, search_query: str, platform: str, author_id: str, *, limit: int = 3\\n        ) -> list[RepositoryDocument]:\\n            return self.__search(\\n                f\"{self.base_url}/api/v1/search/repository_query\",\\n                RepositoryDocument,\\n                search_query,\\n                platform,\\n                author_id,\\n                limit=limit,\\n            )\\n    \\n        def search_post(\\n            self, search_query: str, platform: str, author_id: str, *, limit: int = 3\\n        ) -> list[PostDocument]:\\n            ... # URL: f\"{self.base_url}/api/v1/search/post_query\"\\n    \\n        def search_article(\\n            self, search_query: str, platform: str, author_id: str, *, limit: int = 3\\n        ) -> list[ArticleDocument]:\\n            ... # URL: f\"{self.base_url}/api/v1/search/article_query\"\\n    \\n        def __search(\\n            self, url: str, document_class: type[T], search_query: str, ...\\n        ) -> list[T]:\\n            ...\\n          \\n\\nThe Superlinked server URLs are automatically generated as follows:\\n\\n  * the ingestion URLs are generated based on the data schemas you defined (e.g., repository schema, post schema, etc.)\\n\\n  * the search URLs are created based on the Superlinked queries defined within the application\\n\\n## **6\\\\. Exploring the RAG Superlinked server**\\n\\nAs the RAG Superlinked server is a different component than the Bytewax one,\\nthe implementation sits under the server folder at _6-bonus-superlinked-\\nrag/server/src/app.py._\\n\\n_Here is a step-by-step implementation of the Superlinked application ‚Üì_\\n\\n### **Settings class**\\n\\nUse Pydantic settings to define a global configuration class.\\n\\n    \\n    \\n    class Settings(BaseSettings):\\n        EMBEDDING_MODEL_ID: str = \"sentence-transformers/all-mpnet-base-v2\"\\n    \\n        REDIS_HOSTNAME: str = \"redis\"\\n        REDIS_PORT: int = 6379\\n    \\n    \\n    settings = Settings()\\n\\n### **Schemas**\\n\\nSuperlinked requires you to define your data structure through a set of\\nschemas, which are very similar to data classes or Pydantic models.\\n\\nSuperlinked will use these schemas as ORMs to save your data to a specified\\nvector DB.\\n\\nIt will also use them to define ingestion URLs automatically as POST HTTP\\nmethods that expect the request body to have the same signature as the schema.\\n\\nSimple and effective. Cool, right?\\n\\n    \\n    \\n    @schema\\n    class PostSchema:\\n        id: IdField\\n        platform: String\\n        content: String\\n        author_id: String\\n        type: String\\n    \\n    \\n    @schema\\n    class ArticleSchema:\\n        id: IdField\\n        platform: String\\n        link: String\\n        content: String\\n        author_id: String\\n        type: String\\n    \\n    \\n    @schema\\n    class RepositorySchema:\\n        id: IdField\\n        platform: String\\n        name: String\\n        link: String\\n        content: String\\n        author_id: String\\n        type: String\\n    \\n    \\n    post = PostSchema()\\n    article = ArticleSchema()\\n    repository = RepositorySchema()\\n\\n### **Spaces**\\n\\nThe spaces are where you define your chunking and embedding logic.\\n\\nA space is scoped at the field of a schema. Thus, if you want to embed\\nmultiple attributes of a single schema, you must define multiple spaces and\\ncombine them later into a multi-index.\\n\\nLet‚Äôs take the spaces for the article category as an example:\\n\\n    \\n    \\n    articles_space_content = TextSimilaritySpace(\\n        text=chunk(article.content, chunk_size=500, chunk_overlap=50),\\n        model=settings.EMBEDDING_MODEL_ID,\\n    )\\n    articles_space_plaform = CategoricalSimilaritySpace(\\n        category_input=article.platform,\\n        categories=[\"medium\", \"superlinked\"],\\n        negative_filter=-5.0,\\n    )\\n\\nChunking is done simply by calling the _chunk()_ function on a given schema\\nfield and specifying standard parameters such as ‚Äú _chunk_size‚Äù_ and ‚Äú\\n_chunk_overlap‚Äù_.\\n\\nThe embedding is done through the _TextSimilaritySpace()_ and\\n_CategoricalSimilaritySpace()_ classes.\\n\\nAs the name suggests, the _**TextSimilaritySpace()** _embeds text data using\\nthe model specified within the _‚Äúmodel‚Äù_ parameter. It supports any\\nHuggingFace model. We are using _‚Äúsentence-transformers/all-mpnet-base-v2‚Äù._\\n\\nThe _**CategoricalSimilaritySpace()**_ class uses an _n-hot encoded vector_\\nwith the option to apply a negative filter for unmatched categories, enhancing\\nthe distinction between matching and non-matching category items.\\n\\nYou must also specify all the available categories through the ‚Äú _categories_\\n‚Äù parameter to encode them in n-hot.\\n\\n### **Indexes**\\n\\nThe indexes define how a collection can be queried. They take one or multiple\\nspaces from the same schema.\\n\\nHere is what the article index looks like:\\n\\n    \\n    \\n    article_index = Index(\\n        [articles_space_content, articles_space_plaform],\\n        fields=[article.author_id],\\n    )\\n\\nAs you can see, the vector index combines the article‚Äôs content and the posted\\nplatform. When the article collection is queried, both embeddings will be\\nconsidered.\\n\\nAlso, we index the ‚Äúauthor_id‚Äù field to filter articles written by a specific\\nauthor. It is nothing fancy‚Äîit is just a classic filter. However, indexing the\\nfields used in filters is often good practice.\\n\\n### **Queries**\\n\\nWe will quickly introduce what a query looks like. But in the 14th lesson, we\\nwill insist on the advanced retrieval part, hence on queries.\\n\\nHere is what the article query looks like:\\n\\n    \\n    \\n    article_query = (\\n        Query(\\n            article_index,\\n            weights={\\n                articles_space_content: Param(\"content_weight\"),\\n                articles_space_plaform: Param(\"platform_weight\"),\\n            },\\n        )\\n        .find(article)\\n        .similar(articles_space_content.text, Param(\"search_query\"))\\n        .similar(articles_space_plaform.category, Param(\"platform\"))\\n        .filter(article.author_id == Param(\"author_id\"))\\n        .limit(Param(\"limit\"))\\n    )\\n\\n‚Ä¶and here is what it does:\\n\\n  * it queries the _article_index_ using a weighted multi-index between the content and platform vectors (e.g., `0.9 * content_embedding + 0.1 * platform_embedding` );\\n\\n  * the search text used to compute query content embedding is specified through the ‚Äúsearch_query‚Äù parameter and similar for the platform embedding through the ‚Äúplatform‚Äù parameter;\\n\\n  * we filter the results based on the ‚Äúauthor_id‚Äù;\\n\\n  * take only the top results using the ‚Äúlimit‚Äù parameter.\\n\\nThese parameters are automatically exposed on the REST API endpoint, as seen\\nin the _SuperlinkedClient()_ class.\\n\\n### **Sources**\\n\\nThe sources wrap the schemas and allow you to save that schema in the\\ndatabase.\\n\\nIn reality, the source maps the schema to an ORM and automatically generates\\nREST API endpoints to ingest data points.\\n\\n    \\n    \\n    article_source = RestSource(article)\\n\\n### **Executor**\\n\\nThe last step is to define the executor that wraps all the sources, indices,\\nqueries and vector DB into a single entity:\\n\\n    \\n    \\n    executor = RestExecutor(\\n        sources=[article_source, repository_source, post_source],\\n        indices=[article_index, repository_index, post_index],\\n        queries=[\\n            RestQuery(RestDescriptor(\"article_query\"), article_query),\\n            RestQuery(RestDescriptor(\"repository_query\"), repository_query),\\n            RestQuery(RestDescriptor(\"post_query\"), post_query),\\n        ],\\n        vector_database=InMemoryVectorDatabase(),\\n    )\\n    \\n\\nNow, the last step is to register the executor to the Superlinked engine:\\n\\n    \\n    \\n    SuperlinkedRegistry.register(executor)\\n\\n‚Ä¶and that‚Äôs it!\\n\\nJoking‚Ä¶ there is something more. We have to use a Redis database instead of\\nthe in-memory one.\\n\\n## **7\\\\. Using Redis as a vector DB**\\n\\nFirst, we have to spin up a Redis vector database that we can work with.\\n\\nWe used Docker and attached a Redis image as a service in a _docker-compose_\\nfile along with the Superlinked poller and executor (which comprise the\\nSuperlinked server):\\n\\n    \\n    \\n    version: \"3\"\\n    \\n    services:\\n      poller:\\n        ...\\n    \\n      executor:\\n        ...\\n    \\n      redis:\\n        image: redis/redis-stack:latest\\n        ports:\\n          - \"6379:6379\"\\n          - \"8001:8001\"\\n        volumes:\\n          - redis-data:/data\\n    \\n    volumes:\\n      redis-data:\\n\\nNow, Superlinked makes everything easy. The last step is to define a\\nRedisVectorDatabase connector provided by Superlinked:\\n\\n    \\n    \\n    vector_database = RedisVectorDatabase(\\n        settings.REDIS_HOSTNAME,\\n        settings.REDIS_PORT\\n    )\\n\\n‚Ä¶and swap it in the executor with the _InMemoryVectorDatabase()_ one:\\n\\n    \\n    \\n    executor = RestExecutor(\\n        ...\\n        vector_database=vector_database,\\n    )\\n\\nNow we are done!\\n\\n* * *\\n\\n## **Conclusion**\\n\\n _Congratulations! You learned to write advanced RAG systems\\nusingSuperlinked._\\n\\nMore concretely, in **Lesson 11** , you learned:\\n\\n  * what is Superlinked;\\n\\n  * how to design a streaming pipeline using Bytewax;\\n\\n  * how to design a RAG server using Superlinked;\\n\\n  * how to take a standard RAG feature pipeline and refactor it using Superlinked;\\n\\n  * how to split the feature pipeline into 2 services, one that reads in real-time messages from RabbitMQ and one that chunks, embeds, and stores the data to a vector DB;\\n\\n  * how to use a Redis vector DB.\\n\\n**Lesson 12** will teach you how to implement multi-index queries to optimize\\nthe RAG retrieval layer further.\\n\\n> _üîó**Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è_\\n\\n* * *\\n\\n### Next Steps\\n\\n#### Step 1\\n\\nThis is just the **short version** of **Lesson 11** on **building scalable RAG\\ningestion pipelines.**\\n\\n‚Üí For‚Ä¶\\n\\n  * The full implementation.\\n\\n  * Full deep dive into the code.\\n\\n  * More on the RAG, Bytewax and Superlinked.\\n\\n**Check out** the **full version** of **Lesson 11** on our **Medium\\npublication**. It‚Äôs still FREE:\\n\\nLesson 11 on Medium\\n\\n#### Step 2\\n\\n‚Üí **Consider checking out theLLM Twin GitHub repository and try it yourself\\nü´µ**\\n\\n _Nothing compares with getting your hands dirty and doing it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### Scalable RAG ingestion pipeline using 74.3% less code\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/scalable-rag-ingestion-pipeline-using?r=1ttoeh'), ArticleDocument(id=UUID('0eae1447-70c8-40b2-a5c4-96f6de69f04b'), content={'Title': 'The ultimate MLOps tool - by Paul Iusztin', 'Subtitle': '6 steps to build your AWS infrastructure that will work for 90% of your projects. How to build a real-time news search engine', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The ultimate MLOps tool\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The ultimate MLOps tool\\n\\n### 6 steps to build your AWS infrastructure that will work for 90% of your\\nprojects. How to build a real-time news search engine\\n\\nPaul Iusztin\\n\\nJul 13, 2024\\n\\n18\\n\\nShare this post\\n\\n#### The ultimate MLOps tool\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nBased on your feedback from last week‚Äôs poll, we will post exclusively on\\nSaturdays starting now.\\n\\nEnjoy today‚Äôs article ü§ó\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * The ultimate MLOps tool\\n\\n  * 6 steps to build your AWS infrastructure that will work for 90% of your projects\\n\\n  * How to build a real-time news search engine\\n\\n* * *\\n\\n### The ultimate MLOps tool\\n\\nI tested this ùóºùóøùó∞ùóµùó≤ùòÄùòÅùóøùóÆùòÅùóºùóø ùòÅùóºùóºùóπ for my ùó†ùóü ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ùòÄ and ùóπùóºùòÉùó≤ùó± ùó∂ùòÅ! It is the\\nùòÇùóπùòÅùó∂ùó∫ùóÆùòÅùó≤ ùó†ùóüùó¢ùóΩùòÄ ùòÅùóºùóºùóπ to glue everything together for ùóøùó≤ùóΩùóøùóºùó±ùòÇùó∞ùó∂ùóØùó∂ùóπùó∂ùòÅùòÜ and\\nùó∞ùóºùóªùòÅùó∂ùóªùòÇùóºùòÇùòÄ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥.  \\n  \\nIn the past months, I have tested most of the top orchestrator tools out\\nthere: Airflow, Prefect, Argo, Kubeflow, Metaflow...  \\n  \\nYou name it!  \\n  \\nùóïùòÇùòÅ ùóºùóªùó≤ ùòÄùòÅùóºùóºùó± ùóºùòÇùòÅ ùòÅùóº ùó∫ùó≤.  \\n  \\nI am talking about ZenML!  \\n  \\nùó™ùóµùòÜ?  \\n  \\nThey realized they don\\'t have to compete with tools such as Airflow or AWS in\\nthe orchestrators and MLOps race, but join them!  \\n  \\nInstead of being yet another orchestrator tool, they have built an ùóÆùóØùòÄùòÅùóøùóÆùó∞ùòÅ\\nùóπùóÆùòÜùó≤ùóø ùóºùóª ùòÅùóºùóΩ ùóºùó≥ ùòÅùóµùó≤ ùó†ùóüùó¢ùóΩùòÄ ùó≤ùó∞ùóºùòÄùòÜùòÄùòÅùó≤ùó∫:  \\n  \\n\\\\- experiment trackers & model registries (e.g., Weights & Biases, Comet)  \\n\\\\- orchestrators (e.g., Apache Airflow, Kubeflow)  \\n\\\\- container registries for your Docker images  \\n\\\\- model deployers (Hugging Face , BentoML, Seldon)  \\n  \\nThey wrote a clever wrapper that integrated the whole MLOps ecosystem!  \\n  \\nùòàùò≠ùò¥ùò∞, ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò™ùòØùò® ùò™ùòµ ùò™ùòØùòµùò∞ ùò∫ùò∞ùò∂ùò≥ ùòóùò∫ùòµùò©ùò∞ùòØ ùò§ùò∞ùò•ùò¶ ùò™ùò¥ ùòØùò∞ùòµ ùò™ùòØùòµùò≥ùò∂ùò¥ùò™ùò∑ùò¶.  \\n  \\nAs long your code is modular (which should be anyway), you have to annotate\\nyour DAG:  \\n\\\\- steps with \"Stephen S.\"  \\n\\\\- entry point with james wang  \\n  \\nùòàùò¥ ùò∫ùò∞ùò∂ ùò§ùò¢ùòØ ùò¥ùò¶ùò¶ ùò™ùòØ ùòµùò©ùò¶ ùò§ùò∞ùò•ùò¶ ùò¥ùòØùò™ùò±ùò±ùò¶ùòµùò¥ ùò£ùò¶ùò≠ùò∞ùò∏ ‚Üì  \\n\\nZenML Pipelines\\n\\n.\\n\\nZenML Steps\\n\\n  \\nùóßùóµùó≤ùòÜ ùóÆùóπùòÄùóº ùóΩùóøùóºùòÉùó∂ùó±ùó≤ ùòÅùóµùó≤ ùó∞ùóºùóªùó∞ùó≤ùóΩùòÅ ùóºùó≥ ùóÆ \"ùòÄùòÅùóÆùó∞ùó∏\".  \\n  \\nThis allows you to configure multiple tools and infrastructure sets your\\npipeline can run on.  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶:  \\n  \\n\\\\- ùò¢ ùò≠ùò∞ùò§ùò¢ùò≠ ùò¥ùòµùò¢ùò§ùò¨: that uses a local orchestrator, artifact store, and compute\\nfor quick testing (so you don\\'t have to set up other dependencies)  \\n  \\n\\\\- ùò¢ùòØ ùòàùòûùòö ùò¥ùòµùò¢ùò§ùò¨: that uses AWS SageMaker Orchestrator, Comet, and Seldon\\n\\nZenML Stacks\\n\\n  \\nAs I am still learning ZenML, this was just an intro post to share my\\nexcitement.  \\n  \\nI plan to integrate it into Decoding ML\\'s LLM twin open-source project and\\nshare the process with you!  \\n  \\n.  \\n  \\nùó†ùó≤ùóÆùóªùòÑùóµùó∂ùóπùó≤, ùó∞ùóºùóªùòÄùó∂ùó±ùó≤ùóø ùó∞ùóµùó≤ùó∞ùó∏ùó∂ùóªùó¥ ùóºùòÇùòÅ ùòÅùóµùó≤ùó∂ùóø ùòÄùòÅùóÆùóøùòÅùó≤ùóø ùó¥ùòÇùó∂ùó±ùó≤ ‚Üì  \\n  \\nüîó ùòöùòµùò¢ùò≥ùòµùò¶ùò• ùò®ùò∂ùò™ùò•ùò¶: https://lnkd.in/dPzXHvjH\\n\\n* * *\\n\\n### 6 steps to build your AWS infrastructure that will work for 90% of your\\nprojects\\n\\nùü≤ ùòÄùòÅùó≤ùóΩùòÄ to ùóØùòÇùó∂ùóπùó± your ùóîùó™ùó¶ ùó∂ùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ (using ùóúùóÆùóñ) and a ùóñùóú/ùóñùóó ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ that\\nwill ùòÑùóºùóøùó∏ for ùüµùü¨% of your ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅùòÄ ‚Üì  \\n  \\nWe will use the data collection pipeline from our free digital twin course as\\nan example, but it can easily be extrapolated to most of your projects.  \\n  \\nùòçùò™ùò≥ùò¥ùòµ, ùò≠ùò¶ùòµ\\'ùò¥ ùò¥ùò¶ùò¶ ùò∏ùò©ùò¢ùòµ ùò™ùò¥ ùò™ùòØ ùò∞ùò∂ùò≥ ùòµùò∞ùò∞ùò≠ùò£ùò¶ùò≠ùòµ:  \\n  \\n\\\\- Docker  \\n\\\\- AWS ECR  \\n\\\\- AWS Lambda  \\n\\\\- MongoDB  \\n\\\\- Pulumni  \\n\\\\- GitHub Actions  \\n  \\nùòöùò¶ùò§ùò∞ùòØùò•ùò≠ùò∫, ùò≠ùò¶ùòµ\\'ùò¥ ùò≤ùò∂ùò™ùò§ùò¨ùò≠ùò∫ ùò∂ùòØùò•ùò¶ùò≥ùò¥ùòµùò¢ùòØùò• ùò∏ùò©ùò¢ùòµ ùòµùò©ùò¶ ùò•ùò¢ùòµùò¢ ùò§ùò∞ùò≠ùò≠ùò¶ùò§ùòµùò™ùò∞ùòØ ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùò™ùò¥ ùò•ùò∞ùò™ùòØùò®  \\n  \\nIt automates your digital data collection from LinkedIn, Medium, Substack, and\\nGitHub. The normalized data will be loaded into MongoDB.  \\n  \\nùòïùò∞ùò∏, ùò≠ùò¶ùòµ\\'ùò¥ ùò∂ùòØùò•ùò¶ùò≥ùò¥ùòµùò¢ùòØùò• ùò©ùò∞ùò∏ ùòµùò©ùò¶ ùòàùòûùòö ùò™ùòØùòßùò≥ùò¢ùò¥ùòµùò≥ùò∂ùò§ùòµùò∂ùò≥ùò¶ ùò¢ùòØùò• ùòäùòê/ùòäùòã ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùò∏ùò∞ùò≥ùò¨ùò¥ ‚Üì  \\n  \\n1\\\\. We wrap the application\\'s entry point with a `ùò©ùò¢ùòØùò•ùò≠ùò¶(ùò¶ùò∑ùò¶ùòØùòµ, ùò§ùò∞ùòØùòµùò¶ùòπùòµ:\\nùòìùò¢ùòÆùò£ùò•ùò¢ùòäùò∞ùòØùòµùò¶ùòπùòµ)` function. The AWS Lambda serverless computing service will\\ndefault to the `ùò©ùò¢ùòØùò•ùò≠ùò¶()` function.  \\n  \\n2\\\\. Build a Docker image of your application inheriting the\\n`ùò±ùò∂ùò£ùò≠ùò™ùò§.ùò¶ùò§ùò≥.ùò¢ùò∏ùò¥/ùò≠ùò¢ùòÆùò£ùò•ùò¢/ùò±ùò∫ùòµùò©ùò∞ùòØ:3.11` base Docker image  \\n  \\n‚Üí Now, you can quickly check your AWS Lambda function locally by making HTTP\\nrequests to your Docker container.  \\n  \\n3\\\\. Use Pulumni IaC to create your AWS infrastructure programmatically:  \\n  \\n\\\\- an ECR as your Docker registry  \\n\\\\- an AWS Lambda service  \\n\\\\- a MongoDB cluster  \\n\\\\- the VPC for the whole infrastructure  \\n  \\n4\\\\. Now that we have our Docker image and infrastructure, we can build our\\nCI/CD pipeline using GitHub Actions. The first step is to build the Docker\\nimage inside the CI and push it to ECR when a new PR is merged into the main\\nbranch.  \\n  \\n5\\\\. On the CD part, we will take the fresh Docker image from ECR and deploy it\\nto AWS Lambda.  \\n  \\n6\\\\. Repeat the same logic with the Pulumni code ‚Üí Add a CD GitHub Action that\\nupdates the infrastructure whenever the IaC changes.  \\n  \\nWith ùòÅùóµùó∂ùòÄ ùó≥ùóπùóºùòÑ, you will do fine for ùüµùü¨% of your ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅùòÄ üî•  \\n  \\n.  \\n  \\nùòõùò∞ ùò¥ùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶, ùòµùò©ùò¶ ùòäùòê/ùòäùòã ùò∏ùò™ùò≠ùò≠ ùò≠ùò∞ùò∞ùò¨ ùò≠ùò™ùò¨ùò¶ ùòµùò©ùò™ùò¥:  \\n  \\nfeature PR -> merged to main -> build Docker image -> push to ECR -> deploy to\\nAWS Lambda\\n\\nLLM Twin AWS architecture\\n\\n  \\n  \\nùó™ùóÆùóªùòÅ ùòÅùóº ùóøùòÇùóª ùòÅùóµùó≤ ùó∞ùóºùó±ùó≤ ùòÜùóºùòÇùóøùòÄùó≤ùóπùó≥?  \\n  \\nConsider checking out ùóüùó≤ùòÄùòÄùóºùóª ùüÆ from the FREE ùóüùóüùó† ùóßùòÑùó∂ùóª ùó∞ùóºùòÇùóøùòÄùó≤ hosted by:\\n\\nüîó _The Importance of Data Pipelines in the Era of Generative AI_\\n\\n* * *\\n\\n### How to build a real-time news search engine\\n\\nDecoding ML ùóøùó≤ùóπùó≤ùóÆùòÄùó≤ùó± an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ & ùó∞ùóºùó±ùó≤ on building a ùó•ùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ ùó°ùó≤ùòÑùòÄ ùó¶ùó≤ùóÆùóøùó∞ùóµ\\nùóòùóªùó¥ùó∂ùóªùó≤ using ùóûùóÆùó≥ùó∏ùóÆ, ùó©ùó≤ùó∞ùòÅùóºùóø ùóóùóïùòÄ and ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó≤ùóªùó¥ùó∂ùóªùó≤ùòÄ.  \\n  \\nùòåùò∑ùò¶ùò≥ùò∫ùòµùò©ùò™ùòØùò® ùò™ùòØ ùòóùò∫ùòµùò©ùò∞ùòØ!  \\n  \\nùóßùóµùó≤ ùó≤ùóªùó± ùó¥ùóºùóÆùóπ?  \\n  \\nLearn to build a production-ready semantic search engine for news that is\\nsynced in real-time with multiple news sources using:  \\n\\\\- a streaming engine  \\n\\\\- Kafka  \\n\\\\- a vector DB.  \\n  \\nùóßùóµùó≤ ùóΩùóøùóºùóØùóπùó≤ùó∫?  \\n  \\nAccording to a research study by earthweb.com, the daily influx of news\\narticles, both online and offline, is between 2 and 3 million.  \\n  \\nHow would you constantly sync these data sources with your vector DB to stay\\nin sync with the outside world?  \\n  \\nùóßùóµùó≤ ùòÄùóºùóπùòÇùòÅùó∂ùóºùóª!  \\n  \\n‚Üí Here is where the streaming pipeline kicks in.  \\n  \\nAs soon as a new data point is available, it is:  \\n\\\\- ingested  \\n\\\\- processed  \\n\\\\- loaded to a vector DB  \\n  \\n...in real-time by the streaming pipeline ‚Üê  \\n  \\n.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò™ùò¥ ùò∏ùò©ùò¢ùòµ ùò∫ùò∞ùò∂ ùò∏ùò™ùò≠ùò≠ ùò≠ùò¶ùò¢ùò≥ùòØ ùòßùò≥ùò∞ùòÆ ùòµùò©ùò¶ ùò¢ùò≥ùòµùò™ùò§ùò≠ùò¶ ‚Üì  \\n  \\n‚Üí Set up your own Upstash ùóûùóÆùó≥ùó∏ùóÆ & ùó©ùó≤ùó∞ùòÅùóºùóø ùóóùóï ùó∞ùóπùòÇùòÄùòÅùó≤ùóøùòÄ  \\n  \\n‚Üí ùó¶ùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ & ùòÉùóÆùóπùó∂ùó±ùóÆùòÅùó≤ your ùó±ùóÆùòÅùóÆ points using Pydantic  \\n  \\n‚Üí ùó¶ùó∂ùó∫ùòÇùóπùóÆùòÅùó≤ multiple ùóûùóÆùó≥ùó∏ùóÆ ùóñùóπùó∂ùó≤ùóªùòÅùòÄ using ùòõùò©ùò≥ùò¶ùò¢ùò•ùòóùò∞ùò∞ùò≠ùòåùòπùò¶ùò§ùò∂ùòµùò∞ùò≥ & ùòíùò¢ùòßùò¨ùò¢ùòóùò≥ùò∞ùò•ùò∂ùò§ùò¶ùò≥  \\n  \\n‚Üí ùó¶ùòÅùóøùó≤ùóÆùó∫ ùóΩùóøùóºùó∞ùó≤ùòÄùòÄùó∂ùóªùó¥ using Bytewax \\\\- learn to ùóØùòÇùó∂ùóπùó± ùóÆ ùóøùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ ùó•ùóîùóö ingestion\\nùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n‚Üí ùóïùóÆùòÅùó∞ùóµ-ùòÇùóΩùòÄùó≤ùóøùòÅùó∂ùóªùó¥ ùó≤ùó∫ùóØùó≤ùó±ùó±ùó∂ùóªùó¥ùòÄ + ùó∫ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ to Upstash Vector DB  \\n  \\n‚Üí Build a ùó§&ùóî ùó®I using Streamlit  \\n  \\n‚Üí ùó®ùóªùó∂ùòÅ ùóßùó≤ùòÄùòÅùó∂ùóªùó¥ - Yes, we even added unit testing!\\n\\n  \\nùóñùòÇùóøùó∂ùóºùòÇùòÄ ùòÅùóº ùóπùó≤ùòÉùó≤ùóπ ùòÇùóΩ ùòÜùóºùòÇùóø ùó£ùòÜùòÅùóµùóºùóª, ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ & ùó•ùóîùóö ùó¥ùóÆùó∫ùó≤ ü´µ  \\n  \\nThen, consider checking out ùòµùò©ùò¶ ùò¢ùò≥ùòµùò™ùò§ùò≠ùò¶ & ùò§ùò∞ùò•ùò¶. Everything is free.  \\n  \\n‚Üì‚Üì‚Üì\\n\\nüîó **[Article]** How to build a real-time News Search Engine using Vector DBs\\n\\nüîó ùóöùó∂ùòÅùóõùòÇùóØ ùó∞ùóºùó±ùó≤\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n18\\n\\nShare this post\\n\\n#### The ultimate MLOps tool\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-ultimate-mlops-tool?r=1ttoeh'), ArticleDocument(id=UUID('1436e3e5-eb7c-4632-a538-00fd69c01998'), content={'Title': 'The new king of Infrastructure as Code (IaC)', 'Subtitle': 'Monitoring your DL models while in production. How to build a scalable data collection pipeline', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The new king of Infrastructure as Code (IaC)\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The new king of Infrastructure as Code (IaC)\\n\\n### Monitoring your DL models while in production. How to build a scalable\\ndata collection pipeline\\n\\nPaul Iusztin\\n\\nJun 29, 2024\\n\\n11\\n\\nShare this post\\n\\n#### The new king of Infrastructure as Code (IaC)\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * The new king of Infrastructure as Code (IaC)\\n\\n  * How to build a scalable data collection pipeline\\n\\n  * Monitoring your DL models while in production\\n\\n* * *\\n\\n### The new king of Infrastructure as Code (IaC)\\n\\nThis is ùòÅùóµùó≤ ùóªùó≤ùòÑ ùó∏ùó∂ùóªùó¥ ùóºùó≥ ùóúùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ ùóÆùòÄ ùóñùóºùó±ùó≤ (ùóúùóÆùóñ). Here is ùòÑùóµùòÜ it is ùóØùó≤ùòÅùòÅùó≤ùóø\\nthan ùóßùó≤ùóøùóøùóÆùó≥ùóºùóøùó∫ or ùóñùóóùóû ‚Üì  \\n  \\n‚Üí I am talking about Pulumi ‚Üê  \\n  \\nLet's see what is made of  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùó£ùòÇùóπùòÇùó∫ùó∂ ùóÆùóªùó± ùóµùóºùòÑ ùó∂ùòÄ ùó∂ùòÅ ùó±ùó∂ùó≥ùó≥ùó≤ùóøùó≤ùóªùòÅ?  \\n  \\nUnlike other IaC tools that use YAML, JSON, or a Domain-Specific Language\\n(DSL), Pulumi lets you write code in languages like Python, TypeScript,\\nNode.js, etc.  \\n\\\\- This enables you to leverage existing programming knowledge and tooling for\\nIaC tasks.  \\n\\\\- Pulumi integrates with familiar testing libraries for unit and integration\\ntesting of your infrastructure code.  \\n\\\\- It integrates with most cloud providers (AWS, GCP, Azure, Oracle, etc.)  \\n  \\nùóïùó≤ùóªùó≤ùó≥ùó∂ùòÅùòÄ ùóºùó≥ ùòÇùòÄùó∂ùóªùó¥ ùó£ùòÇùóπùòÇùó∫ùó∂:  \\n  \\nùóôùóπùó≤ùòÖùó∂ùóØùó∂ùóπùó∂ùòÅùòÜ: Use your preferred programming language for IaC + it works for\\nmost clouds out there  \\nùóòùó≥ùó≥ùó∂ùó∞ùó∂ùó≤ùóªùó∞ùòÜ: Leverage existing programming skills and tooling.  \\nùóßùó≤ùòÄùòÅùóÆùóØùó∂ùóπùó∂ùòÅùòÜ: Write unit and integration tests for your infrastructure code.  \\nùóñùóºùóπùóπùóÆùóØùóºùóøùóÆùòÅùó∂ùóºùóª: Enables Dev and Ops to work together using the same language.  \\n  \\nIf you disagree, try to apply OOP or logic (if, for statements) to Terraform\\nHCL's syntax.  \\n  \\nIt works, but it quickly becomes a living hell.  \\n  \\nùóõùóºùòÑ ùó£ùòÇùóπùòÇùó∫ùó∂ ùòÑùóºùóøùó∏ùòÄ:  \\n  \\n\\\\- Pulumi uses a declarative approach. You define the desired state of your\\ninfrastructure.  \\n\\\\- It manages the state of your infrastructure using a state file.  \\n\\\\- When changes are made to the code, Pulumi compares the desired state with\\nthe current state and creates a plan to achieve the desired state.  \\n\\\\- The plan shows what resources will be created, updated, or deleted.  \\n\\\\- You can review and confirm the plan before Pulumi executes it.  \\n  \\n‚Üí It works similarly to Terraform but with all the benefits your favorite\\nprogramming language and existing tooling provides  \\n  \\n‚Üí It works similar to CDK, but faster and for your favorite cloud\\ninfrastructure (not only AWS)\\n\\nPulumi code example\\n\\n _What do you think? Have you used Pulumi?_  \\n  \\nWe started using it for the LLM Twin course, and so far, we love it! I will\\nprobably wholly migrate from Terraform to Pulumi in future projects.\\n\\n> üîó More on Pulumi\\n\\n* * *\\n\\n### How to build a scalable data collection pipeline\\n\\nùóïùòÇùó∂ùóπùó±, ùó±ùó≤ùóΩùóπùóºùòÜ to ùóîùó™ùó¶, ùóúùóÆùóñ, and ùóñùóú/ùóñùóó for a ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ that\\nùó∞ùóøùóÆùòÑùóπùòÄ your ùó±ùó∂ùó¥ùó∂ùòÅùóÆùóπ ùó±ùóÆùòÅùóÆ ‚Üí ùó™ùóµùóÆùòÅ do you need ü§î  \\n  \\nùóßùóµùó≤ ùó≤ùóªùó± ùó¥ùóºùóÆùóπ?  \\n  \\nùòà ùò¥ùò§ùò¢ùò≠ùò¢ùò£ùò≠ùò¶ ùò•ùò¢ùòµùò¢ ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùòµùò©ùò¢ùòµ ùò§ùò≥ùò¢ùò∏ùò≠ùò¥, ùò§ùò∞ùò≠ùò≠ùò¶ùò§ùòµùò¥, ùò¢ùòØùò• ùò¥ùòµùò∞ùò≥ùò¶ùò¥ ùò¢ùò≠ùò≠ ùò∫ùò∞ùò∂ùò≥ ùò•ùò™ùò®ùò™ùòµùò¢ùò≠\\nùò•ùò¢ùòµùò¢ ùòßùò≥ùò∞ùòÆ:  \\n  \\n\\\\- LinkedIn  \\n\\\\- Medium  \\n\\\\- Substack  \\n\\\\- Github  \\n  \\nùóßùóº ùóØùòÇùó∂ùóπùó± ùó∂ùòÅ - ùóµùó≤ùóøùó≤ ùó∂ùòÄ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ‚Üì  \\n  \\nùü≠\\\\. ùó¶ùó≤ùóπùó≤ùóªùó∂ùòÇùó∫: a Python tool for automating web browsers. It‚Äôs used here to\\ninteract with web pages programmatically (like logging into LinkedIn,\\nnavigating through profiles, etc.)  \\n  \\nùüÆ\\\\. ùóïùó≤ùóÆùòÇùòÅùó∂ùó≥ùòÇùóπùó¶ùóºùòÇùóΩ: a Python library for parsing HTML and XML documents. It\\ncreates parse trees that help us extract the data quickly.  \\n  \\nùüØ\\\\. ùó†ùóºùóªùó¥ùóºùóóùóï (ùóºùóø ùóÆùóªùòÜ ùóºùòÅùóµùó≤ùóø ùó°ùóºùó¶ùó§ùóü ùóóùóï): a NoSQL database fits like a glove on our\\nunstructured text data  \\n  \\nùü∞\\\\. ùóîùóª ùó¢ùóóùó†: a technique that maps between an object model in an application\\nand a document database  \\n  \\nùü±\\\\. ùóóùóºùó∞ùó∏ùó≤ùóø & ùóîùó™ùó¶ ùóòùóñùó•: to deploy our code, we have to containerize it, build an\\nimage for every change of the main branch, and push it to AWS ECR  \\n  \\nùü≤\\\\. ùóîùó™ùó¶ ùóüùóÆùó∫ùóØùó±ùóÆ: we will deploy our Docker image to AWS Lambda - a serverless\\ncomputing service that allows you to run code without provisioning or managing\\nservers. It executes your code only when needed and scales automatically, from\\na few daily requests to thousands per second  \\n  \\nùü≥\\\\. ùó£ùòÇùóπùòÇùó∫ùóªùó∂: IaC tool used to programmatically create the AWS infrastructure:\\nMongoDB instance, ECR, Lambdas and the VPC  \\n  \\nùü¥\\\\. ùóöùó∂ùòÅùóõùòÇùóØ ùóîùó∞ùòÅùó∂ùóºùóªùòÄ: used to build our CI/CD pipeline - on any merged PR to the\\nmain branch, it will build & push a new Docker image and deploy it to the AWS\\nLambda service\\n\\nETL architecture to collect digital data from social media platforms\\n\\nùòæùô™ùôßùôûùô§ùô™ùô® ùôùùô§ùô¨ ùô©ùôùùôöùô®ùôö ùô©ùô§ùô§ùô°ùô® ùô¨ùô§ùôßùô† ùô©ùô§ùôúùôöùô©ùôùùôöùôß?\\n\\n> Then...  \\n>  \\n> ‚Üì‚Üì‚Üì  \\n>  \\n> Check out ùóüùó≤ùòÄùòÄùóºùóª ùüÆ from the FREE ùóüùóüùó† ùóßùòÑùó∂ùóª ùóñùóºùòÇùóøùòÄùó≤ created by Decoding ML  \\n>  \\n> ...where we will walk you ùòÄùòÅùó≤ùóΩ-ùóØùòÜ-ùòÄùòÅùó≤ùóΩ through the ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤ and ùó∞ùóºùó±ùó≤ of\\n> the ùó±ùóÆùòÅùóÆ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤:\\n>\\n> üîó The Importance of Data Pipelines in the Era of Generative AI\\n\\n* * *\\n\\n### Monitoring your DL models while in production\\n\\nùó†ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ is ùóßùóõùóò ùó∏ùó≤ùòÜ ùó†ùóüùó¢ùóΩùòÄ ùó≤ùóπùó≤ùó∫ùó≤ùóªùòÅ in ensuring your ùó∫ùóºùó±ùó≤ùóπùòÄ in ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª are\\nùó≥ùóÆùó∂ùóπ-ùòÄùóÆùó≥ùó≤. Here is an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ on ùó†ùóü ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ using Triton, Prometheus and\\nGrafana ‚Üì  \\n  \\n\\nRazvant Alexandru\\n\\nwrote a fantastic ùòÄùòÅùó≤ùóΩ-ùóØùòÜ-ùòÄùòÅùó≤ùóΩ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ in the\\n\\nDecoding ML Newsletter\\n\\non ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ your ùóóùóü ùó∫ùóºùó±ùó≤ùóπùòÄ while in ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª.  \\n  \\nWithin his article, he started with an example where, in one of his projects,\\na main processing task was supposed to take <5 ùò©ùò∞ùò∂ùò≥ùò¥, but while in production,\\nit jumped to >8 ùò©ùò∞ùò∂ùò≥ùò¥.  \\n  \\n‚Üí ùòõùò©ùò™ùò¥ (ùò∞ùò≥ ùò¥ùò∞ùòÆùò¶ùòµùò©ùò™ùòØùò® ùò¥ùò™ùòÆùò™ùò≠ùò¢ùò≥) ùò∏ùò™ùò≠ùò≠ ùò©ùò¢ùò±ùò±ùò¶ùòØ ùòµùò∞ ùò¢ùò≠ùò≠ ùò∞ùòß ùò∂ùò¥.  \\n  \\nEven to the greatest.  \\n  \\nIt's impossible always to anticipate everything that will happen in production\\n(sometimes it is a waste of time even to try to).  \\n  \\nThat is why you always need eyes and years on your production ML system.  \\n  \\nOtherwise, imagine how much $$$ or users he would have lost if he hadn't\\ndetected the ~3-4 hours loss in performance as fast as possible.\\n\\nAfterward, he explained step-by-step how to use:  \\n  \\n\\\\- ùó∞ùóîùó±ùòÉùó∂ùòÄùóºùóø to scrape RAM/CPU usage per container  \\n  \\n\\\\- ùóßùóøùó∂ùòÅùóºùóª ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó¶ùó≤ùóøùòÉùó≤ùóø to serve ML models and yield GPU-specific metrics.  \\n  \\n\\\\- ùó£ùóøùóºùó∫ùó≤ùòÅùóµùó≤ùòÇùòÄ to bind between the metrics generators and the consumer.  \\n  \\n\\\\- ùóöùóøùóÆùó≥ùóÆùóªùóÆ to visualize the metrics\\n\\n> ùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ ùóºùóª ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü  \\n>  \\n> ‚Üì‚Üì‚Üì  \\n>  \\n> üîó How to ensure your models are fail-safe in production?\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n11\\n\\nShare this post\\n\\n#### The new king of Infrastructure as Code (IaC)\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-new-king-of-infrastructure-as?r=1ttoeh'), ArticleDocument(id=UUID('fd48444e-ab32-49b9-afdc-14fe8ecafd41'), content={'Title': 'Data Ingestion Architecture for ML and Marketing Intelligence', 'Subtitle': 'Building a highly scalable data collection pipeline for AI, ML and marketing intelligence leveraging the AWS cloud, Python, data\\xa0crawling, and Docker.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\n### Leveraging AWS Ecosystem and Data Crawling for Scalable and Adaptive Data\\nPipelines\\n\\nRares Istoc\\n\\nJun 27, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n**Today‚Äôs article** is **written** by our **guest** , **Rares Istoc** , a\\nveteran with over 7 years of experience building scalable software and data\\nengineering systems in the industry.\\n\\n‚Üí Here is his üîó LinkedIn.\\n\\nMachine learning without data is like a chef without ingredients - all the\\nskills but nothing to cook.\\n\\nThese days, everything circulates around data, from personalized ads to\\nstreaming recommendations. Data drives decisions in business, healthcare, and\\nsports. Without it, apps would be clueless, smart devices would be dumb, and\\npredictions would be nothing more than guesses. In this digital age, data is\\nthe lifeblood of innovation and efficiency.\\n\\n**Ok, but why another article about data ingestion?**\\n\\nThere are many ways to build data ingestion pipelines, and with all the new\\ntools created over the last decade, selecting the best ones can be\\nchallenging. The answer often depends on your project‚Äôs specific needs.\\n\\nIn this article, you‚Äôll explore an end-to-end solution for marketing\\nintelligence. Using AWS‚Äôs ecosystem, you can create a scalable data-ingestion\\npipeline for data crawling and integrate it into various analytical processes\\nlike sales, competitor analysis, market analysis, and customer insights.\\n\\nI‚Äôll also present the challenges encountered while building this solution.\\nFinding a complete working solution is tough, with most answers scattered\\nacross the Internet. You can access the full solution code on üîó **GitHub**.\\n\\n_**IMPORTANT NOTE:** Before diving into this solution, you must be aware of\\nthe legal implications of ingesting data from some data sources, like social\\nmedia pages, so we can make sure nobody goes to jail. Please read the terms\\nand conditions of each major platform; these will restrict you from crawling\\nuser profiles and private pages._\\n\\n* * *\\n\\n### Table of Contents:\\n\\n  1. Architecture Overview\\n\\n  2. Implementation\\n\\n  3. Challenges & Pitfalls\\n\\n  4. Local Testings\\n\\n  5. Deployment\\n\\n* * *\\n\\n### 1\\\\. Architecture Overview\\n\\nThis is what we are about to build:\\n\\nHere are some non-functional requirements I‚Äôve aimed to achieve with this\\narchitecture:\\n\\n**Scalability:** The solution can process many pages simultaneously and easily\\nadd more, handling growth at any time.\\n\\n**Maintainability & Adaptability:** Each component is designed for easy\\nmodification and expansion without significant development time.\\n\\n**Components Overview:**\\n\\n‚Ä¢ **Scheduler:** Triggers crawler lambdas for each page link.\\n\\n‚Ä¢ **Crawler:** Extracts various posts and information from the page link. If\\nunfamiliar with crawling, look it up before proceeding. Details will follow in\\nthe implementation part.\\n\\n‚Ä¢ **Database:** MongoDB is used for our data lake storage, housing posts for\\nlater use. It excels at handling semi-structured data.\\n\\nThe complete flow: the scheduler triggers a crawler lambda for each page,\\nsending the page name and link. The crawler extracts posts from the past week,\\nstoring the raw content, creation date, link, and name. The scheduler waits\\nfor all lambdas to finish, aggregates the posts from the database, and sends\\nthem to ChatGPT using prompt templates to generate reports.\\n\\n### 2\\\\. Implementation\\n\\nIn this section, I‚Äôll provide a detailed overview of the main components,\\nbreaking them down with code samples and explanations.\\n\\n#### 2.1. Scheduler\\n\\nI‚Äôll not focus much on the reporting part, though you can find it **here**\\nalong with all the code shared in this article. The main focus is the\\nscheduling part, the entry point of the system where the flow starts and is\\norchestrated:\\n\\n    \\n    \\n    import json\\n    import os\\n    import time\\n    from datetime import datetime, timedelta\\n    \\n    import boto3\\n    from aws_lambda_powertools import Logger\\n    from aws_lambda_powertools.utilities.typing import LambdaContext\\n    \\n    from src.constants import PAGE_LINK\\n    from src.db import database\\n    from src.utils import monitor\\n    \\n    logger = Logger(service=\"decodingml/scheduler\")\\n    \\n    _client = boto3.client(\"lambda\")\\n    \\n    \\n    def lambda_handler(event, context: LambdaContext):\\n        correlation_ids = []\\n    \\n        for link in PAGE_LINK:\\n            response = _client.invoke(\\n                FunctionName=\"lambda\",\\n                InvocationType=\"Event\",\\n                Payload=json.dumps({\"link\": link}),\\n            )\\n            logger.info(f\"Triggered crawler for: {link}\")\\n    \\n            correlation_ids.append(response[\"ResponseMetadata\"][\"RequestId\"])\\n    \\n        logger.info(f\"Monitoring: {len(correlation_ids)} crawler processes\")\\n    \\n        while True:\\n            time.sleep(15)\\n            completed = monitor(correlation_ids)\\n    \\n            correlation_ids = [c for c in correlation_ids if c not in completed]\\n    \\n            if not correlation_ids:\\n                break\\n    \\n            logger.info(f\"Still waiting for {len(correlation_ids)} crawlers to complete\")\\n    \\n        now = datetime.now()\\n        posts = list(\\n            database.profiles.find(\\n                {\\n                    \"date\": {\"$gte\": (now - timedelta(days=7)), \"$lte\": now},\\n                }\\n            )\\n        )\\n    \\n        logger.info(f\"Gathered {len(posts)} posts\")\\n    \\n        if not posts:\\n            logger.info(\"Cannot generate report, no new posts available\")\\n            return\\n    \\n        reports = generate_profiles_report(posts)\\n    \\n        logger.info(\"Generated new report!\")\\n\\nThe scheduler acts as a scatterer, iterating over a list of page links and\\ninvoking a crawler asynchronously with the InvocationType parameter set to\\nEvent, ensuring the scheduler won‚Äôt block for a single page. It stores each\\nlambda‚Äôs correlation ID in a list and waits for all lambdas to finish, with a\\n15-second wait time, adjustable based on your crawler‚Äôs average completion\\ntime. Finally, it finds all crawled posts and sends them to the report\\ngeneration phase.\\n\\n#### 2.2. Crawler\\n\\nHere I‚Äôll break down the actual crawling process:\\n\\n    \\n    \\n    import abc\\n    import os\\n    from datetime import datetime, timedelta\\n    from itertools import takewhile, dropwhile\\n    from typing import List, Dict, Any\\n    \\n    import instaloader\\n    \\n    from src.crawlers.base import BaseAbstractCrawler\\n    \\n    class BaseAbstractCrawler(abc.ABC):\\n    \\n        @abc.abstractmethod\\n        def extract(self, link: str, **kwargs) -> None: ...\\n    \\n    \\n    class InstagramCrawler(BaseAbstractCrawler):\\n    \\n        def __init__(self, link: str, proxy=None):\\n            self.link = link\\n            self.loader = instaloader.Instaloader()\\n            self._until = datetime.now()\\n            self._since = self._until - timedelta(days=7)\\n            self._proxy = proxy\\n    \\n        def extract(self, **kwargs) -> List[Dict[str, str | Any]]:\\n            parsed_url = urlparse(self.link)\\n    \\n            if self._proxy:\\n                os.environ[\\'https_proxy\\'] = self._proxy.__dict__().get(\\'http\\')\\n            profile = instaloader.Profile.from_username(self.loader.context, parsed_url.path.strip(\\'/\\').split(\\'/\\')[0])\\n            posts = takewhile(lambda p: p.date > self._since, dropwhile(lambda p: p.date > self._until, profile.get_posts()))\\n    \\n            return [\\n                {\\'content\\': post.caption, \\'date\\': post.date, \\'link\\': self.link}\\n                for post in posts\\n            ]\\n\\nI‚Äôve defined a main abstraction point for all crawlers, establishing a common\\ninterface that all derived crawlers must implement. Each subclass must provide\\nits implementation for the `extract()` method, ensuring reusability and\\nuniformity.\\n\\n    \\n    \\n    import re\\n    \\n    from src.crawlers.base import BaseAbstractCrawler\\n    from src.crawlers.instagram import InstagramCrawler\\n    \\n    \\n    class CrawlerDispatcher:\\n    \\n        def __init__(self) -> None:\\n            self._crawlers = {}\\n    \\n        def register(self, domain: str, crawler: type[BaseAbstractCrawler]) -> None:\\n            self._crawlers[r\"https://(www\\\\.)?{}.com/*\".format(re.escape(domain))] = crawler\\n    \\n        def get_crawler(self, url: str) -> BaseAbstractCrawler:\\n            for pattern, crawler in self._crawlers.items():\\n                if re.match(pattern, url):\\n                    return crawler()\\n            else:\\n                raise ValueError(\"No crawler found for the provided link\")\\n    \\n    \\n    dispatcher = CrawlerDispatcher()\\n    dispatcher.register(\\'instagram\\', InstagramCrawler)\\n\\nTo promote and call each crawler automatically, I‚Äôve built a dispatcher that\\nselects and instantiates the correct crawler class based on the provided link.\\nThis acts as a registry and factory for the crawlers, managed under a unified\\ninterface and structure.\\n\\nAdvantages:\\n\\n‚Ä¢ **Flexibility & Scalability:** Allows easy addition of new domains and\\nspecialized crawlers without modifying the existing codebase.\\n\\n‚Ä¢ **Encapsulation & Modularity:** The dispatcher encapsulates the logic for\\ndetermining which crawler to use, making the system modular and allowing each\\ncrawler to focus on its core business logic.\\n\\n    \\n    \\n    from datetime import datetime, timedelta\\n    \\n    from aws_lambda_powertools import Logger\\n    from aws_lambda_powertools.utilities.typing import LambdaContext\\n    \\n    from src.crawlers import dispatcher\\n    from src.db import database\\n    \\n    logger = Logger(service=\"decodingml/crawler\")\\n    \\n    \\n    def lambda_handler(event, context: LambdaContext):\\n    \\n        link = event.get(\\'link\\')\\n    \\n        logger.info(f\"Start extracting posts for {link}\")\\n    \\n        crawler = dispatcher.get_crawler(event.get(\\'link\\'))\\n    \\n        posts = [{**page, \\'correlation_id\\': context.aws_request_id} for page in crawler.extract()]\\n    \\n        now = datetime.now()\\n        existing_posts = database.profiles.find({\\n            \"date\": {\"$gte\": (now - timedelta(days=7)), \"$lte\": now},\\n            \"name\": link\\n        }, projection={\\'date\\': 1})\\n    \\n        existing_posts = [post.get(\\'date\\') for post in list(existing_posts)]\\n    \\n        posts = [post for post in posts if post.get(\\'date\\') not in existing_posts]\\n    \\n        if not posts:\\n            logger.info(\"No new posts on page\")\\n            return\\n    \\n        logger.info(f\"Successfully extracted {len(posts)} posts\")\\n        database.profiles.insert_many(posts)\\n        logger.info(f\"Successfully inserted data in db\")\\n\\nThe main entry point assembles the link from the event body, selects the\\ncorrect crawler, and starts extraction jobs. After extraction, it checks for\\nexisting posts to avoid duplicates and adds new posts to the database.\\n\\n### 3\\\\. Challenges & Pitfalls\\n\\n#### 3.1. Running headless browser instance with selenium in lambda runtime\\nenvironment\\n\\nThis caused the most headaches. The Lambda execution environment is read-only,\\nso writing to disk requires using a temporary file, complicating automatic\\nbinary driver installation. Therefore, you need to install the driver directly\\nin the Docker image and reference it manually in Selenium‚Äôs driver options.\\nThe only usable driver for this setup was the Google binary driver in my case.\\n\\n    \\n    \\n    FROM  public.ecr.aws/lambda/python:3.11 as build\\n    \\n    # Download chrome driver and browser and manually unpack them in their folders\\n    RUN yum install -y unzip && \\\\\\n        curl -Lo \"/tmp/chromedriver-linux64.zip\" \"https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/119.0.6045.105/linux64/chromedriver-linux64.zip\" && \\\\\\n        curl -Lo \"/tmp/chrome-linux64.zip\" \"https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/119.0.6045.105/linux64/chrome-linux64.zip\" && \\\\\\n        unzip /tmp/chromedriver-linux64.zip -d /opt/ && \\\\\\n        unzip /tmp/chrome-linux64.zip -d /opt/\\n    \\n    \\n    FROM  public.ecr.aws/lambda/python:3.11\\n    \\n    # Install the function\\'s OS dependencies using yum\\n    RUN yum install -y \\\\\\n        atk \\\\\\n        cups-libs \\\\\\n        gtk3 \\\\\\n        libXcomposite \\\\\\n        alsa-lib \\\\\\n        libXcursor \\\\\\n        libXdamage \\\\\\n        libXext \\\\\\n        libXi \\\\\\n        libXrandr \\\\\\n        libXScrnSaver \\\\\\n        libXtst \\\\\\n        pango \\\\\\n        at-spi2-atk \\\\\\n        libXt \\\\\\n        xorg-x11-server-Xvfb \\\\\\n        xorg-x11-xauth \\\\\\n        dbus-glib \\\\\\n        dbus-glib-devel \\\\\\n        nss \\\\\\n        mesa-libgbm \\\\\\n        ffmpeg \\\\\\n        libxext6 \\\\\\n        libssl-dev \\\\\\n        libcurl4-openssl-dev \\\\\\n        libpq-dev\\n    \\n    COPY --from=build /opt/chrome-linux64 /opt/chrome\\n    COPY --from=build /opt/chromedriver-linux64 /opt/\\n    \\n    COPY ./pyproject.toml ./poetry.lock ./\\n    \\n    # Install Poetry, export dependencies to requirements.txt, and install dependencies\\n    # in the Lambda task directory, finally cleanup manifest files.\\n    RUN python3 -m pip install --upgrade pip && pip install poetry\\n    RUN poetry export -f requirements.txt > requirements.txt && \\\\\\n        pip3 install  --no-cache-dir -r requirements.txt --target \"${LAMBDA_TASK_ROOT}\" && \\\\\\n        rm requirements.txt pyproject.toml poetry.lock\\n    \\n    # Copy function code\\n    COPY ./src ${LAMBDA_TASK_ROOT}/src\\n\\nThe main idea in this Dockerfile is that I manually downloaded the Chrome\\ndriver and browser and unpacked them in a location where they can be accessed\\nby Selenium, which usually would‚Äôve done this directly.\\n\\nThis is a mandatory step for the Lambda environment. Since everything is read-\\nonly, in the next code sample I‚Äôll show you how point Selenium to the correct\\ndriver and browser locations:\\n\\n    \\n    \\n    from tempfile import mkdtemp\\n    \\n    def init_driver(self):\\n        options = Options()\\n        # Setup drover binary location manually\\n        options.binary_location = \\'/opt/chrome/chrome\\'\\n        # Run browser in headless mode\\n        options.add_argument(\\'--headless=new\\')\\n        options.add_argument(\\'--no-sandbox\\')\\n        options.add_argument(\\'--single-process\\')\\n        options.add_argument(\\'--window-size=1420,1080\\')\\n        options.add_argument(\\'--disable-dev-shm-usage\\')\\n        options.add_argument(\\'--disable-gpu\\')\\n        options.add_argument(\\'--disable-popup-blocking\\')\\n        options.add_argument(\\'--disable-notifications\\')\\n        options.add_argument(\\'--disable-dev-tools\\')\\n        options.add_argument(\\'--log-level=3\\')\\n        options.add_argument(\\'--ignore-certificate-errors\\')\\n        options.add_argument(\"--no-zygote\")\\n        options.add_argument(f\"--user-data-dir={mkdtemp()}\")\\n        options.add_argument(f\"--data-path={mkdtemp()}\")\\n        options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\\n        options.add_argument(\\'--remote-debugging-port=9222\\')\\n    \\n    \\n        self._driver = webdriver.Chrome(\\n            service=Service(\"/opt/chromedriver\"),\\n            options=options,\\n        )\\n\\nI hardcoded the driver and browser locations in the Dockerfile. Additionally,\\nI pointed several folders (e.g., user-data-dir, disk-cache-dir) to temporary\\ndirectories to prevent Selenium from creating them automatically, which would\\ncause errors due to Lambda‚Äôs disk limitations.\\n\\n#### 3.2. Aggregate Empty Pages\\n\\nMy initial monitoring algorithm was basic, looping over lambda invocation\\ncorrelation IDs and checking the database for generated posts. However, it\\nencountered an infinite loop when no new posts were created for some pages.\\n\\n    \\n    \\n    import datetime\\n    import re\\n    from typing import List\\n    \\n    import boto3\\n    \\n    _client = boto3.client(\\'logs\\')\\n    \\n    \\n    def monitor(correlation_ids: List[str]):\\n        finished = []\\n    \\n        now = int((datetime.datetime.now() datetime.timedelta(days=1)).timestamp() * 1000)\\n    \\n        response = _client.filter_log_events(\\n            logGroupName=\\'/aws/lambda/crawler\\',\\n            startTime=now,\\n            filterPattern=\"REPORT RequestId\"\\n        )\\n    \\n        for event in response[\\'events\\']:\\n            match = re.search(r\\'REPORT RequestId: ([^\\\\s]+)\\', event.get(\\'message\\'))\\n            if match:\\n                correlation_id = match.group(1)\\n                if correlation_id in correlation_ids:\\n                    finished.append(correlation_id)\\n    \\n        return finished\\n\\nHere, I search through all log streams for each lambda generated in that\\ncurrent day and look for the message, which usually has this format: _**REPORT\\nRequestId:**_ <correlation_id>. This indicates that the lambda has reached the\\nend of its execution, and I can mark which correlation IDs have finished.\\n\\n#### 3.3. Avoid being blocked by social media platforms\\n\\nThis was a pity error‚Äîthe kind you would‚Äôve spent days on‚Äîand the solution was\\nto watch it from a different perspective. Popular social media platforms\\nimplement many anti-bot protection mechanisms to prevent crawling, from\\nrequest header analysis to rate limiting to IP blocking.\\n\\nAnd because we run our browser in headless mode to mimic realistic user-\\nbrowser interaction, and all our crawlers send requests under the same IP\\naddress to multiple pages at the same time repeatedly, this screams, please\\nblock me.\\n\\nTo address this, I‚Äôve used a proxy to mask my IP address and location:\\n\\n    \\n    \\n    import os\\n    \\n    \\n    class ProxyConnection:\\n    \\n        def __init__(\\n            self,\\n            host: str = None,\\n            port: str = None,\\n            username: str = None,\\n            password: str = None,\\n            verify_ssl: bool = False\\n        ):\\n            self.host = host or os.getenv(\\'PROXY_HOST\\')\\n            self.port = port or os.getenv(\\'PROXY_PORT\\')\\n            self.username = username or os.getenv(\\'PROXY_USERNAME\\')\\n            self.password = password or os.getenv(\\'PROXY_PASSWORD\\')\\n            self.verify_ssl = verify_ssl\\n            self._url = f\"{self.username}:{self.password}@{self.host}:{self.port}\"\\n    \\n        def __dict__(self):\\n            return {\\n                \\'https\\': \\'https://{}\\'.format(self._url.replace(\" \", \"\")),\\n                \\'http\\': \\'http://{}\\'.format(self._url.replace(\" \", \"\")),\\n                \\'no_proxy\\': \\'localhost, 127.0.0.1\\',\\n                \\'verify_ssl\\': self.verify_ssl\\n            }\\n\\nTo address this, I used a proxy to mask my IP and location. Paid proxies like\\nSmartProxy offer a pool of rotating IPs, assigning a different IP to each\\ncrawler, mimicking regular user behavior. Additionally, using a proxy allows\\nfinding a country without access restrictions to public pages, ensuring smooth\\ncrawling.\\n\\n### 4\\\\. Local Testings\\n\\nTo prove this works, I wrote a makefile containing some simple commands for\\ncrawler and lambda. The problem is that I‚Äôve only managed to test the crawler\\nlocally. Since the scheduler spins up crawlers, they should be already\\ndeployed on AWS.\\n\\n    \\n    \\n    local-test-crawler: # Send test command on local to test  the lambda\\n     curl -X POST \"http://localhost:9000/2015-03-31/functions/function/invocations\" \\\\\\n      -d \\'{\"link\": \"https://www.instagram.com/mcdonalds\"}\\'\\n    \\n    local-test-scheduler: # Send test command on local to test  the lambda\\n     curl -X POST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\n\\nNow, most people, when testing lambda functions on a local environment, use\\nAWS Lambda **RIE (Runtime Interface Emulator)** , which allows you to test\\nyour lambda function packages in a container. Basically, this emulates a\\nlambda execution environment on your local machine. As you can see, I‚Äôve\\nmanaged to do this without using the emulator, which slightly simplified my\\nenvironment.\\n\\nYou can use these commands to test each component. For example, if you would\\nlike to test the crawler, go into your terminal and use this command:\\n\\n    \\n    \\n    > make local-test-crawler\\n\\nAs you can see, the crawling process has started, and for this page, we‚Äôve\\nfound three new posts in the last seven days:\\n\\n### 5\\\\. Deployment\\n\\nThe deployment process is defined in **our GitHub** repository under the\\n**ops** folder, where you can explore the whole solution written in Pulumi.\\n\\nYou can play with the Makefile. It contains all the necessary commands to make\\nyour infrastructure up and running.\\n\\n* * *\\n\\n### Conclusion\\n\\nIn this article, we‚Äôve explored a complete end-to-end robust solution for\\nbuilding a Highly Scalable Data Ingestion pipeline that can leverage existing\\ndata from multiple crawlable sources for various processes like ML training,\\ndata analysis, etc.\\n\\nWe‚Äôve gone through specific challenges you might face and how to overcome them\\nin this process.\\n\\n| _üîó**Check out** the code on GitHub [1] and support us with a _‚≠êÔ∏è\\n\\n* * *\\n\\nWithin our newsletter, we keep things short and sweet.\\n\\nIf you enjoyed reading this article, consider checking out the full version on\\nMedium. It‚Äôs still free ‚Üì\\n\\nFull article on Medium\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### Highly Scalable Data Ingestion Architecture for ML and Marketing\\nIntelligence\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/highly-scalable-data-ingestion-architecture?r=1ttoeh'), ArticleDocument(id=UUID('9c6f5239-fc76-4fe9-a8e2-77f662d0c69f'), content={'Title': '2 Key LLMOps Concepts - by Alex Razvant', 'Subtitle': 'How to monitor LLM & RAG applications. Evaluate your RAG like a pro. Learn about memory/compute requirements on LLMs.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### 2 Key LLMOps Concepts\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# 2 Key LLMOps Concepts\\n\\n### How to monitor LLM & RAG applications. Evaluate your RAG like a pro. Learn\\nabout memory/compute requirements on LLMs.\\n\\nAlex Razvant\\n\\nJun 22, 2024\\n\\n10\\n\\nShare this post\\n\\n#### 2 Key LLMOps Concepts\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * A powerful framework to evaluate RAG pipelines\\n\\n  * Why do LLMs require so much VRAM?\\n\\n  * LLMOps Chain Monitoring\\n\\n* * *\\n\\n### ùó¢ùóªùó≤ ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ ùòÅùóº ùó≤ùòÉùóÆùóπùòÇùóÆùòÅùó≤ ùòÜùóºùòÇùóø ùó•ùóîùóö - ùó•ùóîùóöùóîùòÄ\\n\\nBuilding an RAG pipeline is fairly simple. You just need a Vector-DB knowledge\\nbase, an LLM to process your prompts, plus additional logic for interactions\\nbetween these modules.\\n\\nLesson 10: Evaluating the RAG pipeline. (Image by Author)\\n\\nHowever, reaching a satisfying performance level imposes its challenges due to\\nthe ‚Äúseparate‚Äù components:\\n\\n**Decoding ML Newsletter** is a reader-supported publication. If you enjoy our\\ncontent, please consider becoming a paid subscriber.\\n\\nSubscribe\\n\\n  1. **Retriever** ‚Äî which takes care of querying the Knowledge DB and retrieves additional context that matches the user‚Äôs query. \\n\\n  2. **Generator** ‚Äî which encompasses the LLM module, generating an answer based on the context-augmented prompt. When evaluating a RAG pipeline, we must evaluate both components separately and together. \\n\\nüî∏ **What is RAGAs?**\\n\\nA framework that helps you evaluate your Retrieval Augmented Generation (RAG)\\npipelines. One of the core concepts of RAGAs is Metric-Driven-Development\\n(MDD) which is a product development approach that relies on data to make\\nwell-informed decisions.\\n\\nüî∏ **What metrics do RAGAs expose?**\\n\\nüîΩ For ùó•ùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ Stage :\\n\\n‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùó£ùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª Evaluates the precision of the context used to generate an\\nanswer, ensuring relevant information is selected from the context  \\n‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùó•ùó≤ùóπùó≤ùòÉùóÆùóªùó∞ùòÜ Measures how relevant the selected context is to the\\nquestion. ‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùó•ùó≤ùó∞ùóÆùóπùóπ Measures if all the relevant information required\\nto answer the question was retrieved.  \\n‚Ü≥ ùóñùóºùóªùòÅùó≤ùòÖùòÅ ùóòùóªùòÅùó∂ùòÅùó∂ùó≤ùòÄ ùó•ùó≤ùó∞ùóÆùóπùóπ Evaluates the recall of entities within the context,\\nensuring that no important entities are overlooked.\\n\\nüîΩ For ùóöùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóºùóª Stage :\\n\\n‚Ü≥ ùóôùóÆùó∂ùòÅùóµùó≥ùòÇùóπùóªùó≤ùòÄùòÄ Measures how accurately the generated answer reflects the\\nsource content, ensuring the generated content is truthful and reliable.  \\n‚Ü≥ ùóîùóªùòÄùòÑùó≤ùóø ùó•ùó≤ùóπùó≤ùòÉùóÆùóªùó∞ùó≤ It is validating that the response directly addresses the\\nuser‚Äôs query.  \\n‚Ü≥ ùóîùóªùòÄùòÑùó≤ùóø ùó¶ùó≤ùó∫ùóÆùóªùòÅùó∂ùó∞ ùó¶ùó∂ùó∫ùó∂ùóπùóÆùóøùó∂ùòÅùòÜ Shows that the generated content is semantically\\naligned with expected responses.  \\n‚Ü≥ ùóîùóªùòÄùòÑùó≤ùóø ùóñùóºùóøùóøùó≤ùó∞ùòÅùóªùó≤ùòÄùòÄ Focuses on fact-checking, assessing the factual accuracy\\nof the generated answer.  \\n  \\nüî∏ **How to evaluate using RAGAs?**\\n\\n1\\\\. Prepare your ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØùò¥,ùò¢ùòØùò¥ùò∏ùò¶ùò≥ùò¥,ùò§ùò∞ùòØùòµùò¶ùòπùòµùò¥ and ùò®ùò≥ùò∞ùò∂ùòØùò•_ùòµùò≥ùò∂ùòµùò©ùò¥  \\n2\\\\. Compose a Dataset object  \\n3\\\\. Select metrics  \\n4\\\\. Evaluate  \\n5\\\\. Monitor scores or log the entire evaluation chain to a platform like\\nCometML.\\n\\nFor a full end-to-end workflow of RAGAs evaluation in practice, I\\'ve described\\nit in this LLM-Twin Course Article üëá:\\n\\nHow to Evaluate RAGs Medium Article\\n\\n* * *\\n\\n### Why are LLMs so Memory-hungry?\\n\\nLLMs require lots of GPU memory, but let\\'s see why that\\'s the case. üëá\\n\\nüî∏ What is an LLM parameter?\\n\\nLLMs, like Mistral 7B or LLama3-8B, have billions of parameters. ùóòùóÆùó∞ùóµ\\nùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùó∂ùòÄ ùóÆ ùòÑùó≤ùó∂ùó¥ùóµùòÅ stored and accessed during computation.\\n\\nüî∏ How much GPU VRAM is required? There are three popular precision formats\\nthat LLMs are trained in:\\n\\n‚Üí FP32 - 32bits floating point  \\n‚Üí FP16/BFP16 - 16 bits floating point\\n\\nMost use mixed precision, e.g., matmul in BFP16 and accumulations in FP32.\\n\\nFor this example, we\\'ll use half-precision BFP16.\\n\\nHere\\'s a deeper dive on this topic:  \\nüîó Google BFloat16  \\nüîó LLMs Precision Benchmark\\n\\nüîπ Let\\'s calculate the VRAM required:\\n\\n\\\\\\\\(\\\\begin{align*} \\\\text{VRAM} &= \\\\text{Size}(\\\\text{params}) +\\n\\\\text{Size}(\\\\text{activations}) \\\\\\\\\\\\ \\\\text{Size}(\\\\text{params}) &=\\n\\\\text{Params} \\\\times \\\\text{Precision}(\\\\text{bytes}) \\\\end{align*}\\\\\\\\)\\n\\nAs 1byte=8bits, we\\'ve got:  \\n‚Üí FP32 = 32 bits = 4 bytes  \\n‚Üí FP16/BFP16 = 16bits = 2 bytes\\n\\nNow, for a 7B model, we would require:  \\n‚Üí VRAM = 7 * 10^9 (billion) * 2 bytes = 14 * 10^9 bytes\\n\\nKnowing that 1GB = 10 ^ 9 bytes we have ùü≠ùü∞ùóöùóï as the required VRAM to load a ùü≥ùóï\\nùó∫ùóºùó±ùó≤ùóπ ùó≥ùóºùóø ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ in half BF16 precision.\\n\\nùóßùóµùó∂ùòÄ ùó∂ùòÄ ùóΩùòÇùóøùó≤ùóπùòÜ ùó≥ùóºùóø ùóπùóºùóÆùó±ùó∂ùóªùó¥ ùòÅùóµùó≤ ùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóøùòÄ.  \\n  \\nEver encountered the ùóñùó®ùóóùóî ùó¢ùó¢ùó† Error e.g \"ùòõùò≥ùò™ùò¶ùò• ùòµùò∞ ùò¢ùò≠ùò≠ùò∞ùò§ùò¢ùòµùò¶ +56ùòîùòâ ...\" when\\ninferencing? here\\'s the most plausible cause for that:\\n\\n‚≠ï No GPU VRAM left for the activations. Let\\'s figure out the activation size\\nrequired by using ùóüùóüùóÆùó∫ùóÆùüÆ-ùü≥ùóï as an example.\\n\\nüî∏ Activations are a combination of the following model parameters:  \\n\\\\- Context Length (N)  \\n\\\\- Hidden Size (H)  \\n\\\\- Precision (P)\\n\\nAfter a quick look at the LLama2-7b model configuration, we get these values:  \\n\\\\- Context Length (N) = 4096 tokens  \\n\\\\- Hidden Size (H) = 4096 dims  \\n\\\\- Precision (P) = BF16 = 2bytes  \\nüîó ùóüùóüùóÆùó∫ùóÆùüÆ-ùü≥ùóØ ùó†ùóºùó±ùó≤ùóπ ùó£ùóÆùóøùóÆùó∫ùòÄ: shorturl.at/CWOJ9\\n\\nConsult this interactive LLM-VRAM calculator to check on the different memory\\nsegments reserved when inferencing/training LLMs.\\n\\nüü¢ Inference/Training VRAM Calculator  \\n  \\nüü° For training, things stay a little different, as more factors come into\\nplay, as memory is allocated for:  \\n‚Ü≥ Full Activations considering N(Heads) and N( Layers)  \\n‚Ü≥ Optimizer States which differ based on the optimizer type  \\n‚Ü≥ Gradients\\n\\nHere\\'s a tutorial on PEFT, QLoRA fine-tuning in action üëá:\\n\\nLLM Fine Tuning Medium Article\\n\\nOther Resources:  \\nüìî Model Anatomy: shorturl.at/nJeu0  \\nüìî VRAM for Serving: shorturl.at/9UPBE  \\nüìî LLM VRAM Explorer: shorturl.at/yAcTU\\n\\n* * *\\n\\n### One key LLMOps concept - Chain Monitoring\\n\\nIn traditional ML systems, it is easier to backtrack to a problem compared to\\nGenerative AI ones based on LLMs. When working with LLMs, their generative\\nnature can lead to complex and sometimes unpredictable behavior.\\n\\nüîπ ùóî ùòÄùóºùóπùòÇùòÅùó∂ùóºùóª ùó≥ùóºùóø ùòÅùóµùóÆùòÅ?\\n\\n\"Log prompts or entire chains with representative metadata when\\ntesting/evaluating your LLM.\" ùòñùòØùò¶ ùò±ùò≠ùò¢ùòµùòßùò∞ùò≥ùòÆ ùòµùò©ùò¢ùòµ ùòê ùò≠ùò™ùò¨ùò¶ ùò¢ùòØùò• ùòê\\'ùò∑ùò¶ ùò£ùò¶ùò¶ùòØ ùò∂ùò¥ùò™ùòØùò® ùòßùò∞ùò≥\\nùòµùò©ùò™ùò¥ ùòµùò¢ùò¥ùò¨ ùò™ùò¥ ùóñùóºùó∫ùó≤ùòÅùó†ùóü - ùóüùóüùó†.\\n\\n**üî∏** ùóõùó≤ùóøùó≤ ùóÆùóøùó≤ ùóÆ ùó≥ùó≤ùòÑ ùó∞ùóÆùòÄùó≤ùòÄ ùòÑùóµùó≤ùóøùó≤ ùó∂ùòÅ ùóΩùóøùóºùòÉùó≤ùòÄ ùóØùó≤ùóªùó≤ùó≥ùó∂ùó∞ùó∂ùóÆùóπ**:**\\n\\n‚Üí ùóôùóºùóø ùó¶ùòÇùó∫ùó∫ùóÆùóøùó∂ùòÄùóÆùòÅùó∂ùóºùóª ùóßùóÆùòÄùó∏ùòÄ\\n\\nHere you might have a query that represents the larger text, the LLMs response\\nwhich is the summary, and you could calculate the ROUGE score inline between\\nquery & response and add it to the metadata field. Then you can compose a JSON\\nwith query, response, and rouge_score and log it to comet.\\n\\n‚Üí ùóôùóºùóø ùó§&ùóî ùóßùóÆùòÄùó∏ùòÄ Here, you could log the Q&A pairs separately, or even add an\\nevaluation step using a larger model to evaluate the response. Each pair would\\nbe composed of Q, A, GT, and True/False to mark the evaluation.\\n\\n‚Ü≥ ùóôùóºùóø ùóöùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóºùóª ùóßùóÆùòÄùó∏ùòÄ You could log the query and response, and append in the\\nmetadata a few qualitative metrics (e.g. relevance, cohesiveness).\\n\\n‚Ü≥ùóôùóºùóø ùó•ùóîùóö If you have complex chains within your RAG application, you could log\\nprompt structures (sys_prompt, query), and LLM responses and track the chain\\nexecution step by step.\\n\\n‚Ü≥ ùóôùóºùóø ùó°ùóòùó• You could define the entity fields and log the query, response,\\nentities_list, and extracted_entities in the same prompt payload.\\n\\n‚Ü≥ùóôùóºùóø ùó©ùó∂ùòÄùó∂ùóºùóª ùóßùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùó≤ùóøùòÄ CometML LLM also allows you to log images associated\\nwith a prompt or a chain. If you‚Äôre working with GPT4-Vision for example, you\\ncould log the query and the generated image in the same payload.\\n\\nAlso, besides the actual prompt payload, you could inspect the processing time\\nper each step of a chain.\\n\\nFor example, a 3-step chain in an RAG application might query the Vector DB,\\ncompose the prompt, and pass it to the LLM, and when logging the chain to\\nCometML, you could see the processing time/chain step.\\n\\nüîπ ùóßùóº ùòÄùó≤ùòÅ ùó∂ùòÅ ùòÇùóΩ, ùòÜùóºùòÇ\\'ùóπùóπ ùóªùó≤ùó≤ùó±:\\n\\n\\\\- CometML pip package  \\n\\\\- CometML API key - Workspace name and Project Name\\n\\nI\\'ve used this approach when evaluating a fine-tuned LLM on a custom\\ninstruction dataset. For a detailed walkthrough üëá\\n\\nEvaluating LLMs Medium Article\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n10\\n\\nShare this post\\n\\n#### 2 Key LLMOps Concepts\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/2-key-llmops-concepts?r=1ttoeh'), ArticleDocument(id=UUID('87f34471-9a5b-4641-8272-15b6a18a9be7'), content={'Title': 'The LLM-Twin Free Course on Production-Ready RAG applications.', 'Subtitle': 'Learn how to build a full end-to-end LLM & RAG production-ready system, follow and code along each component by yourself.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\n### Learn how to build a full end-to-end LLM & RAG production-ready system,\\nfollow and code along each component by yourself.\\n\\nAlex Razvant\\n\\nJun 20, 2024\\n\\n13\\n\\nShare this post\\n\\n#### The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n‚Üí the **last lesson** of the LLM Twin free course\\n\\n**What is your LLM Twin?** It is an AI character that writes like yourself by\\nincorporating your style, personality, and voice into an LLM.\\n\\n**Decoding ML Newsletter** is a reader-supported publication. If you enjoy our\\nwork, please consider becoming a paid subscriber.\\n\\nSubscribe\\n\\nImage by DALL-E\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> _More**details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà_\\n\\n# **The LLM-Twin Free Course**\\n\\nThis course teaches you how to design, build, and deploy a production-ready\\nLLM-RAG system. It covers all the components, system design, data ingestion,\\nstreaming pipeline, fine-tuning pipeline, inference pipeline alongside\\nproduction monitoring, and more.\\n\\n## **What is the course about?**\\n\\nWe‚Äôre building a production-ready RAG system, able to write content based on\\nyour unique style, by scrapping previous posts/articles and code snippets\\nwritten by you to construct a fresh and continuously updated knowledge base,\\ngenerate a dataset to fine-tune a capable and efficient open-source LLM, and\\nthen interconnect all components for a full end-to-end deployment while\\nintegrating evaluation and post-deployment monitoring.\\n\\nThis course follows best MLOps & LLMOps practices, focusing on the 3-pipeline-\\ndesign pattern for building ML-centered applications.\\n\\n## **Lesson 1: Presenting the Architecture**\\n\\nPresenting and describing each component, the tooling used, and the intended\\nworkflow of implementation. The first lesson will prepare the ground by\\noffering a wide overview of each component and consideration.\\n\\n**We recommend you start here.**\\n\\nüîó **Lesson 1:** An End-to-End Framework for Production-Ready LLM Systems by\\nBuilding Your LLM Twin\\n\\nLLM twin system architecture [Image by the Author]\\n\\n## **Lesson 2: Data Pipelines**\\n\\nIn this lesson, we‚Äôll start by explaining what a data pipeline is, and the key\\nconcepts of data processing and streaming, and then dive into the data\\nscrapping and processing logic.\\n\\nüîó **Lesson 2:** The Importance of Data Pipelines in the Era of Generative AI\\n\\nLesson 2: The Data Collection Pipeline [Image by author]\\n\\n## **Lesson 3: Change Data Capture and Data Processing**\\n\\nIn this lesson, we‚Äôre showcasing the CDC(Change Data Capture) integration\\nwithin the LLM-Twin data pipeline. We‚Äôre showing how to set up MongoDB, the\\nCDC approach for event-driven processing, RabbitMQ for message queuing, and\\nefficient low-latency database querying using the MongoDB Oplog.\\n\\nüîó **Lesson 3:** CDC Enabling Event-Driven Architectures\\n\\nLesson 3: Event-Driven Processing using RabbitMQ, CDC, and MongoDB (Image by\\nAuthor)\\n\\n## **Lesson 4: Efficient Data Streaming Pipelines**\\n\\nIn this lesson, we‚Äôll focus on the feature pipeline. Here, we‚Äôre showcasing\\nhow we ingest data that we‚Äôve gathered in the previous lesson, and how we‚Äôve\\nbuilt a stream-processing workflow with **Bytewax **that fetches raw samples,\\nstructures them using Pydantic Models, cleans, chunks, encodes, and stores\\nthem in our **Qdrant** Vector Database.\\n\\nüîó **Lesson 4:** SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî\\nin Real-Time!\\n\\nLesson 4: Efficient Data Streaming Pipelines using Bytewax and Qdrant Vector\\nDB. (Image by Author)\\n\\n## **Lesson 5: Advanced RAG Optimization Techniques**\\n\\nIn this lesson, we‚Äôll showcase a few advanced techniques to increase the\\nsimilarity and accuracy of the embedded data samples from our **Qdrant**\\nVector Database. The contents of this lesson could make a significant\\ndifference between a naive RAG application and a production-ready one.\\n\\nüîó **Lesson 5:** The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\nLesson 5: Advanced RAG Optimization Techniques. (Image by Author)\\n\\n## **Lesson 6: Dataset preparation for LLM fine-tuning**\\n\\nIn this lesson, we‚Äôll discuss the core concepts to consider when creating\\ntask-specific custom datasets to fine-tune LLMs. We‚Äôll use our cleaned data\\nfrom our Vector Database, and engineer specific Prompt Templates alongside\\nusing GPT3.5-Turbo API to generate our custom dataset and version it on\\n**Comet ML**.\\n\\nüîó **Lesson 6:** The Role of Feature Stores in Fine-Tuning LLMs\\n\\nLesson 6: Generate custom datasets using Knowledge Distillation.\\n\\n## **Lesson 7: Fine-tuning LLMs on custom datasets**\\n\\nWe‚Äôll show how to implement a fine-tuning workflow for a Mistral7B-Instruct\\nmodel while using the custom dataset we‚Äôve versioned previously. We‚Äôll present\\nin-depth the key concepts including LoRA Adapters, PEFT, Quantisation, and how\\nto deploy on Qwak.\\n\\nüîó **Lesson 7:**How to fine-tune LLMs on custom datasets at Scale using Qwak\\nand CometML\\n\\nLesson 7: Fine-tuning LLMs on custom datasets using Qwak and CometML. (Image\\nby Author)\\n\\n## **Lesson 8: Evaluating the fine-tuned LLM**\\n\\nIn this lesson, we‚Äôre discussing one core concept of ML - **Evaluation**.  \\nWe‚Äôll present the evaluation workflow we‚Äôll showcase the full process of\\nassessing the model‚Äôs performance using the GPT3.5-Turbo model and custom-\\nengineered evaluation templates.\\n\\nüîó **Lesson 8:**Best Practices When Evaluating Fine-Tuned LLMs\\n\\nLesson 8: Evaluating the quality of our custom fine-tuned LLM. (Image by\\nAuthor)\\n\\n## **Lesson 9: Deploying the Inference Pipeline Stack**\\n\\nIn this lesson, we‚Äôll showcase how to design and implement the LLM & RAG\\ninference pipeline based on a set of detached Python microservices. We‚Äôll\\nsplit the ML and business logic into two components, describe each one in\\npart, and show how to wrap up and deploy the inference pipeline on **Qwak** as\\na scalable and reproducible system.\\n\\nüîó **Lesson 9:**Architect scalable and cost-effective LLM & RAG inference\\npipelines\\n\\nLesson 9: Architecturing LLM & RAG inference pipeline. (Image by Author)\\n\\n## **Lesson 10: RAG Pipeline Evaluation**\\n\\nIn this lesson, we‚Äôre covering RAG evaluation ‚Äî which is one of great\\nimportance. If no proper evaluation metrics are monitored or techniques are\\nused, the RAG systems might underperform and hallucinate badly.\\n\\nHere, we‚Äôll describe the workflow of evaluating RAG pipelines using the\\npowerful RAGAs framework, compose the expected RAGAs evaluation format, and\\ncapture eval scores which will be included in full LLM execution chains and\\nlogged on **Comet ML LLM**.\\n\\nüîó **Lesson 10:**Evaluating RAG Systems using the RAGAs Framework\\n\\nLesson 10: Evaluating the RAG pipeline. (Image by Author)\\n\\n### Next Steps\\n\\n#### Step 1\\n\\n**Check out** the **full versions** of all **Lessons 1-11** on our **Medium\\npublication** , under the LLM-Twin Course group tag. _It‚Äôs still FREE:_\\n\\nThe LLM-Twin Course\\n\\n#### Step 2\\n\\n‚Üí **Check out theLLM Twin GitHub repository and try it yourself ü´µ**\\n\\n _Nothing compares with getting your hands dirty and building it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### The LLM-Twin Free Course on Production-Ready RAG applications.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-llm-twin-free-course-on-production?r=1ttoeh'), ArticleDocument(id=UUID('d3cb26a9-45fe-42e0-9a79-7a2f358fc875'), content={'Title': 'A blueprint for designing production LLM systems: From Notebooks to production ', 'Subtitle': 'How to get a GitHub Copilot subscription for FREE (to 5x writing code). Learn to build production ML systems by building an LLM application.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\n### How to get a GitHub Copilot subscription for FREE (to 5x writing code).\\nLearn to build production ML systems by building an LLM application.\\n\\nPaul Iusztin\\n\\nJun 15, 2024\\n\\n13\\n\\nShare this post\\n\\n#### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * How to get a GitHub Copilot subscription for FREE (to 5x writing code)\\n\\n  * A blueprint for designing production LLM systems: From Notebooks to production\\n\\n  * Learn to build production ML systems by building an LLM application\\n\\n* * *\\n\\n### How to get a GitHub Copilot subscription for FREE (to 5x writing code)\\n\\nùóõùóºùòÑ to get a ùóöùó∂ùòÅùóõùòÇùóØ ùóñùóºùóΩùó∂ùóπùóºùòÅ ùòÄùòÇùóØùòÄùó∞ùóøùó∂ùóΩùòÅùó∂ùóºùóª for ùóôùó•ùóòùóò (to 5x writing code) ‚Üì  \\n  \\nThere are other alternatives, but GitHub Copilot is still the leading solution\\ndue to 2 factors: performance & convenience.  \\n  \\nIf you can get it for free, there are 0 reasons not to use it (sneaky move\\nMicrosoft) ‚Üì  \\n  \\nùó¶ùóº ùòÑùóµùóÆùòÅ ùó∂ùòÄ ùòÅùóµùó≤ ùòÄùóºùóπùòÇùòÅùó∂ùóºùóª?  \\n  \\nThere is no secret.  \\n  \\nAs stated in their docs: \"Verified students, teachers, and maintainers of\\npopular open source projects on GitHub are eligible to use Copilot Individual\\nfor free. \"  \\n  \\nüîó Docs  \\n  \\nTo become a student or teacher when you are not is not a solution.  \\n  \\nBut...  \\n  \\nTo become a maintainer of a popular open-source project is!\\n\\nùó¶ùóº ùòÑùóµùóÆùòÅ ùóÆùóøùó≤ ùòÅùóµùó≤ ùó∞ùóøùó∂ùòÅùó≤ùóøùó∂ùóÆ ùó≥ùóºùóø ùóØùó≤ùó∞ùóºùó∫ùó∂ùóªùó¥ ùóÆ \"ùó∫ùóÆùó∂ùóªùòÅùóÆùó∂ùóªùó≤ùóø ùóºùó≥ ùóÆ ùóΩùóºùóΩùòÇùóπùóÆùóø ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤\\nùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ\"?  \\n  \\nI don\\'t know the exact formula, but here are some examples.  \\n  \\nI am eligible for it because I am the owner of a GitHub repository with ~2.2k\\nstars & 350 forks: üîó Hands-on LLMs Course  \\n  \\nAfter digging into some Reddit threads, a dude said that for a repo with ~520\\nstars & 299 forks, you got the free subscription.  \\n  \\nThe idea is that you don\\'t have to be a maintainer of Pandas or PyTorch to\\nbecome eligible.  \\n  \\n.  \\n  \\nùóßùóµùó≤ ùó∞ùóºùóªùó∞ùóπùòÇùòÄùó∂ùóºùóª ùó∂ùòÄ ùòÅùóº...  \\n  \\n‚Üí start contributing to open-source or creating your cool project, which will\\ncomplete the job!  \\n  \\n.  \\n  \\nùòêùòß ùò∫ùò∞ùò∂ ùò£ùò¶ùòµùòµùò¶ùò≥ ùò¨ùòØùò∞ùò∏ ùòµùò©ùò¶ \"ùò¥ùò¶ùò§ùò≥ùò¶ùòµ ùòßùò∞ùò≥ùòÆùò∂ùò≠ùò¢/ùò§ùò≥ùò™ùòµùò¶ùò≥ùò™ùò¢,\" ùò±ùò≠ùò¶ùò¢ùò¥ùò¶ ùò≠ùò¶ùò¢ùò∑ùò¶ ùò™ùòµ ùò™ùòØ ùòµùò©ùò¶\\nùò§ùò∞ùòÆùòÆùò¶ùòØùòµùò¥ ùòßùò∞ùò≥ ùò∞ùòµùò©ùò¶ùò≥ùò¥ ùòµùò∞ ùò¨ùòØùò∞ùò∏.  \\n  \\nAlso, let me know if you know that when contributing to open-source, you must\\ncontribute by \"how much\" until you become eligible.\\n\\n* * *\\n\\n### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\nI am ùóæùòÇùó∂ùòÅùòÅùó∂ùóªùó¥ ùó∞ùóøùó≤ùóÆùòÅùó∂ùóªùó¥ ùó∞ùóºùóªùòÅùó≤ùóªùòÅ... ùóùùóºùó∏ùó∂ùóªùó¥, but here is ùóµùóºùòÑ to ùóØùòÇùó∂ùóπùó± your ùóüùóüùó†\\nùòÅùòÑùó∂ùóª for ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ posts or articles ùòÇùòÄùó∂ùóªùó¥ ùòÜùóºùòÇùóø ùòÉùóºùó∂ùó∞ùó≤ ‚Üì  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóÆùóª ùóüùóüùó† ùòÅùòÑùó∂ùóª?  \\n  \\nIt\\'s an AI character who writes like you, using your writing style and\\npersonality.  \\n  \\nùó™ùóµùòÜ ùóªùóºùòÅ ùó±ùó∂ùóøùó≤ùó∞ùòÅùóπùòÜ ùòÇùòÄùó≤ ùóñùóµùóÆùòÅùóöùó£ùóß? ùó¨ùóºùòÇ ùó∫ùóÆùòÜ ùóÆùòÄùó∏...  \\n  \\nWhen generating content using an LLM, the results tend to:  \\n  \\n\\\\- be very generic and unarticulated,  \\n\\\\- contain misinformation (due to hallucination),  \\n\\\\- require tedious prompting to achieve the desired result.  \\n  \\nùóßùóµùóÆùòÅ ùó∂ùòÄ ùòÑùóµùòÜ, ùó≥ùóºùóø ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ ùó∞ùóºùóªùòÅùó≤ùóªùòÅ, ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùóÆ ùòÄùóΩùó≤ùó∞ùó∂ùóÆùóπùó∂ùòáùó≤ùó± ùòÅùóºùóºùóπ ùòÅùóµùóÆùòÅ:  \\n  \\n‚Üí is fine-tuned on your digital content to replicate your persona  \\n  \\n‚Üí has access to a vector DB (with relevant data) to avoid hallucinating and\\nwrite only about concrete facts\\n\\nùóõùó≤ùóøùó≤ ùóÆùóøùó≤ ùòÅùóµùó≤ ùó∫ùóÆùó∂ùóª ùòÄùòÅùó≤ùóΩùòÄ ùóøùó≤ùóæùòÇùó∂ùóøùó≤ùó± ùòÅùóº ùóØùòÇùó∂ùóπùó± ùòÜùóºùòÇùóø ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÅùòÑùó∂ùóª:  \\n  \\n1\\\\. A data collection pipeline will gather your digital data from Medium,\\nSubstack, LinkedIn and GitHub. It will be normalized and saved to a Mongo DB.  \\n  \\n2\\\\. Using CDC, you listen to any changes made to the Mongo DB and add them as\\nevents to a RabbitMQ queue.  \\n  \\n3\\\\. A Bytewax streaming ingestion pipeline will listen to the queue to clean,\\nchunk, and embed the data in real time.  \\n  \\n4\\\\. The cleaned and embedded data is loaded to a Qdrant vector DB.  \\n  \\n5\\\\. On the training pipeline side, you use a vector DB retrieval client to\\nbuild your training dataset, which consists of the cleaned data (augmented\\nusing RAG).  \\n  \\n6\\\\. You fine-tune an open-source Mistral LLM using QLoRA and push all the\\nexperiment artifacts to a Comet experiment tracker.  \\n  \\n7\\\\. Based on the best experiment, you push the LLM candidate to Comet\\'s model\\nregistry. You carefully evaluate the LLM candidate using Comet\\'s prompt\\nmonitoring dashboard. If the evaluation passes, you tag it as accepted.  \\n  \\n8\\\\. On the inference pipeline side, you deploy the new LLM model by pulling it\\nfrom the model registry, loading it, and quantizing it.  \\n  \\n9\\\\. The inference pipeline is wrapped by a REST API, which allows users to\\nmake ChatGPT-like requests.\\n\\n* * *\\n\\n### Learn to build production ML systems by building an LLM application\\n\\nTaking in mind the _blueprint for designing production LLM systems presented\\nabove_ , we want to let you know that:\\n\\n_‚Üí We are close to wrapping our LLM twin course lessons and code._\\n\\nTo give more context for newcomers, in the past weeks we started ùóøùó≤ùóπùó≤ùóÆùòÄùó∂ùóªùó¥ an\\nùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó∞ùóºùòÇùóøùòÄùó≤ on ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª ùóüùóüùó†ùòÄ by teaching you how to ùóØùòÇùó∂ùóπùó± an ùóüùóüùó† ùòÅùòÑùó∂ùóª:\\nùò†ùò∞ùò∂ùò≥ ùòóùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ-ùòôùò¶ùò¢ùò•ùò∫ ùòàùòê ùòôùò¶ùò±ùò≠ùò™ùò§ùò¢\\n\\nSo‚Ä¶\\n\\nIf you are looking for an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùóôùó•ùóòùóò ùó∞ùóºùòÇùóøùòÄùó≤ on ùóµùóºùòÑ to ùóØùòÇùó∂ùóπùó± ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-\\nùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ, consider checking the course\\'s **first** FREE **lesson**.  \\n  \\nùòõùò©ùò¶ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùò∏ùò™ùò≠ùò≠ ùò∏ùò¢ùò≠ùò¨ ùò∫ùò∞ùò∂ ùòµùò©ùò≥ùò∞ùò∂ùò®ùò© ùò¢ ùòßùò∂ùò≠ùò≠-ùò¥ùòµùò¢ùò§ùò¨ ùò±ùò≥ùò∞ùò§ùò¶ùò¥ùò¥:  \\n  \\n‚Üí from data gathering...  \\n  \\n...until deploying and monitoring your LLM twin using LLMOps ‚Üê  \\n  \\n.  \\n  \\nWith that in mind...  \\n  \\nThe ùü≠ùòÄùòÅ ùóπùó≤ùòÄùòÄùóºùóª will walk you through:  \\n  \\n\\\\- the issues of generating content using ChatGPT (or other similar solutions)  \\n\\\\- the 3-pipeline design  \\n\\\\- the system design and architecture of the LLM twin  \\n  \\n.  \\n  \\nWithin the ùòÄùòÜùòÄùòÅùó≤ùó∫ ùó±ùó≤ùòÄùó∂ùó¥ùóª ùòÄùó≤ùó∞ùòÅùó∂ùóºùóª, we will present all the ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùóÆùóπ\\nùó±ùó≤ùó∞ùó∂ùòÄùó∂ùóºùóªùòÄ on ùóµùóºùòÑ to ùóØùòÇùó∂ùóπùó±:  \\n  \\n\\\\- a data collection pipeline  \\n\\\\- a real-time feature pipeline using a streaming engine  \\n\\\\- hook the data and feature pipelines using the CDC pattern  \\n\\\\- a continuous fine-tuning pipeline  \\n\\\\- an inference pipeline deployed as a REST API  \\n  \\n  \\nA ùóΩùóÆùóøùòÅùó∂ùó∞ùòÇùóπùóÆùóø ùó≥ùóºùó∞ùòÇùòÄ will be on ùó∂ùóªùòÅùó≤ùó¥ùóøùóÆùòÅùó∂ùóªùó¥ ùó†ùóüùó¢ùóΩùòÄ & ùóüùóüùó†ùó¢ùóΩùòÄ ùó¥ùóºùóºùó± ùóΩùóøùóÆùó∞ùòÅùó∂ùó∞ùó≤ùòÄ:  \\n  \\n\\\\- prompt versioning  \\n\\\\- model registries  \\n\\\\- experiment tracker  \\n\\\\- prompt monitoring  \\n\\\\- CI/CD  \\n\\\\- IaC  \\n\\\\- Docker  \\n  \\n.  \\n  \\nùôíùôñùô£ùô© ùô©ùô§ ùôôùôûùôú ùôûùô£ùô©ùô§ ùô©ùôùùôö 1ùô®ùô© ùô°ùôöùô®ùô®ùô§ùô£?  \\n  \\nùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ. It\\'s FREE, and no registration is required  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòìùò¶ùò¥ùò¥ùò∞ùòØ 1 - ùòàùòØ ùòåùòØùò•-ùòµùò∞-ùòåùòØùò• ùòçùò≥ùò¢ùòÆùò¶ùò∏ùò∞ùò≥ùò¨ ùòßùò∞ùò≥ ùòóùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ-ùòôùò¶ùò¢ùò•ùò∫ ùòìùòìùòî ùòöùò∫ùò¥ùòµùò¶ùòÆùò¥ ùò£ùò∫\\nùòâùò∂ùò™ùò≠ùò•ùò™ùòØùò® ùò†ùò∞ùò∂ùò≥ ùòìùòìùòî ùòõùò∏ùò™ùòØ\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### A blueprint for designing production LLM systems: From Notebooks to\\nproduction\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/a-blueprint-for-designing-production?r=1ttoeh'), ArticleDocument(id=UUID('9d858911-52d4-4240-8d6e-91f6b426baa0'), content={'Title': 'The difference between development and continuous training ML environments', 'Subtitle': 'Looking to become a PRO in LangChain? How to write a streaming retrieval system for RAG on social media data.', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The difference between development and continuous training ML\\nenvironments\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The difference between development and continuous training ML environments\\n\\n### Looking to become a PRO in LangChain? How to write a streaming retrieval\\nsystem for RAG on social media data.\\n\\nPaul Iusztin\\n\\nJun 08, 2024\\n\\n7\\n\\nShare this post\\n\\n#### The difference between development and continuous training ML\\nenvironments\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Looking to become a PRO in LangChain?\\n\\n  * The difference between development and continuous training ML environments\\n\\n  * How to write a streaming retrieval system for RAG on social media data\\n\\n* * *\\n\\n _**First** , I want to thank everyone who supported our Hands-on LLMs course\\nrepo_ üôèüèª\\n\\nThe ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ FREE ùó∞ùóºùòÇùóøùòÄùó≤ passed 2.1k+ ‚≠êÔ∏è on GitHub - the place to ùóπùó≤ùóÆùóøùóª\\nthe ùó≥ùòÇùóªùó±ùóÆùó∫ùó≤ùóªùòÅùóÆùóπùòÄ of ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ & ùóüùóüùó†ùó¢ùóΩùòÄ  \\n  \\nùòõùò©ùò¶ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùò™ùò¥ ùòµùò©ùò¶ ùò®ùò∞-ùòµùò∞ ùò©ùò∂ùò£ ùòßùò∞ùò≥ ùò≠ùò¶ùò¢ùò≥ùòØùò™ùòØùò® ùòµùò©ùò¶ ùòßùò∂ùòØùò•ùò¢ùòÆùò¶ùòØùòµùò¢ùò≠ùò¥ ùò∞ùòß ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµùò™ùò∞ùòØ-ùò≥ùò¶ùò¢ùò•ùò∫\\nùòìùòìùòîùò¥ & ùòìùòìùòîùòñùò±ùò¥  \\n  \\nIt will walk you through an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùóΩùóøùóºùó∞ùó≤ùòÄùòÄ...  \\n  \\n...from data preparation to deployment & monitoring:  \\n  \\n\\\\- the 3-pipeline design  \\n\\\\- building your custom financial dataset using GPT-4  \\n\\\\- a streaming pipeline to ingest financial news in real-time  \\n\\\\- fine-tuning an LLM using QLoRA  \\n\\\\- building a custom RAG pipeline  \\n\\\\- deploying the streaming pipeline to AWS  \\n\\\\- deploying the training & inference pipelines to Beam  \\n\\\\- using MLOps components: model registries, experiment trackers, prompt\\nmonitoring  \\n  \\n\\nùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùòèùò¢ùòØùò•ùò¥-ùò∞ùòØ ùòìùòìùòîùò¥ ùòäùò∞ùò∂ùò≥ùò¥ùò¶ - ùòìùò¶ùò¢ùò≥ùòØ ùòµùò∞ ùòõùò≥ùò¢ùò™ùòØ ùò¢ùòØùò• ùòãùò¶ùò±ùò≠ùò∞ùò∫ ùò¢ ùòôùò¶ùò¢ùò≠-ùòõùò™ùòÆùò¶ ùòçùò™ùòØùò¢ùòØùò§ùò™ùò¢ùò≠\\nùòàùò•ùò∑ùò™ùò¥ùò∞ùò≥\\n\\n* * *\\n\\n### Looking to become a PRO in LangChain?\\n\\nThen ùó∞ùóµùó≤ùó∞ùó∏ ùóºùòÇùòÅ this ùóØùóºùóºùó∏ on ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóÆùóªùó¥ùóñùóµùóÆùó∂ùóª: from ùóØùó≤ùó¥ùó∂ùóªùóªùó≤ùóø to ùóÆùó±ùòÉùóÆùóªùó∞ùó≤ùó± ‚Üì  \\n  \\n‚Üí It's called: ùòéùò¶ùòØùò¶ùò≥ùò¢ùòµùò™ùò∑ùò¶ ùòàùòê ùò∏ùò™ùòµùò© ùòìùò¢ùòØùò®ùòäùò©ùò¢ùò™ùòØ: ùòâùò∂ùò™ùò≠ùò• ùòìùòìùòî ùò¢ùò±ùò±ùò¥ ùò∏ùò™ùòµùò© ùòóùò∫ùòµùò©ùò∞ùòØ,\\nùòäùò©ùò¢ùòµùòéùòóùòõ, ùò¢ùòØùò• ùò∞ùòµùò©ùò¶ùò≥ ùòìùòìùòîùò¥ by Ben Auffarth , published by Packt  \\n  \\nùòèùò¶ùò≥ùò¶ ùò™ùò¥ ùò¢ ùò¥ùò©ùò∞ùò≥ùòµ ùò£ùò≥ùò¶ùò¢ùò¨ùò•ùò∞ùò∏ùòØ:  \\n  \\n\\\\- It begins with some theoretical chapters on LLMs & LangChain  \\n  \\n\\\\- It explores the critical components of LangChain: chains, agents, memory,\\ntools  \\n  \\nùóßùóµùó≤ùóª, ùó∫ùòÜ ùó≥ùóÆùòÉùóºùóøùó∂ùòÅùó≤ ùóΩùóÆùóøùòÅ...  \\n  \\nùóúùòÅ ùó∑ùòÇùó∫ùóΩùòÄ ùó±ùó∂ùóøùó≤ùó∞ùòÅùóπùòÜ ùó∂ùóªùòÅùóº ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ - ùó™ùóúùóßùóõ ùó£ùó¨ùóßùóõùó¢ùó° ùóñùó¢ùóóùóò ‚Üì  \\n  \\n\\\\- takes off with beginner-friendly examples of using LangChain with agents,\\nHuggingFace, GCP/VertexAI, Azure, Anthropic, etc.  \\n  \\n\\\\- shows an end-to-end example of building a customer services application\\nwith LangChain & VertexAI  \\n  \\n\\\\- how to mitigate hallucinations using the ùòìùòìùòîùòäùò©ùò¶ùò§ùò¨ùò¶ùò≥ùòäùò©ùò¢ùò™ùòØ class  \\n  \\n\\\\- how to implement map-reduce pipelines  \\n  \\n\\\\- how to monitor token usage & costs  \\n  \\n\\\\- how to extract information from documents such as PDFs  \\n  \\n\\\\- building a Streamlit interface  \\n  \\n\\\\- how reasoning works in agent  \\n  \\n\\\\- building a chatbot like ChatGPT from SCRATCH  \\n  \\n.  \\n  \\nI haven't finished it yet, but I love it so far ‚ÄîI plan to finish it soon.  \\n  \\n.  \\n  \\nùó™ùóµùóº ùó∂ùòÄ ùòÅùóµùó∂ùòÄ ùó≥ùóºùóø?  \\n  \\nIf you are ùòÄùòÅùóÆùóøùòÅùó∂ùóªùó¥ ùóºùòÇùòÅ in the LLM world, this is a great book to ùóøùó≤ùóÆùó± ùó≤ùóªùó±-ùòÅùóº-\\nùó≤ùóªùó±.  \\n  \\nEven if you are ùó≤ùòÖùóΩùó≤ùóøùó∂ùó≤ùóªùó∞ùó≤ùó±, I think it is ùó≤ùòÖùòÅùóøùó≤ùó∫ùó≤ùóπùòÜ ùòÇùòÄùó≤ùó≥ùòÇùóπ to ùòÄùó∏ùó∂ùó∫ ùó∂ùòÅ to\\nrefresh the fundamentals, learn new details, and see how everything is\\nimplemented in LangChain.\\n\\nGenerative AI with LangChain [By Ben Auffarth]\\n\\nùóúùòÄ ùòÅùóµùó∂ùòÄ ùó≥ùóºùóø ùòÜùóºùòÇ? ü´µ  \\n  \\nüîó ùóñùóµùó≤ùó∞ùó∏ ùó∂ùòÅ ùóºùòÇùòÅ: Generative AI with LangChain [By Ben Auffarth]\\n\\n* * *\\n\\n### The difference between development and continuous training ML environments\\n\\nThey might do the same thing, but their design is entirely different ‚Üì  \\n  \\nùó†ùóü ùóóùó≤ùòÉùó≤ùóπùóºùóΩùó∫ùó≤ùóªùòÅ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nAt this point, your main goal is to ingest the raw and preprocessed data\\nthrough versioned artifacts (or a feature store), analyze it & generate as\\nmany experiments as possible to find the best:  \\n\\\\- model  \\n\\\\- hyperparameters  \\n\\\\- augmentations  \\n  \\nBased on your business requirements, you must maximize some specific metrics,\\nfind the best latency-accuracy trade-offs, etc.  \\n  \\nYou will use an experiment tracker to compare all these experiments.  \\n  \\nAfter you settle on the best one, the output of your ML development\\nenvironment will be:  \\n\\\\- a new version of the code  \\n\\\\- a new version of the configuration artifact  \\n  \\nHere is where the research happens. Thus, you need flexibility.  \\n  \\nThat is why we decouple it from the rest of the ML systems through artifacts\\n(data, config, & code artifacts).\\n\\nThe difference between ML development & continuous training environments\\n\\nùóñùóºùóªùòÅùó∂ùóªùòÇùóºùòÇùòÄ ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nHere is where you want to take the data, code, and config artifacts and:  \\n  \\n\\\\- train the model on all the required data  \\n\\\\- output a staging versioned model artifact  \\n\\\\- test the staging model artifact  \\n\\\\- if the test passes, label it as the new production model artifact  \\n\\\\- deploy it to the inference services  \\n  \\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub\\nActions):  \\n  \\n\\\\- builds a docker image from the code artifact (e.g., triggered manually or\\nwhen a new artifact version is created)  \\n\\\\- start the training pipeline inside the docker container that pulls the\\nfeature and config artifacts and outputs the staging model artifact  \\n\\\\- manually look over the training report -> If everything went fine, manually\\ntrigger the testing pipeline  \\n\\\\- manually look over the testing report -> if everything worked fine (e.g.,\\nthe model is better than the previous one), manually trigger the CD pipeline\\nthat deploys the new model to your inference services  \\n  \\nNote how the model registry quickly helps you to decouple all the components.  \\n  \\nAlso, because training and testing metrics are not always black and white, it\\nis challenging to automate the CI/CD pipeline 100%.  \\n  \\nThus, you need a human in the loop when deploying ML models.  \\n  \\nTo conclude...  \\n  \\nThe ML development environment is where you do your research to find better\\nmodels.  \\n  \\nThe continuous training environment is used to train & test the production\\nmodel at scale.\\n\\n* * *\\n\\n### How to write a streaming retrieval system for RAG on social media data\\n\\nùóïùóÆùòÅùó∞ùóµ ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ are the ùóΩùóÆùòÄùòÅ. Here is how to ùòÑùóøùó∂ùòÅùó≤ a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫\\nfor ùó•ùóîùóö on ùòÄùóºùó∞ùó∂ùóÆùóπ ùó∫ùó≤ùó±ùó∂ùóÆ ùó±ùóÆùòÅùóÆ ‚Üì  \\n  \\nùó™ùóµùòÜ ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóºùòÉùó≤ùóø ùóØùóÆùòÅùó∞ùóµ?  \\n  \\nIn environments where data evolves quickly (e.g., social media platforms), the\\nsystem's response time is critical for your application's user experience.  \\n  \\nThat is why TikTok is so addicting. Its recommender system adapts in real-time\\nbased on your interaction with the app.  \\n  \\nHow would it be if the recommendations were updated daily or hourly?  \\n  \\nWell, it would work, but you would probably get bored of the app much faster.  \\n  \\nThe same applies to RAG for highly intensive data sources...  \\n  \\n‚Üí where you must sync your source and vector DB in real time for up-to-date\\nretrievals.  \\n  \\nùòìùò¶ùòµ'ùò¥ ùò¥ùò¶ùò¶ ùò©ùò∞ùò∏ ùò™ùòµ ùò∏ùò∞ùò≥ùò¨ùò¥.  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nI wrote an ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ on how to ùóØùòÇùó∂ùóπùó± a ùóøùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫ for ùó•ùóîùóö on\\nùóüùó∂ùóªùó∏ùó≤ùó±ùóúùóª ùó±ùóÆùòÅùóÆ in collaboration with Superlinked .  \\n  \\nThe ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫ is based on ùüÆ ùó±ùó≤ùòÅùóÆùó∞ùóµùó≤ùó± ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅùòÄ:  \\n\\\\- the streaming ingestion pipeline  \\n\\\\- the retrieval client  \\n  \\nThe ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó∂ùóªùó¥ùó≤ùòÄùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ runs 24/7 to keep the vector DB synced with\\nthe current raw LinkedIn posts data source.  \\n  \\nThe ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùó∞ùóπùó∂ùó≤ùóªùòÅ is used in RAG applications to query the vector DB.  \\n  \\n‚Üí These 2 components are completely decoupled and communicate with each other\\nthrough the vector DB.  \\n  \\n#ùü≠. ùóßùóµùó≤ ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùó∂ùóªùó¥ùó≤ùòÄùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n‚Üí Implemented in Bytewax \\\\- a streaming engine built in Rust (speed&\\nreliability) that exposes a Python interface  \\n  \\nùòîùò¢ùò™ùòØ ùòßùò≠ùò∞ùò∏:  \\n  \\n\\\\- uses CDC to add changes from the source DB to a queue  \\n\\\\- listens to the queue for new events  \\n\\\\- cleans, chunks, and embeds the LI posts  \\n\\\\- loads them to a Qdrant vector DB  \\n  \\nand... everything in real-time!\\n\\nAdvanced RAG architecture [source from Superlinked Vectorhub]\\n\\n#ùüÆ. ùóßùóµùó≤ ùóøùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùó∞ùóπùó∂ùó≤ùóªùòÅ  \\n  \\n‚Üí A standard Python module.  \\n  \\nThe goal is to retrieve similar posts using various query types, such as\\nposts, questions, and sentences.  \\n  \\nùòîùò¢ùò™ùòØ ùòßùò≠ùò∞ùò∏:  \\n  \\n\\\\- preprocess user queries (the same way as they were ingested)  \\n\\\\- search the Qdrant vector DB for the most similar results  \\n\\\\- use rerank to improve the retrieval system's accuracy  \\n\\\\- visualize the results on a 2D plot using UMAP  \\n  \\n.  \\n  \\nYou don't believe me? ü´µ  \\n  \\nùóñùóµùó≤ùó∞ùó∏ ùóºùòÇùòÅ ùòÅùóµùó≤ ùó≥ùòÇùóπùóπ ùóÆùóøùòÅùó∂ùó∞ùóπùó≤ & ùó∞ùóºùó±ùó≤ ùóºùóª ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ‚Üì  \\n  \\nüîó ùòà ùòôùò¶ùò¢ùò≠-ùòµùò™ùòÆùò¶ ùòôùò¶ùòµùò≥ùò™ùò¶ùò∑ùò¢ùò≠ ùòöùò∫ùò¥ùòµùò¶ùòÆ ùòßùò∞ùò≥ ùòôùòàùòé ùò∞ùòØ ùòöùò∞ùò§ùò™ùò¢ùò≠ ùòîùò¶ùò•ùò™ùò¢ ùòãùò¢ùòµùò¢\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n7\\n\\nShare this post\\n\\n#### The difference between development and continuous training ML\\nenvironments\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-difference-between-development?r=1ttoeh'), ArticleDocument(id=UUID('20beb560-6063-4158-b7b5-c2083b299ec5'), content={'Title': 'Architect LLM & RAG inference pipelines - by Paul Iusztin', 'Subtitle': 'Design, build, deploy and monitor LLM and RAG inference pipelines using LLMOps best practices. Integrate it with a model registry and vector DB.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\n### Design, build and deploy RAG inference pipeline using LLMOps best\\npractices.\\n\\nPaul Iusztin\\n\\nJun 06, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n‚Üí the **9th** out of **11 lessons** of the **LLM Twin free course**\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> _More**details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà_\\n\\n### Latest Lessons of the LLM Twin Course\\n\\n**Lesson 6:** The Role of Feature Stores in Fine-Tuning LLMs\\n\\n‚Üí Custom Dataset Generation, Artifact Versioning, GPT3.5-Turbo Distillation,\\nQdrant\\n\\n**Lesson 7:** How to fine-tune LLMs on custom datasets at Scale using Qwak and\\nCometML\\n\\n‚ÜíQLoRA, PEFT, Fine-tuning Mistral-7b-Instruct on custom dataset, Qwak, Comet\\nML\\n\\n**Lesson 8:** Best practices when evaluating fine-tuned LLM models\\n\\n‚Üí LLM Evaluation techniques: Does and don‚Äôts, Quantitive and manual LLM\\nevaluation techniques\\n\\n* * *\\n\\n## **Lesson 9: Architect scalable and cost-effective LLM & RAG inference\\npipelines**\\n\\nIn **Lesson 9,** we will focus on implementing and deploying the inference\\npipeline of the LLM twin system.\\n\\n**First** , we will design and implement a scalable LLM & RAG inference\\npipeline based on microservices, separating the ML and business logic into two\\nlayers.\\n\\n**Secondly** , we will use Comet ML to integrate a prompt monitoring service\\nto capture all input prompts and LLM answers for further debugging and\\nanalysis.\\n\\n**Ultimately** , we will deploy the inference pipeline to Qwak and make the\\nLLM twin service available worldwide.\\n\\n#### **‚Üí Context from previous lessons. What you must know.**\\n\\nThis lesson is part of a more extensive series in which we learn to build an\\nend-to-end LLM system using LLMOps best practices.\\n\\n_If you haven‚Äôt read the whole series, for this one to make sense, you have to\\nknow that we have a:_\\n\\n  * Qdrant vector DB populated with digital data (posts, articles, and code snippets)\\n\\n  * vector DB retrieval module to do advanced RAG\\n\\n  * fine-tuned open-source LLM available in a model registry from Comet ML\\n\\n>  _‚Üí In this lesson, we will focus on gluing everything together into a\\n> scalable inference pipeline and deploying it to the cloud._\\n\\n* * *\\n\\n### **Table of Contents**\\n\\n  1. The architecture of the inference pipeline\\n\\n  2. The training vs. the inference pipeline\\n\\n  3. The RAG business module\\n\\n  4. The LLM microservice\\n\\n  5. Prompt monitoring\\n\\n  6. Deploying and running the inference pipeline\\n\\n  7. Conclusion\\n\\n* * *\\n\\n## 1\\\\. The architecture of the inference pipeline\\n\\nOur inference pipeline contains the following core elements:\\n\\n  * a fine-tuned LLM\\n\\n  * a RAG module\\n\\n  * a monitoring service\\n\\nLet‚Äôs see how to hook these into a scalable and modular system.\\n\\n### **The interface of the inference pipeline**\\n\\nAs we follow the feature/training/inference (FTI) pipeline architecture, the\\ncommunication between the 3 core components is clear.\\n\\nOur LLM inference pipeline needs 2 things:\\n\\n  * a fine-tuned LLM: pulled from the model registry\\n\\n  * features for RAG: pulled from a vector DB (which we modeled as a logical feature store)\\n\\nThis perfectly aligns with the FTI architecture.\\n\\n> _‚Üí If you are unfamiliar with the FTI pipeline architecture, we recommend\\n> you reviewLesson 1‚Äôs section on the 3-pipeline architecture._\\n\\n### **Monolithic vs. microservice inference pipelines**\\n\\nUsually, the inference steps can be split into 2 big layers:\\n\\n  * t**he LLM service:** where the actual inference is being done\\n\\n  * **the business service:** domain-specific logic\\n\\nWe can design our inference pipeline in 2 ways.\\n\\n#### **Option 1: Monolithic LLM & business service**\\n\\nIn a monolithic scenario, we implement everything into a single service.\\n\\n_Pros:_\\n\\n  * easy to implement\\n\\n  * easy to maintain\\n\\n _Cons:_\\n\\n  * harder to scale horizontally based on the specific requirements of each component\\n\\n  * harder to split the work between multiple teams\\n\\n  * not being able to use different tech stacks for the two services\\n\\nMonolithic vs. microservice inference pipelines\\n\\n#### **Option 2: Different LLM & business microservices**\\n\\nThe LLM and business services are implemented as two different components that\\ncommunicate with each other through the network, using protocols such as REST\\nor gRPC.\\n\\n_Pros:_\\n\\n  * each component can scale horizontally individually\\n\\n  * each component can use the best tech stack at hand\\n\\n _Cons:_\\n\\n  * harder to deploy\\n\\n  * harder to maintain\\n\\nLet‚Äôs focus on the ‚Äúeach component can scale individually‚Äù part, as this is\\nthe most significant benefit of the pattern. Usually, LLM and business\\nservices require different types of computing. For example, an LLM service\\ndepends heavily on GPUs, while the business layer can do the job only with a\\nCPU.\\n\\n### **Microservice architecture of the LLM twin inference pipeline**\\n\\nLet‚Äôs understand how we applied the microservice pattern to our concrete LLM\\ntwin inference pipeline.\\n\\nAs explained in the sections above, we have the following components:\\n\\n  1. A business microservice\\n\\n  2. An LLM microservice\\n\\n  3. A prompt monitoring microservice\\n\\n**The business microservice** is implemented as a Python module that:\\n\\n  * contains the advanced RAG logic, which calls the vector DB and GPT-4 API for advanced RAG operations;\\n\\n  * calls the LLM microservice through a REST API using the prompt computed utilizing the user‚Äôs query and retrieved context\\n\\n  * sends the prompt and the answer generated by the LLM to the prompt monitoring microservice.\\n\\nAs you can see, the business microservice is light. It glues all the domain\\nsteps together and delegates the computation to other services.\\n\\nThe end goal of the business layer is to act as an interface for the end\\nclient. In our case, as we will ship the business layer as a Python module,\\nthe client will be a Streamlit application.\\n\\nHowever, you can quickly wrap the Python module with FastAPI and expose it as\\na REST API to make it accessible from the cloud.\\n\\nMicroservice architecture of the LLM twin inference pipeline\\n\\n**The LLM microservice** is deployed on Qwak. This component is wholly niched\\non hosting and calling the LLM. It runs on powerful GPU-enabled machines.\\n\\nHow does the LLM microservice work?\\n\\n  * It loads the fine-tuned LLM twin model from Comet‚Äôs model registry [2].\\n\\n  * It exposes a REST API that takes in prompts and outputs the generated answer.\\n\\n  * When the REST API endpoint is called, it tokenizes the prompt, passes it to the LLM, decodes the generated tokens to a string and returns the answer.\\n\\nThat‚Äôs it!\\n\\n**The prompt monitoring microservice** is based on Comet ML‚Äôs LLM dashboard.\\nHere, we log all the prompts and generated answers into a centralized\\ndashboard that allows us to evaluate, debug, and analyze the accuracy of the\\nLLM.\\n\\n## **2\\\\. The training vs. the inference pipeline**\\n\\nAlong with the obvious reason that the training pipeline takes care of\\ntraining while the inference pipeline takes care of inference (Duh!), there\\nare some critical differences you have to understand.\\n\\n### **The input of the pipeline & How the data is accessed**\\n\\nDo you remember our logical feature store based on the Qdrant vector DB and\\nComet ML artifacts? If not, consider checking out Lesson 6 for a refresher.\\n\\nThe core idea is that **during training** , the data is accessed from an\\noffline data storage in batch mode, optimized for throughput and data lineage.\\n\\nOur LLM twin architecture uses Comet ML artifacts to access, version, and\\ntrack all our data.\\n\\nThe data is accessed in batches and fed to the training loop.\\n\\n**During inference** , you need an online database optimized for low latency.\\nAs we directly query the Qdrant vector DB for RAG, that fits like a glove.\\n\\nDuring inference, you don‚Äôt care about data versioning and lineage. You just\\nwant to access your features quickly for a good user experience.\\n\\nThe data comes directly from the user and is sent to the inference logic.\\n\\nThe training vs. the inference pipeline\\n\\n### **The output of the pipeline**\\n\\nThe **training pipeline‚Äôs** final output is the trained weights stored in\\nComet‚Äôs model registry.\\n\\nThe **inference pipeline‚Äôs** final output is the predictions served directly\\nto the user.\\n\\n### **The infrastructure**\\n\\nThe training pipeline requires more powerful machines with as many GPUs as\\npossible.\\n\\n_Why?_ During training, you batch your data and have to hold in memory all the\\ngradients required for the optimization steps. Because of the optimization\\nalgorithm, the training is more compute-hungry than the inference.\\n\\nThus, more computing and VRAM result in bigger batches, which means less\\ntraining time and more experiments.\\n\\nIf you run a batch pipeline, you will still pass batches to the model but\\ndon‚Äôt perform any optimization steps.\\n\\nIf you run a real-time pipeline, as we do in the LLM twin architecture, you\\npass a single sample to the model or do some dynamic batching to optimize your\\ninference step.\\n\\n### **Are there any overlaps?**\\n\\nYes! This is where the training-serving skew comes in.\\n\\nTo avoid the training-serving skew, you must carefully apply the same\\npreprocessing and postprocessing steps during training and inference.\\n\\n## **3\\\\. The RAG business module**\\n\\nWe will define the RAG business module under the _LLMTwin_ class. The LLM twin\\nlogic is directly correlated with our business logic.\\n\\nWe don‚Äôt have to introduce the word ‚Äúbusiness‚Äù in the naming convention of the\\nclasses.\\n\\nLet‚Äôs dig into the _generate()_ method of the _LLMTwin_ class, where we:\\n\\n  * call the RAG module;\\n\\n  * create the prompt using the prompt template, query and context;\\n\\n  * call the LLM microservice;\\n\\n  * log the prompt, prompt template, and answer to Comet ML‚Äôs prompt monitoring service.\\n\\nInference pipeline business module: generate() method ‚Üí GitHub ‚Üê\\n\\nLet‚Äôs look at how our LLM microservice is implemented using Qwak.\\n\\n## **4\\\\. The LLM microservice**\\n\\nAs the LLM microservice is deployed on Qwak, we must first inherit from the\\n_QwakModel_ class and implement some specific functions.\\n\\n  * _initialize_model()_ : where we load the fine-tuned model from the model registry at serving time\\n\\n  *  _schema():_ where we define the input and output schema\\n\\n  *  _predict()_ : where we implement the actual inference logic\\n\\n**Note:** The _build()_ function contains all the training logic, such as\\nloading the dataset, training the LLM, and pushing it to a Comet experiment.\\nTo see the full implementation, consider checking out Lesson 7, where we\\ndetailed the training pipeline.\\n\\nLLM microservice ‚Üí GitHub ‚Üê\\n\\nLet‚Äôs zoom into the implementation and the life cycle of the Qwak model.\\n\\nThe _schema()_ method is used to define how the input and output of the\\n_predict()_ method look like. This will automatically validate the structure\\nand type of the _predict()_ method. For example, the LLM microservice will\\nthrow an error if the variable instruction is a JSON instead of a string.\\n\\nThe other Qwak-specific methods are called in the following order:\\n\\n  1. ___init__()_ ‚Üí when deploying the model\\n\\n  2.  _initialize_model()_ ‚Üí when deploying the model\\n\\n  3.  _predict()_ ‚Üí on every request to the LLM microservice\\n\\n**> >>** Note that these methods are called only during serving time (and not\\nduring training).\\n\\nQwak exposes your model as a RESTful API, where the _predict()_ method is\\ncalled on each request.\\n\\nInside the prediction method, we perform the following steps:\\n\\n  * map the input text to token IDs using the LLM-specific tokenizer\\n\\n  * move the token IDs to the provided device (GPU or CPU)\\n\\n  * pass the token IDs to the LLM and generate the answer\\n\\n  * extract only the generated tokens from the _generated_ids_ variable by slicing it using the shape of the _input_ids_\\n\\n  * decode the _generated_ids_ back to text\\n\\n  * return the generated text\\n\\nThe final step is to look at Comet‚Äôs prompt monitoring service. ‚Üì\\n\\n## **5\\\\. Prompt monitoring**\\n\\nComet makes prompt monitoring straightforward. There is just one API call\\nwhere you connect to your project and workspace and send the following to a\\nsingle function:\\n\\n  * the prompt and LLM output\\n\\n  * the prompt template and variables that created the final output\\n\\n  * your custom metadata specific to your use case ‚Äî here, you add information about the model, prompt token count, token generation costs, latency, etc.\\n\\n    \\n    \\n    class PromptMonitoringManager:\\n        @classmethod\\n        def log(\\n            cls, prompt: str, output: str,\\n            prompt_template: str | None = None,\\n            prompt_template_variables: dict | None = None,\\n            metadata: dict | None = None,\\n        ) -> None:\\n            metadata = {\\n                \"model\": settings.MODEL_TYPE,\\n                **metadata,\\n            } or {\"model\": settings.MODEL_TYPE}\\n    \\n            comet_llm.log_prompt(\\n                workspace=settings.COMET_WORKSPACE,\\n                project=f\"{settings.COMET_PROJECT}-monitoring\",\\n                api_key=settings.COMET_API_KEY,\\n                prompt=prompt, prompt_template=prompt_template,\\n                prompt_template_variables=prompt_template_variables,\\n                output=output, metadata=metadata,\\n            )\\n\\nThis is how Comet ML‚Äôs prompt monitoring dashboard looks. Here, you can scroll\\nthrough all the prompts that were ever sent to the LLM. ‚Üì\\n\\nYou can click on any prompt and see everything we logged programmatically\\nusing the _PromptMonitoringManager_ class.\\n\\nScreenshot from Comet ML‚Äôs dashboard\\n\\nBesides what we logged, adding various tags and the inference duration can be\\nvaluable.\\n\\n## **6\\\\. Deploying and running the inference pipeline**\\n\\nWe can deploy the LLM microservice using the following Qwak command:\\n\\n    \\n    \\n    qwak models deploy realtime \\\\\\n    --model-id \"llm_twin\" \\\\\\n    --instance \"gpu.a10.2xl\" \\\\ \\n    --timeout 50000 \\\\ \\n    --replicas 2 \\\\\\n    --server-workers 2\\n\\nWe deployed two replicas of the LLM twin. Each replica has access to a machine\\nwith x1 A10 GPU. Also, each replica has two workers running on it.\\n\\nüîó More on Qwak instance types ‚Üê\\n\\nTwo replicas and two workers result in 4 microservices that run in parallel\\nand can serve our users.\\n\\nYou can scale the deployment to more replicas if you need to serve more\\nclients. Qwak provides autoscaling mechanisms triggered by listening to the\\nconsumption of GPU, CPU or RAM.\\n\\nTo conclude, you build the Qwak model once, and based on it, you can make\\nmultiple deployments with various strategies.\\n\\n* * *\\n\\n## **Conclusion**\\n\\n _Congratulations! You are close to the end of the LLM twin series._\\n\\nIn **Lesson 9** of the LLM twin course, you learned to **build** a scalable\\ninference pipeline for serving LLMs and RAG systems.\\n\\n**First** , you learned how to architect an inference pipeline by\\nunderstanding the difference between monolithic and microservice\\narchitectures. We also highlighted the difference in designing the training\\nand inference pipelines.\\n\\n**Secondly** , we walked you through implementing the RAG business module and\\nLLM twin microservice. Also, we showed you how to log all the prompts,\\nanswers, and metadata for Comet‚Äôs prompt monitoring service.\\n\\n**Ultimately** , we showed you how to deploy and run the LLM twin inference\\npipeline on the Qwak AI platform.\\n\\nIn **Lesson 10** , we will show you how to evaluate the whole system by\\nbuilding an advanced RAG evaluation pipeline that analyzes the accuracy of the\\nLLMs ‚Äô answers relative to the query and context.\\n\\nSee you there! ü§ó\\n\\n>  _üîó**Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è_\\n\\n* * *\\n\\n### Next Steps\\n\\n#### Step 1\\n\\nThis is just the **short version** of **Lesson 9** on **architecting scalable\\nand cost-effective LLM & RAG inference pipelines.**\\n\\n‚Üí For‚Ä¶\\n\\n  * The full implementation.\\n\\n  * Full deep dive into the code.\\n\\n  * More on the RAG, LLM and monitoring services.\\n\\n**Check out** the **full version** of **Lesson 9** on our **Medium\\npublication**. It‚Äôs still FREE:\\n\\nLesson 9 on Medium\\n\\n#### Step 2\\n\\n‚Üí **Consider checking out theLLM Twin GitHub repository and try it yourself\\nü´µ**\\n\\n _Nothing compares with getting your hands dirty and doing it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n13\\n\\nShare this post\\n\\n#### Architect scalable and cost-effective LLM & RAG inference pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/architect-scalable-and-cost-effective?r=1ttoeh'), ArticleDocument(id=UUID('95d64d1d-83f2-47e9-8eda-9a687b98e6eb'), content={'Title': '7 tips to reduce your VRAM when training LLMs ', 'Subtitle': '3 techniques you must know to evaluate your LLMs. Introduction to deploying private LLMs with AWS SageMaker.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### 7 tips to reduce your VRAM when training LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# 7 tips to reduce your VRAM when training LLMs\\n\\n### 3 techniques you must know to evaluate your LLMs. Introduction to\\ndeploying private LLMs with AWS SageMaker.\\n\\nPaul Iusztin\\n\\nMay 18, 2024\\n\\n4\\n\\nShare this post\\n\\n#### 7 tips to reduce your VRAM when training LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * 3 techniques you must know to evaluate your LLMs\\n\\n  * 7 tips you must know to reduce your VRAM consumption of your LLMs during training\\n\\n  * Introduction to deploying private LLMs with AWS SageMaker\\n\\n* * *\\n\\nOn the 3rd of May, I ùóµùóºùòÄùòÅùó≤ùó± a ùó≥ùóøùó≤ùó≤ ùòÄùó≤ùòÄùòÄùó∂ùóºùóª on Maven for ùüµùü∞ ùóΩùó≤ùóºùóΩùóπùó≤ on how to\\nùóîùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅ ùó¨ùóºùòÇùóø ùóüùóüùó† ùóßùòÑùó∂ùóª. If you missed it, here is ùóµùóºùòÑ you can ùóÆùó∞ùó∞ùó≤ùòÄùòÄ ùó∂ùòÅ for\\nùó≥ùóøùó≤ùó≤ ‚Üì  \\n  \\n.  \\n  \\nùòíùò¶ùò∫ ùòµùò¢ùò¨ùò¶ùò¢ùò∏ùò¢ùò∫ùò¥ ùò∏ùò¶ùò≥ùò¶:  \\n  \\n‚Üí Why I started building my LLM Twin  \\n  \\n‚Üí The 3 pipeline design / The FTI pipeline architecture  \\n  \\n‚Üí System design of the LLM Twin Architecture  \\n  \\n‚Üí Break down the RAG system of the LLM Twin Architecture  \\n  \\n‚Üí Live Demo  \\n  \\n.  \\n  \\nIf you want the recording, you can watch it for free here:\\nhttps://bit.ly/3PZGV0S  \\n  \\nùòàùò≠ùò¥ùò∞, ùò©ùò¶ùò≥ùò¶ ùò¢ùò≥ùò¶ ùò∞ùòµùò©ùò¶ùò≥ ùò∂ùò¥ùò¶ùòßùò∂ùò≠ ùò≠ùò™ùòØùò¨ùò¥:  \\n  \\n\\\\- ùò¥ùò≠ùò™ùò•ùò¶ùò¥: üîó https://lnkd.in/d_MdqGwS  \\n  \\n\\\\- ùòìùòìùòî ùòõùò∏ùò™ùòØ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùòéùò™ùòµùòèùò∂ùò£: üîó https://lnkd.in/dzat6PB6  \\n  \\n\\\\- ùòìùòìùòî ùòõùò∏ùò™ùòØ ùòçùòôùòåùòå ùò≠ùò¶ùò¥ùò¥ùò∞ùòØùò¥: üîó https://lnkd.in/dX__4mhX\\n\\n* * *\\n\\n### 3 techniques you must know to evaluate your LLMs\\n\\nHere are 3 techniques you must know to evaluate your LLMs quickly.  \\n  \\nManually testing the output of your LLMs is a tedious and painful process ‚Üí\\nyou need to automate it.  \\n  \\nIn generative AI, most of the time, you cannot leverage standard metrics.  \\n  \\nThus, the real question is, how do you evaluate the outputs of an LLM?  \\n  \\n#ùü≠. ùó¶ùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùóÆùóªùòÄùòÑùó≤ùóøùòÄ - ùòÜùóºùòÇ ùó∏ùóªùóºùòÑ ùó≤ùòÖùóÆùó∞ùòÅùóπùòÜ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùòÑùóÆùóªùòÅ ùòÅùóº ùó¥ùó≤ùòÅ  \\n  \\nEven if you use an LLM to generate text, you can ask it to generate a response\\nin a structured format (e.g., JSON) that can be parsed.  \\n  \\nYou know exactly what you want (e.g., a list of products extracted from the\\nuser\\'s question).  \\n  \\nThus, you can easily compare the generated and ideal answers using classic\\napproaches.  \\n  \\nFor example, when extracting the list of products from the user\\'s input, you\\ncan do the following:  \\n\\\\- check if the LLM outputs a valid JSON structure  \\n\\\\- use a classic method to compare the generated and real answers  \\n  \\n#ùüÆ. ùó°ùóº \"ùóøùó∂ùó¥ùóµùòÅ\" ùóÆùóªùòÄùòÑùó≤ùóø (ùó≤.ùó¥., ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ ùó±ùó≤ùòÄùó∞ùóøùó∂ùóΩùòÅùó∂ùóºùóªùòÄ, ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùó≤ùòÄ, ùó≤ùòÅùó∞.)  \\n  \\nWhen generating sentences, the LLM can use different styles, words, etc. Thus,\\ntraditional metrics (e.g., BLUE score) are too rigid to be useful.  \\n  \\nYou can leverage another LLM to test the output of our initial LLM. The trick\\nis in what questions to ask.  \\n  \\nHere, we have another 2 sub scenarios:  \\n  \\n‚Ü≥ ùüÆ.ùü≠ ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº (ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ\\nùóµùóÆùòÉùó≤ ùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nYou don\\'t have access to an expert to write an ideal answer for a given\\nquestion to compare it to.  \\n  \\nBased on the initial prompt and generated answer, you can compile a set of\\nquestions and pass them to an LLM. Usually, these are Y/N questions that you\\ncan easily quantify and check the validity of the generated answer.  \\n  \\nThis is known as \"Rubric Evaluation\"  \\n  \\nFor example:  \\n\"\"\"  \\n\\\\- Is there any disagreement between the response and the context? (Y or N)  \\n\\\\- Count how many questions the user asked. (output a number)  \\n...  \\n\"\"\"  \\n  \\nThis strategy is intuitive, as you can ask the LLM any question you are\\ninterested in as long it can output a quantifiable answer (Y/N or a number).  \\n  \\n‚Ü≥ ùüÆ.ùüÆ. ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóº ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóøùó≤ùòÄùóΩùóºùóªùòÄùó≤ ùòÅùóº (ùòÜùóºùòÇ ùóµùóÆùòÉùó≤\\nùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nWhen you have access to an answer manually created by a group of experts,\\nthings are easier.  \\n  \\nYou will use an LLM to compare the generated and ideal answers based on\\nsemantics, not structure.  \\n  \\nFor example:  \\n\"\"\"  \\n(A) The submitted answer is a subset of the expert answer and entirely\\nconsistent.  \\n...  \\n(E) The answers differ, but these differences don\\'t matter.  \\n\"\"\"\\n\\n* * *\\n\\n### 7 tips you must know to reduce your VRAM consumption of your LLMs during\\ntraining\\n\\nHere are ùü≥ ùòÅùó∂ùóΩùòÄ you must know to ùóøùó≤ùó±ùòÇùó∞ùó≤ your ùó©ùó•ùóîùó† ùó∞ùóºùóªùòÄùòÇùó∫ùóΩùòÅùó∂ùóºùóª of your ùóüùóüùó†ùòÄ\\nduring ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ so you can ùó≥ùó∂ùòÅ it on ùòÖùü≠ ùóöùó£ùó®.  \\n  \\nùü≠\\\\. ùó†ùó∂ùòÖùó≤ùó±-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: During training you use both FP32 and FP16 in the\\nfollowing way: \"FP32 weights\" -> \"FP16 weights\" -> \"FP16 gradients\" -> \"FP32\\ngradients\" -> \"Update weights\" -> \"FP32 weights\" (and repeat). As you can see,\\nthe forward & backward passes are done in FP16, and only the optimization step\\nis done in FP32, which reduces both the VRAM and runtime.  \\n  \\nùüÆ\\\\. ùóüùóºùòÑùó≤ùóø-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: All your computations are done in FP16 instead of FP32.\\nBut the key is using bfloat16 (\"Brain Floating Point\"), a numerical\\nrepresentation Google developed for deep learning. It allows you to represent\\nvery large and small numbers, avoiding overflowing or underflowing scenarios.  \\n  \\nùüØ\\\\. ùó•ùó≤ùó±ùòÇùó∞ùó∂ùóªùó¥ ùòÅùóµùó≤ ùóØùóÆùòÅùó∞ùóµ ùòÄùó∂ùòáùó≤: This one is straightforward. Fewer samples per\\ntraining iteration result in smaller VRAM requirements. The downside of this\\nmethod is that you can\\'t go too low with your batch size without impacting\\nyour model\\'s performance.  \\n  \\nùü∞\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ ùóÆùó∞ùó∞ùòÇùó∫ùòÇùóπùóÆùòÅùó∂ùóºùóª: It is a simple & powerful trick to increase your\\nbatch size virtually. You compute the gradients for \"micro\" batches (forward +\\nbackward passes). Once the accumulated gradients reach the given \"virtual\"\\ntarget, the model weights are updated with the accumulated gradients. For\\nexample, you have a batch size of 4 and a micro-batch size of 1. Then, the\\nforward & backward passes will be done using only x1 sample, and the\\noptimization step will be done using the aggregated gradient of the 4 samples.  \\n  \\nùü±\\\\. ùó®ùòÄùó≤ ùóÆ ùòÄùòÅùóÆùòÅùó≤ùóπùó≤ùòÄùòÄ ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùó≤ùóø: Adam is the most popular optimizer. It is one\\nof the most stable optimizers, but the downside is that it has 2 additional\\nparameters (a mean & variance) for every model parameter. If you use a\\nstateless optimizer, such as SGD, you can reduce the number of parameters by\\n2/3, which is significant for LLMs.  \\n  \\nùü≤\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ (ùóºùóø ùóÆùó∞ùòÅùó∂ùòÉùóÆùòÅùó∂ùóºùóª) ùó∞ùóµùó≤ùó∞ùó∏ùóΩùóºùó∂ùóªùòÅùó∂ùóªùó¥: It drops specific activations\\nduring the forward pass and recomputes them during the backward pass. Thus, it\\neliminates the need to hold all activations simultaneously in VRAM. This\\ntechnique reduces VRAM consumption but makes the training slower.  \\n  \\nùü≥\\\\. ùóñùó£ùó® ùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùóºùó≥ùó≥ùóπùóºùóÆùó±ùó∂ùóªùó¥: The parameters that do not fit on your GPU\\'s\\nVRAM are loaded on the CPU. Intuitively, you can see it as a model parallelism\\nbetween your GPU & CPU.\\n\\nImage by DALL-E\\n\\nMost of these methods are orthogonal, so you can combine them and drastically\\nreduce your VRAM requirements during training.\\n\\n* * *\\n\\n### Introduction to deploying private LLMs with AWS SageMaker\\n\\nEver wondered ùóµùóºùòÑ to ùó±ùó≤ùóΩùóπùóºùòÜ in <ùüØùü¨ ùó∫ùó∂ùóªùòÇùòÅùó≤ùòÄ ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†ùòÄ, such as ùóüùóπùóÆùó∫ùóÆùüÆ,\\non ùóîùó™ùó¶ ùó¶ùóÆùó¥ùó≤ùó†ùóÆùó∏ùó≤ùóø? Then wonder no more ‚Üì\\n\\n#### Step 1: Deploy the LLM to AWS SageMaker\\n\\nThe sweet thing about SageMaker is that it accelerates the development\\nprocess, enabling a more efficient and rapid transition to the production\\nstage.  \\n  \\n\\nVesa Alexandru\\n\\nsmashed with his first article on DML about showing step-by-step how to deploy\\nan LLM from HuggingFace to AWS SageMaker using good practices, such as:  \\n  \\n\\\\- designing a config class for the deployment of the LLM  \\n\\\\- set up AWS and deploy the LLM to SageMaker  \\n\\\\- implement an inference class to call the deployed LLM in real-time through\\na web endpoint  \\n\\\\- define a prompt template function to ensure reproducibility & consistency  \\n  \\n...and, ultimately, how to play yourself with your freshly deployed LLM.\\n\\n_Here is the full article explaining how to deploy the LLM to AWS SageMaker_ ‚Üì\\n\\n#### DML: Introduction to Deploying Private LLMs with AWS SageMaker: Focus on\\nLlama2-7b-chat\\n\\nVesa Alexandru\\n\\n¬∑\\n\\nJan 18\\n\\nRead full story\\n\\n#### Step 2: Call the SageMaker inference endpoint\\n\\nYou\\'ve just deployed your Mistral LLM to SageMaker.  \\n  \\nùòïùò∞ùò∏ ùò∏ùò©ùò¢ùòµ?  \\n  \\nUnfortunately, you are not done.  \\n  \\nThat was just the beginning of the journey.  \\n  \\n‚Üí Now, you have to write a Python client that calls the LLM.  \\n  \\nùóüùó≤ùòÅ\\'ùòÄ ùòÇùòÄùó≤ ùóÆ ùó±ùóºùó∞ùòÇùó∫ùó≤ùóªùòÅ ùòÄùòÇùó∫ùó∫ùóÆùóøùòÜ ùòÅùóÆùòÄùó∏ ùóÆùòÄ ùóÆùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤.  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: Define a Settings object using ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: Create an inference interface that inherits from ùòàùòâùòä  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: Implement an ùòàùòûùòö ùòöùò¢ùò®ùò¶ùòîùò¢ùò¨ùò¶ùò≥ version of the inference interface by\\nspecifying how to construct the HTTP payload and call the SageMaker endpoint.\\nWe want to keep this class independent from the summarization prompt!  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü∞: Create the summarization prompt.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü±: Encapsulate the summarization prompt and Python SageMaker client into\\na ùòöùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶ùòöùò©ùò∞ùò≥ùòµùòãùò∞ùò§ùò∂ùòÆùò¶ùòØùòµ task.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≤: Wrap the ùòöùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶ùòöùò©ùò∞ùò≥ùòµùòãùò∞ùò§ùò∂ùòÆùò¶ùòØùòµ task with a FastAPI endpoint.  \\n  \\n...and bam!  \\n  \\nYou have an LLM for summarizing any document.  \\n  \\n.  \\n  \\nùóõùó≤ùóøùó≤ ùóÆùóøùó≤ ùòÄùóºùó∫ùó≤ ùóÆùó±ùòÉùóÆùóªùòÅùóÆùó¥ùó≤ùòÄ ùóºùó≥ ùòÅùóµùó≤ ùó±ùó≤ùòÄùó∂ùó¥ùóª ùó±ùó≤ùòÄùó∞ùóøùó∂ùóØùó≤ùó± ùóÆùóØùóºùòÉùó≤:  \\n  \\n\\\\- by using an inference interface, you can quickly swap the LLM\\nimplementation  \\n  \\n\\\\- by decoupling the prompt construction logic from the inference class, you\\ncan reuse the inference client with any prompt  \\n  \\n\\\\- by wrapping everything with a ùòöùò∂ùòÆùòÆùò¢ùò≥ùò™ùòªùò¶ùòöùò©ùò∞ùò≥ùòµùòãùò∞ùò§ùò∂ùòÆùò¶ùòØùòµ task you can quickly\\ndefine & configure multiple types of tasks and leverage polymorphism to run\\nthem  \\n  \\n_Here is the full article explaining how to design the inference module_ ‚Üì\\n\\n#### Steal my code to solve real-world problems\\n\\nVesa Alexandru\\n\\n¬∑\\n\\nFeb 29\\n\\nRead full story\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n4\\n\\nShare this post\\n\\n#### 7 tips to reduce your VRAM when training LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/7-tips-to-reduce-your-vram-when-training?r=1ttoeh'), ArticleDocument(id=UUID('d0c592eb-82bc-46c4-9632-388f9dd144ce'), content={'Title': 'Using this Python package, you can x10 your text preprocessing pipelines', 'Subtitle': 'End-to-end framework for production-ready LLMs. Top 6 ML platform features you must know and use in your ML system.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Using this Python package, you can x10 your text preprocessing pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Using this Python package, you can x10 your text preprocessing pipelines\\n\\n### End-to-end framework for production-ready LLMs. Top 6 ML platform features\\nyou must know and use in your ML system.\\n\\nPaul Iusztin\\n\\nMay 11, 2024\\n\\n9\\n\\nShare this post\\n\\n#### Using this Python package, you can x10 your text preprocessing pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Top 6 ML platform features you must know and use in your ML system.\\n\\n  * Using this Python package, you can x10 your text preprocessing pipelines\\n\\n  * End-to-end framework for production-ready LLMs\\n\\n* * *\\n\\n### Top 6 ML platform features you must know and use in your ML system\\n\\nHere they are ‚Üì  \\n  \\n#ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥  \\n  \\nIn your ML development phase, you generate lots of experiments.  \\n  \\nTracking and comparing the metrics between them is crucial in finding the\\noptimal model.  \\n  \\n#ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤  \\n  \\nIts primary purpose is reproducibility.  \\n  \\nTo know how a model was generated, you need to know:  \\n\\\\- the version of the code  \\n\\\\- the version of the packages  \\n\\\\- hyperparameters/config  \\n\\\\- total compute  \\n\\\\- version of the dataset  \\n... and more  \\n  \\n#ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nMost of the time, along with the metrics, you must log a set of visualizations\\nfor your experiment.  \\n  \\nSuch as:  \\n\\\\- images  \\n\\\\- videos  \\n\\\\- prompts  \\n\\\\- t-SNE graphs  \\n\\\\- 3D point clouds  \\n... and more  \\n  \\n#ùü∞. ùó•ùó≤ùóΩùóºùóøùòÅùòÄ  \\n  \\nYou don\\'t work in a vacuum.  \\n  \\nYou have to present your work to other colleges or clients.  \\n  \\nA report lets you take the metadata and visualizations from your experiment...  \\n  \\n...and create, deliver and share a targeted presentation for your clients or\\npeers.  \\n  \\n#ùü±. ùóîùóøùòÅùó∂ùó≥ùóÆùó∞ùòÅùòÄ  \\n  \\nThe most powerful feature out of them all.  \\n  \\nAn artifact is a versioned object that is an input or output for your task.  \\n  \\nEverything can be an artifact, but the most common cases are:  \\n\\\\- data  \\n\\\\- model  \\n\\\\- code  \\n  \\nWrapping your assets around an artifact ensures reproducibility.  \\n  \\nFor example, you wrap your features into an artifact (e.g., features:3.1.2),\\nwhich you can consume into your ML development step.  \\n  \\nThe ML development step will generate config (e.g., config:1.2.4) and code\\n(e.g., code:1.0.2) artifacts used in the continuous training pipeline.  \\n  \\nDoing so lets you quickly respond to questions such as \"What I used to\\ngenerate the model?\" and \"What Version?\"  \\n  \\n#ùü≤. ùó†ùóºùó±ùó≤ùóπ ùó•ùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ  \\n  \\nThe model registry is the ultimate way to make your model accessible to your\\nproduction ecosystem.  \\n  \\nFor example, in your continuous training pipeline, after the model is trained,\\nyou load the weights as an artifact into the model registry (e.g.,\\nmodel:1.2.4).  \\n  \\nYou label this model as \"staging\" under a new version and prepare it for\\ntesting. If the tests pass, mark it as \"production\" under a new version and\\nprepare it for deployment (e.g., model:2.1.5).\\n\\nAll of these features are used in a mature ML system. What is your favorite\\none?\\n\\n* * *\\n\\n### Using this Python package, you can x10 your text preprocessing pipelines\\n\\nAny text preprocessing pipeline has to clean, partition, extract, or chunk\\ntext data to feed it into your LLMs.  \\n  \\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± offers a ùóøùó∂ùó∞ùóµ and ùó∞ùóπùó≤ùóÆùóª ùóîùó£ùóú that allows you to quickly:  \\n  \\n\\\\- ùò±ùò¢ùò≥ùòµùò™ùòµùò™ùò∞ùòØ your data into smaller segments from various data sources (e.g.,\\nHTML, CSV, PDFs, even images, etc.)  \\n\\\\- ùò§ùò≠ùò¶ùò¢ùòØùò™ùòØùò® the text of anomalies (e.g., wrong ASCII characters), any\\nirrelevant information (e.g., white spaces, bullets, etc.), and filling\\nmissing values  \\n\\\\- ùò¶ùòπùòµùò≥ùò¢ùò§ùòµùò™ùòØùò® information from pieces of text (e.g., datetimes, addresses, IP\\naddresses, etc.)  \\n\\\\- ùò§ùò©ùò∂ùòØùò¨ùò™ùòØùò® your text segments into pieces of text that can be inserted into\\nyour embedding model  \\n\\\\- ùò¶ùòÆùò£ùò¶ùò•ùò•ùò™ùòØùò® data (e.g., wrapper over OpenAIEmbeddingEncoder,\\nHuggingFaceEmbeddingEncoders, etc.)  \\n\\\\- ùò¥ùòµùò¢ùò®ùò¶ your data to be fed into various tools (e.g., Label Studio, Label\\nBox, etc.)  \\n  \\nùóîùóπùóπ ùòÅùóµùó≤ùòÄùó≤ ùòÄùòÅùó≤ùóΩùòÄ ùóÆùóøùó≤ ùó≤ùòÄùòÄùó≤ùóªùòÅùó∂ùóÆùóπ ùó≥ùóºùóø:  \\n  \\n\\\\- feeding your data into your LLMs  \\n\\\\- embedding the data and ingesting it into a vector DB  \\n\\\\- doing RAG  \\n\\\\- labeling  \\n\\\\- recommender systems  \\n  \\n... basically for any LLM or multimodal applications  \\n  \\n.  \\n  \\nImplementing all these steps from scratch will take a lot of time.  \\n  \\nI know some Python packages already do this, but the functionality is\\nscattered across multiple packages.\\n\\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± packages everything together under a nice, clean API.\\n\\n* * *\\n\\n### End-to-end framework for production-ready LLMs\\n\\nWant to ùóπùó≤ùóÆùóøùóª to ùóØùòÇùó∂ùóπùó± ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª ùóüùóüùó†ùòÄ in a ùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùòÑùóÆùòÜ? For ùóôùó•ùóòùóò? Then ùòÜùóºùòÇ\\nùòÄùóµùóºùòÇùóπùó± ùòÅùóÆùó∏ùó≤ our ùó°ùóòùó™ ùó∞ùóºùòÇùóøùòÄùó≤ on how to ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ for\\nùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ ‚Üì  \\n  \\nüß† Decoding ML and I are ùòÄùòÅùóÆùóøùòÅùó∂ùóªùó¥ a ùóªùó≤ùòÑ ùóôùó•ùóòùóò ùó∞ùóºùòÇùóøùòÄùó≤ on ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ how to\\nùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅ and ùóØùòÇùó∂ùóπùó± a ùóøùó≤ùóÆùóπ-ùòÑùóºùóøùóπùó± ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ by ùóØùòÇùó∂ùóπùó±ùó∂ùóªùó¥ an ùóüùóüùó† ùóßùòÑùó∂ùóª:  \\n  \\n‚Üí from start to finish - from  \\n‚Üí from data collection to deployment  \\n‚Üí production-ready  \\n‚Üí from NO MLOps to experiment trackers, model registries, prompt monitoring,\\nand versioning\\n\\nThe course is called: ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ  \\n  \\n...and here is what you will learn to build  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüêç 4 ùòóùò∫ùòµùò©ùò∞ùòØ ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥:  \\n  \\n‚Üí ùóßùóµùó≤ ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Crawl your digital data from various social media platforms.  \\n\\\\- Clean, normalize and load the data to a NoSQL DB through a series of ETL\\npipelines.  \\n\\\\- Send database changes to a queue using the CDC pattern.  \\n  \\n‚òÅ Deployed on AWS.\\n\\n  \\n  \\n‚Üí ùóßùóµùó≤ ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Consume messages from a queue through a Bytewax streaming pipeline.  \\n\\\\- Every message will be cleaned, chunked, embedded and loaded into a Qdrant\\nvector DB in real-time.  \\n  \\n‚òÅ Deployed on AWS.  \\n  \\n  \\n‚Üí ùóßùóµùó≤ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Create a custom dataset based on your digital data.  \\n\\\\- Fine-tune an LLM using QLoRA.  \\n\\\\- Use Comet ML\\'s experiment tracker to monitor the experiments.  \\n\\\\- Evaluate and save the best model to Comet\\'s model registry.  \\n  \\n‚òÅ Deployed on Qwak.  \\n  \\n  \\n‚Üí ùóßùóµùó≤ ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\n\\\\- Load and quantize the fine-tuned LLM from Comet\\'s model registry.  \\n\\\\- Deploy it as a REST API  \\n\\\\- Enhance the prompts using RAG  \\n\\\\- Generate content using your LLM twin  \\n\\\\- Monitor the LLM using Comet\\'s prompt monitoring dashboard  \\n  \\n‚òÅ Deployed on Qwak.  \\n  \\n.  \\n  \\nùòàùò≠ùò∞ùòØùò® ùòµùò©ùò¶ 4 ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥, ùò∫ùò∞ùò∂ ùò∏ùò™ùò≠ùò≠ ùò≠ùò¶ùò¢ùò≥ùòØ ùòµùò∞ ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò¶ 3 ùò¥ùò¶ùò≥ùò∑ùò¶ùò≥ùò≠ùò¶ùò¥ùò¥ ùòµùò∞ùò∞ùò≠ùò¥:  \\n  \\n\\\\- Comet as your ML Platform  \\n\\\\- Qdrant as your vector DB  \\n\\\\- Qwak as your ML infrastructure  \\n  \\n.  \\n  \\nTo stay updated on ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ\\ncourse...  \\n  \\nùòæùôùùôöùôòùô† ùôûùô© ùô§ùô™ùô© ùôÇùôûùô©ùôÉùô™ùôó ùôñùô£ùôô ùô®ùô™ùô•ùô•ùô§ùôßùô© ùô™ùô® ùô¨ùôûùô©ùôù ùôñ ‚≠êÔ∏è  \\n  \\n‚Üì‚Üì‚Üì  \\n  \\nüîó ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n9\\n\\nShare this post\\n\\n#### Using this Python package, you can x10 your text preprocessing pipelines\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/using-this-python-package-you-can?r=1ttoeh'), ArticleDocument(id=UUID('46f9a4cc-cf3b-43c6-9026-6c9cddf8674a'), content={'Title': '4 Advanced RAG Algorithms You Must Know - by Paul Iusztin', 'Subtitle': 'Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\n### Implement from scratch 4 advanced RAG methods to optimize your retrieval\\nand post-retrieval algorithm\\n\\nPaul Iusztin\\n\\nMay 09, 2024\\n\\n17\\n\\nShare this post\\n\\n#### The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n1\\n\\nShare\\n\\n _‚Üí the 5th out of 11 lessons of the LLM Twin free course_\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> More **details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà\\n\\n* * *\\n\\n### Latest Lessons of the LLM Twin Course\\n\\n**Lesson 2** : The importance of Data Pipeline in the era of Generative AI\\n\\n‚Üí Data crawling, ETL pipelines, ODM, NoSQL Database\\n\\n**Lesson 3:** CDC: Enabling Event-Driven Architectures\\n\\n‚Üí Change Data Capture (CDC), MongoDB Watcher, RabbitMQ queue\\n\\n**Lesson 4:** Python Streaming Pipelines for Fine-tuning LLMs and RAG - in\\nReal-Time!\\n\\n‚Üí Feature pipeline, Bytewax streaming engine, Pydantic models, The dispatcher\\nlayer\\n\\n* * *\\n\\n### Lesson 5: **The 4 Advanced RAG Algorithms You Must Know to Implement**\\n\\nIn **Lesson 5** , we will focus on building an advanced retrieval module used\\nfor RAG.\\n\\nWe will show you how to implement 4 **retrieval** and **post-retrieval\\nadvanced optimization techniques** to **improve** the **accuracy** of your\\n**RAG retrieval step**.\\n\\nIn this lesson, we will focus only on the retrieval part of the RAG system.\\n\\nIn **Lesson 4** , we showed you how to clean, chunk, embed, and load social\\nmedia data to a Qdrant vector DB (the ingestion part of RAG).\\n\\nIn future lessons, we will integrate this retrieval module into the inference\\npipeline for a full-fledged RAG system.\\n\\nRetrieval Python Module Architecture\\n\\n* * *\\n\\n### 1\\\\. Overview of advanced RAG optimization techniques\\n\\nA production RAG system is split into **3 main components** :\\n\\n  * **ingestion:** clean, chunk, embed, and load your data to a vector DB\\n\\n  * **retrieval:** query your vector DB for context\\n\\n  * **generation:** attach the retrieved context to your prompt and pass it to an LLM\\n\\nThe **ingestion component** sits in the _feature pipeline_ , while the\\n**retrieval** and **generation** **components** are implemented inside the\\n_inference pipeline_.\\n\\nYou can **also** **use** the **retrieval** and **generation** **components**\\nin your _training pipeline_ to fine-tune your LLM further on domain-specific\\nprompts.\\n\\nYou can apply advanced techniques to optimize your RAG system for ingestion,\\nretrieval and generation.\\n\\n_That being said, there are 3 main types of advanced RAG techniques:_\\n\\n  * **Pre-retrieval optimization**[ingestion]: tweak how you create the chunks\\n\\n  * **Retrieval optimization**[retrieval]:**** improve the queries to your vector DB\\n\\n  * **Post-retrieval optimization**[retrieval]**:** process the retrieved chunks to filter out the noise\\n\\n> The **generation step** can be **improved** through fine-tuning or prompt\\n> engineering, which will be explained in future lessons.\\n\\nThe **pre-retrieval optimization techniques** are explained in Lesson 4.\\n\\nIn this lesson, we will show you some **popular** **retrieval** and **post-\\nretrieval** **optimization techniques**.\\n\\n* * *\\n\\n### 2\\\\. Advanced RAG techniques applied to the LLM twin\\n\\n#### **Retrieval optimization**\\n\\n _We will combine 3 techniques:_\\n\\n  * Query Expansion\\n\\n  * Self Query\\n\\n  * Filtered vector search\\n\\n#### **Post-retrieval optimization**\\n\\nWe will **use** the **rerank** pattern **using** **GPT-4** and **prompt\\nengineering** instead of Cohere or an open-source re-ranker cross-encoder [4].\\n\\nI don‚Äôt want to spend too much time on the theoretical aspects. There are\\nplenty of articles on that.\\n\\n_So, we will**jump** straight to **implementing** and **integrating** these\\ntechniques in our LLM twin system._\\n\\nBut first, let‚Äôs clarify why we picked Qdrant as our vector DB ‚Üì\\n\\n#### 2.1. Why Qdrant?\\n\\nThere are many vector DBs out there, too many‚Ä¶\\n\\nBut since we discovered Qdrant, we loved it.\\n\\n**Why?**\\n\\n  * It is built in Rust.\\n\\n  * Apache-2.0 license ‚Äî open-source üî•\\n\\n  * It has a great and intuitive Python SDK.\\n\\n  * It has a freemium self-hosted version to build PoCs for free.\\n\\n  * It supports unlimited document sizes, and vector dims of up to 645536.\\n\\n  * It is production-ready. Companies such as Disney, Mozilla, and Microsoft already use it.\\n\\n  * It is one of the most popular vector DBs out there.\\n\\n_**To** **put that in perspective,**_ Pinecone, one of its biggest\\ncompetitors, supports only documents with up to 40k tokens and vectors with up\\nto 20k dimensions‚Ä¶. and a proprietary license.\\n\\nI could go on and on‚Ä¶\\n\\n‚Ä¶but if you are **curious to find out more** , _check out Qdrant _‚Üê\\n\\n* * *\\n\\n### 3\\\\. Retrieval optimization (1): Query expansion\\n\\nQuery expansion is quite intuitive.\\n\\nYou use an LLM to generate multiple queries based on your initial query.\\n\\nThese queries should contain multiple perspectives of the initial query.\\n\\nThus, when embedded, they hit different areas of your embedding space that are\\nstill relevant to our initial question.\\n\\nYou can do query expansion with a detailed zero-shot prompt.\\n\\nQuery expansion template ‚Üí GitHub Code ‚Üê\\n\\n### 4\\\\. Retrieval optimization (2): Self query\\n\\nWhat if you could extract the tags within the query and use them along the\\nembedded query?\\n\\nThat is what self-query is all about!\\n\\nYou use an LLM to extract various metadata fields that are critical for your\\nbusiness use case (e.g., tags, author ID, number of comments, likes, shares,\\netc.)\\n\\nIn our custom solution, we are extracting just the author ID. Thus, a zero-\\nshot prompt engineering technique will do the job.\\n\\n_Self-queries work hand-in-hand with vector filter searches, which we will\\nexplain in the next section._\\n\\nTo define the _**SelfQueryTemplate**_ , we have to:\\n\\n  * Subclass the base abstract class\\n\\n  * Define the self-query prompt\\n\\n  * Create the LangChain PromptTemplate wrapper\\n\\n    \\n    \\n    class **SelfQueryTemplate**(BasePromptTemplate):\\n        prompt: str = \"\"\"\\n        You are an AI language model assistant. \\n        Your task is to extract information from a user question.\\n        The required information that needs to be extracted is the user id. \\n        Your response should consists of only the extracted id (e.g. 1345256), nothing else.\\n        User question: {question}\\n        \"\"\"\\n    \\n        def create_template(self) -> PromptTemplate:\\n            return PromptTemplate(\\n                template=self.prompt, input_variables=[\"question\"], verbose=True\\n            )\\n\\n### 5\\\\. Retrieval optimization (3): Hybrid & filtered vector search\\n\\nCombine the vector search technique with one (or more) complementary search\\nstrategy, which works great for finding exact words.\\n\\nIt is not defined which algorithms are combined, but the most standard\\nstrategy for hybrid search is to combine the traditional keyword-based search\\nand modern vector search.\\n\\n_How are these combined?_\\n\\n_The**first method** is to merge the similarity scores of the 2 techniques as\\nfollows:_\\n\\n    \\n    \\n    hybrid_score = (1 - alpha) * sparse_score + alpha * dense_score\\n\\nWhere **alpha** takes a value between [0, 1], with:\\n\\n  * **alpha = 1** : Vector Search\\n\\n  * **alpha = 0** : Keyword search\\n\\nAlso, the similarity scores are defined as follows:\\n\\n  * **sparse_score:** is the result of the _keyword search_ that, behind the scenes, uses a BM25 algorithm [7] that sits on top of TF-IDF.\\n\\n  * **dense_score:** is the result of the _vector search_ that most commonly uses a similarity metric such as cosine distance\\n\\n _The**second method** uses the vector search technique as usual and applies a\\nfilter based on your keywords on top of the metadata of retrieved results._\\n\\n> ‚Üí This is also known as**filtered vector search**.\\n\\nIn this use case, the **similar score** is **not changed based** on the\\n**provided** **keywords**.\\n\\nIt is just a fancy word for a simple filter applied to the metadata of your\\nvectors.\\n\\nBut it is **essential** to **understand** the **difference** **between** the\\n**first** and **second** **methods** :\\n\\n  * the**first method** combines the similarity score between the keywords and vectors using the alpha parameter;\\n\\n  * the **second method** is a simple filter on top of your vector search.\\n\\n#### How does this fit into our architecture?\\n\\nRemember that during the self-query step, we extracted the **author_id** as an\\nexact field that we have to match.\\n\\nThus, we will search for the **author_id** using the keyword search algorithm\\nand attach it to the 5 queries generated by the query expansion step.\\n\\n_As we want the**most relevant chunks** from a **given author,** it makes the\\nmost sense to use a **filter** **using** the **author_id** as follows\\n(**filtered vector search**)_ ‚Üì\\n\\n    \\n    \\n    self._qdrant_client.search(\\n          collection_name=\"vector_posts\",\\n          query_filter=models.Filter(\\n              must=[\\n                  models.FieldCondition(\\n                      key=\"author_id\",\\n                      match=models.MatchValue(\\n                          value=metadata_filter_value,\\n                      ),\\n                  )\\n              ]\\n          ),\\n          query_vector=self._embedder.encode(generated_query).tolist(),\\n          limit=k,\\n\\nNote that we can easily extend this with multiple keywords (e.g., tags),\\nmaking the combination of self-query and hybrid search a powerful retrieval\\nduo.\\n\\nThe only **question** you have to **ask yourself** is whether we want to\\n**use** a simple **vector search filter** or the more complex **hybrid\\nsearch** strategy.\\n\\n### 6\\\\. Implement the advanced retrieval Python class\\n\\n _Now that you‚Äôve understood the**advanced retrieval optimization techniques**\\nwe\\'re using, let‚Äôs **combine** them into a **Python retrieval class**._\\n\\nQuery expansion chains wrapper ‚Üí GitHub ‚Üê\\n\\nNow the final step is to call Qdrant for each query generated by the query\\nexpansion step ‚Üì\\n\\nVectorRetriever: main search function ‚Üí GitHub ‚Üê\\n\\n _Note that we have**3 types of data** : posts, articles, and code\\nrepositories._\\n\\nThus, we have to make a query for each collection and combine the results in\\nthe end.\\n\\nWe gathered data from each collection individually and kept the best-retrieved\\nresults using rerank.\\n\\nWhich is the final step of the article.\\n\\n### 7\\\\. Post-retrieval optimization: Rerank using GPT-4\\n\\nWe made a **different search** in the Qdrant vector DB for **N prompts**\\n**generated** by the **query expansion step**.\\n\\n**Each** **search** returns **K results**.\\n\\nThus, we **end up with** **N x K chunks**.\\n\\nIn our particular case, **N = 5** & **K = 3.** Thus, we end up with 15 chunks.\\n\\nPost-retrieval optimization: rerank\\n\\nWe will use **rerank** to order all the **N x K** chunks based on their\\nrelevance relative to the initial question, where the first one will be the\\nmost relevant and the last chunk the least.\\n\\nUltimately, we will pick the TOP K most relevant chunks.\\n\\nRerank works really well when combined with query expansion.\\n\\n_A natural flow when using rerank is as follows:_\\n\\n    \\n    \\n    Search for >K chunks >>> Reorder using rerank >>> Take top K\\n\\nThus, when combined with query expansion, we gather potential useful context\\nfrom multiple points in space rather than just looking for more than K samples\\nin a single location.\\n\\n _Now the flow looks like:_\\n\\n    \\n    \\n    Search for N x K chunks >>> Reoder using rerank >>> Take top K\\n\\nA typical solution for reranking is to use open-source Bi-Encoders from\\nsentence transformers [4].\\n\\nThese solutions take both the question and context as input and return a score\\nfrom 0 to 1.\\n\\nIn this article, we want to take a different approach and use GPT-4 + prompt\\nengineering as our reranker.\\n\\nIf you want to see how to apply rerank using open-source algorithms, check out\\nthis hands-on article from Decoding ML:\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\nPaul Iusztin\\n\\n¬∑\\n\\nMar 7\\n\\nRead full story\\n\\nNow let‚Äôs see our implementation using GPT-4 & prompt engineering.\\n\\nSimilar to what we did for the expansion and self-query chains, we define a\\ntemplate and a chain builder ‚Üì\\n\\n    \\n    \\n    class RerankingTemplate(BasePromptTemplate):\\n        prompt: str = \"\"\"\\n        You are an AI language model assistant. \\n        Your task is to rerank passages related to a query\\n        based on their relevance. The most relevant passages \\n        should be put at the beginning. \\n        You should only pick at max {k} passages.\\n        The following are passages related to this query: {question}.\\n        Passages: {passages}\\n        \"\"\"\\n    \\n        def create_template(self) -> PromptTemplate:\\n            return PromptTemplate(\\n                template=self.prompt, \\n                input_variables=[\"question\", \"passages\"])\\n\\n‚Ä¶and that‚Äôs it!\\n\\n* * *\\n\\n### Conclusion\\n\\n _Congratulations!_\\n\\nIn **Lesson 5** , you learned to **build** an **advanced RAG retrieval\\nmodule** optimized for searching posts, articles, and code repositories from a\\nQdrant vector DB.\\n\\n**First** , you learned about where the RAG pipeline can be optimized:\\n\\n  * pre-retrieval\\n\\n  * retrieval\\n\\n  * post-retrieval\\n\\n**After** you learn how to build from scratch (without using LangChain‚Äôs\\nutilities) the following advanced RAG retrieval & post-retrieval optimization\\ntechniques:\\n\\n  * query expansion\\n\\n  * self query\\n\\n  * hybrid search\\n\\n  * rerank\\n\\n**Ultimately** , you understood where the retrieval component sits in an RAG\\nproduction LLM system, where the code is shared between multiple microservices\\nand doesn‚Äôt sit in a single Notebook.\\n\\n_**Next week** , in **Lesson 6** , we will move to the training pipeline and\\nshow you how to automatically transform the data crawled from LinkedIn,\\nSubstack, Medium, and GitHub into an instruction dataset using GPT-4 to fine-\\ntune your LLM Twin._\\n\\nSee you there! ü§ó\\n\\n* * *\\n\\n### Next Steps\\n\\n#### Step 1\\n\\nThis is just the **short version** of **Lesson 5** on the **advanced RAG\\nretrieval module**.\\n\\n‚Üí For‚Ä¶\\n\\n  * The full implementation.\\n\\n  * Discussion on our custom implementation vs. LangChain.\\n\\n  * More on the problems these 4 advanced RAG techniques solve.\\n\\n  * How to use the retrieval module.\\n\\n**Check out** the **full version** of **Lesson 5** on our **Medium\\npublication**. It‚Äôs still FREE:\\n\\nLesson 5 - FREE Medium Article\\n\\n#### Step 2\\n\\n‚Üí **Check out theLLM Twin GitHub repository and try it yourself ü´µ**\\n\\n _Nothing compares with getting your hands dirty and building it yourself!_\\n\\nLLM Twin Course - GitHub\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n17\\n\\nShare this post\\n\\n#### The 4 Advanced RAG Algorithms You Must Know to Implement\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n1\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Meng LiAI Disruption May 17Great, thanks for sharing!Expand full\\ncommentReplyShare  \\n---|---  \\n  \\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'), ArticleDocument(id=UUID('037e6362-8be7-4860-992f-1f075921a669'), content={'Title': 'Problems deploying your ML models? Here is your solution!', 'Subtitle': 'PyTorch + CUDA ultimate guide. Synthetic data generation. Serverless infrastructure.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Problems deploying your ML models? Here is your solution!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Problems deploying your ML models? Here is your solution!\\n\\n### PyTorch + CUDA ultimate guide. Synthetic data generation. Serverless\\ninfrastructure.\\n\\nPaul Iusztin\\n\\nApr 27, 2024\\n\\n10\\n\\nShare this post\\n\\n#### Problems deploying your ML models? Here is your solution!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * The ultimate guide on installing PyTorch with CUDA support in all possible ways\\n\\n  * Generate a synthetic domain-specific Q&A dataset in <30 minutes\\n\\n  * The power of serverless in the world of ML\\n\\n* * *\\n\\nExciting news üî• I was invited by Maven to speak in their Lighting Lesson\\nseries about how to ùóîùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅ ùó¨ùóºùòÇùóø ùóüùóüùó† ùóßùòÑùó∂ùóª.\\n\\nRegister here (it‚Äôs free) ‚Üê\\n\\nThis 30-min session is for ML & MLOps engineers who want to learn:\\n\\nùóüùóüùó† ùó¶ùòÜùòÄùòÅùó≤ùó∫ ùó±ùó≤ùòÄùó∂ùó¥ùóª ùóºùó≥ ùòÜùóºùòÇùóø ùóüùóüùó† ùóßùòÑùó∂ùóª\\n\\n‚Üí Using the 3-pipeline architecture & MLOps good practices\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆ ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí data crawling, ETLs, CDC, AWS\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆ ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí streaming engine in Python, data ingestion for fine-tuning & RAG, vector DBs\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí create a custom dataset, fine-tuning, model registries, experiment trackers,\\nLLM evaluation\\n\\nùóóùó≤ùòÄùó∂ùó¥ùóª ùóÆùóª ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n‚Üí real-time deployment, REST API, RAG, LLM monitoring\\n\\n‚Üì‚Üì‚Üì\\n\\n> Join LIVE on ùòçùò≥ùò™, ùòîùò¢ùò∫ 3!\\n>\\n> Register here (it‚Äôs free) ‚Üê\\n\\n* * *\\n\\n### The ultimate guide on installing PyTorch with CUDA support in all possible\\nways\\n\\nEver wanted to quit ML while wrestling with ùóñùó®ùóóùóî ùó≤ùóøùóøùóºùóøùòÄ? I know I did. ‚Üí\\nDiscover ùóµùóºùòÑ to install ùóñùó®ùóóùóî & ùó£ùòÜùóßùóºùóøùó∞ùóµ ùóΩùóÆùó∂ùóªùóπùó≤ùòÄùòÄùóπùòÜ in all possible ways.  \\n  \\nHere is the story of most ML people:  \\n  \\n1\\\\. You just got excited about a new model that came out.  \\n  \\n2\\\\. You want to try it out.  \\n  \\n3\\\\. You install everything.  \\n  \\n4\\\\. You run the model.  \\n  \\n5\\\\. Bam... CUDA error.  \\n  \\n6\\\\. You fix the error.  \\n  \\n7\\\\. Bam... Another CUDA error  \\n  \\n7\\\\. You fix the error.  \\n  \\n8\\\\. ...Yet another CUDA error.  \\n  \\nYou get the idea.  \\n  \\n‚Üí Now it is 3:00 am, and you finally solved all your CUDA errors and ran your\\nmodel.  \\n  \\nNow, it\\'s time to do your actual work.  \\n  \\nDo you relate?  \\n  \\nIf so...  \\n  \\nI started a Medium article where I documented good practices and step-by-step\\ninstructions on how to install CUDA & PyTorch with:  \\n  \\n\\\\- Pip  \\n\\\\- Conda (or Mamba)  \\n\\\\- Poetry  \\n\\\\- Docker\\n\\nDocker entry point - bash template\\n\\n> **Check it out** ‚Üì  \\n>  \\n> üîó _**The ultimate guide on installing PyTorch with CUDA support in all\\n> possible ways**_\\n\\nùó°ùóºùòÅùó≤: Feel free to comment with any improvements on how to install CUDA +\\nPyTorch. Let\\'s make the ultimate tutorial on installing these 2 beasts üî•\\n\\n* * *\\n\\n### Generate a synthetic domain-specific Q&A dataset in <30 minutes\\n\\nHow do you ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ a ùòÄùòÜùóªùòÅùóµùó≤ùòÅùó∂ùó∞ ùó±ùóºùó∫ùóÆùó∂ùóª-ùòÄùóΩùó≤ùó∞ùó∂ùó≥ùó∂ùó∞ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ in <ùüØùü¨ ùó∫ùó∂ùóªùòÇùòÅùó≤ùòÄ to\\nùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ your ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†?  \\n  \\nThis method is also known as ùó≥ùó∂ùóªùó≤ùòÅùòÇùóªùó∂ùóªùó¥ ùòÑùó∂ùòÅùóµ ùó±ùó∂ùòÄùòÅùó∂ùóπùóπùóÆùòÅùó∂ùóºùóª. Here are its 3 ùòÆùò¢ùò™ùòØ\\nùò¥ùòµùò¶ùò±ùò¥ ‚Üì  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶, ùò≠ùò¶ùòµ\\'ùò¥ ùò®ùò¶ùòØùò¶ùò≥ùò¢ùòµùò¶ ùò¢ ùòò&ùòà ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùò•ùò¢ùòµùò¢ùò¥ùò¶ùòµ ùò∂ùò¥ùò¶ùò• ùòµùò∞ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò¶ ùò¢\\nùòßùò™ùòØùò¢ùòØùò§ùò™ùò¢ùò≠ ùò¢ùò•ùò∑ùò™ùò¥ùò∞ùò≥ ùòìùòìùòî.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: ùó†ùóÆùóªùòÇùóÆùóπùóπùòÜ ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóÆ ùó≥ùó≤ùòÑ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nGenerate a few input samples (~3) that have the following structure:  \\n\\\\- ùò∂ùò¥ùò¶ùò≥_ùò§ùò∞ùòØùòµùò¶ùòπùòµ: describe the type of investor (e.g., \"I am a 28-year-old\\nmarketing professional\")  \\n\\\\- ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØ: describe the user\\'s intention (e.g., \"Is Bitcoin a good\\ninvestment option?\")  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: ùóòùòÖùóΩùóÆùóªùó± ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ùóµùó≤ùóπùóΩ ùóºùó≥ ùóÆ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó†  \\n  \\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up\\nto +N similar input examples.  \\n  \\nWe generated 100 input examples in our use case, but you can generate more.  \\n  \\nYou will use the manually filled input examples to do few-shot prompting.  \\n  \\nThis will guide the LLM to give you domain-specific samples.  \\n  \\nùòõùò©ùò¶ ùò±ùò≥ùò∞ùòÆùò±ùòµ ùò∏ùò™ùò≠ùò≠ ùò≠ùò∞ùò∞ùò¨ ùò≠ùò™ùò¨ùò¶ ùòµùò©ùò™ùò¥:  \\n\"\"\"  \\n...  \\nGenerate 100 more examples with the following pattern:  \\n  \\n# USER CONTEXT 1  \\n...  \\n  \\n# QUESTION 1  \\n...  \\n  \\n# USER CONTEXT 2  \\n...  \\n\"\"\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: ùó®ùòÄùó≤ ùòÅùóµùó≤ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó† ùòÅùóº ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóºùòÇùòÅùóΩùòÇùòÅùòÄ ùó≥ùóºùóø ùóÆùóπùóπ ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nNow, you will have the same powerful LLM as a teacher, but this time, it will\\nanswer all your N input examples.  \\n  \\nBut first, to introduce more variance, we will use RAG to enrich the input\\nexamples with news context.  \\n  \\nAfterward, we will use the teacher LLM to answer all N input examples.  \\n  \\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual\\nwork.  \\n  \\n.  \\n  \\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a\\nniched task, such as financial advising.  \\n  \\nThis technique is known as finetuning with distillation because you use a\\npowerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data,\\nwhich will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as\\nthe student.\\n\\nGenerate a Q&A dataset in <30 minutes\\n\\n  \\n‚úíÔ∏è ùòïùò∞ùòµùò¶: To ensure that the generated data is of high quality, you can hire a\\ndomain expert to check & refine it.\\n\\n* * *\\n\\n### The power of serverless in the world of ML\\n\\nùóóùó≤ùóΩùóπùóºùòÜùó∂ùóªùó¥ & ùó∫ùóÆùóªùóÆùó¥ùó∂ùóªùó¥ ML models is ùóµùóÆùóøùó±, especially when running your models on\\nGPUs.  \\n  \\nBut ùòÄùó≤ùóøùòÉùó≤ùóøùóπùó≤ùòÄùòÄ makes things ùó≤ùóÆùòÄùòÜ.  \\n  \\nUsing Beam as your serverless provider, deploying & managing ML models can be\\nas easy as ‚Üì  \\n  \\nùóóùó≤ùó≥ùó∂ùóªùó≤ ùòÜùóºùòÇùóø ùó∂ùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ & ùó±ùó≤ùóΩùó≤ùóªùó±ùó≤ùóªùó∞ùó∂ùó≤ùòÄ  \\n  \\nIn a few lines of code, you define the application that contains:  \\n  \\n\\\\- the requirements of your infrastructure, such as the CPU, RAM, and GPU  \\n\\\\- the dependencies of your application  \\n\\\\- the volumes from where you can load your data and store your artifacts  \\n  \\nùóóùó≤ùóΩùóπùóºùòÜ ùòÜùóºùòÇùóø ùó∑ùóºùóØùòÄ  \\n  \\nUsing the Beam application, you can quickly decorate your Python functions to:  \\n  \\n\\\\- run them once on the given serverless application  \\n\\\\- put your task/job in a queue to be processed or even schedule it using a\\nCRON-based syntax  \\n\\\\- even deploy it as a RESTful API endpoint  \\n  \\n.  \\n  \\nAs you can see in the image below, you can have one central function for\\ntraining or inference, and with minimal effort, you can switch from all these\\ndeployment methods.  \\n  \\nAlso, you don\\'t have to bother at all with managing the infrastructure on\\nwhich your jobs run. You specify what you need, and Beam takes care of the\\nrest.  \\n  \\nBy doing so, you can directly start to focus on your application and stop\\ncarrying about the infrastructure.  \\n  \\nThis is the power of serverless!\\n\\nBeam example\\n\\n> ‚Ü≥üîó ùòäùò©ùò¶ùò§ùò¨ ùò∞ùò∂ùòµ ùòâùò¶ùò¢ùòÆ ùòµùò∞ ùò≠ùò¶ùò¢ùò≥ùòØ ùòÆùò∞ùò≥ùò¶\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n10\\n\\nShare this post\\n\\n#### Problems deploying your ML models? Here is your solution!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/problems-deploying-your-ml-models?r=1ttoeh'), ArticleDocument(id=UUID('c91e76e3-774c-43e7-91db-01c0c6bff57a'), content={'Title': 'Streaming Pipelines for LLMs and RAG - by Paul Iusztin', 'Subtitle': 'SOTA streaming pipeline in Python to clean, chunk, embed and load data to a vector DB (feature store)  in real time: for fine-tuning LLMs and RAG (on AWS).', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-\\nTime!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-Time!\\n\\n### Use a Python streaming engine to populate a feature store from 4+ data\\nsources\\n\\nPaul Iusztin\\n\\nApr 25, 2024\\n\\n11\\n\\nShare this post\\n\\n#### SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-\\nTime!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n‚Üí the 4th out of 11 lessons of the LLM Twin free course\\n\\n**What is your LLM Twin?** It is an AI character that writes like yourself by\\nincorporating your style, personality, and voice into an LLM.\\n\\nImage by DALL-E\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> More **details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà\\n\\n* * *\\n\\n### Latest Lessons of the LLM Twin Course\\n\\n**Lesson 1:**` `An End-to-End Framework for Production-Ready LLM Systems by\\nBuilding Your LLM Twin\\n\\n‚Üí LLM Twin Concept, 3-Pipeline Architecture, System Design for LLM Twin\\n\\n**Lesson 2** : The importance of Data Pipeline in the era of Generative AI\\n\\n‚Üí Data crawling, ETL pipelines, ODM, NoSQL Database\\n\\n**Lesson 3:** CDC: Enabling Event-Driven Architectures\\n\\n‚Üí Change Data Capture (CDC), MongoDB Watcher, RabbitMQ queue\\n\\n* * *\\n\\n## Lesson 4: **Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-\\nTime!**\\n\\nIn the **4th lesson** , we will focus on the **feature pipeline.**\\n\\nThe **feature pipeline** is the **first** **pipeline** presented in the **3\\npipeline architecture** : feature, training and inference pipelines.\\n\\nA **feature pipeline** takes raw data as input, processes it into features,\\nand stores it in a feature store, from which the training & inference\\npipelines will use it.\\n\\nThe component is completely isolated from the training and inference code. All\\nthe communication is done through the feature store.\\n\\nBy the **end of this** **article** , you will **learn** to **design** and\\n**build** a **production-ready feature pipeline** that:\\n\\n  * uses Bytewax as a stream engine to process data in real-time;\\n\\n  * ingests data from a RabbitMQ queue;\\n\\n  * uses SWE practices to process multiple data types: posts, articles, code;\\n\\n  * cleans, chunks, and embeds data for LLM fine-tuning and RAG;\\n\\n  * loads the features to a Qdrant vector DB.\\n\\n> Note that we will only cover the **vector DB retrieval client** and\\n> **advanced retrieval techniques** in the **5th lesson**!\\n\\n_Excited? Let‚Äôs get started!_\\n\\n* * *\\n\\n### Table of Contents:\\n\\n  1. Why are we doing this?\\n\\n  2. System design of the feature pipeline\\n\\n  3. The Bytewax streaming flow\\n\\n  4. Pydantic data models\\n\\n  5. Load data to Qdrant (our feature store)\\n\\n  6. The dispatcher layer\\n\\n> üîó **Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è\\n\\n* * *\\n\\n### 1\\\\. Why are we doing this?\\n\\n#### A quick reminder from previous lessons\\n\\nTo give you some context, in Lesson 2, we crawl data from LinkedIn, Medium,\\nand GitHub, normalize it, and load it to MongoDB.\\n\\nIn Lesson 3, we are using CDC to listen to changes to the MongoDB database and\\nemit events in a RabbitMQ queue based on any CRUD operation done on MongoDB.\\n\\n#### The problem we are solving\\n\\nIn our LLM Twin use case, the **feature pipeline** constantly syncs the\\nMongoDB warehouse with the Qdrant vector DB (our feature store) while\\nprocessing the raw data into features.\\n\\n#### Why we are solving it\\n\\nThe **feature store** will be the **central point of access** for all the\\nfeatures used within the training and inference pipelines.\\n\\n‚Üí The **training pipeline** will use the feature store to create **fine-\\ntunin** g datasets for your **LLM** **twin**.\\n\\n‚Üí The **inference pipeline** will use the feature store for **RAG**.\\n\\n### 2\\\\. System design of the feature pipeline: our solution\\n\\n _Our**solution** is based on **CDC** , a **queue,** a **streaming engine,**\\nand a **vector DB:**_\\n\\n‚Üí CDC adds any change made to the Mongo DB to the queue (read more in Lesson\\n3).\\n\\n‚Üí the RabbitMQ queue stores all the events until they are processed.\\n\\n‚Üí The Bytewax streaming engine cleans, chunks, and embeds the data.\\n\\n‚Üí A streaming engine works naturally with a queue-based system.\\n\\n‚Üí The data is uploaded to a Qdrant vector DB on the fly\\n\\n#### **Why is this powerful?**\\n\\nHere are 4 core reasons:\\n\\n  1. The **data** is **processed** in **real-time**.\\n\\n  2. **Out-of-the-box recovery system:** If the streaming pipeline fails to process a message will be added back to the queue \\n\\n  3. **Lightweight:** No need for any diffs between databases or batching too many records\\n\\n  4. **No I/O bottlenecks** on the source database\\n\\n‚Üí **It solves all our problems!**\\n\\nStreaming ingestion pipeline architecture and integration with the rest of the\\ncomponents\\n\\n#### How do we process multiple data types?\\n\\nHow do you **process multiple types** **of** **data** in a **single streaming\\npipeline** **without** **writing** **spaghetti code**?\\n\\nYes, that is for you, data scientists! **Joking‚Ä¶** am I**?**\\n\\nWe have **3 data types** : posts, articles, and code.\\n\\n**Each data type** (and its state) will be **modeled** using **Pydantic**\\n**models**.\\n\\nTo **process** them, we will write a **dispatcher layer** , which will use a\\n**creational** **factory** **pattern **to **instantiate** a **handler**\\nimplemented for that **specific data type** (post, article, code) and\\n**operation** (cleaning, chunking, embedding).\\n\\nThe **handler** follows the **strategy behavioral pattern.**\\n\\n#### Streaming over batch\\n\\nNowadays, using tools such as Bytewax makes implementing streaming pipelines a\\nlot more frictionless than using their JVM alternatives.\\n\\nThe key aspect of choosing a streaming vs. a batch design is real-time\\nsynchronization between your source and destination DBs.\\n\\nIn our particular case, we will process social media data, which changes fast\\nand irregularly.\\n\\nAlso, for our digital twin, it is important to do RAG on up-to-date data. We\\ndon‚Äôt want to have any delay between what happens in the real world and what\\nyour LLM twin sees.\\n\\nThat being said, choosing a streaming architecture seemed natural in our use\\ncase.\\n\\n* * *\\n\\n### 3\\\\. The Bytewax streaming flow\\n\\nThe **Bytewax flow** is the **central point** of the **streaming pipeline**.\\nIt defines all the required steps, following the next simplified pattern:\\n_‚Äúinput - > processing -> output‚Äù._\\n\\nAs I come from the AI world, I like to see it as the **‚Äúgraph‚Äù** of the\\n**streaming pipeline** , where you use the _input()_ , _map()_ , and\\n_output()_ Bytewax functions to define your graph, which in the **Bytewax\\nworld** is **called** a _**‚Äúflow‚Äù**_.\\n\\nAs you can see in the code snippet below, we ingest posts, articles or code\\nmessages from a RabbitMQ queue. After we clean, chunk and embed them.\\nUltimately, we load the cleaned and embedded data to a Qdrant vector DB, which\\nin our LLM twin use case will represent the feature store of our system.\\n\\nTo structure and validate the data, between each Bytewax step, we map and pass\\na different Pydantic model based on its current state: raw, cleaned, chunked,\\nor embedded.\\n\\nBytewax flow ‚Üí GitHub Code  ‚Üê\\n\\nWe have a single streaming pipeline that processes everything.\\n\\nAs we ingest multiple data types (posts, articles, or code snapshots), we have\\nto process them differently.\\n\\nTo do this the right way, we implemented a dispatcher layer that knows how to\\napply data-specific operations based on the type of message.\\n\\nMore on this in the next sections ‚Üì\\n\\n#### Why Bytewax?\\n\\n_Bytewax is an open-source streaming processing framework that:_  \\n\\\\- is built in **Rust** ‚öôÔ∏è for **performance**  \\n\\\\- has **Python** üêç bindings for leveraging its powerful ML ecosystem\\n\\n‚Ä¶ so, for all the Python fanatics out there, no more JVM headaches for you.\\n\\nJokes aside, here is why Bytewax is so powerful ‚Üì\\n\\n\\\\- Bytewax local setup is plug-and-play  \\n\\\\- can quickly be integrated into any Python project (you can go wild ‚Äî even\\nuse it in Notebooks)  \\n\\\\- can easily be integrated with other Python packages (NumPy, PyTorch,\\nHuggingFace, OpenCV, SkLearn, you name it)  \\n\\\\- out-of-the-box connectors for Kafka and local files, or you can quickly\\nimplement your own\\n\\nWe used Bytewax to build the streaming pipeline for the LLM Twin course and\\nloved it.\\n\\n> To **learn more** about **Bytewax** , check out their **Substack** , where\\n> you have the chance to **dive deeper** into **streaming engines**. In\\n> Python. For FREE:\\n>\\n> ‚Üí Bytewax Newsletter ‚Üê\\n\\n* * *\\n\\n### 4\\\\. Pydantic data models\\n\\nLet‚Äôs take a look at what our Pydantic models look like.\\n\\nWe defined a hierarchy of Pydantic models for:\\n\\n  * all our data types: posts, articles, or code\\n\\n  * all our states: raw, cleaned, chunked, and embedded\\n\\nThis is how the set of classes for the posts will look like ‚Üì\\n\\nPydantic posts model structure ‚Üí GitHub Code ‚Üê\\n\\nWe **repeated** the s**ame process** for the **articles** and **code** model\\n**hierarchy**.\\n\\n### 5\\\\. Load data to Qdrant (our feature store)\\n\\nThe first step is to implement our custom Bytewax _DynamicSink_ class ‚Üì\\n\\nQdrant DynamicSink ‚Üí GitHub Code ‚Üê\\n\\nNext, for every type of operation we need (output cleaned or embedded data ),\\nwe have to subclass the _StatelessSinkPartition_ Bytewax class (they also\\nprovide a stateful option ‚Üí more in their docs)\\n\\nAn instance of the class will run on every partition defined within the\\nBytewax deployment.\\n\\nIn the course, we are using a single partition per worker. But, by adding more\\npartitions (and workers), you can quickly scale your Bytewax pipeline\\nhorizontally.\\n\\n**Remember** **why** we upload the **data** to Qdrant in **two stages** , as\\nthe **Qdrant vector DB** will act as our **feature store** :\\n\\n  1. The _cleaned data_ will be used for _LLM fine-tuning_(used by the training pipeline)\\n\\n  2. The _chunked & embedded_ data will be used for _RAG (used by the inference pipeline)_\\n\\nQdrant worker partitions ‚Üí GitHub Code ‚Üê\\n\\nNote that we used**Qdrant‚Äôs** **Batch** method to upload all the available\\npoints simultaneously. By doing so, we **reduce** the **latency** on the\\n**network I/O** side: more on that here\\n\\n### 6\\\\. The dispatcher layer\\n\\nNow that we have the Bytewax flow and all our data models.\\n\\n**How do we map a raw data model to a cleaned data model?**\\n\\n> All our domain logic is modeled by a set of _Handler()_ classes:\\n>\\n> ‚Üí CleaningDataHandler\\n>\\n> ‚Üí ChunkingDataHandler\\n>\\n> ‚Üí EmbeddingDataHandler\\n\\n**Now, to build our dispatcher, we need 2 last components:**\\n\\n  * **a factory class:** instantiates the right handler based on the type of the event\\n\\n  * **a dispatcher class:** the glue code that calls the factory class and handler\\n\\n**Here is what the cleaning dispatcher and factory look like** ‚Üì\\n\\nThe dispatcher and factory classes ‚Üí GitHub Code ‚Üê\\n\\nNote that we will have a different **Handler()** for every (data_type, state)\\npair ‚Äî resulting in 3 x 3 = 9 different handlers.\\n\\nFor Example, we will have 3 handlers based on their data type for the cleaned\\npost state: PostCleaningHandler, ArticleCleaningHandler, and\\nRepositoryCleaningHandler.\\n\\n**By repeating the same logic, we will end up with the following set of\\ndispatchers:**\\n\\n  * _RawDispatcher_ (no factory class required as the data is not processed)\\n\\n  * _CleaningDispatcher_ (with a _ChunkingHandlerFactory_ class)\\n\\n  * _ChunkingDispatcher_ (with a _ChunkingHandlerFactory_ class)\\n\\n  * _EmbeddingDispatcher_ (with an _EmbeddingHandlerFactory_ class)\\n\\n* * *\\n\\n### To Summarize\\n\\nIn **Lesson 4** of the LLM Twin course, we learned how to:\\n\\n  * Design a streaming pipeline in Python using Bytewax\\n\\n  * Load data to a Qdrant vector DB\\n\\n  * Use Pydantic models to add types and validation to the data points\\n\\n  * Implement a dispatcher layer to process multiple data types in a modular way\\n\\n _‚Üí In**Lesson 5, which will be held in two weeks,** we will focus on the\\nvector DB retrieval client and advanced retrieval techniques._\\n\\n* * *\\n\\n### Next Steps\\n\\nTo **dig** **into** the **details** of the **streaming pipeline** and **how**\\nto:\\n\\n  * **implement** **cleaning** , **chunking** , and **embedding** **strategies** for digital data\\n\\n  * **design** the **AWS infrastructure** for the streaming pipeline\\n\\n  * understand how to **run the component**\\n\\n**Check out** the **full-fledged version** of the **article** on our **Medium\\npublication**.\\n\\n‚Üì‚Üì‚Üì\\n\\nLesson 4 - FREE Medium Article\\n\\n* * *\\n\\n#### Images\\n\\nIf not otherwise stated, all images are created by the author.\\n\\n11\\n\\nShare this post\\n\\n#### SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG - in Real-\\nTime!\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/sota-python-streaming-pipelines-for?r=1ttoeh'), ArticleDocument(id=UUID('53bc94d1-8cfd-4e65-b55c-9b3582f6ed64'), content={'Title': 'Ready for production ML? Here are the 4 pillars to build production ML systems', 'Subtitle': 'ML Platforms & MLOps Components. RAG:RAG: What problems does it solve, and how is it integrated into LLM-powered applications', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\n### ML Platforms & MLOps Components. RAG:RAG: What problems does it solve, and\\nhow is it integrated into LLM-powered applications\\n\\nPaul Iusztin\\n\\nApr 13, 2024\\n\\n8\\n\\nShare this post\\n\\n#### Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n### **This week‚Äôs topics:**\\n\\n  * Using an ML Platform is critical to integrating MLOps into your project\\n\\n  * The 4 pillars to build production ML systems\\n\\n  * RAG: What problems does it solve, and how is it integrated into LLM-powered applications?\\n\\n* * *\\n\\n### Using an ML Platform is critical to integrating MLOps into your project\\n\\nHere are 6 ML platform features you must know & use ‚Üì  \\n  \\n...and let\\'s use Comet ML as a concrete example.  \\n  \\n#ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥  \\n  \\nIn your ML development phase, you generate lots of experiments.  \\n  \\nTracking and comparing the metrics between them is crucial in finding the\\noptimal model & hyperparameters.  \\n  \\n#ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤  \\n  \\nIts primary purpose is reproducibility.  \\n  \\nTo know how a model from a specific experiment was generated, you must know:  \\n\\\\- the version of the code  \\n\\\\- version of the dataset  \\n\\\\- hyperparameters/config  \\n\\\\- total compute  \\n... and more  \\n  \\n#ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nMost of the time, along with the scalar metrics, you must log visual results,\\nsuch as:  \\n\\\\- images  \\n\\\\- videos  \\n\\\\- prompts  \\n\\\\- t-SNE graphs  \\n\\\\- 3D point clouds  \\n... and more  \\n  \\n#4. ùêÄùê´ùê≠ùê¢ùêüùêöùêúùê≠ùê¨  \\n  \\nThe most powerful feature out of them all.  \\n  \\nAn artifact is a versioned object that acts as an input or output for your\\njob.  \\n  \\nEverything can be an artifact (data, model, code), but the most common case is\\nfor your data.  \\n  \\nWrapping your assets around an artifact ensures reproducibility and\\nshareability.  \\n  \\nFor example, you wrap your features into an artifact (e.g., features:3.1.2),\\nwhich you can consume and share across multiple ML environments (development\\nor continuous training).  \\n  \\nUsing an artifact to wrap your data allows you to quickly respond to questions\\nsuch as \"What data have I used to generate the model?\" and \"What Version?\"  \\n  \\n#5. ùêåùê®ùêùùêûùê• ùêëùêûùê†ùê¢ùê¨ùê≠ùê´ùê≤  \\n  \\nThe model registry is the ultimate way to version your models and make them\\naccessible to all your services.  \\n  \\nFor example, your continuous training pipeline will log the weights as an\\nartifact into the model registry after it trains the model.  \\n  \\nYou label this model as \"v:1.1.5:staging\" and prepare it for testing. If the\\ntests pass, mark it as \"v:1.1.0:production\" and trigger the CI/CD pipeline to\\ndeploy it to production.  \\n  \\n#6. ùêñùêûùêõùê°ùê®ùê®ùê§ùê¨  \\n  \\nWebhooks lets you integrate the Comet model registry with your CI/CD pipeline.  \\n  \\nFor example, when the model status changes from \"Staging\" to \"Production,\" a\\nPOST request triggers a GitHub Actions workflow to deploy your new model.\\n\\nImage by the Author\\n\\n‚Ü≥üîó Check out **Comet** to learn more\\n\\n* * *\\n\\n### The 4 pillars to build production ML systems\\n\\nBefore building a production-ready system, it is critical to consider a set of\\nquestions that will later determine the nature of your ML system architecture.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò¢ùò≥ùò¶ ùòµùò©ùò¶ 4 ùò±ùò™ùò≠ùò≠ùò¢ùò≥ùò¥ ùòµùò©ùò¢ùòµ ùò∫ùò∞ùò∂ ùò¢ùò≠ùò∏ùò¢ùò∫ùò¥ ùò©ùò¢ùò∑ùò¶ ùòµùò∞ ùò§ùò∞ùòØùò¥ùò™ùò•ùò¶ùò≥ ùò£ùò¶ùòßùò∞ùò≥ùò¶ ùò•ùò¶ùò¥ùò™ùò®ùòØùò™ùòØùò® ùò¢ùòØùò∫\\nùò¥ùò∫ùò¥ùòµùò¶ùòÆ ‚Üì  \\n  \\n‚ûî ùóóùóÆùòÅùóÆ  \\n  \\n\\\\- What data types do you have? (e.g., tabular data, images, text, etc.)  \\n\\\\- What does the data look like? (e.g., for text data, is it in a single\\nlanguage or multiple?)  \\n\\\\- How do you collect the data?  \\n\\\\- At what frequency do you have to collect the data?  \\n\\\\- How do you collect labels for the data? (crucial for how you plan to\\nevaluate and monitor the model in production)  \\n  \\n‚ûî ùóßùóµùóøùóºùòÇùó¥ùóµùóΩùòÇùòÅ  \\n  \\n\\\\- What are the throughput requirements? You must know at least the\\nthroughput\\'s minimum, average, and maximum statistics.  \\n\\\\- How many requests the system must handle simultaneously? (1, 10, 1k, 1\\nmillion, etc.)  \\n  \\n‚ûî ùóüùóÆùòÅùó≤ùóªùó∞ùòÜ  \\n  \\n\\\\- What are the latency requirements? (1 millisecond, 10 milliseconds, 1\\nsecond, etc.)  \\n\\\\- Throughput vs. latency trade-off  \\n\\\\- Accuracy vs. speed trade-off  \\n  \\n‚ûî ùóúùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤  \\n  \\n\\\\- Batch vs. real-time architecture (closely related to the throughput vs.\\nlatency trade-off)  \\n\\\\- How should the system scale? (e.g., based on CPU workload, # of requests,\\nqueue size, data size, etc.)  \\n\\\\- Cost requirements  \\n  \\n.  \\n  \\nDo you see how we shifted the focus from model performance towards how it is\\nintegrated into a more extensive system?  \\n  \\nWhen building production-ready ML, the model\\'s accuracy is no longer the holy\\ngrail but a bullet point in a grander scheme.  \\n  \\n.  \\n  \\nùóßùóº ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùòáùó≤, the 4 pillars to keep in mind before designing an ML\\narchitecture are:  \\n\\\\- Data  \\n\\\\- Throughput  \\n\\\\- Latency  \\n\\\\- Infrastructure\\n\\nImage by the Author\\n\\n* * *\\n\\n### RAG: What problems does it solve, and how is it integrated into LLM-\\npowered applications?\\n\\nLet\\'s find out ‚Üì  \\n  \\nRAG is a popular strategy when building LLMs to add external data to your\\nprompt.  \\n  \\n=== ùó£ùóøùóºùóØùóπùó≤ùó∫ ===  \\n  \\nWorking with LLMs has 3 main issues:  \\n  \\n1\\\\. The world moves fast  \\n  \\nLLMs learn an internal knowledge base. However, the issue is that its\\nknowledge is limited to its training dataset.  \\n  \\nThe world moves fast. New data flows on the internet every second. Thus, the\\nmodel\\'s knowledge base can quickly become obsolete.  \\n  \\nOne solution is to fine-tune the model every minute or day...  \\n  \\nIf you have some billions to spend around, go for it.  \\n  \\n2\\\\. Hallucinations  \\n  \\nAn LLM is full of testosterone and likes to be blindly confident.  \\n  \\nEven if the answer looks 100% legit, you can never fully trust it.  \\n  \\n3\\\\. Lack of reference links  \\n  \\nIt is hard to trust the response of the LLM if we can\\'t see the source of its\\ndecisions.  \\n  \\nEspecially for important decisions (e.g., health, financials)  \\n  \\n=== ùó¶ùóºùóπùòÇùòÅùó∂ùóºùóª ===  \\n  \\n‚Üí Surprize! It is RAG.  \\n  \\n1\\\\. Avoid fine-tuning  \\n  \\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge\\nbase as the main memory (e.g., vector DB).  \\n  \\nThe memory is volatile, so you can quickly introduce or remove data.  \\n  \\n2\\\\. Avoid hallucinations  \\n  \\nBy forcing the LLM to answer solely based on the given context, the LLM will\\nprovide an answer as follows:  \\n  \\n\\\\- use the external data to respond to the user\\'s question if it contains the\\nnecessary insights  \\n\\\\- \"I don\\'t know\" if not  \\n  \\n3\\\\. Add reference links  \\n  \\nUsing RAG, you can easily track the source of the data and highlight it to the\\nuser.  \\n  \\n=== ùóõùóºùòÑ ùó±ùóºùó≤ùòÄ ùó•ùóîùóö ùòÑùóºùóøùó∏? ===  \\n  \\nLet\\'s say we want to use RAG to build a financial assistant.  \\n  \\nùòûùò©ùò¢ùòµ ùò•ùò∞ ùò∏ùò¶ ùòØùò¶ùò¶ùò•?  \\n  \\n\\\\- a data source with historical and real-time financial news (e.g. Alpaca)  \\n\\\\- a stream processing engine (eg. Bytewax)  \\n\\\\- an encoder-only model for embedding the docs (e.g., pick one from\\n`sentence-transformers`)  \\n\\\\- a vector DB (e.g., Qdrant)  \\n  \\nùòèùò∞ùò∏ ùò•ùò∞ùò¶ùò¥ ùò™ùòµ ùò∏ùò∞ùò≥ùò¨?  \\n  \\n‚Ü≥ On the feature pipeline side:  \\n  \\n1\\\\. using Bytewax, you ingest the financial news and clean them  \\n2\\\\. you chunk the news documents and embed them  \\n3\\\\. you insert the embedding of the docs along with their metadata (e.g., the\\ninitial text, source_url, etc.) to Qdrant  \\n  \\n‚Ü≥ On the inference pipeline side:  \\n  \\n4\\\\. the user question is embedded (using the same embedding model)  \\n5\\\\. using this embedding, you extract the top K most similar news documents\\nfrom Qdrant  \\n6\\\\. along with the user question, you inject the necessary metadata from the\\nextracted top K documents into the prompt template (e.g., the text of\\ndocuments & its source_url)  \\n7\\\\. you pass the whole prompt to the LLM for the final answer\\n\\nImage by the Author\\n\\n8\\n\\nShare this post\\n\\n#### Ready for production ML? Here are the 4 pillars to build production ML\\nsystems\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Dr. Jody-Ann S. JonesThe Data Sensei Apr 13Liked by Paul IusztinExcellent\\narticle Paul! Thank you so much for sharing üôèExpand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/ready-for-production-ml-here-are?r=1ttoeh'), ArticleDocument(id=UUID('20a85606-a880-4894-bfb7-6b0cad8b3f1f'), content={'Title': 'My monthly recommendations for leveling up in ML', 'Subtitle': 'In Vector DBs, RAG, MLOps, and LLMs', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### My monthly recommendations for leveling up in ML\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# My monthly recommendations for leveling up in ML\\n\\n### In Vector DBs, RAG, MLOps, and LLMs\\n\\nPaul Iusztin\\n\\nApr 06, 2024\\n\\n12\\n\\nShare this post\\n\\n#### My monthly recommendations for leveling up in ML\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\n**Today is about learning.**\\n\\nHere is a list of learning resources I used and filtered in the past months.\\n\\nIt is one of the most helpful content on Vector DBs, RAG, MLOps and LLMs out\\nthere.\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * Pick the right vector DB for your exact use case\\n\\n  * 4 video lectures on hands-on LLMs\\n\\n  * 7 steps you have to achieve 100% MLOps maturity\\n\\n  * Advanced RAG\\n\\n* * *\\n\\n### Pick the right vector DB for your exact use case\\n\\nThis is the ùóºùóªùóπùòÜ ùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± to ùóΩùó∂ùó∞ùó∏ the ùóøùó∂ùó¥ùóµùòÅ ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï for your exact\\nùòÇùòÄùó≤ ùó∞ùóÆùòÄùó≤.  \\n  \\nSince ChatGPT made AI cool, besides the millions of ChatGPT posts you got\\ntired of and blocked, you realized that a new type of tool started to hit the\\nscene: Vector DBs.  \\n  \\nAs vector DBs play a crucial role in most LLM applications, they popped out\\neverywhere.  \\n  \\nOn this day, there are 37 vector DB solutions that are constantly changing and\\nadding features.  \\n  \\nùòïùò∞ùò∏, ùò©ùò∞ùò∏ ùòµùò©ùò¶ ùò©**ùò≠ ùò¥ùò©ùò∞ùò∂ùò≠ùò• ùòê ùò±ùò™ùò§ùò¨ ùò∞ùòØùò¶?\\n\\nSS from Superlinked\\n\\nùôÉùôöùôßùôö ùôûùô® ùô¨ùôùùôöùôßùôö ùô©ùôùùôö \"ùôëùôöùôòùô©ùô§ùôß ùòøùòΩ ùòæùô§ùô¢ùô•ùôñùôßùôûùô®ùô§ùô£\" ùô†ùôûùôòùô†ùô® ùôûùô£.  \\n  \\nIt is an effort managed by Superlinked, where they carefully compared all\\nthese 37 vector DBs across 29 features, such as:  \\n  \\n\\\\- License  \\n\\\\- GitHub ‚≠ê  \\n\\\\- support for text, image or struct models  \\n\\\\- RAG, RecSys, LangChain or LllamaIndex APIs  \\n\\\\- pricing  \\n\\\\- sharding  \\n\\\\- document size  \\n\\\\- vector dims  \\n  \\n...and more!  \\n  \\nI won\\'t list all 29 features.  \\n  \\nYou have to check it out to see them for yourself ‚Üì\\n\\nVector DB Comparison\\n\\nùó°ùóºùòÅùó≤: To keep the table updated or add more features, you can contribute to it\\nyourself.\\n\\n* * *\\n\\n### 4 video lectures on hands-on LLMs\\n\\nWant to build your first ùóüùóüùó† ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ but don\\'t know where to start?  \\n  \\nHere are ùü∞ ùóôùó•ùóòùóò ùóπùó≤ùó∞ùòÅùòÇùóøùó≤ùòÄ, made by\\n\\nPau Labarta Bajo\\n\\nfrom\\n\\nReal-World Machine Learning\\n\\n, to put you on the right track ‚Üì  \\n  \\n#1. ùêÖùê¢ùêßùêû-ùê≠ùêÆùêßùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû ùêüùê®ùê´ ùê®ùê©ùêûùêß-ùê¨ùê®ùêÆùê´ùêúùêû ùêãùêãùêåùê¨  \\n  \\nYou will learn:  \\n\\\\- What is model fine-tuning?  \\n\\\\- Why is it useful?  \\n\\\\- When to use it?  \\n\\\\- Why to fine-tune an LLM using QLoRA  \\n\\\\- How to architect a fine-tuning pipeline in a real-world project\\n\\n#2. ùêáùêöùêßùêùùê¨-ùê®ùêß ùêüùê¢ùêßùêû-ùê≠ùêÆùêßùê¢ùêßùê†  \\n  \\nLet\\'s apply what we learned in lesson 1 to build our first fine-tuning\\npipeline.\\n\\n#3. ùêÅùêÆùê¢ùê•ùêù & ùêùùêûùê©ùê•ùê®ùê≤ ùêö ùê´ùêûùêöùê•-ùê≠ùê¢ùê¶ùêû ùê¨ùê≠ùê´ùêûùêöùê¶ùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû  \\n  \\nYou will learn:  \\n\\\\- How to transform HTML docs into vector embeddings.  \\n\\\\- How to process data in real-time  \\n\\\\- How to store & retrieve embeddings from a vector DB  \\n\\\\- How to deploy it to AWS.\\n\\n#4. ùêàùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû  \\n  \\nFinally, you will learn how to use LangChain to glue together your fine-tuned\\nLLM and your financial news stored as embeddings in a vector DB to serve\\npredictions behind a RESTful API.\\n\\n* * *\\n\\n### 7 steps you have to achieve 100% MLOps maturity\\n\\nOne of the most ùó∞ùóºùóªùó≥ùòÇùòÄùó∂ùóªùó¥ ùòÑùóºùóøùó±ùòÄ in the ùó†ùóü ùòÑùóºùóøùóπùó± is \"ùó†ùóüùó¢ùóΩùòÄ\", a new &\\ninterdisciplinary process that isn\\'t fully defined yet.  \\n  \\nThe good news is that there is a strong movement in ùó±ùó≤ùó≥ùó∂ùóªùó∂ùóªùó¥ a ùó∞ùóπùó≤ùóÆùóø ùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤\\nin ùòÄùó∞ùóºùóøùó∂ùóªùó¥ the ùóπùó≤ùòÉùó≤ùóπ of ùó†ùóüùó¢ùóΩùòÄ ùó∫ùóÆùòÅùòÇùóøùó∂ùòÅùòÜ within your ùóºùóøùó¥ùóÆùóªùó∂ùòáùóÆùòÅùó∂ùóºùóª or ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ.  \\n  \\n‚Ü≥ Here are ùü≥ ùòÄùòÅùó≤ùóΩùòÄ you have to ùó∞ùóµùó≤ùó∞ùó∏ to ùóÆùó∞ùóµùó∂ùó≤ùòÉùó≤ ùü≠ùü¨ùü¨% ùó†ùóüùó¢ùóΩùòÄ ùó∫ùóÆùòÅùòÇùóøùó∂ùòÅùòÜ ‚Üì  \\n  \\nNo one other than\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\nhas proposed it.  \\n  \\nùóõùó≤ùóøùó≤ ùòÅùóµùó≤ùòÜ ùóÆùóøùó≤ ‚Üì  \\n  \\n=== ùòîùò∂ùò¥ùòµ ùò©ùò¢ùò∑ùò¶ùò¥ ===  \\n  \\nùü≠\\\\. ùóóùóºùó∞ùòÇùó∫ùó≤ùóªùòÅùóÆùòÅùó∂ùóºùóª: project, ML model, and technical documentation  \\n  \\nùüÆ\\\\. ùóßùóøùóÆùó∞ùó≤ùóÆùóØùó∂ùóπùó∂ùòÅùòÜ ùóÆùóªùó± ùóøùó≤ùóΩùóøùóºùó±ùòÇùó∞ùó∂ùóØùó∂ùóπùó∂ùòÅùòÜ: Infrastructure traceability and\\nreproducibility (versioned IaC under CI/CD) and ML code traceability and\\nreproducibility (versioned code, data, and models along with metadata &\\nlineage attached to the data & model)  \\n  \\nùüØ\\\\. ùóñùóºùó±ùó≤ ùóæùòÇùóÆùóπùó∂ùòÅùòÜ: infrastructure code & ML model code quality requirements\\n(tests ran on PRs under the CI pipeline, PR reviews, formatting checks)  \\n  \\nùü∞\\\\. ùó†ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ & ùòÄùòÇùóΩùóΩùóºùóøùòÅ: infrastructure, application, model performance,\\nbusiness KPIs, data drift and outliers monitoring  \\n  \\n=== ùòâùò¶ùò∫ùò∞ùòØùò• ùò£ùò¢ùò¥ùò™ùò§ ùòîùòìùòñùò±ùò¥ ===  \\n  \\nùü±\\\\. ùóóùóÆùòÅùóÆ ùòÅùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùóÆùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ùòÄ & ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùòÄùòÅùóºùóøùó≤: all the features are shared\\n& versioned from a central feature store  \\n  \\nùü≤\\\\. ùó†ùóºùó±ùó≤ùóπ ùóòùòÖùóΩùóπùóÆùó∂ùóªùóÆùóØùó∂ùóπùó∂ùòÅùòÜ: a human can understand the reasoning of the model\\nand not treat it as a black box  \\n  \\nùü≥\\\\. ùóî/ùóï ùòÅùó≤ùòÄùòÅùó∂ùóªùó¥ & ùó≥ùó≤ùó≤ùó±ùóØùóÆùó∞ùó∏ ùóπùóºùóºùóΩ: inputs & outputs of the model are stored\\nautomatically and A/B testing is performed regularly  \\n  \\n.  \\n  \\n‚Ü≥ Check out the entire questionnaire on the\\n\\nMarvelousMLOps\\n\\nblog: üîó MLOps maturity assessment\\n\\n**MLOps Maturity Assessment by Marvelous MLOps**\\n\\nWhat level of MLOps maturity is your organization at? For now, you will rarely\\nsee 100%.\\n\\n* * *\\n\\n### Advanced RAG\\n\\nRAG systems are far from perfect ‚Üí This free course teaches you how to improve\\nyour RAG system.  \\n  \\nI recently finished the ùóîùó±ùòÉùóÆùóªùó∞ùó≤ùó± ùó•ùó≤ùòÅùóøùó∂ùó≤ùòÉùóÆùóπ ùó≥ùóºùóø ùóîùóú ùòÑùó∂ùòÅùóµ ùóñùóµùóøùóºùó∫ùóÆ free course from\\nDeepLearning.AI\\n\\nSS from the Advanced Retrieval for AI with Chroma course\\n\\nIf you are into RAG, I find it among the most valuable learning sources.  \\n  \\nThe course already assumes you know what RAG is.  \\n  \\nIts primary focus is to show you all the current issues of RAG and why it is\\nfar from perfect.  \\n  \\nAfterward, it shows you the latest SoTA techniques to improve your RAG system,\\nsuch as:  \\n\\\\- query expansion  \\n\\\\- cross-encoder re-ranking  \\n\\\\- embedding adaptors  \\n  \\nI am not affiliated with DeepLearning.AI (I wouldn\\'t mind though).  \\n  \\nThis is a great course you should take if you are into RAG systems.  \\n  \\nThe good news is that it is free and takes only 1 hour.  \\n  \\nCheck it out ‚Üì\\n\\nAdvanced Retrieval for AI with Chroma\\n\\n12\\n\\nShare this post\\n\\n#### My monthly recommendations for leveling up in ML\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/my-ml-monthly-learning-resource-recommendations?r=1ttoeh'), ArticleDocument(id=UUID('ab66f3dc-2957-4ab9-9ed7-ece653d3f725'), content={'Title': 'End-to-End Framework for Production-Ready LLMs', 'Subtitle': 'FREE course on designing, training, deploying, and monitoring a production-ready LLM system powered by LLMs, vector DBs & LLMOps by building your LLM twin.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\n### From data gathering to productionizing LLMs using LLMOps good practices.\\n\\nPaul Iusztin\\n\\nMar 28, 2024\\n\\n35\\n\\nShare this post\\n\\n#### An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _‚Üí the 1st out of 11 lessons of**the LLM Twin** free course_\\n\\n**What is your LLM Twin?** It is an AI character that writes like yourself by\\nincorporating your style, personality and voice into an LLM.\\n\\nImage by DALL-E\\n\\n### **Why is this course different?**\\n\\n_By finishing the ‚Äú**LLM Twin: Building Your Production-Ready AI\\nReplica‚Äù**_****_free course, you will learn how to design, train, and deploy a\\nproduction-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps\\ngood practices_.\\n\\n_**Why should you care? ü´µ**_\\n\\n _**‚Üí No more isolated scripts or Notebooks!** Learn production ML by building\\nand deploying an end-to-end production-grade LLM system._\\n\\n> More **details** on what you will **learn** within the **LLM Twin**\\n> **course** , **here** üëà\\n\\nAre you ready to build your AI replica? ü´¢\\n\\n**Let‚Äôs start** with **Lesson 1** ‚Üì‚Üì‚Üì\\n\\n* * *\\n\\n### **Lesson 1: End-to-end framework for production-ready LLM systems**\\n\\nIn the **first lesson** , we will present**** the **project** you will\\n**build** **during** **the** **course** :  _your production-ready LLM Twin/AI\\nreplica._\\n\\n**Afterward** , we will **dig into** the **LLM project system design**.\\n\\nWe will **present** all our **architectural decisions** regarding the design\\nof the _data collection pipeline_ for social media data and how we applied\\n_the 3-pipeline architecture_ to our LLM microservices.\\n\\nIn the **following lessons** , we will **examine** **each component‚Äôs code**\\nand learn **how** to **implement** and **deploy** **it** to AWS and Qwak.\\n\\nLLM twin system architecture [Image by the Author] ‚Üí What you will learn to\\nbuild during this course.\\n\\n### **Table of Contents**\\n\\n  1. What are you going to build? The LLM twin concept\\n\\n  2. LLM twin system design\\n\\n* * *\\n\\n### **1\\\\. What are you going to build? The LLM twin concept**\\n\\nThe **outcome** of this **course** is to learn to **build** your **own AI\\nreplica**. We will use an LLM to do that, hence the name of the course: _**LLM\\nTwin: Building Your Production-Ready AI Replica.**_\\n\\n**But what is an LLM twin?**\\n\\nShortly, your LLM twin will be an AI character who writes like you, using your\\nwriting style and personality.\\n\\nIt will not be you. It will be your writing copycat.\\n\\nMore concretely, you will build an AI replica that writes social media posts\\nor technical articles (like this one) using your own voice.\\n\\n**Why not directly use ChatGPT? You may ask‚Ä¶**\\n\\nWhen trying to generate an article or post using an LLM, the results tend to\\nbe:\\n\\n  * very generic and unarticulated,\\n\\n  * contain misinformation (due to hallucination),\\n\\n  * require tedious prompting to achieve the desired result.\\n\\n_**But here is what we are going to do to fix that** _‚Üì‚Üì‚Üì\\n\\n**First** , we will fine-tune an LLM on your digital data gathered from\\nLinkedIn, Medium, Substack and GitHub.\\n\\nBy doing so, the LLM will align with your writing style and online\\npersonality. It will teach the LLM to talk like the online version of\\nyourself.\\n\\nOur use case will focus on an LLM twin who writes social media posts or\\narticles that reflect and articulate your voice.\\n\\n**Secondly** , we will give the LLM access to a vector DB to access external\\ninformation to avoid hallucinating.\\n\\n**Ultimately** , in addition to accessing the vector DB for information, you\\ncan provide external links that will act as the building block of the\\ngeneration process.\\n\\nExcited? Let‚Äôs get started üî•\\n\\n* * *\\n\\n### **2\\\\. LLM Twin System design**\\n\\nLet‚Äôs understand how to **apply the 3-pipeline architecture** to **our LLM\\nsystem**.\\n\\nThe **architecture** of the **LLM twin** is split into **4 Python\\nmicroservices** :\\n\\n  1. The data collection pipeline\\n\\n  2. The feature pipeline\\n\\n  3. The training pipeline\\n\\n  4. The inference pipeline\\n\\nLLM twin system architecture [Image by the Author]\\n\\n_Now,**let‚Äôs zoom in** on **each component** to understand how they work\\nindividually and interact with each other. ‚Üì‚Üì‚Üì_\\n\\n### **2.1. The data collection pipeline**\\n\\nIts scope is to **crawl data** for **a given user** from:\\n\\n  * Medium (articles)\\n\\n  * Substack (articles)\\n\\n  * LinkedIn (posts)\\n\\n  * GitHub (code)\\n\\nAs every platform is unique, we implemented a different Extract Transform Load\\n(ETL) pipeline for each website.\\n\\nHowever, the **baseline steps** are the **same** for **each platform**.\\n\\n_Thus, for each ETL pipeline, we can abstract away the following baseline\\nsteps:_\\n\\n  * log in using your credentials\\n\\n  * use _selenium_ to crawl your profile\\n\\n  * use _BeatifulSoup_ to parse the HTML\\n\\n  * clean & normalize the extracted HTML\\n\\n  * save the normalized (but still raw) data to Mongo DB\\n\\n> **Important note:** We are crawling only our data, as most platforms do not\\n> allow us to access other people‚Äôs data due to privacy issues. But this is\\n> perfect for us, as to build our LLM twin, we need only our own digital data.\\n\\n**Why Mongo DB?**\\n\\nWe wanted a NoSQL database that quickly allows us to store unstructured data\\n(aka text).\\n\\n**How will the data pipeline communicate with the feature pipeline?**\\n\\nWe will use the **Change Data Capture (CDC) pattern** to inform the feature\\npipeline of any change on our Mongo DB.\\n\\nTo **explain** the **CDC** briefly, a watcher listens 24/7 for any CRUD\\noperation that happens to the Mongo DB.\\n\\nThe watcher will issue an event informing us what has been modified. We will\\nadd that event to a RabbitMQ queue.\\n\\nThe feature pipeline will constantly listen to the queue, process the\\nmessages, and add them to the Qdrant vector DB.\\n\\nFor example, when we write a new document to the Mongo DB, the watcher creates\\na new event. The event is added to the RabbitMQ queue; ultimately, the feature\\npipeline consumes and processes it.\\n\\n**Where will the data pipeline be deployed?**\\n\\nThe data collection pipeline and RabbitMQ service will be deployed to AWS. We\\nwill also use the freemium serverless version of Mongo DB.\\n\\n### **2.2. The feature pipeline**\\n\\nThe feature pipeline is **implemented usingBytewax** (a Rust streaming engine\\nwith a Python interface). Thus, in **our** specific **use case** , we will\\nalso **refer to it** as a **streaming ingestion pipeline**.\\n\\nIt is an **entirely different service** than the data collection pipeline.\\n\\n**How does it communicate with the data pipeline?**\\n\\nAs explained above, the **feature pipeline communicates** with the **data**\\n**pipeline** through a RabbitMQ **queue**.\\n\\nCurrently, the streaming pipeline doesn‚Äôt care how the data is generated or\\nwhere it comes from.\\n\\nIt knows it has to listen to a given queue, consume messages from there and\\nprocess them.\\n\\nBy doing so, we **decouple** **the two components** entirely.\\n\\n**What is the scope of the feature pipeline?**\\n\\nIt represents the **ingestion component** of the **RAG system**.\\n\\nIt will **take** the **raw data** passed through the queue and:\\n\\n  * clean the data;\\n\\n  * chunk it;\\n\\n  * embed it using the embedding models from Superlinked;\\n\\n  * load it to the Qdrant vector DB.\\n\\n**What data will be stored?**\\n\\nThe **training pipeline** will have **access** **only** to the **feature\\nstore** , which, in our case, is represented by the Qdrant vector DB.\\n\\n_With this in mind, we will**store** in Qdrant **2 snapshots of our data:**_\\n\\n1\\\\. The **cleaned data** (without using vectors as indexes ‚Äî store them in a\\nNoSQL fashion).\\n\\n2\\\\. The **cleaned, chunked, and embedded data** (leveraging the vector indexes\\nof Qdrant)\\n\\nThe **training pipeline** needs **access** to the **data** in**both formats**\\nas we want to fine-tune the LLM on standard and augmented prompts.\\n\\n**Why implement a streaming pipeline instead of a batch pipeline?**\\n\\nThere are **2 main reasons.**\\n\\nThe first one is that, coupled with the **CDC pattern** , it is the most\\n**efficient** way to **sync two DBs** between each other.\\n\\nUsing CDC + a streaming pipeline, you process only the changes to the source\\nDB without any overhead.\\n\\nThe second reason is that by doing so, your **source** and **vector DB** will\\n**always be in sync**. Thus, you will always have access to the latest data\\nwhen doing RAG.\\n\\n**Why Bytewax?**\\n\\n**Bytewax** is a streaming engine built in Rust that exposes a Python\\ninterface. We use Bytewax because it combines Rust‚Äôs impressive speed and\\nreliability with the ease of use and ecosystem of Python. It is incredibly\\nlight, powerful, and easy for a Python developer.\\n\\n**Where will the feature pipeline be deployed?**\\n\\nThe feature pipeline will be deployed to AWS. We will also use the freemium\\nserverless version of Qdrant.\\n\\n### **2.3. The training pipeline**\\n\\n**How do we have access to the training features?**\\n\\nAs section 2.2 highlights, all the **training data** will be **accessed** from\\nthe **feature store**. In our case, the feature store is the **Qdrant vector\\nDB** that contains:\\n\\n  * the cleaned digital data from which we will create prompts & answers;\\n\\n  * we will use the chunked & embedded data for RAG to augment the cleaned data.\\n\\n_We will implement a different vector DB retrieval client for each of our main\\ntypes of data (posts, articles, code)._\\n\\n**What will the training pipeline do?**\\n\\nThe training pipeline contains a **data-to-prompt layer** that will preprocess\\nthe data retrieved from the vector DB into prompts.\\n\\nIt will also contain an **LLM fine-tuning module** that inputs a HuggingFace\\ndataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral).\\n\\nAll the experiments will be logged into Comet ML‚Äôs **experiment tracker**.\\n\\nWe will use a bigger LLM (e.g., GPT4) to **evaluate** the results of our fine-\\ntuned LLM. These results will be logged into Comet‚Äôs experiment tracker.\\n\\n**Where will the production candidate LLM be stored?**\\n\\nWe will compare multiple experiments, pick the best one, and issue an LLM\\nproduction candidate for the model registry.\\n\\nAfter, we will inspect the LLM production candidate manually using Comet‚Äôs\\nprompt monitoring dashboard.\\n\\n**Where will the training pipeline be deployed?**\\n\\nThe training pipeline will be deployed to Qwak.\\n\\nQwak is a serverless solution for training and deploying ML models. It makes\\nscaling your operation easy while you can focus on building.\\n\\nAlso, we will use the freemium version of Comet ML for the following:\\n\\n  * experiment tracker;\\n\\n  * model registry;\\n\\n  * prompt monitoring.\\n\\n### **2.4. The inference pipeline**\\n\\nThe inference pipeline is the **final component** of the **LLM system**. It is\\nthe one the **clients** will **interact with**.\\n\\nIt will be **wrapped** under a **REST API**. The clients can call it through\\nHTTP requests, similar to your experience with ChatGPT or similar tools.\\n\\n**How do we access the features?**\\n\\nWe will grab the features solely from the feature store. We will use the same\\nQdrant vector DB retrieval clients as in the training pipeline to use the\\nfeatures we need for RAG.\\n\\n**How do we access the fine-tuned LLM?**\\n\\nThe fine-tuned LLM will always be downloaded from the model registry based on\\nits tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).\\n\\n**What are the components of the inference pipeline?**\\n\\nThe first one is the **retrieval client** used to access the vector DB to do\\nRAG.\\n\\nAfter we have a **query to prompt the layer,** that will map the prompt and\\nretrieved documents from Qdrant into a prompt.\\n\\nAfter the LLM generates its answer, we will log it to Comet‚Äôs **prompt\\nmonitoring dashboard** and return it to the clients.\\n\\nFor example, the client will request the inference pipeline to:\\n\\n‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will\\ngo through all the steps above to return the generated post.\\n\\n**Where will the inference pipeline be deployed?**\\n\\nThe inference pipeline will be deployed to Qwak.\\n\\nAs for the training pipeline, we will use a serverless freemium version of\\nComet for its prompt monitoring dashboard.\\n\\n* * *\\n\\n### **Conclusion**\\n\\nThis is the 1st article of the****_**LLM Twin: Building Your Production-Ready\\nAI Replica**_**** free**** course.\\n\\nIn this lesson, we presented what **you will build** during the course.\\n\\nUltimately, we went through the **system design** of the course and presented\\nthe **architecture** of **each microservice** and how they **interact with\\neach other** :\\n\\n  1. The data collection pipeline\\n\\n  2. The feature pipeline\\n\\n  3. The training pipeline\\n\\n  4. The inference pipeline\\n\\nIn **Lesson 2** , we will dive deeper into the **data collection pipeline** ,\\nlearn how to implement crawlers for various social media platforms, clean the\\ngathered data, store it in a Mongo DB, and finally, show you how to deploy it\\nto AWS.\\n\\n> _üîó**Check out** the code on GitHub [1] and support us with a ‚≠êÔ∏è_\\n\\n* * *\\n\\n#### This is how we can further help you ü´µ\\n\\nIn the **Decoding ML newsletter** , we want to keep things **short & sweet**.\\n\\nTo **dive deeper** into all the **concepts** presented in this article‚Ä¶\\n\\n**Check out** the **full-fledged version** of the **article** on our **Medium\\npublication**.\\n\\n**It‚Äôs FREE** ‚Üì‚Üì‚Üì\\n\\n> üîó Detailed Lesson 1 [on Medium]\\n\\n35\\n\\nShare this post\\n\\n#### An End-to-End Framework for Production-Ready LLM Systems by Building Your\\nLLM Twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/an-end-to-end-framework-for-production?r=1ttoeh'), ArticleDocument(id=UUID('c4ad61cb-4875-41f6-a9d9-f0da74303586'), content={'Title': 'Upskill your LLM knowledge base with these tools.', 'Subtitle': 'Speed-up your LLM inference and dissect the Attention Mechanism with step-by-step animation.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Upskill your LLM knowledge base with these tools.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Upskill your LLM knowledge base with these tools.\\n\\n### Speed-up your LLM inference and dissect the Attention Mechanism with step-\\nby-step animation.\\n\\nAlex Razvant\\n\\nMar 23, 2024\\n\\n10\\n\\nShare this post\\n\\n#### Upskill your LLM knowledge base with these tools.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nThe **LLM-Twin Course** development has taken off! üöÄ\\n\\nJoin aboard and learn how to design, build, and implement an end-to-end LLM\\nreplica, by following along in a step-by-step hands-on manner with the\\ndevelopment of data pipelines, ingestion, LLM fine-tuning, serving,\\nmonitoring, and more.\\n\\nDecoding ML Newsletter is a reader-supported publication. To receive new posts\\nand support my work, consider becoming a free or paid subscriber.\\n\\nSubscribe\\n\\nThe first 2/11 lessons are out, make sure to check them out here:\\n\\n  * Lesson 1: **An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin**\\n\\n  * Lesson 2: **The Importance of Data Pipelines in the Era of Generative AI**\\n\\n* * *\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * **Fast inference on LLMs**\\n\\n  * **Visualize attention mechanism**\\n\\n  * **A commonly misunderstood CUDA issue!**\\n\\n* * *\\n\\n### Fast inference LLMs\\n\\nFor the last few years, LLMs have been a hot topic - new models, RAGs, new\\npapers, the rise of OpenSource models, etc.  \\nThe attention mechanism is easy to understand, but ‚Äúhungry‚Äù to compute - thus\\nmultiple methods aim to fill the performance gap in model-serving.\\n\\nHere are the top 4 LLM inference solutions:\\n\\n  1. ùòÉùóüùóüùó†  \\nA fast and easy-to-use library for LLM inference and serving.\\n\\nùôÜùôöùôÆ ùôñùô®ùô•ùôöùôòùô©ùô® ùôñùôßùôö:\\n\\n     * ‚ûù is open-source \\n\\n     * ‚ûù state-of-the-art serving throughput \\n\\n     * ‚ûù fast model execution with optimized CUDA kernels/graph. \\n\\n     * ‚ûù efficient memory management using PagedAttention \\n\\n     * ‚ûù support for AMD GPUs (ROCm) ‚ûù deploy support with NVIDIA Triton, KServe, Docker\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/nAFPW\\n\\n  2. ùóßùó≤ùóªùòÄùóºùóøùó•ùóß-ùóüùóüùó†  \\nA library that accelerates and optimizes inference performance of the latest\\nLLMs.\\n\\nùôÜùôöùôÆ ùôñùô®ùô•ùôöùôòùô©ùô® ùôñùôßùôö:\\n\\n     * ‚ûù is open-source \\n\\n     * ‚ûù built on a strong TensorRT foundation \\n\\n     * ‚ûù leverages custom-optimized CUDA kernels for transformers ‚ûù enhances customization \\n\\n     * ‚ûù supports various optimization (quant, tensor parallelism) \\n\\n     * ‚ûù takes advantage of the NVIDIA Toolkit (perf-analyzer, Triton)\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/dluMX\\n\\n  3. ùó¢ùóπùóπùóÆùó∫ùóÆ   \\nA tool that allows you to run open-source language models locally.\\n\\nùóûùó≤ùòÜ ùóÆùòÄùóΩùó≤ùó∞ùòÅùòÄ ùóÆùóøùó≤:\\n\\n     * ‚ûù multi-modal model support \\n\\n     * ‚ûù optimizes setup and configuration details, including GPU usage \\n\\n     * ‚ûù bundles weights, configuration, and data into a single Modelfile package\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/dGZ46\\n\\n  4. ùóñùóµùóÆùòÅ ùòÑùó∂ùòÅùóµ ùó•ùóßùó´\\n\\nA solution from NVIDIA that allows users to build their own personalized\\nchatbot experience.\\n\\nùôÜùôöùôÆ ùôñùô®ùô•ùôöùôòùô©ùô® ùôñùôßùôö:\\n\\n     * ‚ûù emphasizes no-code, ChatGPT-like interface \\n\\n     * ‚ûù one can connect custom documents, videos, notes, and PDFs ‚ûù easy to set up RAG (Retrieval Augmented Generation) \\n\\n     * ‚ûù support for the latest LLMs \\n\\n     * ‚ûù leverages TensorRT-LLM and RTX acceleration \\n\\n     * ‚ûù downloadable installer (35GB), out-of-the-box Mistral & LLaMA 7b versions\\n\\nüîó ùòéùò¶ùòµ ùòöùòµùò¢ùò≥ùòµùò¶ùò•: shorturl.at/ekuK6\\n\\n* * *\\n\\n### Visualize attention mechanism\\n\\nùóüùóüùó† models are complex - the key to understanding the process is the ùóÆùòÅùòÅùó≤ùóªùòÅùó∂ùóºùóª\\nùó∫ùó≤ùó∞ùóµùóÆùóªùó∂ùòÄùó∫.\\n\\nHere are ùüØ ùòÅùóºùóºùóπùòÄ to help you interactively visualize attention:\\n\\n  1. ùóîùòÅùòÅùó≤ùóªùòÅùó∂ùóºùóªùó©ùó∂ùòá : shorturl.at/DSY58\\n\\n    1. ùò§ùò∞ùòØùòßùò™ùò®ùò∂ùò≥ùò¢ùò£ùò≠ùò¶ ùòØùò∂ùòÆ ùò©ùò¶ùò¢ùò•ùò¥.\\n\\n    2. ùò§ùò∞ùòØùòßùò™ùò®ùò∂ùò≥ùò¢ùò£ùò≠ùò¶ ùòØùò∂ùòÆ ùò≠ùò¢ùò∫ùò¶ùò≥ùò¥.\\n\\n    3. ùò©ùò¢ùò¥ ùòùùò™ùòõ, ùòâùòåùòôùòõ, ùòéùòóùòõ2 ùò™ùòØùò§ùò≠ùò∂ùò•ùò¶ùò•.\\n\\n    4. ùüÆùóó visualization + ùüØùóó ùòªùò∞ùò∞ùòÆ-ùò™ùòØùò¥ ùò∞ùòØ ùò¥ùò¶ùò≠ùò¶ùò§ùòµùò¶ùò• ùò≠ùò¢ùò∫ùò¶ùò≥ùò¥.\\n\\n  2. ùó£ùòÜùóßùóºùóøùó∞ùóµ ùó†ùó†: shorturl.at/lqJQY\\n\\n     * ùò§ùò∂ùò¥ùòµùò∞ùòÆ ùò∞ùò±ùò¶ùò≥ùò¢ùòµùò™ùò∞ùòØùò¥.\\n\\n     * ùò¶ùòπùòµùò¶ùòØùò¥ùò™ùò£ùò≠ùò¶ ùò™ùòØ ùò®ùò≥ùò¢ùò±ùò©-ùò≠ùò™ùò¨ùò¶ ùòßùò¢ùò¥ùò©ùò™ùò∞ùòØ.\\n\\n     * ùò©ùò¢ùò¥ ùòéùòóùòõ2-ùòØùò¢ùòØùò∞, ùòìùò∞ùòôùòà ùòõùò¶ùò§ùò©ùòØùò™ùò≤ùò∂ùò¶ ùò™ùòØùò§ùò≠ùò∂ùò•ùò¶ùò•.\\n\\n     * 3D\\n\\n  3. ùóïùóïùòÜùóñùóøùóºùó≥ùòÅ: shorturl.at/ivCR1\\n\\n     * ùò™ùòØùò¥ùò±ùò¶ùò§ùòµ ùò¥ùòµùò¶ùò±-ùò£ùò∫-ùò¥ùòµùò¶ùò± 1 ùòµùò∞ùò¨ùò¶ùòØ ùò±ùò≥ùò¶ùò•ùò™ùò§ùòµùò™ùò∞ùòØ.\\n\\n     * ùò©ùò¢ùò¥ ùòéùòóùòõ2-ùò¥ùòÆùò¢ùò≠ùò≠, ùòéùòóùòõ3, ùòéùòóùòõ-ùòØùò¢ùòØùò∞, ùòéùòóùòõ2-ùòüùòì ùò™ùòØùò§ùò≠ùò∂ùò•ùò¶ùò•.\\n\\n     * straight-forward\\n\\n* * *\\n\\n### A commonly misunderstood CUDA issue!\\n\\nThe problem was that ùóªùòÉùó∂ùó±ùó∂ùóÆ-ùòÄùó∫ùó∂ was showing a ùó±ùó∂ùó≥ùó≥ùó≤ùóøùó≤ùóªùòÅ ùóöùó£ùó® ùó±ùó≤ùòÉùó∂ùó∞ùó≤ ùóºùóøùó±ùó≤ùóø\\ncompared to docker or Python. Thus, errors regarding the disjoint memory\\nregions appeared.\\n\\nùóõùó≤ùóøùó≤\\'ùòÄ ùòÅùóµùó≤ ùòÅùóøùó∂ùó∞ùó∏:\\n\\n  * ùó¶ùòÜùòÄùòÅùó≤ùó∫ ùóüùóÆùòÜùó≤ùóø\\n\\n    * ùô£ùô´ùôûùôôùôûùôñ-ùô®ùô¢ùôû works at the system level and orders GPU ùôßùôöùô®ùô•ùôöùôòùô©ùôûùô£ùôú ùô©ùôùùôö ùô©ùô§ùô•-ùôôùô§ùô¨ùô£ ùô§ùôßùôôùôöùôß ùô§ùôõ ùôùùô§ùô¨ ùô©ùôùùôö ùô•ùôùùôÆùô®ùôûùôòùôñùô° ùô´ùôûùôôùôöùô§ ùôòùôñùôßùôô ùôûùô® ùôûùô£ùô®ùôöùôßùô©ùôöùôô ùôûùô£ùô©ùô§ ùô©ùôùùôö ùôãùòæùôÑ_ùôÄùôìùôãùôçùôÄùôéùôé ùô®ùô°ùô§ùô©ùô® ùô§ùô£ ùô©ùôùùôö ùô¢ùô§ùô©ùôùùôöùôßùôóùô§ùôñùôßùôô.\\n\\n  * ùó¶ùóºùó≥ùòÅùòÑùóÆùóøùó≤ ùóüùóÆùòÜùó≤ùóø\\n\\n    * At this layer, python/docker or any other program, by default is seeing the ùôÇùôãùôêùô® ùôûùô£ ùô©ùôùùôö \"ùôÅùòºùôéùôèùôÄùôéùôè_ùôÅùôÑùôçùôéùôè\" ùô§ùôßùôôùôöùôß, meaning it will take the ùôÇùôãùôê ùô¨ùôûùô©ùôù ùô©ùôùùôö ùôùùôûùôúùôùùôöùô®ùô© ùòæùòæ (ùôòùô™ùôôùôñ ùôòùôñùô•ùôñùôóùôûùô°ùôûùô©ùôÆ) ùô§ùô£ ùô©ùôùùôö ùôõùôûùôßùô®ùô© ùôûùô£ùôôùôöùô≠.\\n\\nThe solution here is to condition the applications at the Software Layer to\\nrespect the System Layer ordering by setting the env variable:\\n\\n    \\n    \\n    ùòæùôêùòøùòº_ùòøùôÄùôëùôÑùòæùôÄùôé_ùôäùôçùòøùôÄùôç = \"ùôãùòæùôÑ_ùòΩùôêùôé_ùôÑùòø\"\\n\\nDecoding ML Newsletter is a reader-supported publication. To receive new posts\\nand support my work, consider becoming a free or paid subscriber.\\n\\nSubscribe\\n\\n10\\n\\nShare this post\\n\\n#### Upskill your LLM knowledge base with these tools.\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/upskill-your-llm-knowledge-base-with?r=1ttoeh'), ArticleDocument(id=UUID('4d1d7d1c-ebd2-445e-a8d7-bdfc1c90cfc6'), content={'Title': 'An end-to-end framework for production-ready LLM systems', 'Subtitle': 'Learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Learn an end-to-end framework for production-ready LLM systems by\\nbuilding your LLM twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Learn an end-to-end framework for production-ready LLM systems by building\\nyour LLM twin\\n\\n### Why you should take our new production-ready LLMs course\\n\\nPaul Iusztin\\n\\nMar 16, 2024\\n\\n18\\n\\nShare this post\\n\\n#### Learn an end-to-end framework for production-ready LLM systems by\\nbuilding your LLM twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nWant to ùóπùó≤ùóÆùóøùóª an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ for ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ by\\nùóØùòÇùó∂ùóπùó±ùó∂ùóªùó¥ your ùóüùóüùó† ùòÅùòÑùó∂ùóª?\\n\\nThen you are in luck.\\n\\n‚Üì‚Üì‚Üì\\n\\nThe Decoding ML team and I will ùóøùó≤ùóπùó≤ùóÆùòÄùó≤ (in a few days) a ùóôùó•ùóòùóò ùó∞ùóºùòÇùóøùòÄùó≤ called\\nthe ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø ùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ.\\n\\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóÆùóª ùóüùóüùó† ùóßùòÑùó∂ùóª? It is an AI character that learns to write like somebody\\nby incorporating its style and personality into an LLM.\\n\\n> **Within** the**course,** you**** will**learn how** to**:**\\n>\\n>   * architect\\n>\\n>   * train\\n>\\n>   * deploy\\n>\\n>\\n\\n>\\n> ...a ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùóøùó≤ùóÆùó±ùòÜ ùóüùóüùó† ùòÅùòÑùó∂ùóª of yourself powered by LLMs, vector DBs, and\\n> LLMOps good practices, such as:\\n>\\n>   * experiment trackers\\n>\\n>   * model registries\\n>\\n>   * prompt monitoring\\n>\\n>   * versioning\\n>\\n>   * deploying LLMs\\n>\\n>\\n\\n>\\n> ...and more!\\n\\nIt is an ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùóüùóüùó† ùó∞ùóºùòÇùóøùòÄùó≤ where you will ùóØùòÇùó∂ùóπùó± a ùóøùó≤ùóÆùóπ-ùòÑùóºùóøùóπùó± ùóüùóüùó† ùòÄùòÜùòÄùòÅùó≤ùó∫:\\n\\n‚Üí from start to finish\\n\\n‚Üí from data collection to deployment\\n\\n‚Üí production-ready\\n\\n‚Üí from NO MLOps to experiment trackers, model registries, prompt monitoring,\\nand versioning\\n\\nImage by DALL-E\\n\\n* * *\\n\\n### Who is this for?\\n\\n**Audience:** MLE, DE, DS, or SWE who want to learn to engineer production-\\nready LLM systems using LLMOps good principles.\\n\\n**Level:** intermediate\\n\\n**Prerequisites:** basic knowledge of Python, ML, and the cloud\\n\\n### **How will you learn?**\\n\\nThe course contains **11 hands-on written lessons** and the **open-source\\ncode** you can access on GitHub (WIP).\\n\\nYou can read everything at your own pace.\\n\\n### Costs?\\n\\nThe **articles** and **code** are **completely free**. They will always remain\\nfree.\\n\\nThis time, the Medium articles won't be under any paid wall. I want to make\\nthem entirely available to everyone.\\n\\n### **Meet your teachers!**\\n\\nThe course is created under the Decoding ML umbrella by:\\n\\n  * Paul Iusztin | Senior ML & MLOps Engineer\\n\\n  * Alex Vesa | Senior AI Engineer\\n\\n  * Alex Razvant | Senior ML & MLOps Engineer\\n\\n* * *\\n\\n## What will you learn to build?\\n\\nLM twin system architecture [Image by the Author]\\n\\nüêç ùòõùò©ùò¶ ùòìùòìùòî ùò¢ùò≥ùò§ùò©ùò™ùòµùò¶ùò§ùòµùò∂ùò≥ùò¶ ùò∞ùòß ùòµùò©ùò¶ ùò§ùò∞ùò∂ùò≥ùò¥ùò¶ ùò™ùò¥ ùò¥ùò±ùò≠ùò™ùòµ ùò™ùòØùòµùò∞ 4 ùòóùò∫ùòµùò©ùò∞ùòØ ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥:\\n\\nùóßùóµùó≤ ùó±ùóÆùòÅùóÆ ùó∞ùóºùóπùóπùó≤ùó∞ùòÅùó∂ùóºùóª ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Crawl your digital data from various social media platforms.\\n\\n\\\\- Clean, normalize and load the data to a NoSQL DB through a series of ETL\\npipelines.\\n\\n\\\\- Send database changes to a queue using the CDC pattern.\\n\\n‚òÅ Deployed on AWS.\\n\\nùóßùóµùó≤ ùó≥ùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Consume messages from a queue through a Bytewax streaming pipeline.\\n\\n\\\\- Every message will be cleaned, chunked, embedded (using Superlinked), and\\nloaded into a Qdrant vector DB in real-time.\\n\\n‚òÅ Deployed on AWS.\\n\\nùóßùóµùó≤ ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Create a custom dataset based on your digital data.\\n\\n\\\\- Fine-tune an LLM using QLoRA.\\n\\n\\\\- Use Comet ML's experiment tracker to monitor the experiments.\\n\\n\\\\- Evaluate and save the best model to Comet's model registry.\\n\\n‚òÅ Deployed on Qwak.\\n\\nùóßùóµùó≤ ùó∂ùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\n\\n\\\\- Load and quantize the fine-tuned LLM from Comet's model registry.\\n\\n\\\\- Deploy it as a REST API.\\n\\n\\\\- Enhance the prompts using RAG.\\n\\n\\\\- Generate content using your LLM twin.\\n\\n\\\\- Monitor the LLM using Comet's prompt monitoring dashboard .\\n\\n‚òÅ Deployed on Qwak.\\n\\n.\\n\\nùòàùò≠ùò∞ùòØùò® ùòµùò©ùò¶ 4 ùòÆùò™ùò§ùò≥ùò∞ùò¥ùò¶ùò≥ùò∑ùò™ùò§ùò¶ùò¥, ùò∫ùò∞ùò∂ ùò∏ùò™ùò≠ùò≠ ùò≠ùò¶ùò¢ùò≥ùòØ ùòµùò∞ ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò¶ 3 ùò¥ùò¶ùò≥ùò∑ùò¶ùò≥ùò≠ùò¶ùò¥ùò¥ ùòµùò∞ùò∞ùò≠ùò¥:\\n\\n\\\\- Comet ML as your ML Platform\\n\\n\\\\- Qdrant as your vector DB\\n\\n\\\\- Qwak as your ML infrastructure\\n\\n* * *\\n\\nSoon, we will release the first lesson from the ùóüùóüùó† ùóßùòÑùó∂ùóª: ùóïùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùó¨ùóºùòÇùóø\\nùó£ùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª-ùó•ùó≤ùóÆùó±ùòÜ ùóîùóú ùó•ùó≤ùóΩùóπùó∂ùó∞ùóÆ\\n\\nTo stay updated...\\n\\nùòæùôùùôöùôòùô† ùôûùô© ùô§ùô™ùô© ùôÇùôûùô©ùôÉùô™ùôó ùôñùô£ùôô ùô®ùô™ùô•ùô•ùô§ùôßùô© ùô™ùô® ùô¨ùôûùô©ùôù ùôñ ‚≠êÔ∏è\\n\\n‚Üì‚Üì‚Üì\\n\\nüîó _**LLM Twin: Building Your Production-Ready AI Replica** Course GitHub\\nRepository_\\n\\n18\\n\\nShare this post\\n\\n#### Learn an end-to-end framework for production-ready LLM systems by\\nbuilding your LLM twin\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/want-to-learn-an-end-to-end-framework?r=1ttoeh'), ArticleDocument(id=UUID('1dbefe69-acbf-4b86-8b52-0670b28dbab4'), content={'Title': 'Fix your messy ML configs in your Python projects', 'Subtitle': '2024 MLOps learning roadmap. Python syntax sugar that will help you write cleaner code.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### Fix your messy ML configs in your Python projects\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# Fix your messy ML configs in your Python projects\\n\\n### 2024 MLOps learning roadmap. Python syntax sugar that will help you write\\ncleaner code.\\n\\nPaul Iusztin\\n\\nMar 09, 2024\\n\\n13\\n\\nShare this post\\n\\n#### Fix your messy ML configs in your Python projects\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Decoding ML Notes_\\n\\nThis week our main focus will be a classic.\\n\\n> We will discuss Python.\\n>\\n> More concretely how to write cleaner code and applications in Python. üî•\\n\\nIs that even possible? üíÄ\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * My favorite way to implement a configuration layer in Python\\n\\n  * Some Python syntax sugar that will help you write cleaner code\\n\\n  * 2024 MLOps learning roadmap\\n\\n* * *\\n\\nSince creating content, I learned one crucial thing: \"ùòåùò∑ùò¶ùò≥ùò∫ùò£ùò∞ùò•ùò∫ ùò≠ùò™ùò¨ùò¶ùò¥ ùòµùò∞ ùò≥ùò¶ùò¢ùò•\\nùò¢ùòØùò• ùò≠ùò¶ùò¢ùò≥ùòØ ùò•ùò™ùòßùòßùò¶ùò≥ùò¶ùòØùòµùò≠ùò∫.\"\\n\\n> Do you prefer to read content on Medium?\\n\\nThen, you are in luck.\\n\\nDecoding ML is also on Medium.\\n\\n**Substack vs. Medium?**\\n\\nOn Medium, we plan to post more extended and detailed content, while on\\nSubstack, we will write on the same topics but in a shorter and more\\nconcentrated manner.\\n\\nIf you want more code and less talking‚Ä¶\\n\\n _Check out our Medium publication_ üëÄ\\n\\n‚Üì‚Üì‚Üì\\n\\n‚ûî üîó Decoding ML Medium publication\\n\\nüîó Decoding ML Medium publication\\n\\n* * *\\n\\n### My favorite way to implement a configuration layer in Python\\n\\nThis is my favorite way to ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ a ùó∞ùóºùóªùó≥ùó∂ùó¥ùòÇùóøùóÆùòÅùó∂ùóºùóª/ùòÄùó≤ùòÅùòÅùó∂ùóªùó¥ùòÄ ùòÄùòÜùòÄùòÅùó≤ùó∫ in ùó£ùòÜùòÅùóµùóºùóª\\nfor all my apps ‚Üì\\n\\nThe core is based on ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§, a data validation library for Python.\\n\\nMore precisely, on their ùòâùò¢ùò¥ùò¶ùòöùò¶ùòµùòµùò™ùòØùò®ùò¥ class.\\n\\nùó™ùóµùòÜ ùòÇùòÄùó≤ ùòÅùóµùó≤ ùóΩùòÜùó±ùóÆùóªùòÅùó∂ùó∞ ùóïùóÆùòÄùó≤ùó¶ùó≤ùòÅùòÅùó∂ùóªùó¥ùòÄ ùó∞ùóπùóÆùòÄùòÄ?\\n\\n\\\\- you can quickly load values from .ùò¶ùòØùò∑ files (or even ùòëùòöùòñùòï or ùò†ùòàùòîùòì)\\n\\n\\\\- add default values for the configuration of your application\\n\\n\\\\- the MOST IMPORTANT one ‚Üí It validates the type of the loaded variables.\\nThus, you will always be ensured you use the correct variables to configure\\nyour system.\\n\\nùóõùóºùòÑ ùó±ùóº ùòÜùóºùòÇ ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùó∂ùòÅ?\\n\\nIt is pretty straightforward.\\n\\nYou subclass the ùòâùò¢ùò¥ùò¶ùòöùò¶ùòµùòµùò™ùòØùò®ùò¥ class and define all your settings at the class\\nlevel.\\n\\nIt is similar to a Python ùò•ùò¢ùòµùò¢ùò§ùò≠ùò¢ùò¥ùò¥ but with an extra layer of data validation\\nand factory methods.\\n\\nIf you assign a value to the variable, it makes it optional.\\n\\nIf you leave it empty, providing it in your .ùôöùô£ùô´ file is mandatory.\\n\\nùóõùóºùòÑ ùó±ùóº ùòÜùóºùòÇ ùó∂ùóªùòÅùó≤ùó¥ùóøùóÆùòÅùó≤ ùó∂ùòÅ ùòÑùó∂ùòÅùóµ ùòÜùóºùòÇùóø ùó†ùóü ùó∞ùóºùó±ùó≤?\\n\\nYou often have a training configuration file (or inference) into a JSON or\\nYAML file (I prefer YAML files as they are easier to read).\\n\\nYou shouldn\\'t pollute your ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§ settings class with all the\\nhyperparameters related to the module (as they are a lot, A LOT).\\n\\nAlso, to isolate the application & ML settings, the easiest way is to add the\\nùòµùò≥ùò¢ùò™ùòØùò™ùòØùò®_ùò§ùò∞ùòØùòßùò™ùò®_ùò±ùò¢ùòµùò© in your settings and use a ùòõùò≥ùò¢ùò™ùòØùò™ùòØùò®ùòäùò∞ùòØùòßùò™ùò® class to load\\nit independently.\\n\\nDoing so lets you leverage your favorite way (probably the one you already\\nhave in your ML code) of loading a config file for the ML configuration: plain\\nYAML or JSON files, hydra, or other fancier methods.\\n\\nAnother plus is that you can\\'t hardcode the path anywhere on your system. That\\nis a nightmare when you start using git with multiple people.\\n\\npydantic BaseSettings example [Image by the Author]\\n\\nWhat do you say? Would you start using the ùò±ùò∫ùò•ùò¢ùòØùòµùò™ùò§ ùòâùò¢ùò¥ùò¶ùòöùò¶ùòµùòµùò™ùòØùò®ùò¥ class in your\\nML applications?\\n\\n* * *\\n\\n### Some Python syntax sugar that will help you write cleaner code\\n\\nHere is some ùó£ùòÜùòÅùóµùóºùóª ùòÄùòÜùóªùòÅùóÆùòÖ ùòÄùòÇùó¥ùóÆùóø that will help you ùòÑùóøùó∂ùòÅùó≤ ùó∞ùóπùó≤ùóÆùóªùó≤ùóø ùó∞ùóºùó±ùó≤ ‚Üì\\n\\nI am talking about the ùò∏ùò¢ùò≠ùò≥ùò∂ùò¥ ùò∞ùò±ùò¶ùò≥ùò¢ùòµùò∞ùò≥ denoted by the `:=` symbol.\\n\\nIt was introduced in Python 3.8, but I rarely see it used.\\n\\nThus, as a \"clean code\" freak, I wanted to dedicate a post to it.\\n\\nùó™ùóµùóÆùòÅ ùó±ùóºùó≤ùòÄ ùòÅùóµùó≤ ùòÑùóÆùóπùóøùòÇùòÄ ùóºùóΩùó≤ùóøùóÆùòÅùóºùóø ùó±ùóº?\\n\\nIt\\'s an assignment expression that allows you to assign and return a value in\\nthe same expression.\\n\\nùó™ùóµùòÜ ùòÄùóµùóºùòÇùóπùó± ùòÜùóºùòÇ ùòÇùòÄùó≤ ùó∂ùòÅ?\\n\\nùòäùò∞ùòØùò§ùò™ùò¥ùò¶ùòØùò¶ùò¥ùò¥: It reduces the number of lines needed for variable assignment and\\nchecking, making code more concise.\\n\\nùòôùò¶ùò¢ùò•ùò¢ùò£ùò™ùò≠ùò™ùòµùò∫: It can enhance readability by keeping related logic close,\\nalthough this depends on the context and the reader\\'s familiarity with exotic\\nPython syntax.\\n\\nùôÉùôöùôßùôö ùôñùôßùôö ùô®ùô§ùô¢ùôö ùôöùô≠ùôñùô¢ùô•ùô°ùôöùô®\\n\\n‚Üì‚Üì‚Üì\\n\\n1\\\\. Using the walrus operator, you can directly assign the result of the ùò≠ùò¶ùòØ()\\nfunction inside an if statement.\\n\\n2\\\\. Avoid calling the same function twice in a while loop. The benefit is less\\ncode and makes everything more readable.\\n\\n3\\\\. Another use case arises in list comprehensions where a value computed in a\\nfiltering condition is also needed in the expression body. Before the ùò∏ùò¢ùò≠ùò≥ùò∂ùò¥\\nùò∞ùò±ùò¶ùò≥ùò¢ùòµùò∞ùò≥, if you had to apply a function to an item from a list and filter it\\nbased on some criteria, you had to refactor it to a standard for loop.\\n\\n.\\n\\nWhen writing clean code, the detail matters.\\n\\nThe details make the difference between a codebase that can be read like a\\nbook or one with 10 WTFs / seconds.\\n\\nThe walrus operator examples [Image by the Author]\\n\\nWhat do you think? Does the walrus operator make the Python code more readable\\nand concise?\\n\\n* * *\\n\\n### 2024 MLOps learning roadmap\\n\\nùó™ùóÆùóªùòÅ to ùóπùó≤ùóÆùóøùóª ùó†ùóüùó¢ùóΩùòÄ but got stuck at the 100th tool you think you must know?\\nHere is the ùó†ùóüùó¢ùóΩùòÄ ùóøùóºùóÆùó±ùó∫ùóÆùóΩ ùó≥ùóºùóø ùüÆùü¨ùüÆùü∞ ‚Üì  \\n  \\nùòîùòìùòñùò±ùò¥ ùò∑ùò¥. ùòîùòì ùò¶ùòØùò®ùò™ùòØùò¶ùò¶ùò≥  \\n  \\nIn theory, MLEs focus on deploying models to production while MLOps engineers\\nbuild the platform used by MLEs.  \\n  \\nI think this is heavily dependent on the scale of the company. As the company\\ngets smaller, these 2 roles start to overlap more.  \\n  \\nThis roadmap will teach you how to build such a platform, from programming\\nskills to MLOps components and infrastructure as code.  \\n  \\n.  \\n  \\nHere is the MLOps roadmap for 2024 suggested by\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\n:  \\n  \\nùü≠\\\\. ùó£ùóøùóºùó¥ùóøùóÆùó∫ùó∫ùó∂ùóªùó¥  \\n\\\\- Python & IDEs  \\n\\\\- Bash basics & command line editors  \\n  \\nùüÆ\\\\. ùóñùóºùóªùòÅùóÆùó∂ùóªùó≤ùóøùó∂ùòáùóÆùòÅùó∂ùóºùóª ùóÆùóªùó± ùóûùòÇùóØùó≤ùóøùóªùó≤ùòÅùó≤ùòÄ  \\n\\\\- Docker  \\n\\\\- Kubernetes  \\n  \\nùüØ\\\\. ùó†ùóÆùó∞ùóµùó∂ùóªùó≤ ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ ùó≥ùòÇùóªùó±ùóÆùó∫ùó≤ùóªùòÅùóÆùóπùòÄ  \\n  \\n...until now we laid down the fundamentals. Now let\\'s get into MLOps üî•  \\n  \\nùü∞\\\\. ùó†ùóüùó¢ùóΩùòÄ ùóΩùóøùó∂ùóªùó∞ùó∂ùóΩùóπùó≤ùòÄ  \\n\\\\- reproducible,  \\n\\\\- testable, and  \\n\\\\- evolvable ML-powered software  \\n  \\nùü±\\\\. ùó†ùóüùó¢ùóΩùòÄ ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅùòÄ  \\n\\\\- Version control & CI/CD pipelines  \\n\\\\- Orchestration  \\n\\\\- Experiment tracking and model registries  \\n\\\\- Data lineage and feature stores  \\n\\\\- Model training & serving  \\n\\\\- Monitoring & observability  \\n  \\nùü≤\\\\. ùóúùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ ùóÆùòÄ ùó∞ùóºùó±ùó≤  \\n\\\\- Terraform\\n\\n2024 MLOps Learning Roadmap [Image by the Author]\\n\\nAs a self-learner, I wish I had access to this step-by-step plan when I\\nstarted learning MLOps.  \\n  \\nRemember, you should pick up and tailor this roadmap at the level you are\\ncurrently at.  \\n  \\nFind more details about the roadmap in\\n\\nMaria Vechtomova\\n\\narticle ‚Üì  \\n  \\n‚ûî üîó MLOps roadmap 2024\\n\\n13\\n\\nShare this post\\n\\n#### Fix your messy ML configs in your Python projects\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/my-favorite-way-to-implement-a-configuration?r=1ttoeh'), ArticleDocument(id=UUID('ba6ba94f-b2d0-4ad8-9dbc-638f5eb1a081'), content={'Title': 'A Real-time Retrieval System for RAG on Social Media Data', 'Subtitle': 'Use a Bytewax streaming engine to build a real-time ingestion pipeline to populate a Qdrant vector DB. Implement a RAG retrieval client using rerank.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# A Real-time Retrieval System for RAG on Social Media Data\\n\\n### Use a streaming engine to populate a vector DB in real time. Use rerank &\\nUMAP to improve the accuracy of your retrieved documents.\\n\\nPaul Iusztin\\n\\nMar 07, 2024\\n\\n31\\n\\nShare this post\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n4\\n\\nShare\\n\\n> We are putting in a lot of time to create high-quality content. Thus, we\\n> want to make it as convenient as possible for you to read our content.\\n>\\n> That is why we will experiment with the **posting time** and **move** it to\\n> **Thursday** at **3:00 PM CET**.\\n\\nIn this article, you will learn how to build a real-time retrieval system for\\nsocial media data. In our example, we will use only my LinkedIn posts, but our\\nimplementation can easily be extended to other platforms supporting written\\ncontent, such as X, Instagram, or Medium.\\n\\n**In this article, you will learn how to:**\\n\\n  * build a streaming pipeline that ingests LinkedIn posts into a vector DB in real-time\\n\\n  * clean, chunk, and embed LinkedIn posts\\n\\n  * build a retrieval client to query LinkedIn posts\\n\\n  * use a rerank pattern to improve retrieval accuracy\\n\\n  * visualize content retrieved for a given query in a 2D plot using UMAP\\n\\nOur implementation focuses on just the retrieval part of an RAG system. But\\nyou can quickly hook the retrieved LinkedIn posts to an LLM for post analysis\\nor personalized content generation.\\n\\n* * *\\n\\n## Table of Contents:\\n\\n  1. System Design\\n\\n  2. Data\\n\\n  3. Streaming ingestion pipeline\\n\\n  4. Retrieval client\\n\\n  5. Conclusion\\n\\n* * *\\n\\n### 1\\\\. System Design\\n\\nThe architecture of the retrieval system [Image by the Author - in\\ncollaboration with VectorHub].\\n\\nThe retrieval system is based on 2 detached components:\\n\\n  1. the streaming ingestion pipeline\\n\\n  2. the retrieval client\\n\\nThe **streaming ingestion pipeline** runs 24/7 to keep the vector DB synced up\\nwith current raw LinkedIn posts data source, while the **retrieval client** is\\nused in RAG applications to query the vector DB. These 2 components\\n**communicate with each other only through the vector DB**.\\n\\n#### **1.1. The streaming ingestion pipeline**\\n\\nThe streaming ingestion pipeline implements the Change Data Capture (CDC)\\npattern between a data source containing the raw LinkedIn posts and the vector\\nDB used for retrieval.\\n\\nIn a real-world scenario, the streaming pipeline listens to a queue populated\\nby all the changes made to the source database. But because we are focusing\\nprimarily on the retrieval system, we simulate the data within the queue with\\na couple of JSON files.\\n\\nThe streaming pipeline is built in Python using Bytewax, and cleans, chunks,\\nand embeds the LinkedIn posts before loading them into a Qdrant vector DB.\\n\\n**Why do we need a stream engine?**\\n\\nBecause LinkedIn posts (or any other social media data) evolve frequently,\\nyour vector DB can quickly get out of sync. To handle this, you can build a\\nbatch pipeline that runs every minute. But to really minimize data lag, to\\n**make sure your vector DB stays current with new social media posts** , you\\nneed to use a streaming pipeline that **immediately** takes every new item the\\nmoment it\\'s posted, preprocesses it, and loads it into the vector DB.\\n\\n**Why Bytewax?**\\n\\nBytewax is a streaming engine built in Rust that exposes a Python interface.\\nWe use Bytewax because it combines the impressive speed and reliability of\\nRust with the ease of use and ecosystem of Python.\\n\\n#### 1.2. The retrieval client\\n\\nOur retrieval client is a standard Python module that preprocesses user\\nqueries and searches the vector DB for most similar results. Qdrant vector DB\\nlets us decouple the retrieval client from the streaming ingestion pipeline.\\n\\nUsing a semantic-based retrieval system lets us query our LinkedIn post\\ncollection very flexibly. For example, we can retrieve similar posts using a\\nvariety of query types - e.g., posts, questions, sentences.\\n\\nAlso, to improve the retrieval system\\'s accuracy, we use a rerank pattern.\\n\\nLastly, to better understand and explain the retrieval process for particular\\nqueries, we visualize our results on a 2D plot using UMAP.\\n\\n### 2\\\\. Data\\n\\nWe will ingest 215 LinkedIn posts from my Linked profile - Paul Iusztin.\\nThough we simulate the post ingestion step using JSON files, the posts\\nthemselves are authentic.\\n\\nBefore diving into the code, let\\'s take a look at an example LinkedIn post to\\nfamiliarize ourselves with the challenges it will introduce ‚Üì\\n\\n    \\n    \\n    [\\n        {\\n            \"text\": \"ùó™ùóµùóÆùòÅ do you need to ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ an open-source ùóüùóüùó† to create your own ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùó±ùòÉùó∂ùòÄùóºùóø?\\\\nThis is the ùóüùóüùó† ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùó∏ùó∂ùòÅ you must know ‚Üì\\\\nùóóùóÆùòÅùóÆùòÄùó≤ùòÅ\\\\nThe key component of any successful ML project is the data.\\\\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\\\\nThe best approach is to hire a bunch of experts to create it manually.\\\\nBut, for a PoC, that might get expensive & slow.\\\\nThe good news is that a method called \\\\\"ùòçùò™ùòØùò¶ùòµùò∂ùòØùò™ùòØùò® ùò∏ùò™ùòµùò© ùò•ùò™ùò¥ùòµùò™ùò≠ùò≠ùò¢ùòµùò™ùò∞ùòØ\\\\\" exists.\\\\n \\n    ...\\n    Along with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\\\\n‚Ü≥ Beam: üîó\\\\nhttps://lnkd.in/dedCaMDh\\\\n.\\\\n‚Ü≥ To see all these components in action, check out my FREE ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ ùó∞ùóºùòÇùóøùòÄùó≤ & give it a ‚≠ê:  üîó\\\\nhttps://lnkd.in/dZgqtf8f\\\\nhashtag\\\\n#\\\\nmachinelearning\\\\nhashtag\\\\n#\\\\nmlops\\\\nhashtag\\\\n#\\\\ndatascience\",\\n            \"image\": \"https://media.licdn.com/dms/image/D4D10AQHWQzZcToQQ1Q/image-shrink_800/0/1698388219549?e=1705082400&v=beta&t=9mrDC_NooJgD7u7Qk0PmrTGGaZtuwDIFKh3bEqeBsm0\"\\n        }\\n    ]\\n\\nThe following features of the above post are not compatible with embedding\\nmodels. We\\'ll need to find some way of handling them in our preprocessing\\nstep:\\n\\n  * emojis\\n\\n  * bold, italic text\\n\\n  * other non-ASCII characters\\n\\n  * URLs\\n\\n  * content that exceeds the context window limit of the embedding model\\n\\nEmojis and bolded and italic text are represented by Unicode characters that\\nare not available in the vocabulary of the embedding model. Thus, these items\\ncannot be tokenized and passed to the model; we have to remove them or\\nnormalize them to something that can be parsed by the tokenizer. The same\\nholds true for all other non-ASCII characters.\\n\\nURLs take up space in the context window without providing much semantic\\nvalue. Still, knowing that there\\'s a URL in the sentence may add context. For\\nthis reason, we replace all URLs with a [URL] token. This lets us ingest\\nwhatever value the URL\\'s presence conveys without it taking up valuable space.\\n\\n### 3\\\\. Streaming ingestion pipeline\\n\\nLet\\'s dive into the streaming pipeline, starting from the top and working our\\nway to the bottom ‚Üì\\n\\n#### 3.1. The Bytewax flow\\n\\n**The Bytewax flow** transparently conveys all the steps of the streaming\\npipeline.\\n\\nThe first step is ingesting every LinkedIn post from our JSON files. In the\\nnext steps, every map operation has a single responsibility:\\n\\n  * validate the ingested data using a _RawPost pydantic model_\\n\\n  * clean the posts\\n\\n  * chunk the posts; because chunking will output a list of ChunkedPost objects, we use a flat_map operation to flatten them out\\n\\n  * embed the posts\\n\\n  * load the posts to a Qdrant vector DB\\n\\n    \\n    \\n    def build_flow():\\n        embedding_model = EmbeddingModelSingleton()\\n    \\n        flow = Dataflow(\"flow\")\\n    \\n        stream = op.input(\"input\", flow, JSONSource([\"data/paul.json\"]))\\n        stream = op.map(\"raw_post\", stream, RawPost.from_source)\\n        stream = op.map(\"cleaned_post\", stream, CleanedPost.from_raw_post)\\n        stream = op.flat_map(\\n            \"chunked_post\",\\n            stream,\\n            lambda cleaned_post: ChunkedPost.from_cleaned_post(\\n                cleaned_post, embedding_model=embedding_model\\n            ),\\n        )\\n        stream = op.map(\\n            \"embedded_chunked_post\",\\n            stream,\\n            lambda chunked_post: EmbeddedChunkedPost.from_chunked_post(\\n                chunked_post, embedding_model=embedding_model\\n            ),\\n        )\\n        op.inspect(\"inspect\", stream, print)\\n        op.output(\\n            \"output\", stream, QdrantVectorOutput(vector_size=model.embedding_size)\\n        )\\n        \\n        return flow\\n\\n#### 3.2. The processing steps\\n\\nEvery processing step is incorporated into a _pydantic model_. This way, we\\ncan easily validate the data at each step and reuse the code in the retrieval\\nmodule.\\n\\nWe isolate every step of an ingestion pipeline into its own class:\\n\\n  * cleaning\\n\\n  * chunking\\n\\n  * embedding \\n\\nDoing so, we follow the separation of concerns good SWE practice. Thus, every\\nclass has its own responsibility.\\n\\nNow the code is easy to read and understand. Also, it‚Äôs future-proof, as it‚Äôs\\nextremely easy to change or extend either of the 3 steps: cleaning, chunking\\nand embedding.\\n\\nHere is the interface of the _pydantic models_ :\\n\\n    \\n    \\n    class RawPost(BaseModel):\\n        post_id: str\\n        text: str\\n        image: Optional[str]\\n    \\n        @classmethod\\n        def from_source(cls, k_v: Tuple[str, dict]) -> \"RawPost\":\\n            ... # Mapping a dictionary to a RawPost validated pydantic model.\\n    \\n            return cls(...)\\n    \\n    class CleanedPost(BaseModel):\\n        post_id: str\\n        raw_text: str\\n        text: str\\n        image: Optional[str]\\n    \\n        @classmethod\\n        def from_raw_post(cls, raw_post: RawPost) -> \"CleanedPost\":\\n            ... # Cleaning the raw post\\n    \\n            return cls(...)\\n    \\n    \\n    class ChunkedPost(BaseModel):\\n        post_id: str\\n        chunk_id: str\\n        full_raw_text: str\\n        text: str\\n        image: Optional[str]\\n    \\n        @classmethod\\n        def from_cleaned_post(\\n            cls, cleaned_post: CleanedPost, embedding_model: EmbeddingModelSingleton\\n        ) -> list[\"ChunkedPost\"]:\\n            chunks = ... # Compute chunks\\n    \\n            return [cls(...) for chunk in chunks]\\n    \\n    \\n    class EmbeddedChunkedPost(BaseModel):\\n        post_id: str\\n        chunk_id: str\\n        full_raw_text: str\\n        text: str\\n        text_embedding: list\\n        image: Optional[str] = None\\n        score: Optional[float] = None\\n        rerank_score: Optional[float] = None\\n    \\n        @classmethod\\n        def from_chunked_post(\\n            cls, chunked_post: ChunkedPost, embedding_model: EmbeddingModelSingleton\\n        ) -> \"EmbeddedChunkedPost\":\\n            ... # Compute embedding.\\n    \\n            return cls(...)\\n    \\n\\nNow, the data at each step is validated and has a clear structure.\\n\\n**Note:** Providing different types when instantiating a _pydantic_ model will\\nthrow a validation error. For example, if the  _post_id_ is defined as a\\n_string_ , and we try to instantiate an  _EmbeddedChunkedPost_ with a  _None_\\nor  _int_  _post_id_ , it will throw an error.\\n\\n> Check out the full implementation on our üîó GitHub Articles Hub repository.\\n\\n#### 3.3. Load to Qdrant\\n\\nTo load the LinkedIn posts to Qdrant, you have to override Bytewax\\'s\\n_StatelessSinkPartition_ class (which acts as an **output** in a Bytewax\\nflow):\\n\\n    \\n    \\n    class QdrantVectorSink(StatelessSinkPartition):\\n        def __init__(\\n            self,\\n            client: QdrantClient,\\n            collection_name: str\\n        ):\\n            self._client = client\\n            self._collection_name = collection_name\\n    \\n        def write_batch(self, chunks: list[EmbeddedChunkedPost]):\\n            ... # Map chunks to ids, embeddings, and metadata.\\n    \\n            self._client.upsert(\\n                collection_name=self._collection_name,\\n                points=Batch(\\n                    ids=ids,\\n                    vectors=embeddings,\\n                    payloads=metadata,\\n                ),\\n            )\\n\\nWithin this class, you must overwrite the _write_batch()_ method, where we\\nwill serialize every _EmbeddedChunkedPost_ to a format expected by Qdrant and\\nload it to the vector DB.\\n\\n### 4\\\\. Retrieval client\\n\\nHere, we focus on preprocessing a user\\'s query, searching the vector DB, and\\npostprocessing the retrieved posts for maximum results.\\n\\nTo design the retrieval step, we implement a _QdrantVectorDBRetriever_ class\\nto expose all the necessary features for our retrieval client.\\n\\n    \\n    \\n    class QdrantVectorDBRetriever:\\n        def __init__(\\n            self,\\n            embedding_model: EmbeddingModelSingleton,\\n            vector_db_client: QdrantClient,\\n            cross_encoder_model: CrossEncoderModelSingleton\\n            vector_db_collection: str\\n        ):\\n            self._embedding_model = embedding_model\\n            self._vector_db_client = vector_db_client\\n            self._cross_encoder_model = cross_encoder_model\\n            self._vector_db_collection = vector_db_collection\\n    \\n        def search(\\n            self, query: str, limit: int = 3, return_all: bool = False\\n        ) -> Union[list[EmbeddedChunkedPost], dict[str, list]]:\\n            ... # Search the Qdrant vector DB based on the given query.\\n    \\n        def embed_query(self, query: str) -> list[list[float]]:\\n            ... # Embed the given query.\\n    \\n        def rerank(self, query: str, posts: list[EmbeddedChunkedPost]) -> list[EmbeddedChunkedPost]:\\n            ... # Rerank the posts relative to the given query.\\n    \\n        def render_as_html(self, post: EmbeddedChunkedPost) -> None:\\n            ... # Map the embedded post to HTML to display it.\\n\\n#### 4.1. Embed query\\n\\nWe must embed the query in precisely the same way we ingested our posts into\\nthe vector DB. Because the streaming pipeline is written in Python (thanks to\\nBytewax), and every preprocessing operation is modular, we can quickly\\nreplicate all the steps necessary to embed the query.\\n\\n    \\n    \\n    class QdrantVectorDBRetriever:\\n    \\n        ...\\n    \\n        def embed_query(self, query: str) -> list[list[float]]:\\n            cleaned_query = CleanedPost.clean(query)\\n            chunks = ChunkedPost.chunk(cleaned_query, self._embedding_model)\\n            embdedded_queries = [\\n                self._embedding_model(chunk, to_list=True) for chunk in chunks\\n            ]\\n    \\n            return embdedded_queries\\n\\n> Check out the full implementation on our üîó GitHub repository.\\n\\n#### 4.2. Plain retrieval\\n\\nLet‚Äôs try to retrieve a set of posts without using the rerank algorithm.\\n\\n    \\n    \\n    vector_db_retriever = QdrantVectorDBRetriever(\\n        embedding_model=EmbeddingModelSingleton(),\\n        vector_db_client=build_qdrant_client()\\n    )\\n    \\n    query = \"Posts about Qdrant\"\\n    retrieved_results = vector_db_retriever.search(query=query)\\n    for post in retrieved_results[\"posts\"]:\\n        vector_db_retriever.render_as_html(post)\\n\\nHere are the **top 2 retrieved results** sorted using the cosine similarity\\nscore ‚Üì\\n\\n**Result 1:**\\n\\nResult 1 for the \"Posts about Qdrant\" query (without using reranking) [Image\\nby the Author - in collaboration with VectorHub]\\n\\n**Result 2:**\\n\\nResult 2 for the \"Posts about Qdrant\" query (without using reranking) [Image\\nby the Author - in collaboration with VectorHub]\\n\\nYou can see from the results above, that starting from the second post the\\nresults are irrelevant. Even though it has a cosine similarly score of ~0.69\\nthe posts doesn‚Äôt contain any information about Qdrant or vector DBs.\\n\\n**Note:** We looked over the top 5 retrieved results. Nothing after the first\\npost was relevant. We haven‚Äôt added them here as the article is already too\\nlong.\\n\\n#### 4.3. Visualize retrieval\\n\\nTo visualize our retrieval, we implement a dedicated class that uses the UMAP\\ndimensionality reduction algorithm. We have picked UMAP as it preserves the\\ngeometric properties between points (e.g., the distance) in higher dimensions\\nwhen they are projected onto lower dimensions better than its peers (e.g.,\\nPCA, t-SNE).\\n\\nThe _RetrievalVisualizer_ computes the projected embeddings for the entire\\nvector space once. Afterwards, it uses the render() method to project only the\\ngiven query and retrieved posts, and plot them to a 2D graph.\\n\\n    \\n    \\n    class RetrievalVisualizer:\\n        def __init__(self, posts: list[EmbeddedChunkedPost]):\\n            self._posts = posts\\n    \\n            self._umap_transform = self._fit_model(self._posts)\\n            self._projected_post_embeddings = self.project_posts(self._posts)\\n    \\n        def _fit_model(self, posts: list[EmbeddedChunkedPost]) -> umap.UMAP:\\n            umap_transform = ... # Fit a UMAP model on the given posts.\\n    \\n            return umap_transform\\n    \\n        def project_posts(self, posts: list[EmbeddedChunkedPost]) -> np.ndarray:\\n            embeddings = np.array([post.text_embedding for post in posts])\\n    \\n            return self._project(embeddings=embeddings)\\n    \\n        def _project(self, embeddings: np.ndarray) -> np.ndarray:\\n            ... # Project the embeddings to 2D using UMAP.\\n    \\n            return umap_embeddings\\n    \\n        def render(\\n            self,\\n            embedded_queries: list[list[float]],\\n            retrieved_posts: list[EmbeddedChunkedPost],\\n        ) -> None:\\n          ... # Render the given queries & retrieved posts using matplotlib.\\n\\nLet\\'s take a look at the result to see how the _\" Posts about Qdrant\"_ query\\nlooks ‚Üì\\n\\nVisualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (without reranking)\\n[Image by the Author - in collaboration with VectorHub].\\n\\nOur results are not great. You can see how far the retrieved posts are from\\nour query in the vector space.\\n\\nCan we improve the quality of our retrieval system using the **rerank**\\nalgorithm?\\n\\n#### 4.4. Rerank\\n\\nWe use the _reranking_ algorithm to refine our retrieval for the initial\\nquery. Our initial retrieval step - because it used cosine similarity (or\\nsimilar distance metrics) to compute the distance between a query and post\\nembeddings - may have missed more complex (but essential) relationships\\nbetween the query and the documents in the vector space. Reranking leverages\\nthe power of transformer models that are capable of understanding more nuanced\\nsemantic relationships.\\n\\nWe use a **cross-encoder** model to implement the reranking step, so we can\\nscore the query relative to all retrieved posts individually. These scores\\ntake into consideration more complex relationships than cosine similarity can.\\nUnder the hood is a BERT classifier that outputs a number between 0 and 1\\naccording to how similar the 2 given sentences are. The BERT classifier\\noutputs 0 if they are entirely different and 1 if they are a perfect match.\\n\\nBi-Encoder vs. Cross-Encoder [Image by the Author - in collaboration with\\nVectorHub]\\n\\nBut, you might ask, \"_Why not use the**cross-encoder** model from the start if\\nit is that much better?\"_\\n\\nThe answer, in a word, is speed. Using a cross-encoder model to search your\\nwhole collection is much slower than using cosine similarity. To optimize your\\nretrieval, therefore, your reranking process should involve 2 steps:\\n\\n  1. an initial rough retrieval step using cosine similarity, which retrieves the top N items as potential candidates\\n\\n  2. filtering the rough search using the rerank strategy, which retrieves the top K items as your final results\\n\\nThe implementation is relatively straightforward. For each retrieved post, we\\ncreate a pair consisting of the (cleaned) query and the text of the post. We\\ndo this for all retrieved posts, resulting in a list of pairs.\\n\\nNext, we call a _cross-encoder/ms-marco-MiniLM-L-6-v2_ model (from sentence-\\ntransformers) to give the retrieved posts their rerank score. We then sort the\\nposts in descending order based on their rerank score.\\n\\n> Check out the rerank algorithm implementation on our üîó GitHub repository.\\n\\n#### 4.5. Visualize retrieval with rerank\\n\\nNow that we\\'ve added the rerank pattern to our retrieval system, let\\'s see if\\nit improves the results of our _\" Posts about Qdrant\"_ query ‚Üì\\n\\n**Result 1**\\n\\nResult 1 for the \"Posts about Qdrant\" query (using reranking) [Image by the\\nAuthor - in collaboration with VectorHub]\\n\\n**Result 2:**\\n\\nResult 2 for the \"Posts about Qdrant\" query (using reranking) [Image by the\\nAuthor - in collaboration with VectorHub]\\n\\nThe improvement is remarkable! All our results are about Qdrant and vector\\nDBs.\\n\\n**Note:** We looked over the top 5 retrieved results. The top 4 out of 5 posts\\nare relevant to our query, which is incredible.\\n\\nNow, let\\'s look at the UMAP visualization:\\n\\nVisualization of the ‚ÄúPosts about Qdrant‚Äù query using UMAP (with reranking)\\n[Image by the Author - in collaboration with VectorHub].\\n\\nWhile the returned posts aren\\'t very close to the query, they are **a lot\\ncloser to the query compared to when we weren\\'t reranking the retrieved\\nposts**.\\n\\n* * *\\n\\n### 5\\\\. Conclusion\\n\\nIn this article, we learned how to adapt a RAG retrieval pattern to improve\\nLinkedIn post retrieval. To keep our database up to date with rapidly changing\\nsocial media data, we implemented a real-time streaming pipeline that uses CDC\\nto sync the raw LinkedIn posts data source with a vector DB. You also saw how\\nto use Bytewax to write - using only Python - a streaming pipeline that\\ncleans, chunks, and embeds LinkedIn posts.\\n\\nFinally, you learned how to implement a standard retrieval client for RAG and\\nsaw how to improve it using the rerank pattern. As retrieval is complex to\\nevaluate, you saw how to visualize the retrieval for a given query by\\nrendering all the posts, the query, and the retrieved posts in a 2D space\\nusing UMAP.\\n\\n> This **article** is a **summary** __ of **my contribution** from\\n> **VectorHub**. Check out the full article here to **dig** **into** the\\n> **details,** the**code** and **more experiments**.\\n\\n31\\n\\nShare this post\\n\\n#### A Real-time Retrieval System for RAG on Social Media Data\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n4\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| OlaMar 8Liked by Paul IusztinNice read, full of insights.Expand full\\ncommentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n| VenkataMar 23Liked by Paul IusztinExcellent article. Thanks a lot for\\nposting this.Expand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n2 more comments...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/a-real-time-retrieval-system-for?r=1ttoeh'), ArticleDocument(id=UUID('cb6e689e-e718-42c8-80b1-44db7d568c3b'), content={'Title': '4 key decoding strategies for LLMs that you must know', 'Subtitle': 'The only 6 prompt engineering techniques you need to know. One thing that I do that sets me apart from the crowd.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### 4 key decoding strategies for LLMs that you must know\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# 4 key decoding strategies for LLMs that you must know\\n\\n### The only 6 prompt engineering techniques you need to know. One thing that\\nI do that sets me apart from the crowd.\\n\\nPaul Iusztin\\n\\nFeb 15, 2024\\n\\n9\\n\\nShare this post\\n\\n#### 4 key decoding strategies for LLMs that you must know\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nHello everyone,\\n\\nI hope you enjoyed what Alex R. & Alex V. have prepared for you in their\\nprevious articles.\\n\\nI promised that the 3 of us would dig deeper into more exciting topics about\\nproduction-ready LLM and CV models.\\n\\n_‚Üí But this is just the beginning. Stay tuned for more production ML_ üî•\\n\\n* * *\\n\\n### **This week‚Äôs topics:**\\n\\n  * 4 key decoding strategies for LLMs that you must know\\n\\n  * The only 6 prompt engineering techniques you need to know\\n\\n  * One thing that I do that sets me apart from the crowd\\n\\n* * *\\n\\n> Want to build your first ùóüùóüùó† ùóΩùóøùóºùó∑ùó≤ùó∞ùòÅ but don\\'t know where to start?\\n\\nIf you want to **learn** in a **structured** **way** to **build** hands-on\\n**LLM systems** using good **LLMOps** principles‚Ä¶\\n\\nWe want to **announce** that we just **released** **8 Medium lessons** for the\\n**Hands-on LLMs** **course** that will put you on the right track ‚Üì\\n\\nWithin the **8 Medium lessons** , you will go step-by-step through the\\n**theory** , **system** **design** , and **code** to learn how to build a:\\n\\n  * **real-time streaming pipeline** (deployed on AWS) that uses Bytewax as the stream engine to listen to financial news, cleans & embeds the documents, and loads them to a vector DB\\n\\n  * **fine-tuning pipeline** (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\\n\\n  * **inference pipeline** built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news)\\n\\nWe will also show you how to **integrate** various **serverless tools** , such\\nas:  \\n  \\n‚Ä¢ Comet ML as your ML Platform;  \\n‚Ä¢ Qdrant as your vector DB;  \\n‚Ä¢ Beam as your infrastructure.\\n\\nThe architecture of the system you will learn to build during the **Hands-on\\nLLMs** course [Image by the Author].\\n\\n**Who is this for?**\\n\\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM\\nsystems using LLMOps good principles.\\n\\n**How will you learn?**\\n\\nThe series contains 4 hands-on video lessons and the open-source code you can\\naccess on GitHub.\\n\\n**Curious?** ‚Üì\\n\\nCheck out the 8 Medium lessons of the Hands-on LLMs course and start building\\nyour own LLMs system:\\n\\nüîó The Hands-on LLMs Medium Series\\n\\n* * *\\n\\n### 4 key decoding strategies for LLMs that you must know\\n\\nYou see, LLMs don\\'t just spit out text.  \\n  \\nThey calculate \"logits\", which are mapped to probabilities for every possible\\ntoken in their vocabulary.  \\n  \\nIt uses previous token IDs to predict the next most likely token (the auto-\\nregressive nature of decoder models).  \\n  \\nThe real magic happens in the decoding strategy you pick ‚Üì  \\n  \\n\\\\- Greedy Search  \\n\\\\- Beam Search  \\n\\\\- Top-K Sampling  \\n\\\\- Nucleus Sampling  \\n  \\n.  \\n  \\nùóöùóøùó≤ùó≤ùó±ùòÜ ùó¶ùó≤ùóÆùóøùó∞ùóµ  \\n  \\nIt only holds onto the most likely token at each stage. It\\'s fast and\\nefficient, but it is short-sighted.  \\n  \\nùóïùó≤ùóÆùó∫ ùó¶ùó≤ùóÆùóøùó∞ùóµ  \\n  \\nThis time, you are not looking at just the token with the highest probability.\\nBut you are considering the N most likely tokens.  \\n  \\nThis will create a tree-like structure, where each node will have N children.  \\n  \\nThe procedure repeats until you hit a maximum length or an end-of-sequence\\ntoken.  \\n  \\nUltimately, you pick the leaf with the biggest score and recursively pick its\\nparent until you hit the root node.  \\n  \\nFor example, in the graph below, we have \"ùò£ùò¶ùò¢ùòÆùò¥ = 2\" and \"ùò≠ùò¶ùòØùò®ùòµùò© = 3\".  \\n  \\nùóßùóºùóΩ-ùóû ùó¶ùóÆùó∫ùóΩùóπùó∂ùóªùó¥  \\n  \\nThis technique extends the Beam search strategy and adds a dash of randomness\\nto the generation process.  \\n  \\nInstead of just picking the most likely tokens, it\\'s selecting a token\\nrandomly from the top k most likely choices.  \\n  \\nThus, the tokens with the highest probability will appear more often, but\\nother tokens will be generated occasionally to add some randomness\\n(\"creativity\").  \\n  \\nùó°ùòÇùó∞ùóπùó≤ùòÇùòÄ ùó¶ùóÆùó∫ùóΩùóπùó∂ùóªùó¥  \\n  \\nIn this case, you\\'re not just picking the top k most probable tokens here.\\nYou\\'re picking a cutoff value _p_ and forming a \"nucleus\" of tokens.  \\n  \\nIn other words, rather than selecting the top k most probable tokens, nucleus\\nsampling chooses a cutoff value p such that the sum of the probabilities of\\nthe selected tokens exceeds p.  \\n  \\nThus, at every step, you will have a various number of possible tokens\\nincluded in the \"nucleus\" from which you sample. This introduces even more\\ndiversity and creativity into your output.  \\n  \\n.  \\n  \\nùó°ùóºùòÅùó≤: For ùòµùò∞ùò±-ùò¨ and ùòØùò∂ùò§ùò≠ùò¶ùò∂ùò¥ ùò¥ùò¢ùòÆùò±ùò≠ùò™ùòØùò®, you can also use the \"ùòµùò¶ùòÆùò±ùò¶ùò≥ùò¢ùòµùò¶\"\\nhyperparameter to tweak the output probabilities. It is a parameter that\\nranges from 0 to 1. A low temperature (e.g., 0.1) will decrease the entropy\\n(randomness), making the generation more stable.\\n\\n4 key decoding strategies for LLMs that you must know [Image by the Author].\\n\\nTo summarize...  \\n  \\nThere are 2 main decoding strategies for LLMs:  \\n\\\\- greedy search  \\n\\\\- beam search  \\n  \\nTo add more variability and creativity to beam search, you can use:  \\n\\\\- top-k sampling  \\n\\\\- nucleus sampling\\n\\n* * *\\n\\n### The only 6 prompt engineering techniques you need to know\\n\\nThe whole field of prompt engineering can be reduced to these 6 techniques I\\nuse almost daily when using ChatGPT (or other LLMs).  \\n  \\nHere they are ‚Üì  \\n  \\n#1. ùêÖùêûùê∞ ùê¨ùê°ùê®ùê≠ ùê©ùê´ùê®ùê¶ùê©ùê≠ùê¢ùêßùê†  \\n  \\nAdd in your prompt 2 or 3 high-quality demonstrations, each consisting of both\\ninput and desired output, on the target task.  \\n  \\nThe LLM will better understand your intention and what kind of answers you\\nexpect based on concrete examples.  \\n  \\n#2. ùêíùêûùê•ùêü-ùêúùê®ùêßùê¨ùê¢ùê¨ùê≠ùêûùêßùêúùê≤ ùê¨ùêöùê¶ùê©ùê•ùê¢ùêßùê†  \\n  \\nSample multiple outputs with \"temperature > 0\" and select the best one out of\\nthese candidates.  \\n  \\nHow to pick the best candidate?  \\n  \\nIt will vary from task to task, but here are 2 primary scenarios ‚Üì  \\n  \\n1\\\\. Some tasks are easy to validate, such as programming questions. In this\\ncase, you can write unit tests to verify the correctness of the generated\\ncode.  \\n  \\n2\\\\. For more complicated tasks, you can manually inspect them or use another\\nLLM (or another specialized model) to rank them.  \\n  \\n#3. ùêÇùê°ùêöùê¢ùêß-ùê®ùêü-ùêìùê°ùê®ùêÆùê†ùê°ùê≠ (ùêÇùê®ùêì)  \\n  \\nYou want to force the LLM to explain its thought process, which eventually\\nleads to the final answer, step by step.  \\n  \\nThis will help the LLM to reason complex tasks better.  \\n  \\nYou want to use CoT for complicated reasoning tasks + large models (e.g., with\\nmore than 50B parameters). Simple tasks only benefit slightly from CoT\\nprompting.  \\n  \\nHere are a few methods to achieve CoT:  \\n\\\\- provide a list of bullet points with all the steps you expect the LLM to\\ntake  \\n\\\\- use \"Few shot prompt\" to teach the LLM to think in steps  \\n  \\n... or my favorite: use sentences such as \"Let\\'s think step by step.\"  \\n  \\n#4. ùêÄùêÆùê†ùê¶ùêûùêßùê≠ùêûùêù ùêèùê´ùê®ùê¶ùê©ùê≠ùê¨  \\n  \\nThe LLM\\'s internal knowledge is limited to the data it was trained on. Also,\\noften, it forgets specific details of older training datasets.  \\n  \\nThe most common use case is Retrieval-Augmented Generation (RAG).  \\n  \\nThat is why using the LLM as a reasoning engine is beneficial to parse and\\nextract information from a reliable source of information given as context in\\nthe prompt.  \\n  \\nùòûùò©ùò∫?  \\n\\\\- avoid retraining the model on new data  \\n\\\\- avoid hallucinating  \\n\\\\- access to references on the source  \\n  \\n#5. ùêÄ ùê¨ùê¢ùêßùê†ùê•ùêû ùê´ùêûùê¨ùê©ùê®ùêßùê¨ùê¢ùêõùê¢ùê•ùê¢ùê≠ùê≤ ùê©ùêûùê´ ùê©ùê´ùê®ùê¶ùê©ùê≠  \\n  \\nQuite self-explanatory. It is similar to the DRY principle in SWE.  \\n  \\nHaving only x1 task/prompt is good practice to avoid confusing the LLM.  \\n  \\nIf you have more complex tasks, split them into granular ones and merge the\\nresults later in a different prompt.  \\n  \\n#6. ùêÅùêû ùêöùê¨ ùêûùê±ùê©ùê•ùê¢ùêúùê¢ùê≠ ùêöùê¨ ùê©ùê®ùê¨ùê¨ùê¢ùêõùê•ùêû  \\n  \\nThe LLM cannot read your mind. To maximize the probability of getting\\nprecisely what you want, you can imagine the LLM as a 7-year-old to whom you\\nmust explain everything step-by-step to be sure he understood.  \\n  \\nùòïùò∞ùòµùò¶: The level of detail in the prompt is inversely proportional to the size\\n& complexity of the model.\\n\\n[Image generated by DALL-E]\\n\\nThe truth is that prompt engineering is quite intuitive, and we don\\'t have to\\noverthink it too much.  \\n  \\nWhat would you add to this list?\\n\\n* * *\\n\\n### One thing that I do that sets me apart from the crowd\\n\\nHere is one thing that I do that sets me apart from the crowd:  \\n  \\n\"ùòê ùò¢ùòÆ ùò∞ùò¨ùò¢ùò∫ ùò∏ùò™ùòµùò© ùò£ùò¶ùò™ùòØùò® ùòµùò©ùò¶ ùò•ùò∂ùòÆùò± ùò∞ùòØùò¶ ùòµùò©ùò¢ùòµ ùò¢ùò¥ùò¨ùò¥ ùòÆùò¢ùòØùò∫ ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØùò¥.\"  \\n  \\nùêáùê¶ùê¶... ùêñùê°ùê≤?  \\n  \\nThe reality is that even the brightest minds cannot understand everything from\\nthe first shot.  \\n  \\nIt is not necessarily that you cannot understand the concepts.  \\n  \\nThere are other factors, such as:  \\n\\\\- you are tired  \\n\\\\- you haven\\'t paid enough attention  \\n\\\\- the concept wasn\\'t explained at your level  \\n\\\\- the presenter wasn\\'t clear enough, etc.  \\n  \\nAlso, the truth is that many of us don\\'t understand everything from the first\\nshot when presented with a new concept.  \\n  \\nBut because of our ego, we are afraid to come out and ask something because we\\nare worried that we will sound stupid.  \\n  \\nThe jokes are on you.  \\n  \\nMost people will be grateful you broke the ice and asked to explain the\\nconcept again.  \\n  \\nùêñùê°ùê≤?  \\n  \\nIt will help the team to learn the new concepts better.  \\n  \\nIt will start a discussion to dig deeper into the subject.  \\n  \\nIt will piss off or annoy the people you don\\'t like.  \\n  \\nIt will help other people ask questions next time.  \\n  \\nIt will open up new perspectives on the problem.\\n\\nTo conclude...  \\n  \\nIgnore your ego and what people think of you. Own your curiosity and ask\\nquestions when you feel like it.  \\n  \\nIt is ok not to know everything.  \\n  \\nIt is better to be stupid for 5 minutes than your entire life.\\n\\n* * *\\n\\nCongrats on learning something new today!\\n\\n**Don‚Äôt hesitate to share your thoughts - we would love to hear them.**\\n\\n_**‚Üí** Remember, when ML looks **encoded - we‚Äôll help you decode it.**_\\n\\nSee you next Thursday at 9:00 am CET.\\n\\nHave a fantastic weekend!\\n\\n9\\n\\nShare this post\\n\\n#### 4 key decoding strategies for LLMs that you must know\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/4-key-decoding-strategies-for-llms?r=1ttoeh'), ArticleDocument(id=UUID('50a5a621-5799-4214-990d-3387ecc704e1'), content={'Title': 'DML: New year, the new & improved Decoding ML - What to expect?', 'Subtitle': 'How we plan to grow, provide more qualitative & hands-on content, and real-world ML projects to expand your professional skills', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: New year, the new & improved Decoding ML - What to expect?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: New year, the new & improved Decoding ML - What to expect?\\n\\n### How we plan to grow, provide more qualitative & hands-on content, and\\nreal-world ML projects to expand your professional skills\\n\\nPaul Iusztin\\n\\n,\\n\\nAlex Razvant\\n\\n, and\\n\\nVesa Alexandru\\n\\nJan 11, 2024\\n\\n10\\n\\nShare this post\\n\\n#### DML: New year, the new & improved Decoding ML - What to expect?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\nThis newsletter will differ from the others as I want to share my plans for\\nthe Decoding ML newsletter with you.\\n\\n> From now on, it will cost $1000/month. **Joking.** It will still be free.\\n> It‚Äôs not about the money but about growth, better quality & added value.\\n\\nTo be 100% transparent with you, I started this newsletter as an experiment,\\nbut when I saw people who actually read it, the perfectionist in me screamed\\nthat I should improve it and move to the next step.\\n\\nThis is the next step. And I‚Äôm taking you with me.\\n\\nThe big news is that I will go all in, pouring more time and resources into\\ngrowing the Decoding ML newsletter. My main goals are to:\\n\\n  * push better-quality content every week\\n\\n  * bring more real-world projects to increase your hands-on skills\\n\\n  * increases the number of articles with code examples to make it practical so you can benefit from it even more at your job \\n\\n> As the world constantly changes, especially AI, MLE & MLOps, you cannot\\n> stagnate. Decoding ML‚Äôs growth is about providing you with all the MLE &\\n> MLOps necessary resources to grow with it and smash it at your projects and\\n> job.\\n\\n* * *\\n    \\n    \\n    _So.. How do I plan to grow the Decoding ML newsletter?_\\n\\n## Well, there are 3 main steps ‚Üì\\n\\n## #1. Rebranding\\n\\nFrom now on, my face will no longer be the ‚Äúlogo‚Äù of Decoding ML.\\n\\nThis will be the new logo of Decoding ML ‚Üì\\n\\nSo you don‚Äôt have to see my annoying face every Thursday morning in your email\\nü§£\\n\\n* * *\\n\\n## #2. Bringing in talent\\n\\nAs I wanted to push more content of higher quality, I had to bring in more\\ntalented people to write beside me.\\n\\nI was lucky enough to know Alex Razvant and Alex Vesa, who are 2 fantastic MLE\\n& MLOps engineers with 10 years of hands-on experience in the AI industry.\\n\\nFrom now on, they will start contributing to the Decoding ML newsletter and\\nteam along with me.\\n\\n> Maybe you know this famous saying: ‚Äú**If you want to go fast, go alone; if\\n> you want to go far, go together**.‚Äù ‚Ä¶and I want Decoding ML to go far.\\n\\nOur primary goal is to help you level up in MLE & MLOps by offering hands-on\\nexamples that you can use at your job.\\n\\nI plan to improve the quality of the articles by including more code and\\nconcrete examples besides the system design talks we have discussed so far.\\n\\n‚Ä¶and here enters the scene ‚ÄúThe Alex‚Äôs‚Äù\\n\\nI have worked with them, and I know they are talented experts with fantastic\\nhands-on MLE & MLOps skills and insights to share with you.\\n\\nStarting from now on, Decoding ML will no longer be a one-person brand but a\\nbrand by itself, hosted by the new Decoding ML team:\\n\\n  * myself\\n\\n  * Alex Vesa\\n\\n  * Alex Razvant\\n\\n### #2.1. Now, let the team introduce itself ‚Üì\\n\\n####  _**Alex Vesa**_\\n\\n _Main niche: ‚ÄúDeep Learning/Computer Vision | ML System Infrastructure | Startups | Business‚Äù_\\n\\n‚Ü≥ üîó LinkedIn  \\n\\nHello everyone,\\n\\n  \\nI‚Äôm very grateful for this opportunity. I consider creativity and inspiration\\nto flourish when there's a merger of minds from various individuals.\\n\\nMy professional journey began in 2015, initially focusing on software\\nengineering with a keen interest in Python and AI technologies. I quickly\\nprogressed, taking on challenging roles and AI projects. My experience in\\nvarious startups as a CTO focused on leading teams in developing innovative\\nsoftware solutions. I worked in multiple sectors, notably healthcare and\\nautomotive, where I've implemented AI-driven systems to enhance operational\\nefficiency.\\n\\nMy technical skills are broad, encompassing Python, Django, and AWS. I'm\\ndedicated to leveraging my AI and software development expertise to drive\\norganizational success in this dynamic field.\\n\\nI value knowledge-sharing among our community, and my objective is to bring\\nsolid expertise in practical, real-world AI/ML systems to help you in your\\nday-to-day work and enhance your creativity and vision in product development.\\n\\nUltimately, I want to share with you the endless capabilities you can possess\\nto evolve.\\n\\n#### _Alex Razvant_\\n\\n _Main niche: ‚ÄúML/CV Systems in Production | MLOps_ /_Edge ML Deployments‚Äù_\\n\\n‚Ü≥ üîó LinkedIn\\n\\nHey everyone,\\n\\nI‚Äôm really happy about this merger, as you‚Äôll get 3X more quality content in a\\nconcise, valuable, and actionable manner directly to your inbox!\\n\\nHere are a few words about who I am:\\n\\nI started my journey as a SWE in 2015, diving into full-stack web development.  \\nAfter a few internships, hackathons, and a few failed projects, the ML field\\ncaught my eye, and I haven‚Äôt looked back ever since.\\n\\nMy journey includes over **15+** successful freelance projects, earning a\\n**Top-Rated** ML Engineer badge on **UpWork** , collaborating with **BMW** on\\nAI for self-driving cars, authoring a paper for IEEE RAL 2020, and developing\\nscalable Computer Vision systems to analyze 1000+ hours of CCTV footage.\\n\\nI aim to bring solid expertise via **code tutorials, diagrams, and system\\ndesigns** to help you overcome challenges in building and deploying ML & CV\\nsystems in cloud or edge environments, following the best practices I‚Äôve\\nlearned in SWE, ML, and MLOps.\\n\\n> _Follow them & check them out on LinkedIn to see their incredible experience\\n> in AI._\\n\\n### #2.2. Will we start approaching different topics?\\n\\n_TL/DR: No!_\\n\\nI was meticulous in bringing in more people with the same vision.\\n\\nThus, Decoding ML will approach the same niche as it has done: _‚Äúproduction-\\nready MLE & MLOps topics.‚Äù_\\n\\nSo‚Ä¶ you don‚Äôt have to unsubscribe. We will keep talking about the same topics\\nyou chose to follow in our newsletter: _‚Äúhands-on MLE & MLOps topics‚Äù_\\n\\nHowever, the advantage of having more people with different backgrounds on the\\nteam is that we all come with different perspectives and domain knowledge.\\n\\nFor example:\\n\\n  * Alex Razvant worked a lot with Computer Vision, Deep Learning, and MLOps technologies in the world of retail\\n\\n  * Alex Vesa has a lot of experience with Deep Learning and infrastructure projects in the medical field\\n\\n  * I am passioned about generative AI, MLOps, and SWE\\n\\n‚Ä¶combining our knowledge will result in exciting production-ready MLE & MLOps\\narticles that will significantly benefit you.\\n\\n* * *\\n\\n## #3. Expanding to new distribution channels\\n\\nEvery person consumes content differently.\\n\\nSo, we'd like to give you the best fit to enjoy our content.\\n\\nWe already started a Decoding ML Medium publication, where we will start this\\nmonth to push a deep dive into the code of the Hands-on LLMs Course.\\n\\n‚Ä¶and slowly, we will expand to video format content on:\\n\\n  * Youtube\\n\\n  * Instagram\\n\\n  * TikTok\\n\\nAlso, we started planning a set of eBooks about MLE, MLOps and LLMOps and a\\nnew course about LLMs and LLMOps.\\n\\n* * *\\n\\n### So‚Ä¶ What happens next?\\n\\nI hope you are excited about the news. For sure, I am üî•\\n\\n>  _Next Thursday at 9:00 a.m. CET_ , **Alex Vesa** will make his **grand\\n> opening** by writing a step-by-step article on **how** you can **deploy an\\n> LLaMA2-7b LLM** using **Amazon SageMaker** and **HuggingFace**.\\n\\nTo conclude, you don‚Äôt have to do anything on your side.\\n\\n_Decoding ML follows its natural course by bringing in more people and\\nexpanding to other platforms to give you more value for your time and a more\\npersonalized way to enjoy our content._\\n\\nSee you next Thursday!\\n\\nHave a fantastic weekend! ‚úåüèª\\n\\nPaul\\n\\n10\\n\\nShare this post\\n\\n#### DML: New year, the new & improved Decoding ML - What to expect?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Ahmed BesbesThe Tech Buffet Jan 11Liked by Paul IusztinGreat things coming\\nahead Paul! Looking forward to it!Expand full commentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-new-year-the-new-and-improved?r=1ttoeh'), ArticleDocument(id=UUID('e85a60a3-6667-45fe-81fd-9384322b7cea'), content={'Title': 'DML: 8 types of MLOps tools that must be in your toolbelt to be a successful MLOps engineer', 'Subtitle': 'How to successfully present MLOps ideas to upper management. How I generated PyDocs for 100 Python functions in <1 hour', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 8 types of MLOps tools that must be in your toolbelt to be a successful\\nMLOps engineer\\n\\n### How to successfully present MLOps ideas to upper management. How I\\ngenerated PyDocs for 100 Python functions in <1 hour\\n\\nPaul Iusztin\\n\\nJan 04, 2024\\n\\n18\\n\\nShare this post\\n\\n#### DML: 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\nThe last Hands-on LLM series finished last week. In case you are curious, here\\nare the top 3 out of 9 lessons of the series:\\n\\n  1. Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n  2. Lesson 7: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n\\n  3. Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n* * *\\n\\n#### **This week‚Äôs topics:**\\n\\n  1. 8 types of MLOps tools that must be in your toolbelt to be a successful MLOps engineer\\n\\n  2. How to successfully present MLOps ideas to upper management\\n\\n  3. How I generated PyDocs for 100 Python functions in <1 hour\\n\\n* * *\\n\\n‚Üí Before diving into the topics, I have one important thing to share with you.\\n\\n> We finally finished the code & video lessons for the**Hands-on LLMs** course\\n> üî•\\n\\nBy finishing the **Hands-On LLMs** free course, you will learn how to use the\\n3-pipeline architecture & LLMOps good practices to design, build, and deploy a\\nreal-time financial advisor powered by LLMs & vector DBs.  \\n  \\nWe will primarily focus on the engineering & MLOps aspects.  \\n  \\nThus, by the end of this series, you will know how to build & deploy a real ML\\nsystem, not some isolated code in Notebooks.  \\n  \\nùêåùê®ùê´ùêû ùê©ùê´ùêûùêúùê¢ùê¨ùêûùê•ùê≤, ùê≠ùê°ùêûùê¨ùêû ùêöùê´ùêû ùê≠ùê°ùêû 3 ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ùê¨ ùê≤ùê®ùêÆ ùê∞ùê¢ùê•ùê• ùê•ùêûùêöùê´ùêß ùê≠ùê® ùêõùêÆùê¢ùê•ùêù:  \\n  \\n1\\\\. a ùê´ùêûùêöùê•-ùê≠ùê¢ùê¶ùêû ùê¨ùê≠ùê´ùêûùêöùê¶ùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû (deployed on AWS) that listens to financial\\nnews, cleans & embeds the documents, and loads them to a vector DB  \\n  \\n2\\\\. a ùêüùê¢ùêßùêû-ùê≠ùêÆùêßùê¢ùêßùê† ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû (deployed as a serverless continuous training) that\\nfine-tunes an LLM on financial data using QLoRA, monitors the experiments\\nusing an experiment tracker and saves the best model to a model registry  \\n  \\n3\\\\. an ùê¢ùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû built in LangChain (deployed as a serverless RESTful\\nAPI) that loads the fine-tuned LLM from the model registry and answers\\nfinancial questions using RAG (leveraging the vector DB populated with\\nfinancial news in real-time)  \\n  \\nWe will also show you how to integrate various serverless tools, such as:  \\n  \\n‚Ä¢ Comet ML as your ML Platform;  \\n‚Ä¢ Qdrant as your vector DB;  \\n‚Ä¢ Beam as your infrastructure.  \\n  \\nùêñùê°ùê® ùê¢ùê¨ ùê≠ùê°ùê¢ùê¨ ùêüùê®ùê´?  \\n  \\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM\\nsystems using LLMOps good principles.  \\n  \\nùêáùê®ùê∞ ùê∞ùê¢ùê•ùê• ùê≤ùê®ùêÆ ùê•ùêûùêöùê´ùêß?  \\n  \\nThe series contains 4 hands-on video lessons and the open-source code you can\\naccess on GitHub.  \\n  \\nùêÇùêÆùê´ùê¢ùê®ùêÆùê¨?  \\n  \\n‚Ü≥ üîó Check it out and support us with a ‚≠ê\\n\\nThe architecture of a financial bot powered by LLMs, vector DBs and MLOps\\n[Image by the Authors]\\n\\n* * *\\n\\n### #1. 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\nThese are the ùü¥ ùòÅùòÜùóΩùó≤ùòÄ of ùó†ùóüùó¢ùóΩùòÄ ùòÅùóºùóºùóπùòÄ that must be in your toolbelt to be a\\nùòÄùòÇùó∞ùó∞ùó≤ùòÄùòÄùó≥ùòÇùóπ ùó†ùóüùó¢ùóΩùòÄ ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóø ‚Üì  \\n  \\nIf you are into MLOps, you are aware of the 1000+ tools in the space and think\\nyou have to know.  \\n  \\nThe reality is that all of these tools can be boiled down to 8 main\\ncategories.  \\n  \\nIf you learn the fundamentals and master one tool from each category, you will\\nbe fine.  \\n  \\n.\\n\\nBa≈üak Tuƒü√ße Eskili\\n\\nand\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\nwrote an excellent summary highlighting these 8 categories:  \\n  \\n1\\\\. ùôëùôöùôßùô®ùôûùô§ùô£ ùôòùô§ùô£ùô©ùôßùô§ùô°: crucial for the traceability and reproducibility of an ML\\nmodel deployment or run. Without a version control system, it is difficult to\\nfind out what exact code version was responsible for specific runs or errors\\nyou might have in production. (üîß GitHub, GitLab, etc.)  \\n  \\n2\\\\. ùòæùôÑ/ùòæùòø: automated tests are triggered upon pull request creation &\\ndeployment to production should only occur through the CD pipeline (üîß GitHub\\nActions, GitLab CI/CD, Jenkins, etc.)  \\n  \\n3\\\\. ùôíùô§ùôßùô†ùôõùô°ùô§ùô¨ ùô§ùôßùôòùôùùôöùô®ùô©ùôßùôñùô©ùôûùô§ùô£: manage complex dependencies between different\\ntasks, such as data preprocessing, feature engineering, ML model training (üîß\\nAirflow, ZenML, AWS Step Functions, etc.)  \\n  \\n4\\\\. ùôàùô§ùôôùôöùô° ùôßùôöùôúùôûùô®ùô©ùôßùôÆ: store, version, and share trained ML model artifacts,\\ntogether with additional metadata (üîß Comet ML, W&B, MLFlow, etc.)  \\n  \\n5\\\\. ùòøùô§ùôòùô†ùôöùôß ùôßùôöùôúùôûùô®ùô©ùôßùôÆ: store, version, and share Docker images. Basically, all\\nyour code will be wrapped up in Docker images and shared through this registry\\n(üîß Docker Hub, ECR, etc.)  \\n  \\n6 & 7\\\\. ùôàùô§ùôôùôöùô° ùô©ùôßùôñùôûùô£ùôûùô£ùôú & ùô®ùôöùôßùô´ùôûùô£ùôú ùôûùô£ùôõùôßùôñùô®ùô©ùôßùô™ùôòùô©ùô™ùôßùôö: if on-premise, you will\\nlikely have to go with Kubernetes. There are multiple choices if you are on a\\ncloud provider: Azure ML on Azure, Sagemaker on AWS, and Vertex AI on GCP.  \\n  \\n8\\\\. ùôàùô§ùô£ùôûùô©ùô§ùôßùôûùô£ùôú: Monitoring in ML systems goes beyond what is needed for\\nmonitoring regular software applications. The distinction lies in that the\\nmodel predictions can fail even if all typical health metrics appear in good\\ncondition. (üîß SageMaker, NannyML, Arize, etc.)  \\n  \\nThe secret sauce in MLOps is knowing how to glue all these pieces together\\nwhile keeping things simple.  \\n\\n[Image from Marvelous MLOps]\\n\\n‚Ü≥üîó To read more about these components, check out the article on\\n\\nMarvelousMLOps\\n\\n.\\n\\n* * *\\n\\n### #2. How to successfully present MLOps ideas to upper management\\n\\nHave you ever presented your MLOps ideas to upper management just to get\\nghosted?  \\n  \\nIn that case...  \\n  \\n\\nRapha√´l Hoogvliets\\n\\n,\\n\\nBa≈üak Tuƒü√ße Eskili\\n\\n, and\\n\\nMaria Vechtomova\\n\\nfrom\\n\\nMarvelousMLOps\\n\\npresented a great step-by-step strategy for pitching your MLOps ideas to your\\nupper management and getting attention and resources to implement them.  \\n  \\nHere are the 6 steps you have to know ‚Üì  \\n  \\n1\\\\. ùêÇùê®ùê•ùê•ùêûùêúùê≠ ùêöùê•ùê• ùê≠ùê°ùêû ùê©ùêöùê¢ùêß ùê©ùê®ùê¢ùêßùê≠ùê¨  \\nTalk to data scientists, product owners, and stakeholders in your organization\\nto gather issues such as:  \\n\\\\- time to deployment  \\n\\\\- poor quality deployment  \\n\\\\- non-existing monitoring  \\n\\\\- lack of collaboration  \\n\\\\- external parties  \\n  \\n2\\\\. ùêÑùêùùêÆùêúùêöùê≠ùêû ùê©ùêûùê®ùê©ùê•ùêû  \\nOrganize workshops, meetings, etc., to present what MLOps is and how it can\\nhelp.  \\n  \\nI think it\\'s critical to present it to your target audience. For example, an\\nengineer looks at the problem differently than the business stakeholders.  \\n  \\n3\\\\. ùêèùê´ùêûùê¨ùêûùêßùê≠ ùêõùêûùêüùê®ùê´ùêû ùêöùêßùêù ùêöùêüùê≠ùêûùê´ ùê¨ùêúùêûùêßùêöùê´ùê¢ùê®ùê¨  \\nShow how MLOps can solve the company\\'s challenges and deliver tangible\\nbenefits to the organization, such as:  \\n\\\\- less cost  \\n\\\\- fast deployment  \\n\\\\- better collaboration  \\n\\\\- less risk  \\n  \\n4\\\\. ùêèùê´ùê®ùêØùêû ùê¢ùê≠  \\nUse concrete examples to support your ideas, such as:  \\n\\\\- how a competitor or an organization in the same or related field benefited\\nfrom introducing MLOps  \\n\\\\- build a PoC within your organization  \\n  \\n5\\\\. ùêíùêûùê≠ ùêÆùê© ùê≤ùê®ùêÆùê´ ùê≠ùêûùêöùê¶  \\nChoose 2-3 experienced individuals (not juniors) to set up the foundations in\\nyour team/organization.  \\n  \\nWith an emphasis on starting with experienced engineers and only later\\nbringing more juniors to the party.  \\n  \\n6\\\\. ùêäùêûùêûùê© ùê®ùêß ùê§ùêûùêûùê©ùê¢ùêß\\' ùê®ùêß  \\nOnce you successfully apply MLOps to one use case, you can bring in more\\nresponsibility by growing your team and taking on more projects.  \\n  \\n.  \\n  \\nAll of these are great tips for integrating MLOps in your organization.  \\n  \\nI love their \"Present before and after scenarios\" approach.  \\n  \\nYou can extrapolate this strategy for any other new processes (not only\\nMLOps).  \\n  \\n.  \\n  \\n‚Ü≥üîó To learn the details, check out the full article on\\n\\nMarvelousMLOps\\n\\n.\\n\\n* * *\\n\\n### #3. How I generated PyDocs for 100 Python functions in <1 hour\\n\\nThe most boring programming part is to write PyDocs, so I usually write clean\\ncode and let it speak for itself.  \\n  \\nBut, for open-source projects where you have to generate robust documentation,\\nPyDocs are a must.  \\n  \\nThe good news is that now you can automate this process using Copilot.  \\n  \\nYou can see in the video below an example of how easy it is.  \\n  \\nI tested it on more complex functions/classes, and it works well. I chose this\\nexample because it fits nicely on one screen.  \\n  \\nOnce I tested Copilot\\'s experience, I will never go back.  \\n  \\nIt is true that, in some cases, you have to make some minor adjustments. But\\nthat is still 10000% more efficient than writing it from scratch.  \\n\\nIf you want more examples, check out our **Hands-on LLMs** course, where all\\nthe PyDocs are generated 99% using Copilot in <1 hour.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).  \\n\\n18\\n\\nShare this post\\n\\n#### DML: 8 types of MLOps tools that must be in your toolbelt to be a\\nsuccessful MLOps engineer\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-8-types-of-mlops-tools-that-must?r=1ttoeh'), ArticleDocument(id=UUID('8ff6064c-9c09-494f-a42d-a60b0e80387c'), content={'Title': 'DML: This is what you need to build an inference pipeline for a financial assistant powered by LLMs, vector DBs and LLMOps', 'Subtitle': 'Lesson 9 | The Hands-on LLMs Series', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\n### Lesson 9 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 28, 2023\\n\\n15\\n\\nShare this post\\n\\n#### DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 9 | The Hands-on LLMs Series**\\n\\n> This is the **last lesson** within the **Hands-on LLMs** series... _But\\n> certainly not the last MLE & MLOps series. We are cooking some exciting\\n> stuff._ But I hope you had fun and learned much during this series.\\n\\nNow, let's see how to glue everything we have done so far under the inference\\npipeline. Enjoy! üßÅ\\n\\n#### **Table of Contents:**\\n\\n  1. Inference pipeline video lesson\\n\\n  2. What do you need to build an inference pipeline for a financial assistant powered by LLMs and vector DBs?\\n\\n  3. How can you build & deploy an inference pipeline for a real-time financial advisor while considering good LLMOps practices?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n  * Lesson 7: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n\\n  * Lesson 8: 7-steps on how to fine-tune an open-source LLM to create your real-time financial advisor\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Inference pipeline video lesson\\n\\nWe ùê´ùêûùê•ùêûùêöùê¨ùêûùêù the ùêüùê¢ùêßùêöùê• video ùê•ùêûùê¨ùê¨ùê®ùêß of the ùêáùêöùêßùêùùê¨-ùê®ùêß ùêãùêãùêåùê¨ FREE course that will\\nteach you how to ùêõùêÆùê¢ùê•ùêù & ùêùùêûùê©ùê•ùê®ùê≤ an ùê¢ùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû for a financial advisor\\nusing ùêãùêöùêßùê†ùêÇùê°ùêöùê¢ùêß, ùêãùêãùêåùêéùê©ùê¨, and ùêØùêûùêúùê≠ùê®ùê´ ùêÉùêÅùê¨.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò¢ùò≥ùò¶ ùòµùò©ùò¶ ùò¨ùò¶ùò∫ ùòµùò∞ùò±ùò™ùò§ùò¥ ùò§ùò∞ùò∑ùò¶ùò≥ùò¶ùò• ùò™ùòØ ùòµùò©ùò¶ ùò∑ùò™ùò•ùò¶ùò∞ ùò≠ùò¶ùò¥ùò¥ùò∞ùòØ made by Pau Labarta ùò¢ùòØùò• ùòê\\n‚Üì  \\n  \\n1\\\\. Overview of the architecture of the inference pipeline and how to apply\\nLLMOps good practices  \\n  \\n2\\\\. How to build from scratch a RAG agent using LangChain:\\nContextExtractorChain + FinancialBotQAChain  \\n  \\n3\\\\. How to attach a callback class to log input prompts and LLM answers to\\nComet LLMOps  \\n  \\n4\\\\. Setting up and running the code locally  \\n  \\n5\\\\. Deploying the inference pipeline to Beam as a RESTful API  \\n  \\n.  \\n  \\nùòäùò∂ùò≥ùò™ùò∞ùò∂ùò¥?\\n\\nCheck out the video lesson\\n\\nPau Labarta Bajo\\n\\nand I did ‚Üì\\n\\n* * *\\n\\n### #2. What do you need to build an inference pipeline for a financial\\nassistant powered by LLMs and vector DBs?\\n\\nHere are its ùü≥ ùó∏ùó≤ùòÜ ùó∞ùóºùó∫ùóΩùóºùóªùó≤ùóªùòÅùòÄ ‚Üì  \\n  \\n1\\\\. ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï ùóΩùóºùóΩùòÇùóπùóÆùòÅùó≤ùó± ùòÑùó∂ùòÅùóµ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóªùó≤ùòÑùòÄ: This is the output of the feature\\npipeline. More concretely, a Qdrant vector DB populated with chunks of\\nfinancial news from Alpaca. During the inference pipeline, we will use it to\\nquery valuable chunks of information and do RAG.  \\n  \\n2\\\\. ùó≤ùó∫ùóØùó≤ùó±ùó±ùó∂ùóªùó¥ ùóπùóÆùóªùó¥ùòÇùóÆùó¥ùó≤ ùó∫ùóºùó±ùó≤ùóπ: To embed the user question and query the vector\\nDB, you need the same embedding model used in the feature pipeline, more\\nconcretely `ùò¢ùò≠ùò≠-ùòîùò™ùòØùò™ùòìùòî-ùòì6-ùò∑2` from `ùò¥ùò¶ùòØùòµùò¶ùòØùò§ùò¶-ùòµùò≥ùò¢ùòØùò¥ùòßùò∞ùò≥ùòÆùò¶ùò≥ùò¥`. Using the same\\nencoder-only model is crucial, as the query vector and vector DB index vectors\\nhave to be in the same space.  \\n  \\n3\\\\. ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ùó± ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†: The output of the training pipeline will be a\\nfine-tuned Falcon 7B on financial tasks.  \\n  \\n4\\\\. ùó∫ùóºùó±ùó≤ùóπ ùóøùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ: The fine-tuned model will be shared between the training &\\ninference pipeline through Comet‚Äôs model registry. By doing so, you decouple\\nentirely the 2 components, and the model can easily be shared under specific\\nenvironments (e.g., staging, prod) and versions (e.g., v1.0.1).  \\n  \\n5\\\\. ùóÆ ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ ùó≥ùóºùóø ùóüùóüùó† ùóÆùóΩùóΩùóπùó∂ùó∞ùóÆùòÅùó∂ùóºùóªùòÄ: You need LangChain, as your LLM\\nframework, to glue all the steps together, such as querying the vector DB,\\nstoring the history of the conversation, creating the prompt, and calling the\\nLLM. LangChain provides out-of-the-box solutions to chain all these steps\\ntogether quickly.  \\n  \\n6\\\\. ùó±ùó≤ùóΩùóπùóºùòÜ ùòÅùóµùó≤ ùóüùóüùó† ùóÆùóΩùóΩ ùóÆùòÄ ùóÆ ùó•ùóòùó¶ùóßùó≥ùòÇùóπ ùóîùó£ùóú: One of the final steps is to deploy\\nyour awesome LLM financial assistant under a RESTful API. You can quickly do\\nthis using Beam as your serverless infrastructure provider. Beam specializes\\nin DL. Thus, it offers quick ways to load your LLM application on GPU machines\\nand expose it under a RESTful API.  \\n  \\n7\\\\. ùóΩùóøùóºùó∫ùóΩùòÅ ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥: The last step is to add eyes on top of your system. You\\ncan do this using Comet‚Äôs LLMOps features that allow you to track & monitor\\nall the prompts & responses of the system.\\n\\n> ‚Ü≥üîó Check out how these components are working together in our Hands-on LLMs\\n> free course.\\n\\n* * *\\n\\n### #3. How can you build & deploy an inference pipeline for a real-time\\nfinancial advisor while considering good LLMOps practices?\\n\\nùêáùê®ùê∞ can you ùêõùêÆùê¢ùê•ùêù & ùêùùêûùê©ùê•ùê®ùê≤ an ùê¢ùêßùêüùêûùê´ùêûùêßùêúùêû ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû for a real-time financial\\nadvisor with ùêãùêöùêßùê†ùêÇùê°ùêöùê¢ùêß powered by ùêãùêãùêåùê¨ & ùêØùêûùêúùê≠ùê®ùê´ ùêÉùêÅùê¨ while considering ùê†ùê®ùê®ùêù\\nùêãùêãùêåùêéùê©ùê¨ ùê©ùê´ùêöùêúùê≠ùê¢ùêúùêûùê¨?\\n\\n.\\n\\nAs a quick reminder from previous posts, here is what we already have:  \\n\\\\- a Qdrant vector DB populated with financial news (the output of the feature\\npipeline)  \\n\\\\- fine-tuned Falcon-7B LoRA weights stored in Comet‚Äôs model registry (the\\noutput of the training pipeline)\\n\\nThe Qdrant vectorDB is accessed through a Python client.\\n\\nA specific version of the Falcon-7B LoRA weights is downloaded from Comet‚Äôs\\nmodel registry and loaded in memory using QLoRA.\\n\\nThe goal of the inference pipeline is to use LangChain to glue the 2\\ncomponents into a single `**FinancialAssistant** ` entity.\\n\\n.\\n\\nThe `**FinancialAssistant** ` entity is deployed in a request-response fashion\\nunder a RESTful API. We used Beam to deploy it quickly under a serverless web\\nendpoint.\\n\\nTo deploy any model using Beam as a RESTful API is as easy as writing the\\nfollowing Python decorator:\\n\\n    \\n    \\n    @financial_bot. rest_api(keep_warm_seconds=300, loader=load_bot)def run(**inputs):\\n       ....\\n\\n  \\nùêçùê®ùê∞ ùê•ùêûùê≠‚Äôùê¨ ùêÆùêßùêùùêûùê´ùê¨ùê≠ùêöùêßùêù ùê≠ùê°ùêû ùêüùê•ùê®ùê∞ ùê®ùêü ùê≠ùê°ùêû `ùêÖùê¢ùêßùêöùêßùêúùê¢ùêöùê•ùêÄùê¨ùê¨ùê¢ùê¨ùê≠ùêöùêßùê≠` ùêúùê°ùêöùê¢ùêß‚Üì\\n\\n1\\\\. Clean the user‚Äôs input prompt and use a pre-trained ‚Äú**all-MiniLM-L6-v2**\\n‚Äù encoder-only model to embed it (the same LM used to populate the vector DB).\\n\\n2\\\\. Using the embedded user input, query the Qdrant vector DB and extract the\\ntop 3 most similar financial news based on the cosine similarly distance\\n\\n‚Üí These 2 steps were necessary to do RAG. If you don‚Äôt know how RAG works,\\ncheck out Lesson 3.\\n\\n3\\\\. Build the final prompt using a ‚Äú**PromptTemplate** ‚Äù class (the same one\\nused for training) that formats the following components:  \\n\\\\- a system prompt  \\n\\\\- the user‚Äôs input prompt  \\n\\\\- the financial news context  \\n\\\\- the chat history\\n\\n4\\\\. Now that our prompt contains all the necessary data, we pass it to the\\nfine-tuned Falcon-7B LLM for the final answer.\\n\\nThe input prompt and LLM answer will be logged and monitored by Comet LLMOps.\\n\\n5\\\\. You can get the answer in one shot or use the `TextIteratorStreamer` class\\n(from HuggingFace) to stream it token-by-token.\\n\\n6\\\\. Store the user‚Äôs input prompt and LLM answer in the chat history.\\n\\n7\\\\. Pass the final answer to the client.\\n\\n**Note:** You can use the `**TextIteratorStreamer** ` class & wrap the\\n`**FinancialAssistant** ` under a WebSocket (instead of the RESTful API) to\\nstream the answer of the bot token by token.\\n\\nSimilar to what you see in the interface of ChatGPT.\\n\\nHow | Inference pipeline: Build & deploy an inference pipeline using LangChain powered by LLMs & vector DBs [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nWith this, we concluded the **Hands-On LLMs** series. I hope you enjoyed it üî•\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n15\\n\\nShare this post\\n\\n#### DML: This is what you need to build an inference pipeline for a financial\\nassistant powered by LLMs, vector DBs and LLMOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-this-is-what-you-need-to-build?r=1ttoeh'), ArticleDocument(id=UUID('ceacd8d8-91dc-42a7-ad33-97964bf91387'), content={'Title': 'DML: 7-steps on how to fine-tune an open-source LLM to create your real-time financial advisor', 'Subtitle': 'Lesson 8 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 7-steps on how to fine-tune an open-source LLM to create your real-time\\nfinancial advisor\\n\\n### Lesson 8 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 21, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 8 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. What is Beam? How does serverless make deploying ML models easy?\\n\\n  2. 7 tips you must know to reduce your VRAM consumption of your LLMs during training\\n\\n  3. 7-steps on how to fine-tune an open-source LLM to create your real-time financial advisor\\n\\n#### Previous Lessons:\\n\\n  * Lesson 5: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n  * Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n  * Lesson 7: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. What is Beam? How does serverless make deploying ML models easy?\\n\\nùóóùó≤ùóΩùóπùóºùòÜùó∂ùóªùó¥ & ùó∫ùóÆùóªùóÆùó¥ùó∂ùóªùó¥ ML models is ùóµùóÆùóøùó±, especially when running your models on\\nGPUs.  \\n  \\nBut ùòÄùó≤ùóøùòÉùó≤ùóøùóπùó≤ùòÄùòÄ makes things ùó≤ùóÆùòÄùòÜ.  \\n  \\nUsing Beam as your serverless provider, deploying & managing ML models can be\\nas easy as ‚Üì  \\n  \\nùóóùó≤ùó≥ùó∂ùóªùó≤ ùòÜùóºùòÇùóø ùó∂ùóªùó≥ùóøùóÆùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ & ùó±ùó≤ùóΩùó≤ùóªùó±ùó≤ùóªùó∞ùó∂ùó≤ùòÄ  \\n  \\nIn a few lines of code, you define the application that contains:  \\n  \\n\\\\- the requirements of your infrastructure, such as the CPU, RAM, and GPU  \\n\\\\- the dependencies of your application  \\n\\\\- the volumes from where you can load your data and store your artifacts  \\n  \\nùóóùó≤ùóΩùóπùóºùòÜ ùòÜùóºùòÇùóø ùó∑ùóºùóØùòÄ  \\n  \\nUsing the Beam application, you can quickly decore your Python functions to:  \\n  \\n\\\\- run them once on the given serverless application  \\n\\\\- put your task/job in a queue to be processed or even schedule it using a\\nCRON-based syntax  \\n\\\\- even deploy it as a RESTful API endpoint\\n\\nHow do you use Beam as your serverless provider? [Image by the Author]\\n\\nAs you can see in the image below, you can have one central function for\\ntraining or inference, and with minimal effort, you can switch from all these\\ndeployment methods.  \\n  \\nAlso, you don\\'t have to bother at all with managing the infrastructure on\\nwhich your jobs run. You specify what you need, and Beam takes care of the\\nrest.  \\n  \\nBy doing so, you can directly start to focus on your application and stop\\ncarrying about the infrastructure.  \\n  \\nThis is the power of serverless!  \\n  \\n‚Ü≥üîó Check out Beam to learn more\\n\\n* * *\\n\\n### #2. 7 tips you must know to reduce your VRAM consumption of your LLMs\\nduring training\\n\\nHere are ùü≥ ùòÅùó∂ùóΩùòÄ you must know to ùóøùó≤ùó±ùòÇùó∞ùó≤ your ùó©ùó•ùóîùó† ùó∞ùóºùóªùòÄùòÇùó∫ùóΩùòÅùó∂ùóºùóª of your ùóüùóüùó†ùòÄ\\nduring ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ so you can ùó≥ùó∂ùòÅ it on ùòÖùü≠ ùóöùó£ùó®.  \\n  \\nWhen training LLMs, one of the pain points is to have enough VRAM on your\\nsystem.  \\n  \\nThe good news is that the gods of DL are with us, and there are methods to\\nlower your VRAM consumption without a significant impact on your performance ‚Üì  \\n  \\nùü≠\\\\. ùó†ùó∂ùòÖùó≤ùó±-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: During training you use both FP32 and FP16 in the\\nfollowing way: \"FP32 weights\" -> \"FP16 weights\" -> \"FP16 gradients\" -> \"FP32\\ngradients\" -> \"Update weights\" -> \"FP32 weights\" (and repeat). As you can see,\\nthe forward & backward passes are done in FP16, and only the optimization step\\nis done in FP32, which reduces both the VRAM and runtime.  \\n  \\nùüÆ\\\\. ùóüùóºùòÑùó≤ùóø-ùóΩùóøùó≤ùó∞ùó∂ùòÄùó∂ùóºùóª: All your computations are done in FP16 instead of FP32.\\nBut the key is using bfloat16 (\"Brain Floating Point\"), a numerical\\nrepresentation Google developed for deep learning. It allows you to represent\\nvery large and small numbers, avoiding overflowing or underflowing scenarios.  \\n  \\nùüØ\\\\. ùó•ùó≤ùó±ùòÇùó∞ùó∂ùóªùó¥ ùòÅùóµùó≤ ùóØùóÆùòÅùó∞ùóµ ùòÄùó∂ùòáùó≤: This one is straightforward. Fewer samples per\\ntraining iteration result in smaller VRAM requirements. The downside of this\\nmethod is that you can\\'t go too low with your batch size without impacting\\nyour model\\'s performance.  \\n  \\nùü∞\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ ùóÆùó∞ùó∞ùòÇùó∫ùòÇùóπùóÆùòÅùó∂ùóºùóª: It is a simple & powerful trick to increase your\\nbatch size virtually. You compute the gradients for \"micro\" batches (forward +\\nbackward passes). Once the accumulated gradients reach the given \"virtual\"\\ntarget, the model weights are updated with the accumulated gradients. For\\nexample, you have a batch size of 4 and a micro-batch size of 1. Then, the\\nforward & backward passes will be done using only x1 sample, and the\\noptimization step will be done using the aggregated gradient of the 4 samples.  \\n  \\nùü±\\\\. ùó®ùòÄùó≤ ùóÆ ùòÄùòÅùóÆùòÅùó≤ùóπùó≤ùòÄùòÄ ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùó≤ùóø: Adam is the most popular optimizer. It is one\\nof the most stable optimizers, but the downside is that it has 2 additional\\nparameters (a mean & variance) for every model parameter. If you use a\\nstateless optimizer, such as SGD, you can reduce the number of parameters by\\n2/3, which is significant for LLMs.  \\n  \\nùü≤\\\\. ùóöùóøùóÆùó±ùó∂ùó≤ùóªùòÅ (ùóºùóø ùóÆùó∞ùòÅùó∂ùòÉùóÆùòÅùó∂ùóºùóª) ùó∞ùóµùó≤ùó∞ùó∏ùóΩùóºùó∂ùóªùòÅùó∂ùóªùó¥: It drops specific activations\\nduring the forward pass and recomputes them during the backward pass. Thus, it\\neliminates the need to hold all activations simultaneously in VRAM. This\\ntechnique reduces VRAM consumption but makes the training slower.  \\n  \\nùü≥\\\\. ùóñùó£ùó® ùóΩùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùóºùó≥ùó≥ùóπùóºùóÆùó±ùó∂ùóªùó¥: As the name suggests, the parameters that do not\\nfit on your GPU\\'s VRAM are loaded on the CPU. Intuitively, you can see it as a\\nmodel parallelism between your GPU & CPU.\\n\\nA happy dude going for a walk with his GPU [Image by DALL-E]\\n\\nMost of these methods are orthogonal, so you can combine them and drastically\\nreduce your VRAM requirements during training.\\n\\n* * *\\n\\n### #3. 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\nIn the past weeks, we covered ùòÑùóµùòÜ you have to fine-tune an LLM and ùòÑùóµùóÆùòÅ\\nresources & tools you need:  \\n\\\\- Q&A dataset  \\n\\\\- pre-trained LLM (Falcon 7B) & QLoRA  \\n\\\\- MLOps: experiment tracker, model registry, prompt monitoring (Comet ML)  \\n\\\\- compute platform (Beam)  \\n  \\n.  \\n  \\nNow, let\\'s see how you can hook all of these pieces together into a single\\nfine-tuning module ‚Üì  \\n  \\nùü≠\\\\. ùóüùóºùóÆùó± ùòÅùóµùó≤ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ  \\n  \\nOur Q&A samples have the following structure keys: \"about_me,\" \"user_context,\"\\n\"question,\" and \"answer.\"  \\n  \\nFor task-specific fine-tuning, you need only 100-1000 samples. Thus, you can\\ndirectly load the whole JSON in memory.  \\n  \\nAfter you map every sample to a list of Python ùò•ùò¢ùòµùò¢ùò§ùò≠ùò¢ùò¥ùò¥ùò¶ùò¥ to validate the\\nstructure & type of the ingested instances.  \\n  \\nùüÆ\\\\. ùó£ùóøùó≤ùóΩùóøùóºùó∞ùó≤ùòÄùòÄ ùòÅùóµùó≤ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ ùó∂ùóªùòÅùóº ùóΩùóøùóºùó∫ùóΩùòÅùòÄ  \\n  \\nThe first step is to use ùò∂ùòØùò¥ùòµùò≥ùò∂ùò§ùòµùò∂ùò≥ùò¶ùò• to clean every sample by removing\\nredundant characters.  \\n  \\nAfter, as every sample consists of multiple fields, you must map it to a\\nsingle piece of text, also known as the prompt.  \\n  \\nTo do so, you define a ùòóùò≥ùò∞ùòÆùò±ùòµùòõùò¶ùòÆùò±ùò≠ùò¢ùòµùò¶ class to manage all your prompts. You\\nwill use it to map all the sample keys to a prompt using a Python f-string.  \\n  \\nThe last step is to map the list of Python ùò•ùò¢ùòµùò¢ùò§ùò≠ùò¢ùò¥ùò¥ùò¶ùò¥ to a HuggingFace\\ndataset and map every sample to a prompt, as discussed above.  \\n  \\nùüØ\\\\. ùóüùóºùóÆùó± ùòÅùóµùó≤ ùóüùóüùó† ùòÇùòÄùó∂ùóªùó¥ ùó§ùóüùóºùó•ùóî  \\n  \\nLoad a pretrained Falcon 7B LLM by passing a ùò£ùò™ùòµùò¥ùò¢ùòØùò•ùò£ùò∫ùòµùò¶ùò¥ quantization\\nconfiguration that loads all the weights on 4 bits.  \\n  \\nAfter using LoRA, you freeze the weights of the original Falcon LLM and attach\\nto it a set of trainable adapters.  \\n  \\nùü∞\\\\. ùóôùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥  \\n  \\nThe ùòµùò≥ùò≠ Python package makes this step extremely simple.  \\n  \\nYou pass to the ùòöùòçùòõùòõùò≥ùò¢ùò™ùòØùò¶ùò≥ class the training arguments, the dataset and the\\nmodel and call the ùòµùò≥ùò¢ùò™ùòØ() method.  \\n  \\nOne crucial aspect is configuring an experiment tracker, such as Comet ML, to\\nlog the loss and other vital metrics & artifacts.  \\n  \\nùü±\\\\. ùó£ùòÇùòÄùóµ ùòÅùóµùó≤ ùóØùó≤ùòÄùòÅ ùó∫ùóºùó±ùó≤ùóπ ùòÅùóº ùòÅùóµùó≤ ùó∫ùóºùó±ùó≤ùóπ ùóøùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ  \\n  \\nOne of the final steps is to attach a callback to the ùòöùòçùòõùòõùò≥ùò¢ùò™ùòØùò¶ùò≥ class that\\nruns when the training ends to push the model with the lowest loss to the\\nmodel registry as the new production candidate.  \\n  \\nùü≤\\\\. ùóòùòÉùóÆùóπùòÇùóÆùòÅùó≤ ùòÅùóµùó≤ ùóªùó≤ùòÑ ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª ùó∞ùóÆùóªùó±ùó∂ùó±ùóÆùòÅùó≤  \\n  \\nEvaluating generative AI models can be pretty tricky.  \\n  \\nYou can run the LLM on the test set and log the prompts & answers to Comet\\nML\\'s monitoring system to check them manually.  \\n  \\nIf the provided answers are valid, using the model registry dashboard, you\\nwill manually release it to replace the old LLM.  \\n  \\nùü≥\\\\. ùóóùó≤ùóΩùóπùóºùòÜ ùòÅùóº ùóïùó≤ùóÆùó∫  \\n  \\nIt is as easy as wrapping the training & inference functions (or classes) with\\na Python \"@ùò¢ùò±ùò±.ùò≥ùò∂ùòØ()\" decorator.\\n\\nA step-by-step guide on fine-tuning an LLM to create a real-time financial\\nadvisor [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 9** ,**** the last lesson of the **Hands-\\nOn LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: 7-steps on how to fine-tune an open-source LLM to create your real-\\ntime financial advisor\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-7-steps-on-how-to-fine-tune-an?r=1ttoeh'), ArticleDocument(id=UUID('dffed5e0-c824-40db-9388-a26fa09f7b49'), content={'Title': 'DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?', 'Subtitle': 'Lesson 7 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\n### Lesson 7 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 14, 2023\\n\\n5\\n\\nShare this post\\n\\n#### DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 7 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. Real-time feature pipeline video lesson\\n\\n  2. How do you generate a synthetic domain-specific Q&A dataset in <30 minutes to fine-tune your open-source LLM?\\n\\n  3. My personal list of filtered resources about LLMs & vector DBs\\n\\n#### Previous Lessons:\\n\\n  * Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n  * Lesson 5: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n  * Lesson 6: What do you need to fine-tune an open-source LLM to create your financial advisor?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Real-time feature pipeline video lesson\\n\\nI know we are currently talking about the training pipeline and Q&A dataset\\ngeneration, but sometimes, mixing the information to remember and make new\\nconnections is healthy.\\n\\n‚Ä¶or maybe that is only an excuse to share the video lesson about the feature\\npipeline that wasn‚Äôt ready when I started this series.\\n\\nIt will teach you how to ùó∂ùóªùó¥ùó≤ùòÄùòÅ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóªùó≤ùòÑùòÄ in ùóøùó≤ùóÆùóπ-ùòÅùó∂ùó∫ùó≤ from Alpaca, ùó∞ùóπùó≤ùóÆùóª\\n& ùó≤ùó∫ùóØùó≤ùó± the ùó±ùóºùó∞ùòÇùó∫ùó≤ùóªùòÅùòÄ, and ùóπùóºùóÆùó± them in a ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï.\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùóÆùóª ùóºùòÉùó≤ùóøùòÉùó∂ùó≤ùòÑ ùóºùó≥ ùòÅùóµùó≤ ùòÉùó∂ùó±ùó≤ùóº ‚Üì  \\n  \\n1\\\\. Step-by-step instructions on how to set up the streaming pipeline code & a\\nQdrant vector DB serverless cluster  \\n2\\\\. Why we used Bytewax to build the streaming pipeline  \\n3\\\\. How we used Bytewax to ingest financial news in real-time leveraging a\\nWebSocket, clean the documents, chunk them, embed them and ingest them in the\\nQdrant vector DB  \\n4\\\\. How we adapted the Bytewax streaming pipeline to also work in batch mode\\nto populate the vector DB with historical data  \\n5\\\\. How to run the code  \\n6\\\\. How to deploy the code to AWS\\n\\nHere it is ‚Üì Enjoy üëÄ\\n\\n* * *\\n\\n## #2. How do you generate a synthetic domain-specific Q&A dataset in <30\\nminutes to fine-tune your open-source LLM?\\n\\nThis method is also known as ùó≥ùó∂ùóªùó≤ùòÅùòÇùóªùó∂ùóªùó¥ ùòÑùó∂ùòÅùóµ ùó±ùó∂ùòÄùòÅùó∂ùóπùóπùóÆùòÅùó∂ùóºùóª. Here are its 3 ùòÆùò¢ùò™ùòØ\\nùò¥ùòµùò¶ùò±ùò¥ ‚Üì  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶, ùò≠ùò¶ùòµ\\'ùò¥ ùò®ùò¶ùòØùò¶ùò≥ùò¢ùòµùò¶ ùò¢ ùòò&ùòà ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùò•ùò¢ùòµùò¢ùò¥ùò¶ùòµ ùò∂ùò¥ùò¶ùò• ùòµùò∞ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò¶ ùò¢\\nùòßùò™ùòØùò¢ùòØùò§ùò™ùò¢ùò≠ ùò¢ùò•ùò∑ùò™ùò¥ùò∞ùò≥ ùòìùòìùòî.  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: ùó†ùóÆùóªùòÇùóÆùóπùóπùòÜ ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóÆ ùó≥ùó≤ùòÑ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nGenerate a few input samples (~3) that have the following structure:  \\n\\\\- ùò∂ùò¥ùò¶ùò≥_ùò§ùò∞ùòØùòµùò¶ùòπùòµ: describe the type of investor (e.g., \"I am a 28-year-old\\nmarketing professional\")  \\n\\\\- ùò≤ùò∂ùò¶ùò¥ùòµùò™ùò∞ùòØ: describe the user\\'s intention (e.g., \"Is Bitcoin a good\\ninvestment option?\")  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: ùóòùòÖùóΩùóÆùóªùó± ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ùóµùó≤ùóπùóΩ ùóºùó≥ ùóÆ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó†  \\n  \\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up\\nto +N similar input examples.  \\n  \\nWe generated 100 input examples in our use case, but you can generate more.  \\n  \\nYou will use the manually filled input examples to do few-shot prompting.  \\n  \\nThis will guide the LLM to give you domain-specific samples.  \\n  \\nùòõùò©ùò¶ ùò±ùò≥ùò∞ùòÆùò±ùòµ ùò∏ùò™ùò≠ùò≠ ùò≠ùò∞ùò∞ùò¨ ùò≠ùò™ùò¨ùò¶ ùòµùò©ùò™ùò¥:  \\n\"\"\"  \\n...  \\nGenerate 100 more examples with the following pattern:  \\n  \\n# USER CONTEXT 1  \\n...  \\n  \\n# QUESTION 1  \\n...  \\n  \\n# USER CONTEXT 2  \\n...  \\n\"\"\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: ùó®ùòÄùó≤ ùòÅùóµùó≤ ùòÅùó≤ùóÆùó∞ùóµùó≤ùóø ùóüùóüùó† ùòÅùóº ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ ùóºùòÇùòÅùóΩùòÇùòÅùòÄ ùó≥ùóºùóø ùóÆùóπùóπ ùòÅùóµùó≤ ùó∂ùóªùóΩùòÇùòÅ ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ  \\n  \\nNow, you will have the same powerful LLM as a teacher, but this time, it will\\nanswer all your N input examples.  \\n  \\nBut first, to introduce more variance, we will use RAG to enrich the input\\nexamples with news context.  \\n  \\nAfterward, we will use the teacher LLM to answer all N input examples.  \\n  \\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual\\nwork.  \\n  \\n.  \\n  \\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a\\nniched task, such as financial advising.  \\n  \\nThis technique is known as finetuning with distillation because you use a\\npowerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data,\\nwhich will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as\\nthe student.  \\n  \\n‚úíÔ∏è ùòïùò∞ùòµùò¶: To ensure that the generated data is of high quality, you can hire a\\ndomain expert to check & refine it.\\n\\nHow do you generate a Q&A dataset in <30 minutes to fine-tune your LLMs?\\n[Image by the Author].\\n\\n‚Ü≥ To learn more about this technique, check out ‚ÄúHow to generate a Q&A dataset\\nin less than 30 minutes‚Äù Pau Labarta\\'s article from\\n\\nReal-World Machine Learning\\n\\n.\\n\\n* * *\\n\\n### #3. My personal list of filtered resources about LLMs & vector DBs\\n\\nThe internet is full of ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ ùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ùòÄ about ùóüùóüùó†ùòÄ & ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóïùòÄ. But ùó∫ùóºùòÄùòÅ\\nùóºùó≥ ùó∂ùòÅ is ùòÅùóøùóÆùòÄùóµ.  \\n  \\nAfter ùü≤ ùó∫ùóºùóªùòÅùóµùòÄ of ùóøùó≤ùòÄùó≤ùóÆùóøùó∞ùóµùó∂ùóªùó¥ ùóüùóüùó†ùòÄ & ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóïùòÄ, here is a ùóπùó∂ùòÄùòÅ ùóºùó≥ ùó≥ùó∂ùóπùòÅùó≤ùóøùó≤ùó±\\nùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ùòÄ that I ùóΩùó≤ùóøùòÄùóºùóªùóÆùóπùóπùòÜ ùòÇùòÄùó≤ ‚Üì  \\n  \\nùòâùò≠ùò∞ùò®ùò¥:  \\n  \\n\\\\- philschmid  \\n\\\\- Chip Huyen  \\n\\\\- eugeneyan  \\n\\\\- LLM Learning Lab  \\n\\\\- Lil\\'Log  \\n\\\\- VectorHub by SuperLinked  \\n\\\\- Qdrant Blog  \\n  \\nùòàùò≥ùòµùò™ùò§ùò≠ùò¶ùò¥:  \\n  \\n\\\\- Patterns for Building LLM-based Systems & Products  \\n\\\\- RLHF: Reinforcement Learning from Human Feedback  \\n\\\\- Illustrating Reinforcement Learning from Human Feedback (RLHF)  \\n\\\\- Understanding Encoder And Decoder LLMs  \\n\\\\- Building LLM applications for production  \\n\\\\- Prompt Engineering  \\n\\\\- Transformers  \\n\\\\- Bidirectional Encoder Representations from Transformers (BERT)  \\n\\\\- Multimodality and Large Multimodal Models (LMMs) by Chip Huyen  \\n  \\nùòùùò™ùò•ùò¶ùò∞ùò¥:  \\n  \\n\\\\- Word Embedding and Word2Vec, Clearly Explained!!!  \\n\\\\- Let\\'s build GPT: from scratch, in code, spelled out  \\n\\\\- Transformer Neural Networks, ChatGPT\\'s foundation, Clearly Explained!!!  \\n\\\\- Large Language Models with Semantic Search  \\n\\\\- Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly\\nExplained!!!  \\n  \\nùòäùò∞ùò•ùò¶ ùòôùò¶ùò±ùò∞ùò¥ùò™ùòµùò∞ùò≥ùò™ùò¶ùò¥:  \\n  \\n\\\\- OpenAI Cookbook  \\n\\\\- generative-ai-for-beginners  \\n  \\nùòäùò∞ùò∂ùò≥ùò¥ùò¶ùò¥:  \\n  \\n\\\\- LangChain for LLM Application Development  \\n\\\\- Building Systems with the ChatGPT API  \\n\\\\- ChatGPT Prompt Engineering for Developers  \\n  \\n.  \\n  \\n...and hopefully, my üîó Hands-on LLMs course will soon appear along them.\\n\\nImage by DALL-E\\n\\nLet me know what you think of this list and have fun learning üî•\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 8** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n5\\n\\nShare this post\\n\\n#### DML: How do you generate a Q&A dataset in <30 minutes to fine-tune your\\nLLMs?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-do-you-generate-a-q-and-a?r=1ttoeh'), ArticleDocument(id=UUID('15c3831b-67fd-4279-970a-a720aafefa67'), content={'Title': 'DML: What do you need to fine-tune an open-source LLM to create your financial advisor?', 'Subtitle': 'Lesson 6 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\n### Lesson 6 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nDec 07, 2023\\n\\n4\\n\\nShare this post\\n\\n#### DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 6 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. The difference between encoders, decoders, and encoder-decoder LLMs.\\n\\n  2. You must know these 3 main stages of training an LLM to train your own LLM on your proprietary data.\\n\\n  3. What do you need to fine-tune an open-source LLM to create your own financial advisor?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 3: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?\\n\\n  * Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n  * Lesson 5: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. The difference between encoders, decoders, and encoder-decoder LLMs\\n\\nLet\\'s see when to use each architecture ‚Üì  \\n  \\nAs embeddings are everywhere, both encoders and decoders use self-attention\\nlayers to encode word tokens into embeddings.  \\n  \\nThe devil is in the details. Let\\'s clarify it ‚Üì  \\n  \\nùóßùóµùó≤ ùó¢ùóøùó∂ùó¥ùó∂ùóªùóÆùóπ ùóßùóøùóÆùóªùòÄùó≥ùóºùóøùó∫ùó≤ùóø  \\n  \\nIt is an encoder-decoder setup. The encoder processes the input text and hands\\noff its understanding as embeddings to the decoder, which will generate the\\nfinal output.  \\n  \\nThe key difference between an encoder & decoder is in how it processes its\\ninputs & outputs.  \\n  \\n=== ùóòùóªùó∞ùóºùó±ùó≤ùóøùòÄ ===  \\n  \\nThe role of an encoder is to extract relevant information from the whole input\\nand encode it into an embedding (e.g., BERT, RoBERTa).  \\n  \\nWithin the \"Multi-head attention\" of the transformer, all the tokens are\\nallowed to speak to each other.  \\n  \\nA token at position t can talk to all other previous tokens [0, t-1] and\\nfuture tokens [t+1, T]. This means that the attention mask is computed along\\nthe whole vector.  \\n  \\nThus, because the encoder processes the whole input, it is helpful for\\nclassification tasks (e.g., sentiment analysis) and creates embeddings for\\nclustering, recommender systems, vector DB indexes, etc.  \\n  \\n=== ùóóùó≤ùó∞ùóºùó±ùó≤ùóøùòÄ ===  \\n  \\nOn the flip side, if you want to generate text, use decoder-only models (e.g.,\\nGPT family).  \\n  \\nOnly the current and previous tokens (not the whole input) are used to predict\\nthe next token.  \\n  \\nWithin the \"Masked Multi-head attention,\" the future positions are masked to\\nmaintain the autoregressive property of the decoding process.  \\n  \\nFor example, within the \"Masked Multi-head attention,\" instead of all the\\ntokens talking to each other, a token at position t will have access only to\\nprevious tokens at positions t-1, t-2, t-3, ..., 0.  \\n  \\n=== ùóòùóªùó∞ùóºùó±ùó≤ùóø-ùó±ùó≤ùó∞ùóºùó±ùó≤ùóø ===  \\n  \\nThis technique is used when you have to understand the entire input sequence\\n(encoder) and the previously generated sequence (decoder -> autoregressive).  \\n  \\nTypical use cases are text translation & summarization (the original\\ntransformer was built for text translation), where the output heavily relies\\non the input.  \\n  \\nWhy? Because the decoding step always has to be conditioned by the encoded\\ninformation. Also known as cross-attention, the decoder queries the encoded\\ninformation for information to guide the decoding process.  \\n  \\nFor example, when translating English to Spanish, every Spanish token\\npredicted is conditioned by the previously predicted Spanish tokens & the\\nentire English sentence.\\n\\nEncoder vs. Decoder vs. Encoder-Decoder LLMs [Image by the Author].\\n\\nTo conclude...  \\n  \\n\\\\- a decoder takes as input previous tokens and predicts the next one (in an\\nautoregressive way)  \\n\\\\- by dropping the \"Masked\" logic from the \"Masked Multi-head attention,\" you\\nprocess the whole input, transforming the decoder into an encoder  \\n\\\\- if you hook the encoder to the decoder through a cross-attention layer, you\\nhave an encoder-decoder architecture\\n\\n* * *\\n\\n### #2. You must know these 3 main stages of training an LLM to train your own\\nLLM on your proprietary data\\n\\nYou must know these ùüØ ùó∫ùóÆùó∂ùóª ùòÄùòÅùóÆùó¥ùó≤ùòÄ of ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóÆùóª ùóüùóüùó† to train your own ùóüùóüùó† on\\nyour ùóΩùóøùóºùóΩùóøùó∂ùó≤ùòÅùóÆùóøùòÜ ùó±ùóÆùòÅùóÆ.  \\n  \\n# ùó¶ùòÅùóÆùó¥ùó≤ ùü≠: ùó£ùóøùó≤ùòÅùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó≥ùóºùóø ùó∞ùóºùó∫ùóΩùóπùó≤ùòÅùó∂ùóºùóª  \\n  \\nYou start with a bear foot randomly initialized LLM.  \\n  \\nThis stage aims to teach the model to spit out tokens. More concretely, based\\non previous tokens, the model learns to predict the next token with the\\nhighest probability.  \\n  \\nFor example, your input to the model is \"The best programming language is\\n___\", and it will answer, \"The best programming language is Rust.\"  \\n  \\nIntuitively, at this stage, the LLM learns to speak.  \\n  \\nùòãùò¢ùòµùò¢: >1 trillion token (~= 15 million books). The data quality doesn\\'t have\\nto be great. Hence, you can scrape data from the internet.  \\n  \\n# ùó¶ùòÅùóÆùó¥ùó≤ ùüÆ: ùó¶ùòÇùóΩùó≤ùóøùòÉùó∂ùòÄùó≤ùó± ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ (ùó¶ùóôùóß) ùó≥ùóºùóø ùó±ùó∂ùóÆùóπùóºùó¥ùòÇùó≤  \\n  \\nYou start with the pretrained model from stage 1.  \\n  \\nThis stage aims to teach the model to respond to the user\\'s questions.  \\n  \\nFor example, without this step, when prompting: \"What is the best programming\\nlanguage?\", it has a high probability of creating a series of questions such\\nas: \"What is MLOps? What is MLE? etc.\"  \\n  \\nAs the model mimics the training data, you must fine-tune it on Q&A (questions\\n& answers) data to align the model to respond to questions instead of\\npredicting the following tokens.  \\n  \\nAfter the fine-tuning step, when prompted, \"What is the best programming\\nlanguage?\", it will respond, \"Rust\".  \\n  \\nùòãùò¢ùòµùò¢: 10K - 100K Q&A example  \\n  \\nùòïùò∞ùòµùò¶: After aligning the model to respond to questions, you can further\\nsingle-task fine-tune the model, on Q&A data, on a specific use case to\\nspecialize the LLM.  \\n  \\n# ùó¶ùòÅùóÆùó¥ùó≤ ùüØ: ùó•ùó≤ùó∂ùóªùó≥ùóºùóøùó∞ùó≤ùó∫ùó≤ùóªùòÅ ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥ ùó≥ùóøùóºùó∫ ùóµùòÇùó∫ùóÆùóª ùó≥ùó≤ùó≤ùó±ùóØùóÆùó∞ùó∏ (ùó•ùóüùóõùóô)  \\n  \\nDemonstration data tells the model what kind of responses to give but doesn\\'t\\ntell the model how good or bad a response is.  \\n  \\nThe goal is to align your model with user feedback (what users liked or didn\\'t\\nlike) to increase the probability of generating answers that users find\\nhelpful.  \\n  \\nùòôùòìùòèùòç ùò™ùò¥ ùò¥ùò±ùò≠ùò™ùòµ ùò™ùòØ 2:  \\n  \\n1\\\\. Using the LLM from stage 2, train a reward model to act as a scoring\\nfunction using (prompt, winning_response, losing_response) samples (=\\ncomparison data). The model will learn to maximize the difference between\\nthese 2. After training, this model outputs rewards for (prompt, response)\\ntuples.  \\n  \\nùòãùò¢ùòµùò¢: 100K - 1M comparisons  \\n  \\n2\\\\. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here,\\nyou will use the reward model trained above to give a score for every:\\n(prompt, response). The RL algorithm will align the LLM to generate prompts\\nwith higher rewards, increasing the probability of generating responses that\\nusers liked.  \\n  \\nùòãùò¢ùòµùò¢: 10K - 100K prompts\\n\\nThe 3 main stages of training an LLM that you must know [Image by the Author].\\n\\n**Note:** Post inspired by Chip Huyen\\'s üîó RLHF: Reinforcement Learning from\\nHuman Feedback\" article.\\n\\n* * *\\n\\n### #3. What do you need to fine-tune an open-source LLM to create your own\\nfinancial advisor?\\n\\nThis is the ùóüùóüùó† ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùó∏ùó∂ùòÅ you must know ‚Üì  \\n  \\nùóóùóÆùòÅùóÆùòÄùó≤ùòÅ  \\n  \\nThe key component of any successful ML project is the data.  \\n  \\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial\\nscenarios.  \\n  \\nThe best approach is to hire a bunch of experts to create it manually.  \\n  \\nBut, for a PoC, that might get expensive & slow.  \\n  \\nThe good news is that a method called \"ùòçùò™ùòØùò¶ùòµùò∂ùòØùò™ùòØùò® ùò∏ùò™ùòµùò© ùò•ùò™ùò¥ùòµùò™ùò≠ùò≠ùò¢ùòµùò™ùò∞ùòØ\" exists.  \\n  \\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to\\ngenerate your fine-tuning data. After, use this data to fine-tune a smaller\\nmodel (e.g., Falcon 7B).\"  \\n  \\nFor specializing smaller LLMs on specific use cases (e.g., financial\\nadvisors), this is an excellent method to kick off your project.  \\n  \\nùó£ùóøùó≤-ùòÅùóøùóÆùó∂ùóªùó≤ùó± ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó†  \\n  \\nYou never want to start training your LLM from scratch (or rarely).  \\n  \\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.  \\n  \\nYou want to fine-tune your LLM on your specific task.  \\n  \\nThe good news is that you can find a plethora of open-source LLMs on\\nHuggingFace (e.g., Falcon, LLaMa, etc.)  \\n  \\nùó£ùóÆùóøùóÆùó∫ùó≤ùòÅùó≤ùóø ùó≤ùó≥ùó≥ùó∂ùó∞ùó∂ùó≤ùóªùòÅ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥  \\n  \\nAs LLMs are big... duh...  \\n  \\n... they don\\'t fit on a single GPU.  \\n  \\nAs you want only to fine-tune the LLM, the community invented clever\\ntechniques that quantize the LLM (to fit on a single GPU) and fine-tune only a\\nset of smaller adapters.  \\n  \\nOne popular approach is QLoRA, which can be implemented using HF\\'s `ùò±ùò¶ùòßùòµ`\\nPython package.  \\n  \\nùó†ùóüùó¢ùóΩùòÄ  \\n  \\nAs you want your project to get to production, you have to integrate the\\nfollowing MLOps components:  \\n  \\n\\\\- experiment tracker to monitor & compare your experiments  \\n\\\\- model registry to version & share your models between the FTI pipelines  \\n\\\\- prompts monitoring to debug & track complex chains  \\n  \\n‚Ü≥üîó All of them are available on ML platforms, such as Comet ML  \\n  \\nùóñùóºùó∫ùóΩùòÇùòÅùó≤ ùóΩùóπùóÆùòÅùó≥ùóºùóøùó∫  \\n  \\nThe most common approach is to train your LLM on your on-prem Nivida GPUs\\ncluster or rent them on cloud providers such as AWS, Paperspace, etc.  \\n  \\nBut what if I told you that there is an easier way?  \\n  \\nThere is! It is called serverless.  \\n  \\nFor example, Beam is a GPU serverless provider that makes deploying your\\ntraining pipeline as easy as decorating your Python function with\\n`@ùò¢ùò±ùò±.ùò≥ùò∂ùòØ()`.  \\n  \\nAlong with ease of deployment, you can easily add your training code to your\\nCI/CD to add the final piece of the MLOps puzzle, called CT (continuous\\ntraining).  \\n  \\n‚Ü≥üîó Beam\\n\\nWhat | Training Pipeline [Image by the Author].\\n\\n> ‚Ü≥üîó To see all these components in action, check out our FREE ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ\\n> ùó∞ùóºùòÇùóøùòÄùó≤ & give it a ‚≠ê\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 7** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n4\\n\\nShare this post\\n\\n#### DML: What do you need to fine-tune an open-source LLM to create your\\nfinancial advisor?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-what-do-you-need-to-fine-tune?r=1ttoeh'), ArticleDocument(id=UUID('174d6f07-42f4-4190-9150-bb4ad35f8413'), content={'Title': 'DML: Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?', 'Subtitle': 'Lesson 5 | The Hands-on LLMs Series', 'Content': \"#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Why & when do you need to fine-tune open-source LLMs? What about\\nfine-tuning vs. prompt engineering?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Why & when do you need to fine-tune open-source LLMs? What about fine-\\ntuning vs. prompt engineering?\\n\\n### Lesson 5 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 30, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: Why & when do you need to fine-tune open-source LLMs? What about\\nfine-tuning vs. prompt engineering?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 5 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. Using this Python package, you can x10 your text preprocessing pipeline development.\\n\\n  2. Why & when do you need to fine-tune open-source LLMs? What about fine-tuning vs. prompt engineering?\\n\\n  3. Fine-tuning video lessons\\n\\n#### Previous Lessons:\\n\\n  * Lesson 2: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n  * Lesson 3: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?\\n\\n  * Lesson 4: How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Using this Python package, you can x10 your text preprocessing\\npipeline development\\n\\nAny text preprocessing pipeline has to clean, partition, extract, or chunk\\ntext data to feed it into your LLMs.  \\n  \\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± offers a ùóøùó∂ùó∞ùóµ and ùó∞ùóπùó≤ùóÆùóª ùóîùó£ùóú that allows you to quickly:  \\n  \\n\\\\- ùò±ùò¢ùò≥ùòµùò™ùòµùò™ùò∞ùòØ your data into smaller segments from various data sources (e.g.,\\nHTML, CSV, PDFs, even images, etc.)  \\n\\\\- ùò§ùò≠ùò¶ùò¢ùòØùò™ùòØùò® the text of anomalies (e.g., wrong ASCII characters), any\\nirrelevant information (e.g., white spaces, bullets, etc.), and filling\\nmissing values  \\n\\\\- ùò¶ùòπùòµùò≥ùò¢ùò§ùòµùò™ùòØùò® information from pieces of text (e.g., datetimes, addresses, IP\\naddresses, etc.)  \\n\\\\- ùò§ùò©ùò∂ùòØùò¨ùò™ùòØùò® your text segments into pieces of text that can be inserted into\\nyour embedding model  \\n\\\\- ùò¶ùòÆùò£ùò¶ùò•ùò•ùò™ùòØùò® data (e.g., wrapper over OpenAIEmbeddingEncoder,\\nHuggingFaceEmbeddingEncoders, etc.)  \\n\\\\- ùò¥ùòµùò¢ùò®ùò¶ your data to be fed into various tools (e.g., Label Studio, Label\\nBox, etc.)\\n\\nUnstructured [Image by the Author].\\n\\nùóîùóπùóπ ùòÅùóµùó≤ùòÄùó≤ ùòÄùòÅùó≤ùóΩùòÄ ùóÆùóøùó≤ ùó≤ùòÄùòÄùó≤ùóªùòÅùó∂ùóÆùóπ ùó≥ùóºùóø:  \\n  \\n\\\\- feeding your data into your LLMs  \\n\\\\- embedding the data and ingesting it into a vector DB  \\n\\\\- doing RAG  \\n\\\\- labeling  \\n\\\\- recommender systems  \\n  \\n... basically for any LLM or multimodal applications  \\n  \\n.  \\n  \\nImplementing all these steps from scratch will take a lot of time.  \\n  \\nI know some Python packages already do this, but the functionality is\\nscattered across multiple packages.  \\n  \\nùòÇùóªùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± packages everything together under a nice, clean API.  \\n  \\n‚Ü≥ Check it out.\\n\\n* * *\\n\\n### #2. Why & when do you need to fine-tune open-source LLMs? What about fine-\\ntuning vs. prompt engineering?\\n\\nFine-tuning is the process of taking a pre-trained model and further refining\\nit on a specific task.  \\n  \\nùóôùó∂ùóøùòÄùòÅ, ùóπùó≤ùòÅ'ùòÄ ùó∞ùóπùóÆùóøùó∂ùó≥ùòÜ ùòÑùóµùóÆùòÅ ùó∫ùó≤ùòÅùóµùóºùó±ùòÄ ùóºùó≥ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùóÆùóª ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó† ùó≤ùòÖùó∂ùòÄt ‚Üì  \\n  \\n\\\\- ùòäùò∞ùòØùòµùò™ùòØùò∂ùò¶ùò• ùò±ùò≥ùò¶-ùòµùò≥ùò¢ùò™ùòØùò™ùòØùò®: utilize domain-specific data to apply the same pre-\\ntraining process (next token prediction) on the pre-trained (base) model  \\n\\\\- ùòêùòØùò¥ùòµùò≥ùò∂ùò§ùòµùò™ùò∞ùòØ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò®: the pre-trained (base) model is fine-tuned on a\\nQ&A dataset to learn to answer questions  \\n\\\\- ùòöùò™ùòØùò®ùò≠ùò¶-ùòµùò¢ùò¥ùò¨ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò®: the pre-trained model is refined for a specific\\ntask, such as toxicity detection, coding, medicine advice, etc.  \\n\\\\- ùòôùòìùòèùòç: It requires collecting human preferences (e.g., pairwise\\ncomparisons), which are then used to train a reward model. The reward model is\\nused to fine-tune the LLM via RL techniques such as PPO.  \\n  \\nCommon approaches are to take a pre-trained LLM (next-word prediction) and\\napply instruction & single-task fine-tuning.  \\n  \\nùó™ùóµùòÜ ùó±ùóº ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùòÅùóº ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùòÅùóµùó≤ ùóüùóüùó†?  \\n  \\nYou do instruction fine-tuning to make the LLM learn to answer your questions.  \\n  \\nThe exciting part is when you want to fine-tune your LLM on a single task.  \\n  \\nHere is why ‚Üì  \\n  \\nùò±ùò¶ùò≥ùòßùò∞ùò≥ùòÆùò¢ùòØùò§ùò¶: it will improve your LLM performance on given use cases (e.g.,\\ncoding, extracting text, etc.). Mainly, the LLM will specialize in a given\\ntask (a specialist will always beat a generalist in its domain)  \\n  \\nùò§ùò∞ùòØùòµùò≥ùò∞ùò≠: you can refine how your model should behave on specific inputs and\\noutputs, resulting in a more robust product  \\n  \\nùòÆùò∞ùò•ùò∂ùò≠ùò¢ùò≥ùò™ùòªùò¢ùòµùò™ùò∞ùòØ: you can create an army of smaller models, where each is\\nspecialized on a particular task, increasing the overall system's performance.\\nUsually, when you fine-tune one task, it reduces the performance of the other\\ntasks (known as the  \\nalignment tax). Thus, having an expert system of multiple smaller models can\\nimprove the overall performance.  \\n  \\nùó™ùóµùóÆùòÅ ùóÆùóØùóºùòÇùòÅ ùóΩùóøùóºùó∫ùóΩùòÅ ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóøùó∂ùóªùó¥ ùòÉùòÄ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥?  \\n  \\nùò•ùò¢ùòµùò¢: use prompting when you don't have data available (~2 examples are\\nenough). Fine-tuning needs at least >=100 examples to work.  \\n  \\nùò§ùò∞ùò¥ùòµ: prompting forces you to write long & detailed prompts to achieve your\\nlevel of performance. You pay per token (API or compute-wise). Thus, when a\\nprompt gets bigger, your costs increase. But, when fine-tuning an LLM, you\\nincorporate all that knowledge inside the model. Hence, you can use smaller\\nprompts with similar performance.\\n\\nFine-tuning LLMs [Image by the Author].\\n\\nWhen you start a project, a good strategy is to write a wrapper over an API\\n(e.g., OpenAI's GPT-4, Anyscale, etc.) that defines a desired interface that\\ncan easily be swapped with your open-source implementation in future\\niterations.\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\n### #3. Fine-tuning video lessons  \\n\\nAs you might know,\\n\\nPau Labarta Bajo\\n\\nfrom\\n\\nReal-World Machine Learning\\n\\nand I are also working on a free Hands-on LLMs course that contains the open-\\nsource code + a set of video lessons.\\n\\nHere are the 2 video lessons about fine-tuning ‚Üì\\n\\n#### 01 Hands-on LLMS | Theoretical Part\\n\\nHere is a ùò¥ùò∂ùòÆùòÆùò¢ùò≥ùò∫ of the 1ùò¥ùòµ ùò∑ùò™ùò•ùò¶ùò∞ ùò≠ùò¶ùò¥ùò¥ùò∞ùòØ ‚Üì\\n\\nùó™ùóµùòÜ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùóπùóÆùóøùó¥ùó≤ ùóπùóÆùóªùó¥ùòÇùóÆùó¥ùó≤ ùó∫ùóºùó±ùó≤ùóπùòÄ?  \\n  \\n1\\\\. ùòóùò¶ùò≥ùòßùò∞ùò≥ùòÆùò¢ùòØùò§ùò¶: Fine-tuning a large language model (LLM) can improve\\nperformance, especially for specialized tasks.  \\n  \\n2\\\\. ùòåùò§ùò∞ùòØùò∞ùòÆùò™ùò§ùò¥: Fine-tuned models are smaller and thus cheaper to run. This is\\ncrucial, given that LLMs can have billions of parameters.  \\n  \\nùó™ùóµùóÆùòÅ ùó±ùóº ùòÜùóºùòÇ ùóªùó≤ùó≤ùó± ùòÅùóº ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ ùóÆ ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤?  \\n  \\n1\\\\. ùòãùò¢ùòµùò¢ùò¥ùò¶ùòµ: You need a dataset of input-output examples. This dataset can be\\ncreated manually or semi-automatically using existing LLMs like GPT-3.5.  \\n  \\n2\\\\. ùòâùò¢ùò¥ùò¶ ùòìùòìùòî: Choose an open-source LLM from repositories like Hugging Face's\\nModel Hub (e.g., Falcon 7B)  \\n  \\n3\\\\. ùòçùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùò¥ùò§ùò≥ùò™ùò±ùòµ: Data loader + Trainer  \\n  \\n4\\\\. ùòàùò•ùò∑ùò¢ùòØùò§ùò¶ùò• ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò™ùòØùò® ùòµùò¶ùò§ùò©ùòØùò™ùò≤ùò∂ùò¶ùò¥ ùòµùò∞ ùòßùò™ùòØùò¶-ùòµùò∂ùòØùò¶ ùòµùò©ùò¶ ùòÆùò∞ùò•ùò¶ùò≠ ùò∞ùòØ ùò§ùò©ùò¶ùò¢ùò± ùò©ùò¢ùò≥ùò•ùò∏ùò¢ùò≥ùò¶:\\nQLoRA  \\n  \\n5\\\\. ùòîùòìùòñùò±ùò¥: Experiment Tracker + Model Registry  \\n  \\n6\\\\. ùòêùòØùòßùò≥ùò¢ùò¥ùòµùò≥ùò∂ùò§ùòµùò∂ùò≥ùò¶: Comet \\\\+ Beam\\n\\n#### 02 Hands-on LLMS | Diving into the code\\n\\nùóõùó≤ùóøùó≤ ùó∂ùòÄ ùóÆ ùòÄùóµùóºùóøùòÅ ùòÑùóÆùóπùó∏ùòÅùóµùóøùóºùòÇùó¥ùóµ ùóºùó≥ ùòÅùóµùó≤ ùóπùó≤ùòÄùòÄùóºùóª ‚Üì  \\n  \\n1\\\\. How to set up the code and environment using Poetry  \\n2\\\\. How to configure Comet & Beam  \\n3\\\\. How to start the training pipeline locally (if you have a CUDA-enabled\\nGPU) or on Beam (for running your training pipeline on a serverless\\ninfrastructure -> doesn't matter what hardware you have).  \\n4\\\\. An overview of the code  \\n5\\\\. Clarifying why we integrated Poetry, a model registry and linting within\\nthe training pipeline.  \\n  \\n‚ùóThis video is critical for everyone who wants to replicate the training\\npipeline of our course on their system. The previous lesson focused on the\\ntheoretical parts of the training pipeline.\\n\\n> ‚Ü≥üîó To find out the code & all the videos, check out the **Hands-on LLMs**\\n> GitHub repository.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 6** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: Why & when do you need to fine-tune open-source LLMs? What about\\nfine-tuning vs. prompt engineering?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n\", 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-why-and-when-do-you-need-to-fine?r=1ttoeh'), ArticleDocument(id=UUID('b6d86294-1bcc-4226-8218-3a63cab813a2'), content={'Title': 'DML: How to implement a streaming pipeline to populate a vector DB for real-time RAG?', 'Subtitle': 'Lesson 4 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How to implement a streaming pipeline to populate a vector DB for real-\\ntime RAG?\\n\\n### Lesson 4 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 23, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 4 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. What is Bytewax?\\n\\n  2. Why have vector DBs become so popular? Why are they so crucial for most ML applications?\\n\\n  3. How to implement a streaming pipeline to populate a vector DB for real-time RAG?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 1: How to design an LLM system for a financial assistant using the 3-pipeline design\\n\\n  * Lesson 2: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n  * Lesson 3: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. What is Bytewax?\\n\\nAre you afraid of writing ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ùòÄ? Or do you think they are hard\\nto implement?  \\n  \\nI did until I discovered Bytewax üêù. Let me show you ‚Üì  \\n  \\nBytewax üêù is an ùóºùóΩùó≤ùóª-ùòÄùóºùòÇùóøùó∞ùó≤ ùòÄùòÅùóøùó≤ùóÆùó∫ ùóΩùóøùóºùó∞ùó≤ùòÄùòÄùó∂ùóªùó¥ ùó≥ùóøùóÆùó∫ùó≤ùòÑùóºùóøùó∏ that:  \\n\\\\- is built in Rust ‚öôÔ∏è for performance  \\n\\\\- has Python üêç binding for ease of use  \\n  \\n... so for all the Python fanatics out there, no more JVM headaches for you.  \\n  \\nJokes aside, here is why Bytewax üêù is so powerful ‚Üì  \\n  \\n\\\\- Bytewax local setup is plug-and-play  \\n\\\\- can quickly be integrated into any Python project (you can go wild -- even\\nuse it in Notebooks)  \\n\\\\- can easily be integrated with other Python packages (NumPy, PyTorch,\\nHuggingFace, OpenCV, SkLearn, you name it)  \\n\\\\- out-of-the-box connectors for Kafka, local files, or you can quickly\\nimplement your own  \\n\\\\- CLI tool to easily deploy it to K8s, AWS, or GCP.  \\n  \\nùòçùò∞ùò≥ ùò¶ùòπùò¢ùòÆùò±ùò≠ùò¶ (ùò≠ùò∞ùò∞ùò¨ ùò¢ùòµ ùòµùò©ùò¶ ùò™ùòÆùò¢ùò®ùò¶ ùò£ùò¶ùò≠ùò∞ùò∏):  \\n1\\\\. We defined a streaming app in a few lines of code.  \\n2\\\\. We run the streaming app with one command.  \\n  \\n.  \\n  \\nThe thing is that I worked in Kafka Streams (in Kotlin) for one year.  \\n  \\nI loved & understood the power of building streaming applications. The only\\nthing that stood in my way was, well... Java.  \\n  \\nI don\\'t have something with Java; it is a powerful language. However, building\\nan ML application in Java + Python takes much time due to a more significant\\nresistance to integrating the two.  \\n  \\n...and that\\'s where Bytewax üêù kicks in.  \\n  \\nWe used Bytewax üêù for building the streaming pipeline for the ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ\\ncourse and loved it.\\n\\nWhat is Bytewax? [Iamge by the Author].\\n\\n* * *\\n\\n### #2. Why have vector DBs become so popular? Why are they so crucial for\\nmost ML applications?\\n\\nIn the world of ML, everything can be represented as an embedding.  \\n  \\nA vector DB is an intelligent way to use your data embeddings as an index and\\nperform fast and scalable searches between unstructured data points.  \\n  \\nSimply put, a vector DB allows you to find matches between anything and\\nanything (e.g., use an image as a query to find similar pieces of text, video,\\nother images, etc.).  \\n  \\n.  \\n  \\nùòêùòØ ùò¢ ùòØùò∂ùòµùò¥ùò©ùò¶ùò≠ùò≠, ùòµùò©ùò™ùò¥ ùò™ùò¥ ùò©ùò∞ùò∏ ùò∫ùò∞ùò∂ ùò§ùò¢ùòØ ùò™ùòØùòµùò¶ùò®ùò≥ùò¢ùòµùò¶ ùò¢ ùò∑ùò¶ùò§ùòµùò∞ùò≥ ùòãùòâ ùò™ùòØ ùò≥ùò¶ùò¢ùò≠-ùò∏ùò∞ùò≥ùò≠ùò•\\nùò¥ùò§ùò¶ùòØùò¢ùò≥ùò™ùò∞ùò¥ ‚Üì  \\n  \\nUsing various DL techniques, you can project your data points (images, videos,\\ntext, audio, user interactions) into the same vector space (aka the embeddings\\nof the data).  \\n  \\nYou will load the embeddings along a payload (e.g., a URL to the image, date\\nof creation, image description, properties, etc.) into the vector DB, where\\nthe data will be indexed along the:  \\n\\\\- vector  \\n\\\\- payload  \\n\\\\- text within the payload  \\n  \\nNow that the embedding indexes your data, you can query the vector DB by\\nembedding any data point.  \\n  \\nFor example, you can query the vector DB with an image of your cat and use a\\nfilter to retrieve only \"black\" cats.  \\n  \\nTo do so, you must embed the image using the same model you used to embed the\\ndata within your vector DB. After you query the database using a given\\ndistance (e.g., cosine distance between 2 vectors) to find similar embeddings.  \\n  \\nThese similar embeddings have attached to them their payload that contains\\nvaluable information such as the URL to an image, a URL to a site, an ID of a\\nuser, a chapter from a book about the cat of a witch, etc.  \\n  \\n.  \\n  \\nUsing this technique, I used Qdrant to implement RAG for a financial assistant\\npowered by LLMs.  \\n  \\nBut vector DBs go beyond LLMs & RAG.  \\n  \\nùòèùò¶ùò≥ùò¶ ùò™ùò¥ ùò¢ ùò≠ùò™ùò¥ùòµ ùò∞ùòß ùò∏ùò©ùò¢ùòµ ùò∫ùò∞ùò∂ ùò§ùò¢ùòØ ùò£ùò∂ùò™ùò≠ùò• ùò∂ùò¥ùò™ùòØùò® ùò∑ùò¶ùò§ùòµùò∞ùò≥ ùòãùòâùò¥ (e.g., Qdrant ):  \\n  \\n\\\\- similar image search  \\n\\\\- semantic text search (instead of plain text search)  \\n\\\\- recommender systems  \\n\\\\- RAG for chatbots  \\n\\\\- anomalies detection  \\n  \\n‚Ü≥üîó ùòäùò©ùò¶ùò§ùò¨ ùò∞ùò∂ùòµ ùòòùò•ùò≥ùò¢ùòØùòµ\\'ùò¥ ùò®ùò∂ùò™ùò•ùò¶ùò¥ ùò¢ùòØùò• ùòµùò∂ùòµùò∞ùò≥ùò™ùò¢ùò≠ùò¥ ùòµùò∞ ùò≠ùò¶ùò¢ùò≥ùòØ ùòÆùò∞ùò≥ùò¶ ùò¢ùò£ùò∞ùò∂ùòµ ùò∑ùò¶ùò§ùòµùò∞ùò≥ ùòãùòâùò¥.\\n\\nQdrant‚Äôs Architecture [Image from Qdrant docs].\\n\\n* * *\\n\\n### #3. How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\nThis is ùóµùóºùòÑ you can ùó∂ùó∫ùóΩùóπùó≤ùó∫ùó≤ùóªùòÅ a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ to populate a ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï to\\ndo ùó•ùóîùóö for a ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùòÄùòÄùó∂ùòÄùòÅùóÆùóªùòÅ powered by ùóüùóüùó†ùòÄ.  \\n  \\nIn a previous post, I covered ùòÑùóµùòÜ you need a streaming pipeline over a batch\\npipeline when implementing RAG.  \\n  \\nNow, we will focus on the ùóµùóºùòÑ, aka implementation details ‚Üì  \\n  \\nüêù All the following steps are wrapped in Bytewax functions and connected in a\\nsingle streaming pipeline.  \\n  \\nùóòùòÖùòÅùóøùóÆùó∞ùòÅ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóªùó≤ùòÑùòÄ ùó≥ùóøùóºùó∫ ùóîùóπùóΩùóÆùó∞ùóÆ  \\n  \\nYou need 2 types of inputs:  \\n  \\n1\\\\. A WebSocket API to listen to financial news in real-time. This will be\\nused to listen 24/7 for new data and ingest it as soon as it is available.  \\n  \\n2\\\\. A RESTful API to ingest historical data in batch mode. When you deploy a\\nfresh vector DB, you must populate it with data between a given range\\n[date_start; date_end].  \\n  \\nYou wrap the ingested HTML document and its metadata in a `pydantic`\\nNewsArticle model to validate its schema.  \\n  \\nRegardless of the input type, the ingested data is the same. Thus, the\\nfollowing steps are the same for both data inputs ‚Üì  \\n  \\nùó£ùóÆùóøùòÄùó≤ ùòÅùóµùó≤ ùóõùóßùó†ùóü ùó∞ùóºùóªùòÅùó≤ùóªùòÅ  \\n  \\nAs the ingested financial news is in HTML, you must extract the text from\\nparticular HTML tags.  \\n  \\n`unstructured` makes it as easy as calling `partition_html(document)`, which\\nwill recursively return the text within all essential HTML tags.  \\n  \\nThe parsed NewsArticle model is mapped into another `pydantic` model to\\nvalidate its new schema.  \\n  \\nThe elements of the news article are the headline, summary and full content.  \\n  \\nùóñùóπùó≤ùóÆùóª ùòÅùóµùó≤ ùòÅùó≤ùòÖùòÅ  \\n  \\nNow we have a bunch of text that has to be cleaned. Again, `unstructured`\\nmakes things easy. Calling a few functions we clean:  \\n\\\\- the dashes & bullets  \\n\\\\- extra whitespace & trailing punctuation  \\n\\\\- non ascii chars  \\n\\\\- invalid quotes  \\n  \\nFinally, we standardize everything to lowercase.  \\n  \\nùóñùóµùòÇùóªùó∏ ùòÅùóµùó≤ ùòÅùó≤ùòÖùòÅ  \\n  \\nAs the text can exceed the context window of the embedding model, we have to\\nchunk it.  \\n  \\nYet again, `unstructured` provides a valuable function that splits the text\\nbased on the tokenized text and expected input length of the embedding model.  \\n  \\nThis strategy is naive, as it doesn\\'t consider the text\\'s structure, such as\\nchapters, paragraphs, etc. As the news is short, this is not an issue, but\\nLangChain provides a `RecursiveCharacterTextSplitter` class that does that if\\nrequired.  \\n  \\nùóòùó∫ùóØùó≤ùó± ùòÅùóµùó≤ ùó∞ùóµùòÇùóªùó∏ùòÄ  \\n  \\nYou pass all the chunks through an encoder-only model.  \\n  \\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model\\nthat can run on a CPU and outputs a 384 embedding.  \\n  \\nBut based on the size and complexity of your data, you might need more complex\\nand bigger models.  \\n  \\nùóüùóºùóÆùó± ùòÅùóµùó≤ ùó±ùóÆùòÅùóÆ ùó∂ùóª ùòÅùóµùó≤ ùó§ùó±ùóøùóÆùóªùòÅ ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï  \\n  \\nFinally, you insert the embedded chunks and their metadata into the Qdrant\\nvector DB.  \\n  \\nThe metadata contains the embedded text, the source_url and the publish date.\\n\\nHow to implement a streaming pipeline to populate a vector DB for real-time\\nRAG [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 5** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: How to implement a streaming pipeline to populate a vector DB for\\nreal-time RAG?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-to-implement-a-streaming?r=1ttoeh'), ArticleDocument(id=UUID('b2296169-eed0-4b28-864a-08b061f5ee45'), content={'Title': 'DML: Why & what do you need a streaming pipeline when implementing RAG in your LLM applications?', 'Subtitle': 'Lesson 3 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\n### Lesson 3 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 16, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 3 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. RAG: What problems does it solve, and how it\\'s integrated into LLM-powered applications?\\n\\n  2. Why do you need a streaming pipeline instead of a batch pipeline when implementing RAG in your LLM applications?\\n\\n  3. What do you need to implement a streaming pipeline for a financial assistant?\\n\\n#### Previous Lessons:\\n\\n  * Lesson 1: How to design an LLM system for a financial assistant using the 3-pipeline design\\n\\n  * Lesson 2: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. RAG: What problems does it solve, and how it\\'s integrated into LLM-\\npowered applications?\\n\\nLet\\'s find out ‚Üì  \\n  \\nRAG is a popular strategy when building LLMs to add external data to your\\nprompt.  \\n  \\n=== ùó£ùóøùóºùóØùóπùó≤ùó∫ ===  \\n  \\nWorking with LLMs has 3 main issues:  \\n  \\n1\\\\. The world moves fast  \\n  \\nAn LLM learns an internal knowledge base. However, the issue is that its\\nknowledge is limited to its training dataset.  \\n  \\nThe world moves fast. New data flows on the internet every second. Thus, the\\nmodel\\'s knowledge base can quickly become obsolete.  \\n  \\nOne solution is to fine-tune the model every minute or day...  \\n  \\nIf you have some billions to spend around, go for it.  \\n  \\n2\\\\. Hallucinations  \\n  \\nAn LLM is full of testosterone and likes to be blindly confident.  \\n  \\nEven if the answer looks 100% legit, you can never fully trust it.  \\n  \\n3\\\\. Lack of reference links  \\n  \\nIt is hard to trust the response of the LLM if we can\\'t see the source of its\\ndecisions.  \\n  \\nEspecially for important decisions (e.g., health, financials)  \\n  \\n=== ùó¶ùóºùóπùòÇùòÅùó∂ùóºùóª ===  \\n  \\n‚Üí Surprize! It is RAG.  \\n  \\n1\\\\. Avoid fine-tuning  \\n  \\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge\\nbase as the main memory (e.g., vector DB).  \\n  \\nThe memory is volatile, so you can quickly introduce or remove data.  \\n  \\n2\\\\. Avoid hallucinations  \\n  \\nBy forcing the LLM to answer solely based on the given context, the LLM will\\nprovide an answer as follows:  \\n\\\\- use the external data to respond to the user\\'s question if it contains the\\nnecessary insights  \\n\\\\- \"I don\\'t know\" if not  \\n  \\n3\\\\. Add reference links  \\n  \\nUsing RAG, you can easily track the source of the data and highlight it to the\\nuser.  \\n  \\n=== ùóõùóºùòÑ ùó±ùóºùó≤ùòÄ ùó•ùóîùóö ùòÑùóºùóøùó∏? ===  \\n  \\nLet\\'s say we want to use RAG to build a financial assistant.  \\n  \\nùòûùò©ùò¢ùòµ ùò•ùò∞ ùò∏ùò¶ ùòØùò¶ùò¶ùò•?  \\n  \\n\\\\- a data source with historical and real-time financial news (e.g. Alpaca)  \\n\\\\- a stream processing engine (e.g., Bytewax)  \\n\\\\- an encoder-only model for embedding the documents (e.g., pick one from\\n`sentence-transformers`)  \\n\\\\- a vector DB (e.g., Qdrant)  \\n  \\nùòèùò∞ùò∏ ùò•ùò∞ùò¶ùò¥ ùò™ùòµ ùò∏ùò∞ùò≥ùò¨?  \\n  \\n‚Ü≥ On the feature pipeline side:  \\n  \\n1\\\\. using Bytewax, you ingest the financial news and clean them  \\n2\\\\. you chunk the news documents and embed them  \\n3\\\\. you insert the embedding of the docs along with their metadata (e.g., the\\ninitial text, source_url, etc.) to Qdrant  \\n  \\n‚Ü≥ On the inference pipeline side:  \\n  \\n4\\\\. the user question is embedded (using the same embedding model)  \\n5\\\\. using this embedding, you extract the top K most similar news documents\\nfrom Qdrant  \\n6\\\\. along with the user question, you inject the necessary metadata from the\\nextracted top K documents into the prompt template (e.g., the text of\\ndocuments & its source_url)  \\n7\\\\. you pass the whole prompt to the LLM for the final answer\\n\\nWhat is Retrieval Augmented Generation (RAG)? [Image by the Author].\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\n### #2. Why do you need a streaming pipeline instead of a batch pipeline when\\nimplementing RAG in your LLM applications?\\n\\nThe quality of your RAG implementation is as good as the quality & freshness\\nof your data.  \\n  \\nThus, depending on your use case, you have to ask:  \\n\"How fresh does my data from the vector DB have to be to provide accurate\\nanswers?\"  \\n  \\nBut for the best user experience, the data has to be as fresh as possible, aka\\nreal-time data.  \\n  \\nFor example, when implementing a financial assistant, being aware of the\\nlatest financial news is critical. A new piece of information can completely\\nchange the course of your strategy.  \\n  \\nHence, when implementing RAG, one critical aspect is to have your vector DB\\nsynced with all your external data sources in real-time.  \\n  \\nA batch pipeline will work if your use case accepts a particular delay (e.g.,\\none hour, one day, etc.).  \\n  \\nBut with tools like Bytewax üêù, building streaming applications becomes much\\nmore accessible. So why not aim for the best?\\n\\nStreaming vs. batch pipelines when doing RAG [Image by the Author]\\n\\n* * *\\n\\n### #3. What do you need to implement a streaming pipeline for a financial\\nassistant?\\n\\n\\\\- A financial news data source exposed through a web socket (e.g., Alpaca)  \\n  \\n\\\\- A Python streaming processing framework. For example, Bytewax üêù is built in\\nRust for efficiency and exposes a Python interface for ease of use - you don\\'t\\nneed the Java ecosystem to implement real-time pipelines anymore.  \\n  \\n\\\\- A Python package to process, clean, and chunk documents. `unstructured`\\noffers a rich set of features that makes parsing HTML documents extremely\\nconvenient.  \\n  \\n\\\\- An encoder-only language model that maps your chunked documents into\\nembeddings. `setence-transformers` is well integrated with HuggingFace and has\\na huge list of models of various sizes.  \\n  \\n\\\\- A vector DB, where to insert your embeddings and their metadata (e.g., the\\nembedded text, the source_url, the creation date, etc.). For example, Qdrant\\nprovides a rich set of features and a seamless experience.  \\n  \\n\\\\- A way to deploy your streaming pipeline. Docker + AWS will never disappoint\\nyou.  \\n  \\n\\\\- A CI/CD pipeline for continuous tests & deployments. GitHub Actions is a\\ngreat serverless option with a rich ecosystem.  \\n  \\nThis is what you need to build & deploy a streaming pipeline solely in Python\\nüî•\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 4** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: Why & what do you need a streaming pipeline when implementing RAG in\\nyour LLM applications?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-why-and-what-do-you-need-a-streaming?r=1ttoeh'), ArticleDocument(id=UUID('032f3296-b891-484d-9e00-c2872bbb9bbe'), content={'Title': 'DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps', 'Subtitle': 'Lesson 2 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\n### Lesson 2 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 09, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n### **Lesson 2 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. Introduction video lessons \\n\\n  2. What is LLMOps? MLOps vs. LLMOps\\n\\n  3. Unwrapping step-by-step the 3-pipeline design of a financial assistant powered by LLMs\\n\\n#### Previous Lessons:\\n\\n  * Lesson 1: How to design an LLM system for a financial assistant using the 3-pipeline design\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #1. Introduction video lessons\\n\\nWe started releasing the first video lessons of the course.\\n\\nThis is a recording of me, where I presented at a webinar hosted by Gathers, a\\n1.5-hour overview of the ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ course.\\n\\nCheck it out to get a gut feeling of the LLM system ‚Üì\\n\\nThis is the **1st official lesson** of the **Hands-on LLMs** course presented\\nby no other but\\n\\nPau Labarta Bajo\\n\\nfrom the **Real-World Machine Learning** newsletter (if you wonder, the course\\nis the result of our collaboration).\\n\\nPau is one of the best teachers I know. If you have some spare time, it is\\nworth it ‚Üì\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course and support it with a ‚≠ê.\\n\\n* * *\\n\\n### #2. What is LLMOps? MLOps vs. LLMOps\\n\\nLLMOps here, LLMOps there, but did you take the time to see how it differs\\nfrom MLOps?  \\n  \\nIf not, here is a 2-min LLMOps vs. MLOps summary ‚Üì  \\n  \\nùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóüùóüùó†ùó¢ùóΩùòÄ?  \\n  \\nWell, everything revolves around the idea that \"Size matters.\"  \\n  \\nLLMOps is about best practices for efficient deployment, monitoring and\\nmaintenance, but this time for large language models.  \\n  \\nLLMOps is a subset of MLOps, focusing on training & deploying large models\\ntrained on big data.  \\n  \\nIntuitive right?  \\n  \\nùóïùòÇùòÅ ùóµùó≤ùóøùó≤ ùóÆùóøùó≤ ùü± ùóüùóüùó†ùó¢ùóΩùòÄ ùòÇùóªùó∂ùóæùòÇùó≤ ùó≥ùóÆùó∞ùòÅùóºùóøùòÄ ùòÅùóµùóÆùòÅ ùòÄùó≤ùòÅ ùó∂ùòÅ ùóÆùóΩùóÆùóøùòÅ ùó≥ùóøùóºùó∫ ùó†ùóüùó¢ùóΩùòÄ ‚Üì  \\n  \\nùü≠\\\\. ùóñùóºùó∫ùóΩùòÇùòÅùóÆùòÅùó∂ùóºùóªùóÆùóπ ùóøùó≤ùòÄùóºùòÇùóøùó∞ùó≤ùòÄ: training your models on CUDA-enabled GPUs is more\\ncritical than ever, along with knowing how to run your jobs on a cluster of\\nGPUs leveraging data & model parallelism using techniques such as ZeRO from\\nDeepSpeed. Also, the high cost of inference makes model compression techniques\\nessential for deployment.  \\n  \\nùüÆ\\\\. ùóßùóøùóÆùóªùòÄùó≥ùó≤ùóø ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥: training models from scratch is a thing of the past. In\\nmost use cases, you will fine-tune the model on specific tasks, leveraging\\ntechniques such as LLaMA-Adapters or QLora.  \\n  \\nùüØ\\\\. ùóõùòÇùó∫ùóÆùóª ùó≥ùó≤ùó≤ùó±ùóØùóÆùó∞ùó∏: reinforcement learning from human feedback (RLHF) showed\\nmuch potential in improving the quality of generated outputs. But to do RLHF,\\nyou have to introduce a feedback loop within your ML system that lets you\\nevaluate the generated results based on human feedback, which are even further\\nused to fine-tune your LLMs.  \\n  \\nùü∞\\\\. ùóöùòÇùóÆùóøùó±ùóøùóÆùó∂ùóπùòÄ: to create safe systems, you must protect your systems against\\nharmful or violent inputs and outputs. Also, when designing your prompt\\ntemplates, you must consider hallucinations and prompt hacking.  \\n  \\nùü±\\\\. ùó†ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ & ùóÆùóªùóÆùóπùòÜùòáùó∂ùóªùó¥ ùóΩùóøùóºùó∫ùóΩùòÅùòÄ: most ML platforms (e.g., Comet ML)\\nintroduced specialized logging tools to debug and monitor your LLMs to help\\nyou find better prompt templates and protect against hallucination and\\nhacking.\\n\\nWhat is LLMOps? LLMOps vs. MLOps [Image by the Author]\\n\\nTo conclude...  \\n  \\nLLMOps isn\\'t anything new for those familiar with MLOps and Deep Learning.  \\n  \\nFor example, training deep learning models on clusters of GPUs or fine-tuning\\nthem isn\\'t new, but now it is more important than ever to master these skills\\nas models get bigger and bigger.  \\n  \\nBut it indeed introduced novel techniques to fine-tune models (e.g., QLora),\\nto merge the fields of RL and DL, and a plethora of tools around prompt\\nmanipulation & storing, such as:  \\n\\\\- vector DBs (e.g., Qdrant)  \\n\\\\- prompt chaining (e.g., LangChain)  \\n\\\\- prompt logging & analytics (e.g., Comet LLMOps)  \\n  \\n.  \\n  \\nBut with the new multi-modal large models trend, these tips & tricks will\\nconverge towards all deep learning models (e.g., computer vision), and soon,\\nwe will change the name of LLMOps to DLOps or LMOps.  \\n  \\nWhat do you think? Is the term of LLMOps going to stick around?\\n\\n* * *\\n\\n### #3. Unwrapping step-by-step the 3-pipeline design of a financial assistant\\npowered by LLMs\\n\\nHere is a step-by-step guide on designing the architecture of a financial\\nassistant powered by LLMs, vector DBs and MLOps.  \\n  \\nThe 3-pipeline design, also known as the FTI architecture, makes things simple\\n‚Üì  \\n  \\n=== ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWe want to build a streaming pipeline that listens to real-time financial\\nnews, embeds the news, and loads everything in a vector DB. The goal is to add\\nup-to-date news to the user\\'s questions using RAG to avoid retraining.  \\n  \\n1\\\\. We listen 24/7 to financial news from Alpaca through a WebSocket wrapped\\nover a Bytewax connector  \\n2\\\\. Once any financial news is received, these are passed to the Bytewax flow\\nthat:  \\n\\\\- extracts & cleans the necessary information from the news HTML document  \\n\\\\- chunks the text based on the LLM\\'s max context window  \\n\\\\- embeds all the chunks using the \"all-MiniLM-L6-v2\" encoder-only model from\\nsentence-transformers  \\n\\\\- inserts all the embeddings along their metadata to Qdrant  \\n3\\\\. The streaming pipeline is deployed to an EC2 machine that runs multiple\\nBytewax processes. It can be deployed to K8s into a multi-node setup to scale\\nup.  \\n  \\n=== ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWe want to fine-tune a pretrained LLM to specialize the model to answer\\nfinancial-based questions.  \\n  \\n1\\\\. Manually fill ~100 financial questions.  \\n2\\\\. Use RAG to enrich the questions using the financial news from the Qdrant\\nvector DB.  \\n3\\\\. Use a powerful model such as GPT-4 to answer them, or hire an expert if\\nyou have more time and resources.  \\n4\\\\. Load Falcon from HuggingFace using QLoRA to fit on a single GPU.  \\n5\\\\. Preprocess the Q&A dataset into prompts.  \\n6\\\\. Fine-tune the LLM and log all the artifacts to Comet\\'s experiment tracker\\n(loss, model weights, etc.)  \\n7\\\\. For every epoch, run the LLM on your test set, log the prompts to Comet\\'s\\nprompt logging feature and compute the metrics.  \\n8\\\\. Send the best LoRA weights to the model registry as the next production\\ncandidate.  \\n9\\\\. Deploy steps 4-8 to Beam to run the training on an A10G or A100 Nvidia\\nGPU.  \\n  \\n=== ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWe want to hook the financial news stored in the Qdrant Vector DB and the\\nFalcon fine-tuned model into a single entity exposed under a RESTful API.  \\n  \\nSteps 1-7 are all chained together using LangChain.  \\n  \\n1\\\\. Use the \"all-MiniLM-L6-v2\" encoder-only model to embed the user\\'s\\nquestion.  \\n2\\\\. Using the question embedding, query the Qdrant vector DB to find the top 3\\nrelated financial news.  \\n3\\\\. Attach the text (stored as metadata along the embeddings) of the news to\\nthe prompt (aka RAG).  \\n4\\\\. Download Falcon\\'s pretrained weights from HF & LoRA\\'s fine-tuned weights\\nfrom Comet\\'s model registry.  \\n5\\\\. Load the LLM and pass the prompt (= the user\\'s question, financial news,\\nhistory) to it.  \\n6\\\\. Store the conversation in LangChain\\'s memory.  \\n7\\\\. Deploy steps 1-7 under a RESTful API using Beam.\\n\\n3-pipeline architecture [Image by the Author]\\n\\n> ‚Ü≥üîó Check out the **Hands-on LLMs** course to see this in action.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 3** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: Unwrapping the 3-pipeline design of a financial assistant powered by LLMs | LLMOps vs. MLOps\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-unwrapping-the-3-pipeline-design?r=1ttoeh'), ArticleDocument(id=UUID('21c92489-204c-4791-b4dd-f0c2487f7e82'), content={'Title': 'DML: How to design an LLM system for a financial assistant using the 3-pipeline design', 'Subtitle': 'Lesson 1 | The Hands-on LLMs Series', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\n### Lesson 1 | The Hands-on LLMs Series\\n\\nPaul Iusztin\\n\\nNov 02, 2023\\n\\n5\\n\\nShare this post\\n\\n#### DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n> As promised, starting this week, we will **begin** the **series** based on\\n> the **Hands-on LLMs FREE course**.\\n\\nNote that this is not the course itself. It is an overview for all the busy\\npeople who will focus on the key aspects.\\n\\nThe entire course will soon be available on üîó GitHub.\\n\\n* * *\\n\\n### **Lesson 1 | The Hands-on LLMs Series**\\n\\n#### **Table of Contents:**\\n\\n  1. What is the 3-pipeline design\\n\\n  2. How to apply the 3-pipeline design in architecting a financial assistant powered by LLMs\\n\\n  3. The tech stack used to build an end-to-end LLM system for a financial assistant \\n\\n* * *\\n\\nAs the Hands-on LLMs course is still a ùòÑùóºùóøùó∏ ùó∂ùóª ùóΩùóøùóºùó¥ùóøùó≤ùòÄùòÄ, we want to ùó∏ùó≤ùó≤ùóΩ ùòÜùóºùòÇ\\nùòÇùóΩùó±ùóÆùòÅùó≤ùó± on our progress ‚Üì  \\n\\n> ‚Ü≥ Thus, we opened up the ùó±ùó∂ùòÄùó∞ùòÇùòÄùòÄùó∂ùóºùóª ùòÅùóÆùóØ under the course\\'s GitHub\\n> Repository, where we will ùó∏ùó≤ùó≤ùóΩ ùòÜùóºùòÇ ùòÇùóΩùó±ùóÆùòÅùó≤ùó± with everything is happening.\\n\\n  \\nAlso, if you have any ùó∂ùó±ùó≤ùóÆùòÄ, ùòÄùòÇùó¥ùó¥ùó≤ùòÄùòÅùó∂ùóºùóªùòÄ, ùóæùòÇùó≤ùòÄùòÅùó∂ùóºùóªùòÄ or want to ùó∞ùóµùóÆùòÅ, we\\nencourage you to ùó∞ùóøùó≤ùóÆùòÅùó≤ ùóÆ \"ùóªùó≤ùòÑ ùó±ùó∂ùòÄùó∞ùòÇùòÄùòÄùó∂ùóºùóª\".  \\n  \\n‚Üì We want the course to fill your real needs ‚Üì  \\n  \\n‚Ü≥ Hence, if your suggestion fits well with our hands-on course direction, we\\nwill consider implementing it.\\n\\nHands-on LLMs course discussions section [Image by the Author].\\n\\nCheck it out and leave a ‚≠ê if you like what you see:  \\n‚Ü≥üîó Hands-on LLMs course\\n\\n* * *\\n\\n### #1. What is the 3-pipeline design\\n\\nWe all know how ùó∫ùó≤ùòÄùòÄùòÜ ùó†ùóü ùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ can get. That is where the ùüØ-ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤\\nùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤ ùó∏ùó∂ùó∞ùó∏ùòÄ ùó∂ùóª.  \\n  \\nThe 3-pipeline design is a way to bring structure & modularity to your ML\\nsystem and improve your MLOps processes.  \\n  \\nThis is how ‚Üì  \\n  \\n=== ùó£ùóøùóºùóØùóπùó≤ùó∫ ===  \\n  \\nDespite advances in MLOps tooling, transitioning from prototype to production\\nremains challenging.  \\n  \\nIn 2022, only 54% of the models get into production. Auch.  \\n  \\nSo what happens?  \\n  \\nSometimes the model is not mature enough, sometimes there are some security\\nrisks, but most of the time...  \\n  \\n...the architecture of the ML system is built with research in mind, or the ML\\nsystem becomes a massive monolith that is extremely hard to refactor from\\noffline to online.  \\n  \\nSo, good processes and a well-defined architecture are as crucial as good\\ntools and models.  \\n  \\n  \\n=== ùó¶ùóºùóπùòÇùòÅùó∂ùóºùóª ===  \\n  \\nùòõùò©ùò¶ 3-ùò±ùò™ùò±ùò¶ùò≠ùò™ùòØùò¶ ùò¢ùò≥ùò§ùò©ùò™ùòµùò¶ùò§ùòµùò∂ùò≥ùò¶.  \\n  \\nFirst, let\\'s understand what the 3-pipeline design is.  \\n  \\nIt is a mental map that helps you simplify the development process and split\\nyour monolithic ML pipeline into 3 components:  \\n1\\\\. the feature pipeline  \\n2\\\\. the training pipeline  \\n3\\\\. the inference pipeline  \\n  \\n...also known as the Feature/Training/Inference (FTI) architecture.  \\n  \\n.  \\n  \\n#ùü≠. The feature pipeline transforms your data into features & labels, which\\nare stored and versioned in a feature store.  \\n  \\n#ùüÆ. The training pipeline ingests a specific version of the features & labels\\nfrom the feature store and outputs the trained models, which are stored and\\nversioned inside a model registry.  \\n  \\n#ùüØ. The inference pipeline takes a given version of the features and trained\\nmodels and outputs the predictions to a client.  \\n  \\n.  \\n  \\nThis is why the 3-pipeline design is so beautiful:  \\n  \\n\\\\- it is intuitive  \\n\\\\- it brings structure, as on a higher level, all ML systems can be reduced to\\nthese 3 components  \\n\\\\- it defines a transparent interface between the 3 components, making it\\neasier for multiple teams to collaborate  \\n\\\\- the ML system has been built with modularity in mind since the beginning  \\n\\\\- the 3 components can easily be divided between multiple teams (if\\nnecessary)  \\n\\\\- every component can use the best stack of technologies available for the\\njob  \\n\\\\- every component can be deployed, scaled, and monitored independently  \\n\\\\- the feature pipeline can easily be either batch, streaming or both  \\n  \\nBut the most important benefit is that...  \\n  \\n...by following this pattern, you know 100% that your ML model will move out\\nof your Notebooks into production.\\n\\nWhat is the 3-pipeline design & Why should you adopt it in your ML systems?\\n[Image by the Author].\\n\\nWhat do you think about the 3-pipeline architecture? Have you used it?  \\n  \\nIf you want to know more about the 3-pipeline design, I recommend this awesome\\narticle from Hopsworks ‚Üì  \\n‚Ü≥üîó From MLOps to ML Systems with Feature/Training/Inference Pipelines\\n\\n* * *\\n\\n### #2. How to apply the 3-pipeline design in architecting a financial\\nassistant powered by LLMs\\n\\nBuilding ML systems is hard, right? Wrong.  \\n  \\nHere is how the ùüØ-ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ùó±ùó≤ùòÄùó∂ùó¥ùóª can make ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùó∂ùóªùó¥ the ùó†ùóü ùòÄùòÜùòÄùòÅùó≤ùó∫ for a\\nùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóÆùòÄùòÄùó∂ùòÄùòÅùóÆùóªùòÅ ùó≤ùóÆùòÄùòÜ ‚Üì  \\n  \\n.  \\n  \\nI already covered the concepts of the 3-pipeline design in my previous post,\\nbut here is a quick recap:  \\n  \\n\"\"\"  \\nIt is a mental map that helps you simplify the development process and split\\nyour monolithic ML pipeline into 3 components:  \\n1\\\\. the feature pipeline  \\n2\\\\. the training pipeline  \\n3\\\\. the inference pipeline  \\n...also known as the Feature/Training/Inference (FTI) architecture.  \\n\"\"\"  \\n  \\n.  \\n  \\nNow, let\\'s see how you can use the FTI architecture to build a financial\\nassistant powered by LLMs ‚Üì  \\n  \\n#ùü≠. ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\nThe feature pipeline is designed as a streaming pipeline that extracts real-\\ntime financial news from Alpaca and:  \\n  \\n\\\\- cleans and chunks the news documents  \\n\\\\- embeds the chunks using an encoder-only LM  \\n\\\\- loads the embeddings + their metadata in a vector DB  \\n\\\\- deploys it to AWS  \\n  \\nIn this architecture, the vector DB acts as the feature store.  \\n  \\nThe vector DB will stay in sync with the latest news to attach real-time\\ncontext to the LLM using RAG.  \\n  \\n#ùüÆ. ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\nThe training pipeline is split into 2 main steps:  \\n  \\n‚Ü≥ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ ùòÄùó≤ùó∫ùó∂-ùóÆùòÇùòÅùóºùó∫ùóÆùòÅùó≤ùó± ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóºùóª ùòÄùòÅùó≤ùóΩ  \\n  \\nIt takes the vector DB (feature store) and a set of predefined questions\\n(manually written) as input.  \\n  \\nAfter, you:  \\n  \\n\\\\- use RAG to inject the context along the predefined questions  \\n\\\\- use a large & powerful model, such as GPT-4, to generate the answers  \\n\\\\- save the generated dataset under a new version  \\n  \\n‚Ü≥ ùóôùó∂ùóªùó≤-ùòÅùòÇùóªùó∂ùóªùó¥ ùòÄùòÅùó≤ùóΩ  \\n  \\n\\\\- download a pre-trained LLM from Huggingface  \\n\\\\- load the LLM using QLoRA  \\n\\\\- preprocesses the generated Q&A dataset into a format expected by the LLM  \\n\\\\- fine-tune the LLM  \\n\\\\- push the best QLoRA weights (model) to a model registry  \\n\\\\- deploy it using a serverless solution as a continuous training pipeline  \\n  \\n#ùüØ. ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤  \\n  \\nThe inference pipeline is the financial assistant that the clients actively\\nuse.  \\n  \\nIt uses the vector DB (feature store) and QLoRA weights (model) from the model\\nregistry in the following way:  \\n  \\n\\\\- download the pre-trained LLM from Huggingface  \\n\\\\- load the LLM using the pretrained QLoRA weights  \\n\\\\- connect the LLM and vector DB into a chain  \\n\\\\- use RAG to add relevant financial news from the vector DB  \\n\\\\- deploy it using a serverless solution under a RESTful API\\n\\nThe architecture of a financial assistant using the 3 pipeline design [Image\\nby the Author].\\n\\nHere are the main benefits of using the FTI architecture:  \\n\\\\- it defines a transparent interface between the 3 modules  \\n\\\\- every component can use different technologies to implement and deploy the\\npipeline  \\n\\\\- the 3 pipelines are loosely coupled through the feature store & model\\nregistry  \\n\\\\- every component can be scaled independently\\n\\n> See this architecture in action in my üîó ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ FREE course.\\n\\n* * *\\n\\n### #3. The tech stack used to build an end-to-end LLM system for a financial\\nassistant\\n\\nThe tools are divided based on the ùüØ-ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ (aka ùóôùóßùóú) ùóÆùóøùó∞ùóµùó∂ùòÅùó≤ùó∞ùòÅùòÇùóøùó≤:  \\n  \\n=== ùóôùó≤ùóÆùòÅùòÇùóøùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWhat do you need to build a streaming pipeline?  \\n  \\n‚Üí streaming processing framework: Bytewax (brings the speed of Rust into our\\nbeloved Python ecosystem)  \\n  \\n‚Üí parse, clean, and chunk documents: unstructured  \\n  \\n‚Üí validate document structure: pydantic  \\n  \\n‚Üí encoder-only language model: HuggingFace sentence-transformers, PyTorch  \\n  \\n‚Üí vector DB: Qdrant  \\n  \\n‚Üídeploy: Docker, AWS  \\n  \\n‚Üí CI/CD: GitHub Actions  \\n  \\n  \\n=== ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWhat do you need to build a fine-tuning pipeline?  \\n  \\n‚Üí pretrained LLM: HuggingFace Hub  \\n  \\n‚Üí parameter efficient tuning method: peft (= LoRA)  \\n  \\n‚Üí quantization: bitsandbytes (= QLoRA)  \\n  \\n‚Üí training: HuggingFace transformers, PyTorch, trl  \\n  \\n‚Üí distributed training: accelerate  \\n  \\n‚Üí experiment tracking: Comet ML  \\n  \\n‚Üí model registry: Comet ML  \\n  \\n‚Üí prompt monitoring: Comet ML  \\n  \\n‚Üí continuous training serverless deployment: Beam  \\n  \\n  \\n=== ùóúùóªùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùó£ùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ===  \\n  \\nWhat do you need to build a financial assistant?  \\n  \\n‚Üí framework for developing applications powered by language models: LangChain  \\n  \\n‚Üí model registry: Comet ML  \\n  \\n‚Üí inference: HuggingFace transformers, PyTorch, peft (to load the LoRA\\nweights)  \\n  \\n‚Üí quantization: bitsandbytes  \\n  \\n‚Üí distributed inference: accelerate  \\n  \\n‚Üí encoder-only language model: HuggingFace sentence-transformers  \\n  \\n‚Üí vector DB: Qdrant  \\n  \\n‚Üí prompt monitoring: Comet ML  \\n  \\n‚Üí RESTful API serverless service: Beam  \\n  \\n.  \\n  \\nAs you can see, some tools overlap between the FTI pipelines, but not all.  \\n  \\nThis is the beauty of the 3-pipeline design, as every component represents a\\ndifferent entity for which you can pick the best stack to build, deploy, and\\nmonitor.  \\n  \\nYou can go wild and use Tensorflow in one of the components if you want your\\ncolleges to hate you üòÇ\\n\\n> See the tools in action in my üîó ùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ FREE course.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for **Lesson 2** of the **Hands-On LLMs series** üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n5\\n\\nShare this post\\n\\n#### DML: How to design an LLM system for a financial assistant using the\\n3-pipeline design\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-to-design-an-llm-system-for?r=1ttoeh'), ArticleDocument(id=UUID('007833f1-fb36-470f-adad-78143f817fee'), content={'Title': 'DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG in Your LLM Applications', 'Subtitle': 'Hello there, I am Paul Iusztin üëãüèº', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG\\nin Your LLM Applications\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG in\\nYour LLM Applications\\n\\nPaul Iusztin\\n\\nOct 26, 2023\\n\\n4\\n\\nShare this post\\n\\n#### DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG\\nin Your LLM Applications\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time Rag in Your LLM Applications\\n\\n> **Story:** If anyone told you that ML or MLOps is easy, they were right. A\\n> simple trick I learned the hard way.\\n\\n* * *\\n\\nThis week‚Äôs newsletter is shorter than usual, but I have some great news üî•\\n\\n> Next week, within the Decoding ML newsletter, I will start a step-by-step\\n> series based on the Hands-On LLMs course I am developing.\\n>\\n> By the end of this series, you will know how to design, build, and deploy a\\n> financial assistant powered by LLMs.\\n>\\n> ‚Ä¶all of this for FREE inside the Decoding ML newsletter\\n\\n‚Ü≥üîó Check out the Hands-On LLMs course GitHub page and give it a star to stay\\nupdated with our progress.\\n\\n* * *\\n\\n### #1. Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time Rag\\nin Your LLM Applications\\n\\nTo successfully use ùó•ùóîùóö in your ùóüùóüùó† ùóÆùóΩùóΩùóπùó∂ùó∞ùóÆùòÅùó∂ùóºùóªùòÄ, your ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï must\\nconstantly be updated with the latest data.  \\n  \\nHere is how you can implement a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ to keep your vector DB in\\nsync with your datasets ‚Üì  \\n  \\n.  \\n  \\nùó•ùóîùóö is a popular strategy when building LLMs to add context to your prompt\\nabout your private datasets.  \\n  \\nLeveraging your domain data using RAG provides 2 significant benefits:  \\n\\\\- you don\\'t need to fine-tune your model as often (or at all)  \\n\\\\- avoid hallucinations  \\n  \\n.  \\n  \\nOn the ùóØùóºùòÅ ùòÄùó∂ùó±ùó≤, to implement RAG, you have to:  \\n  \\n3\\\\. Embed the user\\'s question using an embedding model (e.g., BERT). Use the\\nembedding to query your vector DB and find the most similar vectors using a\\ndistance function (e.g., cos similarity).  \\n4\\\\. Get the top N closest vectors and their metadata.  \\n5\\\\. Attach the extracted top N vectors metadata + the chat history to the\\ninput prompt.  \\n6\\\\. Pass the prompt to the LLM.  \\n7\\\\. Insert the user question + assistant answer to the chat history.  \\n  \\n.  \\n  \\nBut the question is, ùóµùóºùòÑ do you ùó∏ùó≤ùó≤ùóΩ ùòÜùóºùòÇùóø ùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï ùòÇùóΩ ùòÅùóº ùó±ùóÆùòÅùó≤ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ùóπùóÆùòÅùó≤ùòÄùòÅ\\nùó±ùóÆùòÅùóÆ?  \\n  \\n‚Ü≥ You need a real-time streaming pipeline.  \\n  \\nHow do you implement it?  \\n  \\nYou need 2 components:  \\n  \\n‚Ü≥ A streaming processing framework. For example, Bytewax is built in Rust for\\nefficiency and exposes a Python interface for ease of use - you don\\'t need\\nJava to implement real-time pipelines anymore.  \\n  \\nüîó Bytewax  \\n  \\n‚Ü≥ A vector DB. For example, Qdrant provides a rich set of features and a\\nseamless experience.  \\n  \\nüîó Qdrant  \\n  \\n.  \\n  \\nHere is an example of how to implement a streaming pipeline for financial news\\n‚Üì  \\n  \\n#ùü≠. Financial news data source (e.g., Alpaca):  \\n  \\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to\\nadd data to your vector DB in batch mode between a desired [start_date,\\nend_date] range. You can tweak the number of workers to parallelize this step\\nas much as possible.  \\n‚Üí You run this once in the beginning.  \\n  \\nYou need the data exposed under a web socket to ingest news in real time. So,\\nyou\\'ll be able to listen to the news and ingest it in your vector DB as soon\\nas they are available.  \\n‚Üí Listens 24/7 for financial news.  \\n  \\n#ùüÆ. Build the streaming pipeline using Bytewax:  \\n  \\nImplement 2 input connectors for the 2 different types of APIs: RESTful API &\\nweb socket.  \\n  \\nThe rest of the steps can be shared between both connectors ‚Üì  \\n  \\n\\\\- Clean financial news documents.  \\n\\\\- Chunk the documents.  \\n\\\\- Embed the documents (e.g., using Bert).  \\n\\\\- Insert the embedded documents + their metadata to the vector DB (e.g.,\\nQdrant).  \\n  \\n#ùüØ-ùü≥. When the users ask a financial question, you can leverage RAG with an\\nup-to-date vector DB to search for the latest news in the industry.\\n\\nSynced Vector DBs - A Guide to Streaming Pipelines for Real-Time Rag in Your\\nLLM Applications [Image by the Author]\\n\\n* * *\\n\\n### #Story. If anyone told you that ML or MLOps is easy, they were right. A\\nsimple trick I learned the hard way.\\n\\nIf anyone told you that ùó†ùóü or ùó†ùóüùó¢ùóΩùòÄ is ùó≤ùóÆùòÄùòÜ, they were ùóøùó∂ùó¥ùóµùòÅ.  \\n  \\nHere is a simple trick that I learned the hard way ‚Üì  \\n  \\nIf you are in this domain, you already know that everything changes fast:  \\n  \\n\\\\- a new tool every month  \\n\\\\- a new model every week  \\n\\\\- a new project every day  \\n  \\nYou know what I did? I stopped caring about all these changes and switched my\\nattention to the real gold.  \\n  \\nWhich is ‚Üí \"ùóôùóºùó∞ùòÇùòÄ ùóºùóª ùòÅùóµùó≤ ùó≥ùòÇùóªùó±ùóÆùó∫ùó≤ùóªùòÅùóÆùóπùòÄ.\"  \\n  \\n.  \\n  \\nLet me explain ‚Üì  \\n  \\nWhen you constantly chase the latest models (aka FOMO), you will only have a\\nshallow understanding of that new information (except if you are a genius or\\nalready deep into that niche).  \\n  \\nBut the joke\\'s on you. In reality, most of what you think you need to know,\\nyou don\\'t.  \\n  \\nSo you won\\'t use what you learned and forget most of it after 1-2 months.  \\n  \\nWhat a waste of time, right?  \\n  \\n.  \\n  \\nBut...  \\n  \\nIf you master the fundamentals of the topic, you want to learn.  \\n  \\nFor example, for deep learning, you have to know:  \\n  \\n\\\\- how models are built  \\n\\\\- how they are trained  \\n\\\\- groundbreaking architectures (Resnet, UNet, Transformers, etc.)  \\n\\\\- parallel training  \\n\\\\- deploying a model, etc.  \\n  \\n...when in need (e.g., you just moved on to a new project), you can easily\\npick up the latest research.  \\n  \\nThus, after you have laid the foundation, it is straightforward to learn SoTA\\napproaches when needed (if needed).  \\n  \\nMost importantly, what you learn will stick with you, and you will have the\\nflexibility to jump from one project to another quickly.  \\n  \\n.  \\n  \\nI am also guilty. I used to FOMO into all kinds of topics until I was honest\\nwith myself and admitted I am no Leonardo Da Vinci.  \\n  \\nBut here is what I did and worked well:  \\n  \\n\\\\- building projects  \\n\\\\- replicating the implementations of famous papers  \\n\\\\- teaching the subject I want to learn  \\n... and most importantly, take my time to relax and internalize the\\ninformation.\\n\\nTo conclude:  \\n  \\n\\\\- learn ahead only the fundamentals  \\n\\\\- learn the latest trend only when needed\\n\\n[Image by the Author]\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\n‚Ä¶and see you next week for the beginning of the Hands-On LLMs series üî•\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n4\\n\\nShare this post\\n\\n#### DML: Synced Vector DBs - A Guide to Streaming Pipelines for Real-Time RAG\\nin Your LLM Applications\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-synced-vector-dbs-a-guide-to?r=1ttoeh'), ArticleDocument(id=UUID('e9353901-9ba9-483c-8c59-2de649c9743a'), content={'Title': 'DML: What is the difference between your ML development and continuous training environments?', 'Subtitle': '3 techniques you must know to evaluate your LLMs quickly. Experimentation vs. continuous training environments.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\n### 3 techniques you must know to evaluate your LLMs quickly. Experimentation\\nvs. continuous training environments.\\n\\nPaul Iusztin\\n\\nOct 19, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. 3 techniques you must know to evaluate your LLMs quickly\\n\\n  2. What is the difference between your ML development and continuous training environments?\\n\\n> **Story:** Job roles tell you there is just one type of MLE, but there are\\n> actually 3.\\n\\n* * *\\n\\n> But first, I want to let you know that after 1 year of making content, I\\n> finally decided to share my content on **Twitter/X**.\\n\\nI took this decision because everybody has a different way of reading and\\ninteracting with their socials.  \\n  \\n...and I want everyone to enjoy my content on their favorite platform.\\n\\nI even bought that stu*** blue ticker to see that I am serious about this üòÇ\\n\\nSo...  \\n\\n> If **you like my content** and you are a **Twitter/X** **person** ‚Üì\\n>\\n> ‚Ü≥üîó **follow** at @ùê¢ùêÆùê¨ùê≥ùê≠ùê¢ùêßùê©ùêöùêÆùê•\\n\\n* * *\\n\\n###  #1. 3 techniques you must know to evaluate your LLMs quickly\\n\\nManually testing the output of your LLMs is a tedious and painful process ‚Üí\\nyou need to automate it.  \\n  \\nIn generative AI, most of the time, you cannot leverage standard metrics.  \\n  \\nThus, the real question is, how do you evaluate the outputs of an LLM?  \\n  \\nDepending on your problem, here is what you can do ‚Üì  \\n  \\n#ùü≠. ùó¶ùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùóÆùóªùòÄùòÑùó≤ùóøùòÄ - ùòÜùóºùòÇ ùó∏ùóªùóºùòÑ ùó≤ùòÖùóÆùó∞ùòÅùóπùòÜ ùòÑùóµùóÆùòÅ ùòÜùóºùòÇ ùòÑùóÆùóªùòÅ ùòÅùóº ùó¥ùó≤ùòÅ  \\n  \\nEven if you use an LLM to generate text, you can ask it to generate a response\\nin a structured format (e.g., JSON) that can be parsed.  \\n  \\nYou know exactly what you want (e.g., a list of products extracted from the\\nuser\\'s question).  \\n  \\nThus, you can easily compare the generated and ideal answers using classic\\napproaches.  \\n  \\nFor example, when extracting the list of products from the user\\'s input, you\\ncan do the following:  \\n\\\\- check if the LLM outputs a valid JSON structure  \\n\\\\- use a classic method to compare the generated and real answers  \\n  \\n#ùüÆ. ùó°ùóº \"ùóøùó∂ùó¥ùóµùòÅ\" ùóÆùóªùòÄùòÑùó≤ùóø (ùó≤.ùó¥., ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó∂ùóªùó¥ ùó±ùó≤ùòÄùó∞ùóøùó∂ùóΩùòÅùó∂ùóºùóªùòÄ, ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùó≤ùòÄ, ùó≤ùòÅùó∞.)  \\n  \\nWhen generating sentences, the LLM can use different styles, words, etc. Thus,\\ntraditional metrics (e.g., BLUE score) are too rigid to be useful.  \\n  \\nYou can leverage another LLM to test the output of our initial LLM. The trick\\nis in what questions to ask.  \\n  \\nWhen testing LLMs, you won\\'t have a big testing split size as you are used to.\\nA set of 10-100 tricky examples usually do the job (it won\\'t be costly).  \\n  \\nHere, we have another 2 sub scenarios:  \\n  \\n‚Ü≥ ùüÆ.ùü≠ ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº (ùòÜùóºùòÇ ùó±ùóºùóª\\'ùòÅ\\nùóµùóÆùòÉùó≤ ùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nYou don\\'t have access to an expert to write an ideal answer for a given\\nquestion to compare it to.  \\n  \\nBased on the initial prompt and generated answer, you can compile a set of\\nquestions and pass them to an LLM. Usually, these are Y/N questions that you\\ncan easily quantify and check the validity of the generated answer.  \\n  \\nThis is known as \"Rubric Evaluation\"  \\n  \\nFor example:  \\n\"\"\"  \\n\\\\- Is there any disagreement between the response and the context? (Y or N)  \\n\\\\- Count how many questions the user asked. (output a number)  \\n...  \\n\"\"\"  \\n  \\nThis strategy is intuitive, as you can ask the LLM any question you are\\ninterested in as long it can output a quantifiable answer (Y/N or a number).  \\n  \\n‚Ü≥ ùüÆ.ùüÆ. ùó™ùóµùó≤ùóª ùòÜùóºùòÇ ùó±ùóº ùóµùóÆùòÉùó≤ ùóÆùóª ùó∂ùó±ùó≤ùóÆùóπ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóº ùó∞ùóºùó∫ùóΩùóÆùóøùó≤ ùòÅùóµùó≤ ùóøùó≤ùòÄùóΩùóºùóªùòÄùó≤ ùòÅùóº (ùòÜùóºùòÇ ùóµùóÆùòÉùó≤\\nùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ)  \\n  \\nWhen you can access an answer manually created by a group of experts, things\\nare easier.  \\n  \\nYou will use an LLM to compare the generated and ideal answers based on\\nsemantics, not structure.  \\n  \\nFor example:  \\n\"\"\"  \\n(A) The submitted answer is a subset of the expert answer and entirely\\nconsistent.  \\n...  \\n(E) The answers differ, but these differences don\\'t matter.  \\n\"\"\"\\n\\n3 techniques you must know to evaluate your LLMs quickly [Image by the\\nAuthor].\\n\\n* * *\\n\\n### #2. What is the difference between your ML development and continuous\\ntraining environments?\\n\\nThey might do the same thing, but their design is entirely different ‚Üì  \\n  \\nùó†ùóü ùóóùó≤ùòÉùó≤ùóπùóºùóΩùó∫ùó≤ùóªùòÅ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nAt this point, your main goal is to ingest the raw and preprocessed data\\nthrough versioned artifacts (or a feature store), analyze it & generate as\\nmany experiments as possible to find the best:  \\n\\\\- model  \\n\\\\- hyperparameters  \\n\\\\- augmentations  \\n  \\nBased on your business requirements, you must maximize some specific metrics,\\nfind the best latency-accuracy trade-offs, etc.  \\n  \\nYou will use an experiment tracker to compare all these experiments.  \\n  \\nAfter you settle on the best one, the output of your ML development\\nenvironment will be:  \\n\\\\- a new version of the code  \\n\\\\- a new version of the configuration artifact  \\n  \\nHere is where the research happens. Thus, you need flexibility.  \\n  \\nThat is why we decouple it from the rest of the ML systems through artifacts\\n(data, config, & code artifacts).  \\n  \\nùóñùóºùóªùòÅùó∂ùóªùòÇùóºùòÇùòÄ ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ ùóòùóªùòÉùó∂ùóøùóºùóªùó∫ùó≤ùóªùòÅ  \\n  \\nHere is where you want to take the data, code, and config artifacts and:  \\n  \\n\\\\- train the model on all the required data  \\n\\\\- output a staging versioned model artifact  \\n\\\\- test the staging model artifact  \\n\\\\- if the test passes, label it as the new production model artifact  \\n\\\\- deploy it to the inference services  \\n  \\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub\\nActions):  \\n  \\n\\\\- builds a docker image from the code artifact (e.g., triggered manually or\\nwhen a new artifact version is created)  \\n\\\\- start the training pipeline inside the docker container that pulls the\\nfeature and config artifacts and outputs the staging model artifact  \\n\\\\- manually look over the training report -> If everything went fine, manually\\ntrigger the testing pipeline  \\n\\\\- manually look over the testing report -> if everything worked fine (e.g.,\\nthe model is better than the previous one), manually trigger the CD pipeline\\nthat deploys the new model to your inference services  \\n  \\nNote how the model registry quickly helps you to decouple all the components.  \\n  \\nAlso, because training and testing metrics are not always black & white, it is\\ntough to 100% automate the CI/CD pipeline.  \\n  \\nThus, you need a human in the loop when deploying ML models.\\n\\n. What is the difference between your ML development and continuous training\\nenvironments [Image by the Author]\\n\\nTo conclude...  \\n  \\nThe ML development environment is where you do your research to find better\\nmodels:  \\n\\\\- ùò™ùòØùò±ùò∂ùòµ: data artifact  \\n\\\\- ùò∞ùò∂ùòµùò±ùò∂ùòµ: code & config artifacts  \\n  \\nThe continuous training environment is used to train & test the production\\nmodel at scale:  \\n\\\\- ùò™ùòØùò±ùò∂ùòµ: data, code, config artifacts  \\n\\\\- ùò∞ùò∂ùòµùò±ùò∂ùòµ: model artifact\\n\\n> This is not a fixed solution, as ML systems are still an open question.\\n>\\n> But if you want to see this strategy in action ‚Üì  \\n>  \\n> ‚Ü≥üîó Check out my **The Full Stack 7-Steps MLOps Framework** FREE Course.\\n\\n* * *\\n\\n### Story: Job roles tell you there is just one type of MLE, but there are\\nactually 3\\n\\nHere they are ‚Üì  \\n  \\nThese are the 3 ML engineering personas I found while working with different\\nteams in the industry:  \\n  \\n#ùü≠. ùó•ùó≤ùòÄùó≤ùóÆùóøùó∞ùóµùó≤ùóøùòÄ ùòÇùóªùó±ùó≤ùóøùó∞ùóºùòÉùó≤ùóø  \\n  \\nThey like to stay in touch with the latest papers, understand the architecture\\nof models, optimize them, run experiments, etc.  \\n  \\nThey are great at picking the best models but not that great at writing clean\\ncode and scaling the solution.  \\n  \\n#ùüÆ. ùó¶ùó™ùóò ùòÇùóªùó±ùó≤ùóøùó∞ùóºùòÉùó≤ùóø  \\n  \\nThey pretend they read papers but don\\'t (maybe only when they have to). They\\nare more concerned with writing modular code and data quality than the latest\\nhot models. Usually, these are the \"data-centric\" people.  \\n  \\nThey are great at writing clean code & processing data at scale but lack deep\\nmathematical skills to develop complex DL solutions.  \\n  \\n#ùüØ. ùó†ùóüùó¢ùóΩùòÄ ùó≥ùóøùó≤ùóÆùó∏ùòÄ  \\n  \\nThey ultimately don\\'t care about the latest research & hot models. They are\\nmore into the latest MLOps tools and building ML systems. They love to\\nautomate everything and use as many tools as possible.  \\n  \\nGreat at scaling the solution and building ML pipelines, but not great at\\nrunning experiments & tweaking ML models. They love to treat the ML model as a\\nblack box.\\n\\nImage by the Author.\\n\\nI started as #1. , until I realized I hated it - now I am a mix of:  \\n  \\n‚Üí #ùü≠. 20%  \\n‚Üí #ùüÆ. 40%  \\n‚Üí #ùüØ. 40%  \\n  \\nBut that doesn\\'t mean one is better - these types are complementary.  \\n  \\nA great ML team should have at least one of each persona.  \\n  \\nWhat do you think? Did I get it right?\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: What is the difference between your ML development and continuous\\ntraining environments?\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-what-is-the-difference-between?r=1ttoeh'), ArticleDocument(id=UUID('aa199018-9dcc-4768-9e99-1b2356af2c21'), content={'Title': 'DML: 7-steps to build a production-ready financial assistant using LLMs ', 'Subtitle': 'How to fine-tune any LLM at scale in under 5 minutes. 7 steps to build a production-ready financial assistant using LLMs.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\n### How to fine-tune any LLM at scale in under 5 minutes. 7 steps to build a\\nproduction-ready financial assistant using LLMs.\\n\\nPaul Iusztin\\n\\nOct 12, 2023\\n\\n5\\n\\nShare this post\\n\\n#### DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Writing your own ML models is history. How to fine-tune any LLM at scale in under 5 minutes.\\n\\n  2. 7 steps to chain your prompts to build a production-ready financial assistant using LLMs.\\n\\n> **Extra:** 3 key resources on how to monitor your ML models\\n\\n* * *\\n\\n### #1. Writing your own ML models is history. How to fine-tune any LLM at\\nscale in under 5 minutes.\\n\\nWriting your own ML models is history.  \\n  \\nThe true value is in your data, how you prepare it, and your computer power.  \\n  \\nTo demonstrate my statement. Here is how you can write a Python script to\\ntrain your LLM at scale in under 5 minutes ‚Üì  \\n  \\n#ùü≠. Load your data in JSON format and convert it into a Hugging Dataset  \\n  \\n#ùüÆ. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with\\nthe tokenizer and training & evaluation datasets.  \\n  \\n#ùüØ. Wrap your training script with a serverless solution, such as Beam, which\\nquickly lets you access a cluster of GPUs to train large models.  \\n  \\nüö® As you can see, the secret ingredients are not the LLM but:  \\n\\\\- the amount of data  \\n\\\\- the quality of data  \\n\\\\- how you process the data  \\n\\\\- $$$ for compute power  \\n\\\\- the ability to scale the system\\n\\n3-steps to write a Python script to train your LLMs at scale [Image by the\\nAuthor].\\n\\nüí° My advice  \\n  \\n‚Ü≥ If you don\\'t plan to become an ML researcher, shift your focus from the\\nlatest models to your data and infrastructure.  \\n  \\n.  \\n  \\nùó°ùóºùòÅùó≤: Integrating serverless services, such as Beam, makes the deployment of\\nyour training pipeline fast & seamless, leaving you to focus only on the last\\npiece of the puzzle: your data.\\n\\n  \\n‚Ü≥üîó Check out Beam\\'s docs to find out more.\\n\\n* * *\\n\\n### #2. 7 steps to chain your prompts to build a production-ready financial\\nassistant using LLMs.\\n\\nùü≥ ùòÄùòÅùó≤ùóΩùòÄ on how to ùó∞ùóµùóÆùó∂ùóª your ùóΩùóøùóºùó∫ùóΩùòÅùòÄ to build a production-ready ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ\\nùóÆùòÄùòÄùó∂ùòÄùòÅùóÆùóªùòÅ using ùóüùóüùó†ùòÄ ‚Üì  \\n  \\nWhen building LLM applications, you frequently have to divide your application\\ninto multiple steps & prompts, which are known as \"chaining prompts\".  \\n  \\nHere are 7 standard steps when building a financial assistant using LLMs (or\\nany other assistant) ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: Check if the user\\'s question is safe using OpenAI\\'s Moderation API  \\n  \\nIf the user\\'s query is safe, move to ùó¶ùòÅùó≤ùóΩ ùüÆ ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: Query your proprietary data (e.g., financial news) to enrich the\\nprompt with fresh data & additional context.  \\n  \\nTo do so, you have to:  \\n\\\\- use an LM to embed the user\\'s input  \\n\\\\- use the embedding to query your proprietary data stored in a vector DB  \\n  \\nùòïùò∞ùòµùò¶: You must use the same LM model to embed:  \\n\\\\- the data that will be stored in the vector DB  \\n\\\\- the user\\'s question used to query the vector DB  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: Build the prompt using:  \\n\\\\- a predefined template  \\n\\\\- the user\\'s question  \\n\\\\- extracted financial news as context  \\n\\\\- your conversation history as context  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü∞: Call the LLM  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü±: Check if the assistant\\'s answer is safe using the OpenAI\\'s Moderation\\nAPI.  \\n  \\nIf the assistant\\'s answer is safe, move to ùó¶ùòÅùó≤ùóΩ ùü± ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≤: Use an LLM to check if the final answer is satisfactory.  \\n  \\nTo do so, you build a prompt using the following:  \\n\\\\- a validation predefined template  \\n\\\\- the user\\'s initial question  \\n\\\\- the assistants answer  \\n  \\nThe LLM has to give a \"yes\" or \"no\" answer.  \\n  \\nThus, if it answers \"yes,\" we show the final answer to the user. Otherwise, we\\nwill return a predefined response, such as:  \\n\"Sorry, we couldn\\'t answer your question because we don\\'t have enough\\ninformation.\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≥: Add the user\\'s question and assistant\\'s answer to a history cache.\\nWhich will be used to enrich the following prompts with the current\\nconversation.  \\n  \\nJust to remind you, the assistant should support a conversation. Thus, it\\nneeds to know what happened in the previous questions.  \\n  \\n‚Üí In practice, you usually keep only the latest N (question, answer) tuples or\\na conversation summary to keep your context length under control.\\n\\n7 Steps to Build a Production-Ready Financial Assistant Using LLMs [Image by\\nthe Author]\\n\\n‚Ü≥ If you want to see this strategy in action, check out our new FREE Hands-on\\nLLMs course (work in progress) & give it a ‚≠ê on GitHub to stay updated with\\nits latest progress.\\n\\n* * *\\n\\n### Extra: 3 key resources on how to monitor your ML models\\n\\nIn the last month, I read 100+ ML monitoring articles.  \\n  \\nI trimmed them for you to 3 key resources:  \\n  \\n1\\\\. A series of excellent articles made by Arize AI that will make you\\nunderstand what ML monitoring is all about.  \\n  \\n‚Ü≥üîó Arize Articles  \\n  \\n2\\\\. The Evidently AI Blog, where you can find answers to all your questions\\nregarding ML monitoring.  \\n  \\n‚Ü≥üîó Evidently Blog  \\n  \\n3\\\\. The monitoring hands-on examples hosted by DataTalksClub will teach you\\nhow to implement an ML monitoring system.  \\n  \\n‚Ü≥üîó DataTalks Course  \\n  \\nAfter wasting a lot of time reading other resources...  \\n  \\nUsing these 3 resources is a solid start for learning about monitoring ML\\nsystems.\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n5\\n\\nShare this post\\n\\n#### DML: 7-steps to build a production-ready financial assistant using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh'), ArticleDocument(id=UUID('de3f1dc2-70e9-4621-825b-56dd9a8f99be'), content={'Title': 'DML: Chain of Thought Reasoning: Write robust & explainable prompts for your LLM', 'Subtitle': 'Everything you need to know about chaining prompts: increase your LLMs accuracy & debug and explain your LLM.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Chain of Thought Reasoning: Write robust & explainable prompts for your\\nLLM\\n\\n### Everything you need to know about chaining prompts: increase your LLMs\\naccuracy & debug and explain your LLM.\\n\\nPaul Iusztin\\n\\nOct 05, 2023\\n\\n1\\n\\nShare this post\\n\\n#### DML: Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Chaining Prompts to Reduce Costs, Increase Accuracy & Easily Debug Your LLMs\\n\\n  2. Chain of Thought Reasoning: Write robust & explainable prompts for your LLM\\n\\n> **Extra:** Why**** any ML system should use an ML platform as its central\\n> nervous system\\n\\n* * *\\n\\nBut first, I want to share with you this quick 7-minute guide teaching you how\\nstable diffusion models are trained and generate new images.  \\n  \\nDiffusion models are the cornerstone of most modern computer vision generative\\nAI applications.  \\n  \\nThus, if you are into generative AI, it is essential to have an intuition of\\nhow a diffusion model works.  \\n  \\nCheck out my article to quickly understand:  \\n\\\\- the general picture of how diffusion models work  \\n\\\\- how diffusion models generate new images  \\n\\\\- how they are trained  \\n\\\\- how they are controlled by a given context (e.g., text)  \\n  \\n‚Ü≥üîó Busy? This Is Your Quick Guide to Opening the Diffusion Models Black Box\\n\\n* * *\\n\\n### #1. Chaining Prompts to Reduce Costs, Increase Accuracy & Easily Debug\\nYour LLMs\\n\\n> Here it is ‚Üì\\n\\nùóñùóµùóÆùó∂ùóªùó∂ùóªùó¥ ùóΩùóøùóºùó∫ùóΩùòÅùòÄ is an intuitive technique that states that you must split\\nyour prompts into multiple calls.  \\n  \\nùó™ùóµùòÜ? ùóüùó≤ùòÅ\\'ùòÄ ùòÇùóªùó±ùó≤ùóøùòÄùòÅùóÆùóªùó± ùòÅùóµùó∂ùòÄ ùòÑùó∂ùòÅùóµ ùòÄùóºùó∫ùó≤ ùóÆùóªùóÆùóπùóºùó¥ùó∂ùó≤ùòÄ.  \\n  \\nWhen cooking, you are following a recipe split into multiple steps. You want\\nto move to the next step only when you know what you have done so far is\\ncorrect.  \\n  \\n‚Ü≥ You want every prompt to be simple & focused.  \\n  \\nAnother analogy is between reading all the code in one monolith/god class and\\nusing DRY to separate the logic between multiple modules.  \\n  \\n‚Ü≥ You want to understand & debug every prompt easily.  \\n  \\n.  \\n  \\nChaining prompts is a ùóΩùóºùòÑùó≤ùóøùó≥ùòÇùóπ ùòÅùóºùóºùóπ ùó≥ùóºùóø ùóØùòÇùó∂ùóπùó±ùó∂ùóªùó¥ ùóÆ ùòÄùòÅùóÆùòÅùó≤ùó≥ùòÇùóπ ùòÄùòÜùòÄùòÅùó≤ùó∫ where you\\nmust take different actions depending on the current state.  \\n  \\nIn other words, you control what happens between 2 chained prompts.  \\n  \\nùòâùò∫ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµùò¥ ùò∞ùòß ùò§ùò©ùò¢ùò™ùòØùò™ùòØùò® ùò±ùò≥ùò∞ùòÆùò±ùòµùò¥:  \\n  \\n\\\\- increase in accuracy  \\n\\\\- reduce the number of tokens -> lower costs (skips steps of the workflow\\nwhen not needed)  \\n\\\\- avoid context limitations  \\n\\\\- easier to include a human-in-the-loop -> easier to control, moderate, test\\n& debug  \\n\\\\- use external tools/plugins (web search, API, databases, calculator, etc.)  \\n  \\n.  \\n  \\nùóòùòÖùóÆùó∫ùóΩùóπùó≤  \\n  \\nYou want to build a virtual assistant to respond to customer service queries.  \\n  \\nInstead of adding in one single prompt the system message, all the available\\nproducts, and the user inquiry, you can split it into the following:  \\n1\\\\. Use a prompt to extract the products and categories of interest.  \\n2\\\\. Enrich the context only with the products of interest.  \\n3\\\\. Call the LLM for the final answer.  \\n  \\nYou can evolve this example by adding another prompt that classifies the\\nnature of the user inquiry. Based on that, redirect it to billing, technical\\nsupport, account management, or a general LLM (similar to the complex system\\nof GPT-4).\\n\\nChaining Prompts to Reduce Costs, Increase Accuracy & Easily Debug Your LLMs\\n[Image by the Author].\\n\\nùóßùóº ùòÄùòÇùó∫ùó∫ùóÆùóøùó∂ùòáùó≤:  \\n  \\nInstead of writing a giant prompt that includes multiple steps:  \\n  \\nSplit the god prompt into multiple modular prompts that let you keep track of\\nthe state externally and orchestrate the program.  \\n  \\nIn other words, you want modular prompts that you can combine easily (same as\\nin writing standard functions/classes)  \\n  \\n.  \\n  \\nTo ùóÆùòÉùóºùó∂ùó± ùóºùòÉùó≤ùóøùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóøùó∂ùóªùó¥, use this technique when your prompt contains >=\\ninstruction.  \\n  \\nYou can leverage the DRY principle from software -> one prompt = one\\ninstruction.  \\n  \\n‚Ü≥üîó Tools to chain prompts: LangChain  \\n‚Ü≥üîó Tools to monitor and debug prompts: Comet LLMOps Tools\\n\\n* * *\\n\\n### #2. Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\nùóñùóµùóÆùó∂ùóª ùóºùó≥ ùóßùóµùóºùòÇùó¥ùóµùòÅ ùó•ùó≤ùóÆùòÄùóºùóªùó∂ùóªùó¥ is a ùóΩùóºùòÑùó≤ùóøùó≥ùòÇùóπ ùóΩùóøùóºùó∫ùóΩùòÅ ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóøùó∂ùóªùó¥ ùòÅùó≤ùó∞ùóµùóªùó∂ùóæùòÇùó≤ to\\nùó∂ùó∫ùóΩùóøùóºùòÉùó≤ ùòÜùóºùòÇùóø ùóüùóüùó†\\'ùòÄ ùóÆùó∞ùó∞ùòÇùóøùóÆùó∞ùòÜ ùóÆùóªùó± ùó≤ùòÖùóΩùóπùóÆùó∂ùóª ùó∂ùòÅùòÄ ùóÆùóªùòÄùòÑùó≤ùóø.  \\n\\n> Let me explain ‚Üì\\n\\n  \\nIt is a method to force the LLM to follow a set of predefined steps.  \\n  \\nüß† ùó™ùóµùòÜ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùóñùóµùóÆùó∂ùóª ùóºùó≥ ùóßùóµùóºùòÇùó¥ùóµùòÅ ùó•ùó≤ùóÆùòÄùóºùóªùó∂ùóªùó¥?  \\n  \\nIn complex scenarios, the LLM must thoroughly reason about a problem before\\nresponding to the question.  \\n  \\nOtherwise, the LLM might rush to an incorrect conclusion.  \\n  \\nBy forcing the model to follow a set of steps, we can guide the model to\\n\"think\" more methodically about the problem.  \\n  \\nAlso, it helps us explain and debug how the model reached a specific answer.  \\n  \\n.  \\n  \\nüí° ùóúùóªùóªùó≤ùóø ùó†ùóºùóªùóºùóπùóºùó¥ùòÇùó≤  \\n  \\nThe inner monologue is all the steps needed to reach the final answer.  \\n  \\nOften, we want to hide all the reasoning steps from the end user.  \\n  \\nIn fancy words, we want to mimic an \"inner monologue\" and output only the\\nfinal answer.  \\n  \\nEach reasoning step is structured into a parsable format.  \\n  \\nThus, we can quickly load it into a data structure and output only the desired\\nsteps to the user.  \\n  \\n.  \\n  \\n‚Ü≥ ùóüùó≤ùòÅ\\'ùòÄ ùóØùó≤ùòÅùòÅùó≤ùóø ùòÇùóªùó±ùó≤ùóøùòÄùòÅùóÆùóªùó± ùòÅùóµùó∂ùòÄ ùòÑùó∂ùòÅùóµ ùóÆùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤:  \\n  \\nThe input prompt to the LLM consists of a system message + the user\\'s\\nquestion.  \\n  \\nThe secret is in defining the system message as follows:  \\n  \\n\"\"\"  \\nYou are a virtual assistant helping clients...  \\n  \\nFollow the next steps to answer the customer queries.  \\n  \\nStep 1: Decide if it is a question about a product ...  \\nStep 2: Retrieve the product ...  \\nStep 3: Extract user assumptions ...  \\nStep 4: Validate user assumptions ...  \\nStep 5: Answer politely ...  \\n  \\nMake sure to answer in the following format:  \\nStep 1: <ùò¥ùòµùò¶ùò±_1_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\nStep 2: <ùò¥ùòµùò¶ùò±_2_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\nStep 3: <ùò¥ùòµùò¶ùò±_3_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\nStep 4: <ùò¥ùòµùò¶ùò±_4_ùò¢ùòØùò¥ùò∏ùò¶ùò≥>  \\n  \\nResponse to the user: <ùòßùò™ùòØùò¢ùò≠_ùò≥ùò¶ùò¥ùò±ùò∞ùòØùò¥ùò¶>  \\n\"\"\"  \\n  \\nEnforcing the LLM to follow a set of steps, we ensured it would answer the\\nright questions.  \\n  \\nUltimately, we will show the user only the <ùòßùò™ùòØùò¢ùò≠_ùò≥ùò¶ùò¥ùò±ùò∞ùòØùò¥ùò¶> subset of the\\nanswer.  \\n  \\nThe other steps (aka \"inner monologue\") help:  \\n\\\\- the model to reason  \\n\\\\- the developer to debug  \\n  \\nHave you used this technique when writing prompts?\\n\\nChain of Thought Reasoning: Write robust & explainable prompts for your LLM\\n[Image by the Author].\\n\\n* * *\\n\\n### Extra: Why**** any ML system should use an ML platform as its central\\nnervous system\\n\\nAny ML system should use an ML platform as its central nervous system.  \\n  \\nHere is why ‚Üì  \\n  \\nThe primary role of an ML Platform is to bring structure to your:  \\n\\\\- experiments  \\n\\\\- visualizations  \\n\\\\- models  \\n\\\\- datasets  \\n\\\\- documentation  \\n  \\nAlso, its role is to decouple your data preprocessing, experiment, training,\\nand inference pipelines.  \\n  \\n.  \\n  \\nAn ML platform helps you automate everything mentioned above using these 6\\nfeatures:  \\n  \\n1\\\\. experiment tracking: log & compare experiments  \\n2\\\\. metadata store: know how a model (aka experiment) was generated  \\n3\\\\. visualisations: a central hub for your visualizations  \\n4\\\\. reports: create documents out of your experiments  \\n5\\\\. artifacts: version & share your datasets  \\n6\\\\. model registry: version & share your models\\n\\nWhy**** any ML system should use an ML platform as its central nervous system\\n[GIF by the Author].\\n\\nI have used many ML Platforms before, but lately, I started using Comet, and I\\nlove it.\\n\\n‚Ü≥üîó Comet ML  \\n  \\nWhat is your favorite ML Platform?\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n1\\n\\nShare this post\\n\\n#### DML: Chain of Thought Reasoning: Write robust & explainable prompts for\\nyour LLM\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-chain-of-thought-reasoning-write?r=1ttoeh'), ArticleDocument(id=UUID('3d7e4ad6-60d2-4e20-bf42-e158930d168c'), content={'Title': 'DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs', 'Subtitle': 'Stop Manually Creating Your ML AWS Infrastructure - use Terraform! Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\n### Stop Manually Creating Your ML AWS Infrastructure - use Terraform! Build &\\nServe a Production-Ready Classifier in 1 Hour Using LLMs.\\n\\nPaul Iusztin\\n\\nSep 21, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. Stop Manually Creating Your ML AWS Infrastructure. Use Terraform!\\n\\n  2. Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs.\\n\\n* * *\\n\\n> Before going into our subject of the day, I have some news to share with you\\n> üëÄ\\n\\nIf you want to ùóæùòÇùó∂ùó∞ùó∏ùóπùòÜ ùóπùó≤ùóÆùóøùóª in a ùòÄùòÅùóøùòÇùó∞ùòÅùòÇùóøùó≤ùó± ùòÑùóÆùòÜ how to ùóØùòÇùó∂ùóπùó± ùó≤ùóªùó±-ùòÅùóº-ùó≤ùóªùó± ùó†ùóü\\nùòÄùòÜùòÄùòÅùó≤ùó∫ùòÄ ùòÇùòÄùó∂ùóªùó¥ ùóüùóüùó†ùòÄ, emphasizing ùóøùó≤ùóÆùóπ-ùòÑùóºùóøùóπùó± ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ?\\n\\nI want to let you know that ‚Üì\\n\\nI am invited on ùó¶ùó≤ùóΩùòÅùó≤ùó∫ùóØùó≤ùóø ùüÆùü¥ùòÅùóµ to a ùòÑùó≤ùóØùó∂ùóªùóÆùóø to present an overview of the\\nùóõùóÆùóªùó±ùòÄ-ùóºùóª ùóüùóüùó†ùòÄ course I am creating.\\n\\nI will show you a ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùó≤ùòÖùóÆùó∫ùóΩùóπùó≤ of how to ùóØùòÇùó∂ùóπùó± ùóÆ ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóØùóºùòÅ ùòÇùòÄùó∂ùóªùó¥ ùóüùóüùó†ùòÄ.\\nHere is what I will cover ‚Üì\\n\\n  * creating your Q&A dataset in a semi-automated way (OpenAI GPT) \\n\\n  * fine-tuning an LLM on your new dataset using QLoRA (HuggingFace, Peft, Comet ML, Beam)\\n\\n  * build a streaming pipeline to ingest news in real time into a vector DB (Bytewax, Qdrant, AWS)\\n\\n  * build a financial bot based on the fine-tuned model and real-time financial news (LangChain, Comet ML, Beam) \\n\\n  * build a simple UI to interact with the financial bot \\n\\n‚ùóNo Notebooks or fragmented examples.\\n\\n‚úÖ I want to show you how to build a real product.\\n\\n‚Üí More precisely, I will focus on the engineering and system design, showing\\nyou how the components described above work together.\\n\\n.\\n\\nIf this is something you want to learn, be sure to register using the link\\nbelow ‚Üì\\n\\n‚Ü≥üîó Engineering an End-to-End ML System for a Financial Assistant Using LLMs\\n(September 28th).\\n\\nSee you there üëÄ\\n\\n> Now back to business üî•\\n\\n* * *\\n\\n### #1. Stop Manually Creating Your ML AWS Infrastructure. Use Terraform!\\n\\nI was uselessly spending 1000$ dollars every month on cloud machines until I\\nstarted using this tool üëá  \\n  \\nTerraform!  \\n  \\n.  \\n  \\nùêÖùê¢ùê´ùê¨ùê≠, ùê•ùêûùê≠\\'ùê¨ ùêÆùêßùêùùêûùê´ùê¨ùê≠ùêöùêßùêù ùê∞ùê°ùê≤ ùê∞ùêû ùêßùêûùêûùêù ùêìùêûùê´ùê´ùêöùêüùê®ùê´ùê¶.  \\n  \\nWhen you want to deploy a software application, there are two main steps:  \\n1\\\\. Provisioning infrastructure  \\n2\\\\. Deploying applications  \\n  \\nA regular workflow would be that before deploying your applications or\\nbuilding your CI/CD pipelines, you manually go and spin up your, let\\'s say,\\nAWS machines.  \\n  \\nInitially, this workflow should be just fine, but there are two scenarios when\\nit could get problematic.  \\n  \\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome\\nand might yield bugs in manually replicating it.  \\n  \\n#2. In the world of AI, there are many cases when you want to spin up a GPU\\nmachine to train your models, and afterward, you don\\'t need it anymore. Thus,\\nif you forget to close it, you will end up uselessly paying a lot of $$$.  \\n  \\nWith Terraform, you can solve both of these issues.  \\n  \\n.  \\n  \\nSo...  \\n  \\nùêñùê°ùêöùê≠ ùê¢ùê¨ ùêìùêûùê´ùê´ùêöùêüùê®ùê´ùê¶?  \\n  \\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as\\ncode\" tool that:  \\n  \\n\\\\- is declarative (you focus on the WHAT, not on the HOW)  \\n\\\\- automates and manages your infrastructure  \\n\\\\- is open source  \\n  \\nYeah... yeah... that sounds fancy. But ùê∞ùê°ùêöùê≠ ùêúùêöùêß ùêà ùêùùê® ùê∞ùê¢ùê≠ùê° ùê¢ùê≠?  \\n  \\nLet\\'s take AWS as an example, where you have to:  \\n\\\\- create a VPC  \\n\\\\- create AWS users and permissions  \\n\\\\- spin up EC2 machines  \\n\\\\- install programs (e.g., Docker)  \\n\\\\- create a K8s cluster  \\n  \\nUsing Terraform...  \\n  \\nYou can do all that just by providing a configuration file that reflects the\\nstate of your infrastructure.  \\n  \\nBasically, it helps you create all the infrastructure you need\\nprogrammatically. Isn\\'t that awesome?\\n\\nTerraform [Image by the Author].\\n\\nIf you want to quickly understand Terraform enough to start using it in your\\nown projects:  \\n  \\n‚Ü≥ check out my 7-minute read article: üîó Stop Manually Creating Your AWS\\nInfrastructure. Use Terraform!\\n\\n* * *\\n\\n### #2. Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\nùòìùòìùòîùò¥ ùò¢ùò≥ùò¶ ùò¢ ùò≠ùò∞ùòµ ùòÆùò∞ùò≥ùò¶ ùòµùò©ùò¢ùòØ ùò§ùò©ùò¢ùòµùò£ùò∞ùòµùò¥. ùòõùò©ùò¶ùò¥ùò¶ ùòÆùò∞ùò•ùò¶ùò≠ùò¥ ùò¢ùò≥ùò¶ ùò≥ùò¶ùò∑ùò∞ùò≠ùò∂ùòµùò™ùò∞ùòØùò™ùòªùò™ùòØùò® ùò©ùò∞ùò∏ ùòîùòì\\nùò¥ùò∫ùò¥ùòµùò¶ùòÆùò¥ ùò¢ùò≥ùò¶ ùò£ùò∂ùò™ùò≠ùòµ.  \\n  \\n.  \\n  \\nUsing the standard approach when building an end-to-end ML application, you\\nhad to:  \\n\\\\- get labeled data: 1 month  \\n\\\\- train the model: 2 months  \\n\\\\- serve de model: 3 months  \\n  \\nThese 3 steps might take ~6 months to implement.  \\n  \\nSo far, it worked great.  \\n  \\nBut here is the catch ‚Üì  \\n  \\n.  \\n  \\nùò†ùò∞ùò∂ ùò§ùò¢ùòØ ùò≥ùò¶ùò¢ùò§ùò© ùò¢ùò≠ùòÆùò∞ùò¥ùòµ ùòµùò©ùò¶ ùò¥ùò¢ùòÆùò¶ ùò≥ùò¶ùò¥ùò∂ùò≠ùòµ ùò™ùòØ ùò¢ ùòßùò¶ùò∏ ùò©ùò∞ùò∂ùò≥ùò¥ ùò∞ùò≥ ùò•ùò¢ùò∫ùò¥ ùò∂ùò¥ùò™ùòØùò® ùò¢ ùò±ùò≥ùò∞ùòÆùò±ùòµ-\\nùò£ùò¢ùò¥ùò¶ùò• ùò≠ùò¶ùò¢ùò≥ùòØùò™ùòØùò® ùò¢ùò±ùò±ùò≥ùò∞ùò¢ùò§ùò©.  \\n  \\nLet\\'s take a classification task as an example ‚Üì  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü≠: You write a system prompt explaining the model and what types of\\ninputs and outputs it will get.  \\n  \\n\"  \\nYou will be provided with customer service queries.  \\n  \\nClassify each query into the following categories:  \\n\\\\- Billing  \\n\\\\- Account Management  \\n\\\\- General Inquiry  \\n\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüÆ: You can give the model an example to make sure it understands the task\\n(known as one-shot learning):  \\n  \\n\"  \\nUser: I want to know the price of the pro subscription plan.  \\nAssistant: Billing  \\n\"  \\n  \\nùó¶ùòÅùó≤ùóΩ ùüØ: Attach the user prompt and create the input prompt, which now consists\\nof the following:  \\n\\\\- system  \\n\\\\- example  \\n\\\\- user  \\n...prompts  \\n  \\nùó¶ùòÅùó≤ùóΩ ùü∞: Call the LLM\\'s API... and boom, you built a classifier in under one\\nhour.  \\n  \\nCool, right? üî•  \\n  \\nUsing this approach, the only time-consuming step is to tweak the prompt until\\nit reaches the desired result.\\n\\nHow to quickly build a classifier using LLMs [GIF by the Author].\\n\\nTo conclude...  \\n  \\nIn today\\'s LLMs world, to build a classifier, you have to write:  \\n\\\\- a system prompt  \\n\\\\- an example  \\n\\\\- attach the user prompt  \\n\\\\- pass the input prompt to the LLM API\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: Build & Serve a Production-Ready Classifier in 1 Hour Using LLMs\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-build-and-serve-a-production?r=1ttoeh'), ArticleDocument(id=UUID('49e2912f-313d-439d-8de6-522dc8379cb2'), content={'Title': 'DML: 4 key ideas you must know to train an LLM successfully', 'Subtitle': 'My time series forecasting Python code was a disaster until I started using this package. 4 key ideas you must know to train an LLM successfully.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: 4 key ideas you must know to train an LLM successfully\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: 4 key ideas you must know to train an LLM successfully\\n\\n### My time series forecasting Python code was a disaster until I started\\nusing this package. 4 key ideas you must know to train an LLM successfully.\\n\\nPaul Iusztin\\n\\nSep 14, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: 4 key ideas you must know to train an LLM successfully\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n**This week‚Äôs ML & MLOps topics:**\\n\\n  1. My time series forecasting Python code was a disaster until I started using this package\\n\\n  2. 4 key ideas you must know to train an LLM successfully\\n\\n> **Extra** : My favorite ML & MLOps newsletter\\n\\n* * *\\n\\n### #1. My time series forecasting Python code was a disaster until I started\\nusing this package\\n\\nDoes building time series models sound more complicated than modeling standard\\ntabular datasets?  \\n  \\nWell... maybe it is... but that is precisely why you need to learn more about\\nùòÄùó∏ùòÅùó∂ùó∫ùó≤!  \\n  \\nWhen I first built forecasting models, I manually coded the required\\npreprocessing and postprocessing steps. What a newbie I was...  \\n  \\nHow easy would my life have been if I had started from the beginning to use\\nùòÄùó∏ùòÅùó∂ùó∫ùó≤?  \\n  \\n.  \\n  \\nùêñùê°ùêöùê≠ ùê¢ùê¨ ùê¨ùê§ùê≠ùê¢ùê¶ùêû?  \\n  \\nùòÄùó∏ùòÅùó∂ùó∫ùó≤ is a Python package that adds time-series functionality over well-known\\npackages such as statsmodels, fbprophet, scikit-learn, autoarima, xgboost,\\netc.  \\n  \\nThus, all of a sudden, all your beloved packages will support time series\\nfeatures such as:  \\n\\\\- easily swap between different models (e.g., xgboost, lightgbm, decision\\ntrees, etc.)  \\n\\\\- out-of-the-box windowing transformations & aggregations  \\n\\\\- functionality for multivariate, panel, and hierarchical learning  \\n\\\\- cross-validation adapted to time-series  \\n\\\\- cool visualizations  \\nand more...\\n\\nSktime example [Image by the Author].\\n\\n‚Ü≥ If you want to see ùòÄùó∏ùòÅùó∂ùó∫ùó≤ in action, check out my article: üîó A Guide to\\nBuilding Effective Training Pipelines for Maximum Results\\n\\n* * *\\n\\n### #2. 4 key ideas you must know to train an LLM successfully\\n\\nThese are 4 key ideas you must know to train an LLM successfully  \\n  \\nüìñ ùóõùóºùòÑ ùó∂ùòÄ ùòÅùóµùó≤ ùó∫ùóºùó±ùó≤ùóπ ùóπùó≤ùóÆùóøùóªùó∂ùóªùó¥?  \\n  \\nLLMs still leverage supervised learning.  \\n  \\nA standard NLP task is to build a classifier.  \\nFor example, you have a sequence of tokens as inputs and, as output, a set of\\nclasses (e.g., negative and positive).  \\n  \\nWhen training an LLM for text generation, you have as input a sequence of\\ntokens, and its task is to predict the next token:  \\n\\\\- Input: JavaScript is all you [...]  \\n\\\\- Output: Need  \\n  \\nThis is known as an autoregressive process.  \\n  \\n‚öîÔ∏è ùòÑùóºùóøùó±ùòÄ != ùòÅùóºùó∏ùó≤ùóªùòÄ  \\n  \\nTokens are created based on the frequency of sequences of characters.  \\n  \\nFor example:  \\n\\\\- In the sentence: \"Learning new things is fun!\" every work is a different\\ntoken as each is frequently used.  \\n\\\\- In the sentence: \"Prompting is a ...\" the word \\'prompting\\' is divided into\\n3 tokens: \\'prom\\', \\'pt\\', and \\'ing\\'  \\n  \\nThis is important because different LLMs have different limits for the input\\nnumber of tokens.\\n\\nHow to train an LLM cheatsheet [Image by the Author].\\n\\nüß† ùóßùòÜùóΩùó≤ùòÄ ùóºùó≥ ùóüùóüùó†ùòÄ  \\n  \\nThere are 3 primary types of LLMs:  \\n\\\\- base LLM  \\n\\\\- instruction tuned LLM  \\n\\\\- RLHF tuned LLM  \\n  \\nùòöùòµùò¶ùò±ùò¥ ùòµùò∞ ùò®ùò¶ùòµ ùòßùò≥ùò∞ùòÆ ùò¢ ùò£ùò¢ùò¥ùò¶ ùòµùò∞ ùò¢ùòØ ùò™ùòØùò¥ùòµùò≥ùò∂ùò§ùòµùò™ùò∞ùòØ-ùòµùò∂ùòØùò¶ùò• ùòìùòìùòî:  \\n  \\n1\\\\. Train the Base LLM on a lot of data (trillions of tokens) - trained for\\nmonths on massive GPU clusters  \\n  \\n2\\\\. Fine-tune the Base LLM on a Q&A dataset (millions of tokens) - trained for\\nhours or days on modest-size computational resources  \\n  \\n3\\\\. [Optional] Fine-tune the LLM further on human ratings reflecting the\\nquality of different LLM outputs, on criteria such as if the answer is\\nhelpful, honest and harmless using RLHF. This will increase the probability of\\ngenerating a more highly rated output.  \\n  \\nüèóÔ∏è ùóõùóºùòÑ ùòÅùóº ùóØùòÇùó∂ùóπùó± ùòÅùóµùó≤ ùóΩùóøùóºùó∫ùóΩùòÅ ùòÅùóº ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùòÅùóµùó≤ ùóüùóüùó† ùóºùóª ùóÆ ùó§&ùóî ùó±ùóÆùòÅùóÆùòÄùó≤ùòÅ  \\n  \\nThe most common approach consists of 4 steps:  \\n1\\\\. A system message that sets the general tone & behavior.  \\n2\\\\. The context that adds more information to help the model to answer\\n(Optional).  \\n3\\\\. The user\\'s question.  \\n4\\\\. The answer to the question.  \\n  \\nNote that you need to know the answer to the question during training. You can\\nintuitively see it as your label.\\n\\n* * *\\n\\n### Extra: My favorite ML & MLOps newsletter\\n\\nDo you want to learn ML & MLOps from real-world experience?  \\n  \\nThen I suggest you join Pau Labarta Bajo\\'s Real-World Machine Learning  \\nweekly newsletter, along with another 8k+ ML developers.  \\n  \\nPau Labarta Bajo inspired me to start my weekly newsletter and is a great\\nteacher who makes learning seamless ‚úå\\n\\n> üîó **Real-World Machine Learning -**Every Saturday Morning\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 a.m. CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where all my work is aggregated in one place (courses, articles, webinars, podcasts, etc.).\\n\\n3\\n\\nShare this post\\n\\n#### DML: 4 key ideas you must know to train an LLM successfully\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Pau Labarta BajoReal-World Machine Learning Sep 14, 2023Liked by Paul\\nIusztinThanks for the shout out Paul. I love the content you shareExpand full\\ncommentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-4-key-ideas-you-must-know-to?r=1ttoeh'), ArticleDocument(id=UUID('0b152bfd-0a90-4220-a1b8-77709ecb06d0'), content={'Title': 'DML: How to add real-time monitoring & metrics to your ML System', 'Subtitle': 'How to easily add retry policies to your Python code. How to add real-time monitoring & metrics to your ML System.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: How to add real-time monitoring & metrics to your ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: How to add real-time monitoring & metrics to your ML System\\n\\n### How to easily add retry policies to your Python code. How to add real-time\\nmonitoring & metrics to your ML System.\\n\\nPaul Iusztin\\n\\nSep 07, 2023\\n\\n6\\n\\nShare this post\\n\\n#### DML: How to add real-time monitoring & metrics to your ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\n _This week‚Äôs ML & MLOps topics:_\\n\\n  1. How to add real-time monitoring & metrics to your ML System\\n\\n  2. How to easily add retry policies to your Python code\\n\\n _Storytime:_ How am I writing code in 2023? ùóú ùó±ùóºùóª\\'ùòÅ.\\n\\n* * *\\n\\n> But first, I have some big news to share with you üéâ\\n\\n‚Äî> Want to learn how to ùó≥ùó∂ùóªùó≤-ùòÅùòÇùóªùó≤ ùóÆùóª ùóüùóüùó†, build a ùòÄùòÅùóøùó≤ùóÆùó∫ùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤, use a\\nùòÉùó≤ùó∞ùòÅùóºùóø ùóóùóï, build a ùó≥ùó∂ùóªùóÆùóªùó∞ùó∂ùóÆùóπ ùóØùóºùòÅ and ùó±ùó≤ùóΩùóπùóºùòÜ ùó≤ùòÉùó≤ùóøùòÜùòÅùóµùó∂ùóªùó¥ using a serverless\\nsolution?\\n\\nThen you will enjoy looking at this new free course that me and\\n\\nPau Labarta Bajo\\n\\n(from the RWML newsletter) are cooking.\\n\\n  \\n‚Ü≥ The course will teach you how to build an end-to-end LLM solution.  \\n  \\nIt is structured into 4 modules ‚Üì  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùü≠: Learn how to generate a financial Q&A dataset in a semi-automated\\nway using the OpenAI API.  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùüÆ: Fine-tune the LLM (e.g., Falcon, Llama2, etc.) using HuggingFace &\\nPeft. Also, we will show you how to integrate an experiment tracker, model\\nregistry, and monitor the prompts using Comet.  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùüØ: Build a streaming pipeline using Bytewax that listens to financial\\nnews through a web socket, cleans it, embeds it, and loads it to a vector\\ndatabase using Qdrant.  \\n  \\nùó†ùóºùó±ùòÇùóπùó≤ ùü∞: Wrap the fine-tuned model and vector DB into a financial bot using\\nLangChain and deploy it under a RESTful API.  \\n  \\n‚ùóÔ∏è But all of this is useless if it isn\\'t deployed.  \\n  \\n‚Üí We will use Beam to deploy everything quickly - Beam is a serverless\\nsolution that lets you focus on your problem and quickly serve all your ML\\ncomponents. Say bye-bye to access policies and network configuration.  \\n  \\nùó°ùóºùòÅùó≤: This is still a work in progress, but the first 3 modules are almost\\ndone.\\n\\nArchitecture built during the **Hands-On LLMs Course** [GIF by the Author].\\n\\nCurious?\\n\\nThen, check out the repository and give it a ‚≠ê ‚Üì\\n\\n‚Ü≥ üîó Course GitHub Repository\\n\\n* * *\\n\\n### #1. How to add real-time monitoring & metrics to your ML System\\n\\nYour model is exposed to performance degradation after it is deployed to\\nproduction.  \\n  \\nThat is why you need to monitor it constantly.  \\n  \\nThe most common way to monitor an ML model is to compute its metrics.  \\n  \\nBut for that, you need the ground truth.  \\n  \\nùóúùóª ùóΩùóøùóºùó±ùòÇùó∞ùòÅùó∂ùóºùóª, ùòÜùóºùòÇ ùó∞ùóÆùóª ùóÆùòÇùòÅùóºùó∫ùóÆùòÅùó∂ùó∞ùóÆùóπùóπùòÜ ùóÆùó∞ùó∞ùó≤ùòÄùòÄ ùòÅùóµùó≤ ùó¥ùóøùóºùòÇùóªùó± ùòÅùóøùòÇùòÅùóµ ùó∂ùóª ùüØ ùó∫ùóÆùó∂ùóª\\nùòÄùó∞ùó≤ùóªùóÆùóøùó∂ùóºùòÄ:  \\n1\\\\. near real-time: you can access it quite quickly  \\n2\\\\. delayed: you can access it after a considerable amount of time (e.g., one\\nmonth)  \\n3\\\\. never: you have to label the data manually  \\n  \\n.  \\n  \\nùóôùóºùóø ùòÇùòÄùó≤ ùó∞ùóÆùòÄùó≤ùòÄ ùüÆ. ùóÆùóªùó± ùüØ. ùòÜùóºùòÇ ùó∞ùóÆùóª ùóæùòÇùó∂ùó∞ùó∏ùóπùòÜ ùó∞ùóºùó∫ùóΩùòÇùòÅùó≤ ùòÜùóºùòÇùóø ùó∫ùóºùóªùó∂ùòÅùóºùóøùó∂ùóªùó¥ ùóΩùó∂ùóΩùó≤ùóπùó∂ùóªùó≤ ùó∂ùóª\\nùòÅùóµùó≤ ùó≥ùóºùóπùóπùóºùòÑùó∂ùóªùó¥ ùòÑùóÆùòÜ:  \\n  \\n\\\\- store the model predictions and GT as soon as they are available (these 2\\nwill be out of sync -> you can\\'t compute the metrics right away)  \\n  \\n\\\\- build a DAG (e.g., using Airflow) that extracts the predictions & GT\\ncomputes the metrics in batch mode and loads them into another storage (e.g.,\\nGCS)  \\n  \\n\\\\- use an orchestration tool to run the DAG in the following scenarios:  \\n1\\\\. scheduled: if the GT is available in near real-time (e.g., hourly), then\\nit makes sense to run your monitoring pipeline based on the known frequency  \\n2\\\\. triggered: if the GT is delayed and you don\\'t know when it may come up,\\nthen you can implement a webhook to trigger your monitoring pipeline  \\n  \\n\\\\- attach a consumer to your storage to use and display the metrics (e.g.,\\ntrigger alarms and display them in a dashboard)\\n\\nHow to add real-time monitoring & metrics to your ML system [Image by the\\nAuthor].\\n\\nIf you want to see how to implement a near real-time monitoring pipeline using\\nAirflow and GCS, check out my article ‚Üì\\n\\n‚Ü≥ üîó Ensuring Trustworthy ML Systems With Data Validation and Real-Time\\nMonitoring\\n\\n* * *\\n\\n### #2. How to easily add retry policies to your Python code\\n\\nOne strategy that makes the ùó±ùó∂ùó≥ùó≥ùó≤ùóøùó≤ùóªùó∞ùó≤ ùóØùó≤ùòÅùòÑùó≤ùó≤ùóª ùó¥ùóºùóºùó± ùó∞ùóºùó±ùó≤ ùóÆùóªùó± ùó¥ùóøùó≤ùóÆùòÅ ùó∞ùóºùó±ùó≤ is\\nadding ùóøùó≤ùòÅùóøùòÜ ùóΩùóºùóπùó∂ùó∞ùó∂ùó≤ùòÄ.  \\n  \\nTo manually implement them can get tedious and complicated.  \\n  \\nRetry policies are a must when you:  \\n\\\\- make calls to an external API  \\n\\\\- read from a queue, etc.  \\n  \\n.  \\n  \\nùó®ùòÄùó∂ùóªùó¥ ùòÅùóµùó≤ ùóßùó≤ùóªùóÆùó∞ùó∂ùòÅùòÜ ùó£ùòÜùòÅùóµùóºùóª ùóΩùóÆùó∞ùó∏ùóÆùó¥ùó≤...  \\n  \\nùò†ùò∞ùò∂ ùò§ùò¢ùòØ ùò≤ùò∂ùò™ùò§ùò¨ùò≠ùò∫ ùò•ùò¶ùò§ùò∞ùò≥ùò¢ùòµùò¶ ùò∫ùò∞ùò∂ùò≥ ùòßùò∂ùòØùò§ùòµùò™ùò∞ùòØùò¥ ùò¢ùòØùò• ùò¢ùò•ùò• ùò§ùò∂ùò¥ùòµùò∞ùòÆùò™ùòªùò¢ùò£ùò≠ùò¶ ùò≥ùò¶ùòµùò≥ùò∫ ùò±ùò∞ùò≠ùò™ùò§ùò™ùò¶ùò¥,\\nùò¥ùò∂ùò§ùò© ùò¢ùò¥:  \\n  \\n1\\\\. Add fixed and random wait times between multiple retries.  \\n  \\n2\\\\. Add a maximum number of attempts or computation time.  \\n  \\n3\\\\. Retry only when specific errors are thrown (or not thrown).  \\n  \\n... as you can see, you easily compose these policies between them.  \\n  \\nThe cherry on top is that you can access the statistics of the retries of a\\nspecific function:  \\n\"  \\nprint(raise_my_exception.retry.statistics)  \\n\"\\n\\nExamples of the retry policies using tenacity [Image by the Author].\\n\\n‚Ü≥ üîó tenacity repository\\n\\n* * *\\n\\n###  _Storytime:_ How am I writing code in 2023? I don‚Äôt\\n\\nAs an engineer, you are paid to think and solve problems. How you do that, it\\ndoesn\\'t matter. Let me explain ‚Üì  \\n  \\n.  \\n  \\nThe truth is that I am lazy.  \\n  \\nThat is why I am a good engineer.  \\n  \\nWith the rise of LLMs, my laziness hit all times highs.  \\n  \\n.  \\n  \\nùóßùóµùòÇùòÄ, ùòÅùóµùó∂ùòÄ ùó∂ùòÄ ùóµùóºùòÑ ùóú ùòÑùóøùó∂ùòÅùó≤ ùó∫ùòÜ ùó∞ùóºùó±ùó≤ ùòÅùóµùó≤ùòÄùó≤ ùó±ùóÆùòÜùòÄ ‚Üì  \\n  \\n\\\\- 50% Copilot (tab is the new CTRL-C + CTRL-V)  \\n\\\\- 30% ChatGPT/Bard  \\n\\\\- 10% Stackoverflow (call me insane, but I still use StackOverflow from time\\nto time)  \\n\\\\- 10% Writing my own code  \\n  \\nThe thing is that I am more productive than ever.  \\n  \\n... and that 10% of \"writing my own code\" is the final step that connects all\\nthe dots and brings real value to the table.  \\n  \\n.  \\n  \\nùóúùóª ùóøùó≤ùóÆùóπùó∂ùòÅùòÜ, ùóÆùòÄ ùóÆùóª ùó≤ùóªùó¥ùó∂ùóªùó≤ùó≤ùóø, ùòÜùóºùòÇ ùó∫ùóºùòÄùòÅùóπùòÜ ùóµùóÆùòÉùó≤ ùòÅùóº:  \\n  \\n\\\\- ask the right questions  \\n\\\\- understand & improve the architecture of the system  \\n\\\\- debug code  \\n\\\\- understand business requirements  \\n\\\\- communicate with other teams  \\n  \\n...not to write code.\\n\\n[Image by the Author]\\n\\nWriting code as we know it most probably will disappear with the rise of AI\\n(it kind of already did).  \\n  \\n.  \\n  \\nWhat do you think? How do you write code these days?\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 am CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: here, I approach in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where I will constantly aggregate all my work (courses, articles, webinars, podcasts, etc.).\\n\\n6\\n\\nShare this post\\n\\n#### DML: How to add real-time monitoring & metrics to your ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-how-to-add-real-time-monitoring?r=1ttoeh'), ArticleDocument(id=UUID('a520fdac-65b4-4340-9ee2-d16a1390b838'), content={'Title': 'DML: Top 6 ML Platform Features You Must Know to Build an ML System', 'Subtitle': 'Why serving an ML model using a batch architecture is so powerful? Top 6 ML platform features you must know.', 'Content': '#\\n\\nSubscribeSign in\\n\\nShare this post\\n\\n#### DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n# DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\n### Why serving an ML model using a batch architecture is so powerful? Top 6\\nML platform features you must know.\\n\\nPaul Iusztin\\n\\nAug 31, 2023\\n\\n3\\n\\nShare this post\\n\\n#### DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\n _Hello there, I am Paul Iusztin üëãüèº_\\n\\n _Within this newsletter, I will help you decode complex topics about ML &\\nMLOps one week at a time üî•_\\n\\nThis week we will cover:\\n\\n  1. Top 6 ML platform features you must know to build an ML system\\n\\n  2. Why serving an ML model using a batch architecture is so powerful?\\n\\n_Story:_ ‚ÄúI never forget anything‚Äù - said no one but your second brain.\\n\\n* * *\\n\\nThis week, no shameless promotion üëÄ\\n\\n* * *\\n\\n### #1. Top 6 ML platform features you must know to build an ML system\\n\\nHere they are ‚Üì  \\n  \\n#ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥  \\n  \\nIn your ML development phase, you generate lots of experiments.  \\n  \\nTracking and comparing the metrics between them is crucial in finding the\\noptimal model.  \\n  \\n#ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤  \\n  \\nIts primary purpose is reproducibility.  \\n  \\nTo know how a model was generated, you need to know:  \\n\\\\- the version of the code  \\n\\\\- the version of the packages  \\n\\\\- hyperparameters/config  \\n\\\\- total compute  \\n\\\\- version of the dataset  \\n... and more  \\n  \\n#ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ  \\n  \\nMost of the time, along with the metrics, you must log a set of visualizations\\nfor your experiment.  \\n  \\nSuch as:  \\n\\\\- images  \\n\\\\- videos  \\n\\\\- prompts  \\n\\\\- t-SNE graphs  \\n\\\\- 3D point clouds  \\n... and more  \\n  \\n#ùü∞. ùó•ùó≤ùóΩùóºùóøùòÅùòÄ  \\n  \\nYou don\\'t work in a vacuum.  \\n  \\nYou have to present your work to other colleges or clients.  \\n  \\nA report lets you take the metadata and visualizations from your experiment...  \\n  \\n...and create, deliver and share a targeted presentation for your clients or\\npeers.  \\n  \\n#ùü±. ùóîùóøùòÅùó∂ùó≥ùóÆùó∞ùòÅùòÄ  \\n  \\nThe most powerful feature out of them all.  \\n  \\nAn artifact is a versioned object that is an input or output for your task.  \\n  \\nEverything can be an artifact, but the most common cases are:  \\n\\\\- data  \\n\\\\- model  \\n\\\\- code  \\n  \\nWrapping your assets around an artifact ensures reproducibility.  \\n  \\nFor example, you wrap your features into an artifact (e.g., features:3.1.2),\\nwhich you can consume into your ML development step.  \\n  \\nThe ML development step will generate config (e.g., config:1.2.4) and code\\n(e.g., code:1.0.2) artifacts used in the continuous training pipeline.  \\n  \\nDoing so lets you quickly respond to questions such as \"What I used to\\ngenerate the model?\" and \"What Version?\"  \\n  \\n#ùü≤. ùó†ùóºùó±ùó≤ùóπ ùó•ùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ  \\n  \\nThe model registry is the ultimate way to make your model accessible to your\\nproduction ecosystem.  \\n  \\nFor example, in your continuous training pipeline, after the model is trained,\\nyou load the weights as an artifact into the model registry (e.g.,\\nmodel:1.2.4).  \\n  \\nYou label this model as \"staging\" under a new version and prepare it for\\ntesting. If the tests pass, mark it as \"production\" under a new version and\\nprepare it for deployment (e.g., model:2.1.5).\\n\\nTop 6 ML platform features you must know [Image by the Author].\\n\\n.  \\n  \\nAll of these features are used in a mature ML system. What is your favorite\\none?  \\n  \\n‚Ü≥ You can see all these features in action in my: üîó **The Full Stack 7-Steps\\nMLOps Framework** FREE course.\\n\\n* * *\\n\\n### #2. Why serving an ML model using a batch architecture is so powerful?\\n\\nWhen you first start deploying your ML model, you want an initial end-to-end\\nflow as fast as possible.  \\n  \\nDoing so lets you quickly provide value, get feedback, and even collect data.  \\n  \\n.  \\n  \\nBut here is the catch...  \\n  \\nSuccessfully serving an ML model is tricky as you need many iterations to\\noptimize your model to work in real-time:  \\n\\\\- low latency  \\n\\\\- high throughput  \\n  \\nInitially, serving your model in batch mode is like a hack.  \\n  \\nBy storing the model\\'s predictions in dedicated storage, you automatically\\nmove your model from offline mode to a real-time online model.  \\n  \\nThus, you no longer have to care for your model\\'s latency and throughput. The\\nconsumer will directly load the predictions from the given storage.  \\n  \\nùêìùê°ùêûùê¨ùêû ùêöùê´ùêû ùê≠ùê°ùêû ùê¶ùêöùê¢ùêß ùê¨ùê≠ùêûùê©ùê¨ ùê®ùêü ùêö ùêõùêöùê≠ùêúùê° ùêöùê´ùêúùê°ùê¢ùê≠ùêûùêúùê≠ùêÆùê´ùêû:  \\n\\\\- extracts raw data from a real data source  \\n\\\\- clean, validate, and aggregate the raw data within a feature pipeline  \\n\\\\- load the cleaned data into a feature store  \\n\\\\- experiment to find the best model + transformations using the data from the\\nfeature store  \\n\\\\- upload the best model from the training pipeline into the model registry  \\n\\\\- inside a batch prediction pipeline, use the best model from the model\\nregistry to compute the predictions  \\n\\\\- store the predictions in some storage  \\n\\\\- the consumer will download the predictions from the storage  \\n\\\\- repeat the whole process hourly, daily, weekly, etc. (it depends on your\\ncontext)  \\n.  \\n  \\nùòõùò©ùò¶ ùòÆùò¢ùò™ùòØ ùò•ùò∞ùò∏ùòØùò¥ùò™ùò•ùò¶ of deploying your model in batch mode is that the\\npredictions will have a level of lag.  \\n  \\nFor example, in a recommender system, if you make your predictions daily, it\\nwon\\'t capture a user\\'s behavior in real-time, and it will update the\\npredictions only at the end of the day.  \\n  \\nMoving to other architectures, such as request-response or streaming, will be\\nnatural after your system matures in batch mode.\\n\\nML Batch Architecture Design [Image by the Author].\\n\\nSo remember, when you initially deploy your model, using a batch mode\\narchitecture will be your best shot for a good user experience.\\n\\n* * *\\n\\n### _Story:_ ‚ÄúI never forget anything‚Äù - said no one but your second brain.\\n\\nAfter 6+ months of refinement, this is my second brain strategy üëá  \\n  \\nTiago\\'s Forte book inspired me, but I adapted his system to my needs.  \\n  \\n.  \\n  \\n#ùü¨. ùóñùóºùóπùóπùó≤ùó∞ùòÅ  \\n  \\nThis is where you are bombarded with information from all over the place.  \\n  \\n#ùü≠. ùóßùóµùó≤ ùóöùóøùóÆùòÉùó≤ùòÜùóÆùóøùó±  \\n  \\nThis is where I save everything that looks interesting.  \\n  \\nI won\\'t use 90% of what is here, but it satisfied my urge to save that \"cool\\narticle\" I saw on LinkedIn.  \\n  \\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists,\\netc.  \\n  \\n#ùüÆ. ùóßùóµùó≤ ùóïùóºùóÆùóøùó±  \\n  \\nHere, I start converging the information and planning what to do next.  \\n  \\nTools: Notion  \\n  \\n#ùüØ. ùóßùóµùó≤ ùóôùó∂ùó≤ùóπùó±  \\n  \\nHere is where I express myself through learning, coding, writing, etc.  \\n  \\nTools: whatever you need to express yourself.  \\n  \\n2 & 3 are iterative processes. Thus I often bounce between them until the\\ninformation is distilled.  \\n  \\n#ùü∞. ùóßùóµùó≤ ùó™ùóÆùóøùó≤ùóµùóºùòÇùòÄùó≤  \\n  \\nHere is where I take the distilled information and write it down for cold\\nstorage.  \\n  \\nTools: Notion, Google Drive  \\n  \\n.  \\n  \\nWhen I want to search for a piece of information, I start from the Warehouse\\nand go backward until I find what I need.  \\n  \\nAs a minimalist, I kept my tools to a minimum. I primarily use only: Brave,\\nNotion, and Google Drive.  \\n  \\nYou don\\'t need 100+ tools to be productive. They just want to take your money\\nfrom you.\\n\\nMy second brain strategy [Image by the Author].\\n\\nSo remember...  \\n  \\nYou have to:  \\n\\\\- collect  \\n\\\\- link  \\n\\\\- plan  \\n\\\\- distill  \\n\\\\- store\\n\\n* * *\\n\\nThat‚Äôs it for today üëæ\\n\\nSee you next Thursday at 9:00 am CET.\\n\\nHave a fantastic weekend!\\n\\nPaul\\n\\n* * *\\n\\n#### Whenever you‚Äôre ready, here is how I can help you:\\n\\n  1. **The Full Stack 7-Steps MLOps Framework :** a 7-lesson FREE course that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code + 2.5 hours of reading & video materials on Medium.\\n\\n  2. **Machine Learning& MLOps Blog**: here, I approach in-depth topics about designing and productionizing ML systems using MLOps.\\n\\n  3. **Machine Learning& MLOps Hub**: a place where I will constantly aggregate all my work (courses, articles, webinars, podcasts, etc.),\\n\\n3\\n\\nShare this post\\n\\n#### DML: Top 6 ML Platform Features You Must Know to Build an ML System\\n\\ndecodingml.substack.com\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\n2\\n\\nShare\\n\\nPreviousNext\\n\\n#### Discussion about this post\\n\\nComments\\n\\nRestacks\\n\\n| Ahmed BesbesThe Tech Buffet Aug 31, 2023Liked by Paul IusztinHello Paul!\\nGreat newsletter. It\\'d be even more useful to suggest tools for each of these\\nfeatures (e.g. the model registry, the feature store, etc)Expand full\\ncommentReplyShare  \\n---|---  \\n  \\n1 reply by Paul Iusztin\\n\\n1 more comment...\\n\\nTop\\n\\nLatest\\n\\nDiscussions\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe\\n\\n¬© 2024 Paul Iusztin\\n\\nPrivacy ‚àô Terms ‚àô Collection notice\\n\\nStart WritingGet the app\\n\\nSubstack is the home for great culture\\n\\nShare\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNote\\n\\nOther\\n\\nThis site requires JavaScript to run correctly. Please turn on JavaScript or\\nunblock scripts\\n\\n', 'language': 'en'}, platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-top-6-ml-platform-features-you?r=1ttoeh')]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7150d0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [doc for query_result in results.values() for doc in query_result]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "097ceafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleDocument(id=UUID('34978aea-e179-44b5-975c-7deb64456380'), content={'Title': 'An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin', 'Subtitle': 'From data gathering to productionizing LLMs using LLMOps good practices.', 'Content': \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.1K13ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLet‚Äôs start with Lesson 1 ‚Üì‚Üì‚ÜìLesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component‚Äôs code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask‚Ä¶When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ‚Üì‚Üì‚ÜìFirst, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: ‚ÄúWrite me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].‚ÄùExcited? Let‚Äôs get started üî•2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios‚Ä¶‚Ä¶the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution‚Üí The 3-pipeline architectureLet‚Äôs understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline‚Ä¶also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that‚Ä¶‚Ä¶by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.‚Ü≥ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet‚Äôs understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesn‚Äôt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let‚Äôs say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let‚Äôs zoom in on each component to understand how they work individually and interact with each other. ‚Üì‚Üì‚Üì3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.üîó 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people‚Äôs data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.üîó 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn‚Äôt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes ‚Äî store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren‚Äôt scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust‚Äôs impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won‚Äôt focus too much on any specific LLM.All the experiments will be logged into Comet ML‚Äôs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet‚Äôs experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet‚Äôs prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet‚Äôs prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èHave you enjoyed this article? Then‚Ä¶‚Üì‚Üì‚ÜìJoin 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferences[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps: Metadata storeIntroductionJul 17ListsAI Regulation6 stories¬∑593 savesNatural Language Processing1766 stories¬∑1367 savesPredictive Modeling w/ Python20 stories¬∑1607 savesPractical Guides to Machine Learning10 stories¬∑1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}, platform='medium', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760fc175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DataCategory.ARTICLES: 'articles'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].get_collection_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87197fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_documents': 50,\n",
       " <DataCategory.ARTICLES: 'articles'>: {'authors': ['Paul Iusztin'],\n",
       "  'num_documents': 50}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_metadata(documents: list[Document]) -> dict:\n",
    "    metadata = {\n",
    "        \"num_documents\": len(documents),\n",
    "    }\n",
    "    for document in documents:\n",
    "        collection = document.get_collection_name()\n",
    "        if collection not in metadata:\n",
    "            metadata[collection] = {}\n",
    "        if \"authors\" not in metadata[collection]:\n",
    "            metadata[collection][\"authors\"] = list()\n",
    "\n",
    "        metadata[collection][\"num_documents\"] = metadata[collection].get(\"num_documents\", 0) + 1\n",
    "        metadata[collection][\"authors\"].append(document.author_full_name)\n",
    "\n",
    "    for value in metadata.values():\n",
    "        if isinstance(value, dict) and \"authors\" in value:\n",
    "            value[\"authors\"] = list(set(value[\"authors\"]))\n",
    "\n",
    "    return metadata\n",
    "\n",
    "_get_metadata(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e68ad",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "527beeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mLoad pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngohongthai/Library/Caches/pypoetry/virtualenvs/llm-engineering-tL-FPc5M-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm_engineering.application.preprocessing import CleaningDispatcher\n",
    "from llm_engineering.domain.cleaned_documents import CleanedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7750dd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin',\n",
       " 'Subtitle': 'From data gathering to productionizing LLMs using LLMOps good practices.',\n",
       " 'Content': \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.1K13ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLet‚Äôs start with Lesson 1 ‚Üì‚Üì‚ÜìLesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component‚Äôs code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask‚Ä¶When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ‚Üì‚Üì‚ÜìFirst, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: ‚ÄúWrite me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].‚ÄùExcited? Let‚Äôs get started üî•2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios‚Ä¶‚Ä¶the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution‚Üí The 3-pipeline architectureLet‚Äôs understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline‚Ä¶also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that‚Ä¶‚Ä¶by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.‚Ü≥ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet‚Äôs understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesn‚Äôt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let‚Äôs say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let‚Äôs zoom in on each component to understand how they work individually and interact with each other. ‚Üì‚Üì‚Üì3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.üîó 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people‚Äôs data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.üîó 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn‚Äôt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes ‚Äî store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren‚Äôt scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust‚Äôs impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won‚Äôt focus too much on any specific LLM.All the experiments will be logged into Comet ML‚Äôs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet‚Äôs experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet‚Äôs prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet‚Äôs prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èHave you enjoyed this article? Then‚Ä¶‚Üì‚Üì‚ÜìJoin 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferences[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps: Metadata storeIntroductionJul 17ListsAI Regulation6 stories¬∑593 savesNatural Language Processing1766 stories¬∑1367 savesPredictive Modeling w/ Python20 stories¬∑1607 savesPractical Guides to Machine Learning10 stories¬∑1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eeafdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin', 'From data gathering to productionizing LLMs using LLMOps good practices.', \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.1K13ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLet‚Äôs start with Lesson 1 ‚Üì‚Üì‚ÜìLesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component‚Äôs code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask‚Ä¶When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ‚Üì‚Üì‚ÜìFirst, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: ‚ÄúWrite me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].‚ÄùExcited? Let‚Äôs get started üî•2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios‚Ä¶‚Ä¶the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution‚Üí The 3-pipeline architectureLet‚Äôs understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline‚Ä¶also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that‚Ä¶‚Ä¶by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.‚Ü≥ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet‚Äôs understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesn‚Äôt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let‚Äôs say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let‚Äôs zoom in on each component to understand how they work individually and interact with each other. ‚Üì‚Üì‚Üì3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.üîó 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people‚Äôs data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.üîó 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn‚Äôt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes ‚Äî store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren‚Äôt scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust‚Äôs impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won‚Äôt focus too much on any specific LLM.All the experiments will be logged into Comet ML‚Äôs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet‚Äôs experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet‚Äôs prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet‚Äôs prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èHave you enjoyed this article? Then‚Ä¶‚Üì‚Üì‚ÜìJoin 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferences[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps: Metadata storeIntroductionJul 17ListsAI Regulation6 stories¬∑593 savesNatural Language Processing1766 stories¬∑1367 savesPredictive Modeling w/ Python20 stories¬∑1607 savesPractical Guides to Machine Learning10 stories¬∑1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_content = documents[0].content.values()\n",
    "valid_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc13f90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin #### From data gathering to productionizing LLMs using LLMOps good practices. #### End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.1K13ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet ML‚Äôs experiment tracker to monitor the experiments. Evaluate and save the best model to Comet‚Äôs model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comet‚Äôs model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet‚Äôs prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessons‚Üí Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine-Tuning LLMsHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine-Tuned LLMsArchitect scalable and cost-effective LLM & RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èLet‚Äôs start with Lesson 1 ‚Üì‚Üì‚ÜìLesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component‚Äôs code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏è1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask‚Ä¶When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ‚Üì‚Üì‚ÜìFirst, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: ‚ÄúWrite me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].‚ÄùExcited? Let‚Äôs get started üî•2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios‚Ä¶‚Ä¶the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution‚Üí The 3-pipeline architectureLet‚Äôs understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline‚Ä¶also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that‚Ä¶‚Ä¶by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.‚Ü≥ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet‚Äôs understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesn‚Äôt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let‚Äôs say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let‚Äôs zoom in on each component to understand how they work individually and interact with each other. ‚Üì‚Üì‚Üì3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.üîó 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people‚Äôs data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.üîó 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn‚Äôt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes ‚Äî store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren‚Äôt scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust‚Äôs impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won‚Äôt focus too much on any specific LLM.All the experiments will be logged into Comet ML‚Äôs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet‚Äôs experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet‚Äôs prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet‚Äôs prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:‚ÄúWrite a 1000-word LinkedIn post about LLMs,‚Äù and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.üîó Check out the code on GitHub [1] and support us with a ‚≠êÔ∏èHave you enjoyed this article? Then‚Ä¶‚Üì‚Üì‚ÜìJoin 5k+ engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle-tested content on production-grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For‚Ä¶decodingml.substack.comReferences[1] Your LLM Twin Course ‚Äî GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers¬∑Editor for Decoding MLSenior ML & MLOps Engineer ‚Ä¢ Founder @ Decoding ML ~ Content about building production-grade ML/AI systems ‚Ä¢ DML Newsletter: https://decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost-effective LLM & RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What‚Äôs Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps: Metadata storeIntroductionJul 17ListsAI Regulation6 stories¬∑593 savesNatural Language Processing1766 stories¬∑1367 savesPredictive Modeling w/ Python20 stories¬∑1607 savesPractical Guides to Machine Learning10 stories¬∑1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI: Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine-tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine-tune a Mistral7b-Instruct using PEFT & QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" #### \".join(valid_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7efc860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin From data gathering to productionizing LLMs using LLMOps good practices. End to End Framework for Production Ready LLMs Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course Building Your Production Ready AI ReplicaAn End to End Framework for Production Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin FollowPublished inDecoding ML 16 min read Mar 16, 20242.1K13ListenShare the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL EWhy is this course different?By finishing the LLM Twin Building Your Production Ready AI Replica free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real world LLM system from start to finish from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices the data collection pipeline crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. deployed on AWS the feature pipeline consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded using Superlinked , and loaded into a Qdrant vector DB in real time. deployed on AWS the training pipeline create a custom dataset based on your digital data. Fine tune an LLM using QLoRA. Use Comet ML s experiment tracker to monitor the experiments. Evaluate and save the best model to Comet s model registry. deployed on Qwak the inference pipeline load and quantize the fine tuned LLM from Comet s model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet s prompt monitoring dashboard. deployed on Qwak LLM twin system architecture Image by the Author Along the 4 microservices, you will learn to integrate 3 serverless tools Comet ML as your ML Platform Qdrant as your vector DB Qwak as your ML infrastructure Who is this for?Audience MLE, DE, DS, or SWE who want to learn to engineer production ready LLM systems using LLMOps good principles.Level intermediatePrerequisites basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands on written lessons and the open source code you can access on GitHub, showing how to build an end to end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace. To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms AWS, Qwak have a pay as you go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools Qdrant, Comet , we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by Paul Iusztin Senior ML MLOps EngineerAlex Vesa Senior AI EngineerAlex Razvant Senior ML MLOps EngineerLessons Quick overview of each lesson of the LLM Twin free course.The course is split into 12 lessons. Every Medium article will be its own lesson An End to End Framework for Production Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture Enabling Event Driven ArchitecturesSOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine Tuning LLMsHow to fine tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine Tuned LLMsArchitect scalable and cost effective LLM RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework Bonus Build a scalable RAG ingestion pipeline using 74.3 less code Bonus Build Multi Index Advanced RAG Apps Check out the code on GitHub 1 and support us with a Let s start with Lesson 1 Lesson 1 End to end framework for production ready LLM systemsIn the first lesson, we will present the project you will build during the course your production ready LLM Twin AI replica.Afterward, we will explain what the 3 pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3 pipeline architecture to our LLM microservices.In the following lessons, we will examine each component s code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture Image by the Author Table of ContentsWhat are you going to build? The LLM twin conceptThe 3 pipeline architectureLLM twin system design Check out the code on GitHub 1 and support us with a 1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course LLM Twin Building Your Production Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles like this one using your own voice.Why not directly use ChatGPT? You may ask When trying to generate an article or post using an LLM, the results tend to be very generic and unarticulated,contain misinformation due to hallucination ,require tedious prompting to achieve the desired result.But here is what we are going to do to fix that First, we will fine tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here 2 .To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs e.g., what ChatGPT will do , it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to Write me a 1000 word LinkedIn post about LLMs based on the article from this link URL . Excited? Let s get started 2. The 3 pipeline architectureWe all know how messy ML systems can get. That is where the 3 pipeline architecture kicks in.The 3 pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54 of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are the model is not mature enoughsecurity risks e.g., data privacy not enough dataTo some extent, these are true.But the reality is that in many scenarios the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well defined architecture are as crucial as using suitable tools and models with high accuracy.Solution The 3 pipeline architectureLet s understand what the 3 pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components 1. the feature pipeline2. the training pipeline3. the inference pipeline also known as the Feature Training Inference FTI architecture. 1. The feature pipeline transforms your data into features labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store. 2. The training pipeline ingests a specific version of the features labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry. 3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3 pipeline architecture Image by the Author .This is why the 3 pipeline design is so beautiful it is intuitive it brings structure, as on a higher level, all ML systems can be reduced to these 3 components it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate the ML system has been built with modularity in mind since the beginning the 3 components can easily be divided between multiple teams if necessary every component can use the best stack of technologies available for the job every component can be deployed, scaled, and monitored independently the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that by following this pattern, you know 100 that your ML model will move out of your Notebooks into production. If you want to learn more about the 3 pipeline design, I recommend this excellent article 3 written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet s understand how to apply the 3 pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture Image by the Author As you can see, the data collection pipeline doesn t follow the 3 pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let s say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let s zoom in on each component to understand how they work individually and interact with each other. 3.1. The data collection pipelineIts scope is to crawl data for a given user from Medium articles Substack articles LinkedIn posts GitHub code As every platform is unique, we implemented a different Extract Transform Load ETL pipeline for each website. 1 min read on ETL pipelines 4 However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean normalize the extracted HTMLsave the normalized but still raw data to Mongo DBImportant note We are crawling only our data, as most platforms do not allow us to access other people s data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data aka text .How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture CDC pattern to inform the feature pipeline of any change on our Mongo DB. 1 min read on the CDC pattern 5 To explain the CDC briefly, a watcher listens 24 7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline our data pipeline to a streaming pipeline our feature pipeline .Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax a Rust streaming engine with a Python interface . Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn t care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and clean the data chunk it embed it using the embedding models from Superlinked load it to the Qdrant vector DB.Every type of data post, article, code will be processed independently through its own set of classes.Even though all of them are text based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data 1. The cleaned data without using vectors as indexes store them in a NoSQL fashion .2. The cleaned, chunked, and embedded data leveraging the vector indexes of Qdrant The training pipeline needs access to the data in both formats as we want to fine tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts aka RAG .Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren t scalable when working with big data.Using CDC a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust s impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains the cleaned digital data from which we will create prompts answers we will use the chunked embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data posts, articles, code .We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data to prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine tuning module that inputs a HuggingFace dataset and uses QLoRA to fine tune a given LLM e.g., Mistral . By using HuggingFace, we can easily switch between different LLMs so we won t focus too much on any specific LLM.All the experiments will be logged into Comet ML s experiment tracker.We will use a bigger LLM e.g., GPT4 to evaluate the results of our fine tuned LLM. These results will be logged into Comet s experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet s prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following experiment tracker model registry prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine tuned LLM?The fine tuned LLM will always be downloaded from the model registry based on its tag e.g., accepted and version e.g., v1.0.2, latest, etc. .How will the fine tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM s speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet s prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to Write a 1000 word LinkedIn post about LLMs, and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin Building Your Production Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3 pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS. Check out the code on GitHub 1 and support us with a Have you enjoyed this article? Then Join 5k engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle tested content on production grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏ Decoding ML Newsletter Paul Iusztin SubstackJoin for battle tested content on designing, coding, and deploying production grade ML MLOps systems. Every week. For decodingml.substack.comReferences 1 Your LLM Twin Course GitHub Repository 2024 , Decoding ML GitHub Organization 2 Introducing new AI experiences from Meta 2023 , Meta 3 Jim Dowling, From MLOps to ML Systems with Feature Training Inference Pipelines 2023 , Hopsworks 4 Extract Transform Load ETL , Databricks Glossary 5 Daniel Svonava and Paolo Perrone, Understanding the different Data Modality Types 2023 , SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for 5 monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers Editor for Decoding MLSenior ML MLOps Engineer Founder Decoding ML Content about building production grade ML AI systems DML Newsletter https decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost effective LLM RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What s Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps Metadata storeIntroductionJul 17ListsAI Regulation6 stories 593 savesNatural Language Processing1766 stories 1367 savesPredictive Modeling w Python20 stories 1607 savesPractical Guides to Machine Learning10 stories 1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine tune a Mistral7b Instruct using PEFT QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications Serving LLMs Part 9 Learn Large Language Models LLM through the lens of a Retrieval Augmented Generation RAG Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step by Step Tutorial on Implementing Retrieval Augmented Generation RAG , Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams To make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_engineering.application.preprocessing.operations import clean_text\n",
    "clean_content = clean_text(\" #### \".join(valid_content))\n",
    "clean_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cea3e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96327dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65d914fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 05:35:29.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n",
      "\u001b[32m2025-07-14 05:35:29.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.application.preprocessing.dispatchers\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDocument cleaned successfully.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CleanedArticleDocument(id=UUID('a520fdac-65b4-4340-9ee2-d16a1390b838'), content='DML Top 6 ML Platform Features You Must Know to Build an ML System Why serving an ML model using a batch architecture is so powerful? Top 6 ML platform features you must know. SubscribeSign in Share this post DML Top 6 ML Platform Features You Must Know to Build an ML System decodingml.substack.com Copy link Facebook Email Note Other DML Top 6 ML Platform Features You Must Know to Build an ML System Why serving an ML model using a batch architecture is so powerful? Top 6 ML platform features you must know. Paul Iusztin Aug 31, 2023 3 Share this post DML Top 6 ML Platform Features You Must Know to Build an ML System decodingml.substack.com Copy link Facebook Email Note Other 2 Share _Hello there, I am Paul Iusztin _ _Within this newsletter, I will help you decode complex topics about ML MLOps one week at a time _ This week we will cover 1. Top 6 ML platform features you must know to build an ML system 2. Why serving an ML model using a batch architecture is so powerful? _Story _ I never forget anything said no one but your second brain. This week, no shameless promotion 1. Top 6 ML platform features you must know to build an ML system Here they are ùü≠. ùóòùòÖùóΩùó≤ùóøùó∂ùó∫ùó≤ùóªùòÅ ùóßùóøùóÆùó∞ùó∏ùó∂ùóªùó¥ In your ML development phase, you generate lots of experiments. Tracking and comparing the metrics between them is crucial in finding the optimal model. ùüÆ. ùó†ùó≤ùòÅùóÆùó±ùóÆùòÅùóÆ ùó¶ùòÅùóºùóøùó≤ Its primary purpose is reproducibility. To know how a model was generated, you need to know the version of the code the version of the packages hyperparameters config total compute version of the dataset ... and more ùüØ. ùó©ùó∂ùòÄùòÇùóÆùóπùó∂ùòÄùóÆùòÅùó∂ùóºùóªùòÄ Most of the time, along with the metrics, you must log a set of visualizations for your experiment. Such as images videos prompts t SNE graphs 3D point clouds ... and more ùü∞. ùó•ùó≤ùóΩùóºùóøùòÅùòÄ You don t work in a vacuum. You have to present your work to other colleges or clients. A report lets you take the metadata and visualizations from your experiment... ...and create, deliver and share a targeted presentation for your clients or peers. ùü±. ùóîùóøùòÅùó∂ùó≥ùóÆùó∞ùòÅùòÄ The most powerful feature out of them all. An artifact is a versioned object that is an input or output for your task. Everything can be an artifact, but the most common cases are data model code Wrapping your assets around an artifact ensures reproducibility. For example, you wrap your features into an artifact e.g., features 3.1.2 , which you can consume into your ML development step. The ML development step will generate config e.g., config 1.2.4 and code e.g., code 1.0.2 artifacts used in the continuous training pipeline. Doing so lets you quickly respond to questions such as What I used to generate the model? and What Version? ùü≤. ùó†ùóºùó±ùó≤ùóπ ùó•ùó≤ùó¥ùó∂ùòÄùòÅùóøùòÜ The model registry is the ultimate way to make your model accessible to your production ecosystem. For example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry e.g., model 1.2.4 . You label this model as staging under a new version and prepare it for testing. If the tests pass, mark it as production under a new version and prepare it for deployment e.g., model 2.1.5 . Top 6 ML platform features you must know Image by the Author . . All of these features are used in a mature ML system. What is your favorite one? You can see all these features in action in my The Full Stack 7 Steps MLOps Framework FREE course. 2. Why serving an ML model using a batch architecture is so powerful? When you first start deploying your ML model, you want an initial end to end flow as fast as possible. Doing so lets you quickly provide value, get feedback, and even collect data. . But here is the catch... Successfully serving an ML model is tricky as you need many iterations to optimize your model to work in real time low latency high throughput Initially, serving your model in batch mode is like a hack. By storing the model s predictions in dedicated storage, you automatically move your model from offline mode to a real time online model. Thus, you no longer have to care for your model s latency and throughput. The consumer will directly load the predictions from the given storage. ùêìùê°ùêûùê¨ùêû ùêöùê´ùêû ùê≠ùê°ùêû ùê¶ùêöùê¢ùêß ùê¨ùê≠ùêûùê©ùê¨ ùê®ùêü ùêö ùêõùêöùê≠ùêúùê° ùêöùê´ùêúùê°ùê¢ùê≠ùêûùêúùê≠ùêÆùê´ùêû extracts raw data from a real data source clean, validate, and aggregate the raw data within a feature pipeline load the cleaned data into a feature store experiment to find the best model transformations using the data from the feature store upload the best model from the training pipeline into the model registry inside a batch prediction pipeline, use the best model from the model registry to compute the predictions store the predictions in some storage the consumer will download the predictions from the storage repeat the whole process hourly, daily, weekly, etc. it depends on your context . ùòõùò©ùò¶ ùòÆùò¢ùò™ùòØ ùò•ùò∞ùò∏ùòØùò¥ùò™ùò•ùò¶ of deploying your model in batch mode is that the predictions will have a level of lag. For example, in a recommender system, if you make your predictions daily, it won t capture a user s behavior in real time, and it will update the predictions only at the end of the day. Moving to other architectures, such as request response or streaming, will be natural after your system matures in batch mode. ML Batch Architecture Design Image by the Author . So remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience. _Story _ I never forget anything said no one but your second brain. After 6 months of refinement, this is my second brain strategy Tiago s Forte book inspired me, but I adapted his system to my needs. . ùü¨. ùóñùóºùóπùóπùó≤ùó∞ùòÅ This is where you are bombarded with information from all over the place. ùü≠. ùóßùóµùó≤ ùóöùóøùóÆùòÉùó≤ùòÜùóÆùóøùó± This is where I save everything that looks interesting. I won t use 90 of what is here, but it satisfied my urge to save that cool article I saw on LinkedIn. Tools Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists, etc. ùüÆ. ùóßùóµùó≤ ùóïùóºùóÆùóøùó± Here, I start converging the information and planning what to do next. Tools Notion ùüØ. ùóßùóµùó≤ ùóôùó∂ùó≤ùóπùó± Here is where I express myself through learning, coding, writing, etc. Tools whatever you need to express yourself. 2 3 are iterative processes. Thus I often bounce between them until the information is distilled. ùü∞. ùóßùóµùó≤ ùó™ùóÆùóøùó≤ùóµùóºùòÇùòÄùó≤ Here is where I take the distilled information and write it down for cold storage. Tools Notion, Google Drive . When I want to search for a piece of information, I start from the Warehouse and go backward until I find what I need. As a minimalist, I kept my tools to a minimum. I primarily use only Brave, Notion, and Google Drive. You don t need 100 tools to be productive. They just want to take your money from you. My second brain strategy Image by the Author . So remember... You have to collect link plan distill store That s it for today See you next Thursday at 9 00 am CET. Have a fantastic weekend! Paul Whenever you re ready, here is how I can help you 1. The Full Stack 7 Steps MLOps Framework a 7 lesson FREE course that will walk you step by step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code 2.5 hours of reading video materials on Medium. 2. Machine Learning MLOps Blog here, I approach in depth topics about designing and productionizing ML systems using MLOps. 3. Machine Learning MLOps Hub a place where I will constantly aggregate all my work courses, articles, webinars, podcasts, etc. , 3 Share this post DML Top 6 ML Platform Features You Must Know to Build an ML System decodingml.substack.com Copy link Facebook Email Note Other 2 Share PreviousNext Discussion about this post Comments Restacks Ahmed BesbesThe Tech Buffet Aug 31, 2023Liked by Paul IusztinHello Paul! Great newsletter. It d be even more useful to suggest tools for each of these features e.g. the model registry, the feature store, etc Expand full commentReplyShare 1 reply by Paul Iusztin 1 more comment... Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Note Other This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en', platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-top-6-ml-platform-features-you?r=1ttoeh')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for document in documents:\n",
    "    cleaned_document = CleaningDispatcher.dispatch(document)\n",
    "    cleaned_documents.append(cleaned_document)\n",
    "cleaned_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d9eea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_metadata(cleaned_documents: list[CleanedDocument]) -> dict:\n",
    "    metadata = {\"num_documents\": len(cleaned_documents)}\n",
    "    for document in cleaned_documents:\n",
    "        category = document.get_category()\n",
    "        if category not in metadata:\n",
    "            metadata[category] = {}\n",
    "        if \"authors\" not in metadata[category]:\n",
    "            metadata[category][\"authors\"] = list()\n",
    "\n",
    "        metadata[category][\"num_documents\"] = metadata[category].get(\"num_documents\", 0) + 1\n",
    "        metadata[category][\"authors\"].append(document.author_full_name)\n",
    "\n",
    "    for value in metadata.values():\n",
    "        if isinstance(value, dict) and \"authors\" in value:\n",
    "            value[\"authors\"] = list(set(value[\"authors\"]))\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f580b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_documents': 50,\n",
       " <DataCategory.ARTICLES: 'articles'>: {'authors': ['Paul Iusztin'],\n",
       "  'num_documents': 50}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_metadata(cleaned_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693b056",
   "metadata": {},
   "source": [
    "## Chunk and embed cleaned documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f611f",
   "metadata": {},
   "source": [
    "### Chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6f377cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 1000\n",
    "max_length = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54c10547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin From data gathering to productionizing LLMs using LLMOps good practices. End to End Framework for Production Ready LLMs Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course Building Your Production Ready AI ReplicaAn End to End Framework for Production Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin FollowPublished inDecoding ML 16 min read Mar 16, 20242.1K13ListenShare the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL EWhy is this course different?By finishing the LLM Twin Building Your Production Ready AI Replica free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_content = cleaned_documents[0].content\n",
    "cleaned_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96fa9abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin From data gathering to productionizing LLMs using LLMOps good practices.',\n",
       " 'End to End Framework for Production Ready LLMs Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course Building Your Production Ready AI ReplicaAn End to End Framework for Production Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin FollowPublished inDecoding ML 16 min read Mar 16, 20242.1K13ListenShare the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin?',\n",
       " 'It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL EWhy is this course different?By finishing the LLM Twin Building Your Production Ready AI Replica free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care?',\n",
       " 'No more isolated scripts or Notebooks!',\n",
       " 'Learn production ML by building and deploying an end to end production grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real world LLM system from start to finish from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal?']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", cleaned_content)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e05e168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin From data gathering to productionizing LLMs using LLMOps good practices. End to End Framework for Production Ready LLMs Decoding MLOpen in appSign upSign inWriteSign upSign inTop highlightLLM Twin Course Building Your Production Ready AI ReplicaAn End to End Framework for Production Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin FollowPublished inDecoding ML 16 min read Mar 16, 20242.1K13ListenShare the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL EWhy is this course different?By finishing the LLM Twin Building Your Production Ready AI Replica free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real world LLM system from start to finish from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices the data collection pipeline crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. deployed on AWS the feature pipeline consume messages from a queue through a Bytewax streaming pipeline.',\n",
       " 'Every message will be cleaned, chunked, embedded using Superlinked , and loaded into a Qdrant vector DB in real time. deployed on AWS the training pipeline create a custom dataset based on your digital data. Fine tune an LLM using QLoRA. Use Comet ML s experiment tracker to monitor the experiments. Evaluate and save the best model to Comet s model registry. deployed on Qwak the inference pipeline load and quantize the fine tuned LLM from Comet s model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comet s prompt monitoring dashboard. deployed on Qwak LLM twin system architecture Image by the Author Along the 4 microservices, you will learn to integrate 3 serverless tools Comet ML as your ML Platform Qdrant as your vector DB Qwak as your ML infrastructure Who is this for?Audience MLE, DE, DS, or SWE who want to learn to engineer production ready LLM systems using LLMOps good principles.Level intermediatePrerequisites basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 10 hands on written lessons and the open source code you can access on GitHub, showing how to build an end to end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace. To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms AWS, Qwak have a pay as you go pricing plan. Qwak offers a few hours of free computing.',\n",
       " 'Every Medium article will be its own lesson An End to End Framework for Production Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture Enabling Event Driven ArchitecturesSOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time!The 4 Advanced RAG Algorithms You Must Know to ImplementThe Role of Feature Stores in Fine Tuning LLMsHow to fine tune LLMs on custom datasets at Scale using Qwak and CometMLBest Practices When Evaluating Fine Tuned LLMsArchitect scalable and cost effective LLM RAG inference pipelinesHow to evaluate your RAG pipeline using the RAGAs Framework Bonus Build a scalable RAG ingestion pipeline using 74.3 less code Bonus Build Multi Index Advanced RAG Apps Check out the code on GitHub 1 and support us with a Let s start with Lesson 1 Lesson 1 End to end framework for production ready LLM systemsIn the first lesson, we will present the project you will build during the course your production ready LLM Twin AI replica.Afterward, we will explain what the 3 pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3 pipeline architecture to our LLM microservices.In the following lessons, we will examine each component s code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture Image by the Author Table of ContentsWhat are you going to build? The LLM twin conceptThe 3 pipeline architectureLLM twin system design Check out the code on GitHub 1 and support us with a 1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica.',\n",
       " 'We will use an LLM to do that, hence the name of the course LLM Twin Building Your Production Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles like this one using your own voice.Why not directly use ChatGPT? You may ask When trying to generate an article or post using an LLM, the results tend to be very generic and unarticulated,contain misinformation due to hallucination ,require tedious prompting to achieve the desired result.But here is what we are going to do to fix that First, we will fine tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here 2 .To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs e.g., what ChatGPT will do , it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to Write me a 1000 word LinkedIn post about LLMs based on the article from this link URL . Excited? Let s get started 2.',\n",
       " 'The 3 pipeline architectureWe all know how messy ML systems can get. That is where the 3 pipeline architecture kicks in.The 3 pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54 of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are the model is not mature enoughsecurity risks e.g., data privacy not enough dataTo some extent, these are true.But the reality is that in many scenarios the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well defined architecture are as crucial as using suitable tools and models with high accuracy.Solution The 3 pipeline architectureLet s understand what the 3 pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components 1. the feature pipeline2. the training pipeline3. the inference pipeline also known as the Feature Training Inference FTI architecture. 1. The feature pipeline transforms your data into features labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store. 2. The training pipeline ingests a specific version of the features labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry. 3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry.',\n",
       " 'Its final goal is to output the predictions to a client.The 3 pipeline architecture Image by the Author .This is why the 3 pipeline design is so beautiful it is intuitive it brings structure, as on a higher level, all ML systems can be reduced to these 3 components it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate the ML system has been built with modularity in mind since the beginning the 3 components can easily be divided between multiple teams if necessary every component can use the best stack of technologies available for the job every component can be deployed, scaled, and monitored independently the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that by following this pattern, you know 100 that your ML model will move out of your Notebooks into production. If you want to learn more about the 3 pipeline design, I recommend this excellent article 3 written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet s understand how to apply the 3 pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture Image by the Author As you can see, the data collection pipeline doesn t follow the 3 pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let s say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let s zoom in on each component to understand how they work individually and interact with each other.',\n",
       " '3.1. The data collection pipelineIts scope is to crawl data for a given user from Medium articles Substack articles LinkedIn posts GitHub code As every platform is unique, we implemented a different Extract Transform Load ETL pipeline for each website. 1 min read on ETL pipelines 4 However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean normalize the extracted HTMLsave the normalized but still raw data to Mongo DBImportant note We are crawling only our data, as most platforms do not allow us to access other people s data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data aka text .How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture CDC pattern to inform the feature pipeline of any change on our Mongo DB. 1 min read on the CDC pattern 5 To explain the CDC briefly, a watcher listens 24 7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline our data pipeline to a streaming pipeline our feature pipeline .Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB.',\n",
       " 'This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax a Rust streaming engine with a Python interface . Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn t care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and clean the data chunk it embed it using the embedding models from Superlinked load it to the Qdrant vector DB.Every type of data post, article, code will be processed independently through its own set of classes.Even though all of them are text based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data 1.',\n",
       " 'The cleaned data without using vectors as indexes store them in a NoSQL fashion .2. The cleaned, chunked, and embedded data leveraging the vector indexes of Qdrant The training pipeline needs access to the data in both formats as we want to fine tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts aka RAG .Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren t scalable when working with big data.Using CDC a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust s impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store.',\n",
       " 'In our case, the feature store is the Qdrant vector DB that contains the cleaned digital data from which we will create prompts answers we will use the chunked embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data posts, articles, code .We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data to prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine tuning module that inputs a HuggingFace dataset and uses QLoRA to fine tune a given LLM e.g., Mistral . By using HuggingFace, we can easily switch between different LLMs so we won t focus too much on any specific LLM.All the experiments will be logged into Comet ML s experiment tracker.We will use a bigger LLM e.g., GPT4 to evaluate the results of our fine tuned LLM. These results will be logged into Comet s experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet s prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models.',\n",
       " 'It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following experiment tracker model registry prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine tuned LLM?The fine tuned LLM will always be downloaded from the model registry based on its tag e.g., accepted and version e.g., v1.0.2, latest, etc. .How will the fine tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM s speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG.',\n",
       " 'This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet s prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to Write a 1000 word LinkedIn post about LLMs, and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin Building Your Production Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3 pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS. Check out the code on GitHub 1 and support us with a Have you enjoyed this article? Then Join 5k engineers in the ùóóùó≤ùó∞ùóºùó±ùó∂ùóªùó¥ ùó†ùóü ùó°ùó≤ùòÑùòÄùóπùó≤ùòÅùòÅùó≤ùóø for battle tested content on production grade ML. ùóòùòÉùó≤ùóøùòÜ ùòÑùó≤ùó≤ùó∏ Decoding ML Newsletter Paul Iusztin SubstackJoin for battle tested content on designing, coding, and deploying production grade ML MLOps systems. Every week.',\n",
       " 'Find your audience.Sign up for freeMembershipRead member only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for 5 monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.1K2.1K13FollowWritten by Paul Iusztin5.1K Followers Editor for Decoding MLSenior ML MLOps Engineer Founder Decoding ML Content about building production grade ML AI systems DML Newsletter https decodingml.substack.comFollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post retrieval algorithmMay 41.8K12Paul IusztininDecoding MLThe 6 MLOps foundational principlesThe core MLOps guidelines for production MLSep 21442Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataMar 236725Paul IusztininDecoding MLArchitect scalable and cost effective LLM RAG inference pipelinesDesign, build and deploy RAG inference pipeline using LLMOps best practices.Jun 15601See all from Paul IusztinSee all from Decoding MLRecommended from MediumVishal RajputinAIGuysWhy GEN AI Boom Is Fading And What s Next?Every technology has its hype and cool down period.Sep 42.3K72DerckData architecture for MLOps Metadata storeIntroductionJul 17ListsAI Regulation6 stories 593 savesNatural Language Processing1766 stories 1367 savesPredictive Modeling w Python20 stories 1607 savesPractical Guides to Machine Learning10 stories 1961 savesIda Silfverski√∂ldinLevel Up CodingAgentic AI Build a Tech Research AgentUsing a custom data pipeline with millions of textsSep 679610Alex RazvantinDecoding MLHow to fine tune LLMs on custom datasets at Scale using Qwak and CometMLHow to fine tune a Mistral7b Instruct using PEFT QLoRA, leveraging best MLOps practices deploying on Qwak.ai and tracking with CometML.May 185922Vipra SinghBuilding LLM Applications Serving LLMs Part 9 Learn Large Language Models LLM through the lens of a Retrieval Augmented Generation RAG Application.Apr 188666Steve HeddeninTowards Data ScienceHow to Implement Graph RAG Using Knowledge Graphs and Vector DatabasesA Step by Step Tutorial on Implementing Retrieval Augmented Generation RAG , Semantic Search, and RecommendationsSep 61.4K18See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams To make Medium work, we log user data.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracts = []\n",
    "current_chunk = \"\"\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    if not sentence:\n",
    "        continue\n",
    "\n",
    "    if len(current_chunk) + len(sentence) <= max_length:\n",
    "        current_chunk += sentence + \" \"\n",
    "    else:\n",
    "        if len(current_chunk) >= min_length:\n",
    "            extracts.append(current_chunk.strip())\n",
    "        current_chunk = sentence + \" \"\n",
    "\n",
    "if len(current_chunk) >= min_length:\n",
    "    extracts.append(current_chunk.strip())\n",
    "\n",
    "extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7965f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering-tL-FPc5M-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
