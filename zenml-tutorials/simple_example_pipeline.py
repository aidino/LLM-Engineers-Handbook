#!/usr/bin/env python3
"""
Simple ZenML Pipeline - KhÃ´ng cáº§n pandas, trÃ¡nh lá»—i _ctypes

Cháº¡y pipeline nÃ y:
1. cd learning-code  
2. poetry run python simple_example_pipeline.py

Pipeline nÃ y sá»­ dá»¥ng pure Python vÃ  built-in libraries
"""

import json
import random
from datetime import datetime
from typing import Any, Dict, List

from zenml import pipeline, step

# ===============================
# STEPS: CÃ¡c bÆ°á»›c xá»­ lÃ½ Ä‘Æ¡n giáº£n
# ===============================

@step
def generate_simple_data() -> Dict[str, Any]:
    """
    ğŸ² Táº¡o dá»¯ liá»‡u Ä‘Æ¡n giáº£n vá»›i pure Python
    """
    print("ğŸ“Š Äang táº¡o dá»¯ liá»‡u máº«u vá»›i pure Python...")
    
    # Set seed cho reproducible results
    random.seed(42)
    
    # Táº¡o 100 samples
    n_samples = 100
    data = {
        'samples': [],
        'metadata': {
            'count': n_samples,
            'created_at': datetime.now().isoformat(),
            'features': ['value', 'category', 'score']
        }
    }
    
    # Táº¡o samples
    categories = ['A', 'B', 'C']
    for i in range(n_samples):
        sample = {
            'id': i,
            'value': random.uniform(0, 100),
            'category': random.choice(categories),
            'score': random.randint(1, 10)
        }
        data['samples'].append(sample)
    
    print(f"âœ… ÄÃ£ táº¡o {n_samples} samples")
    return data

@step
def process_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸ”„ Xá»­ lÃ½ dá»¯ liá»‡u Ä‘Æ¡n giáº£n
    """
    print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    
    samples = raw_data['samples']
    
    # TÃ­nh thá»‘ng kÃª cÆ¡ báº£n
    values = [s['value'] for s in samples]
    scores = [s['score'] for s in samples]
    
    # TÃ­nh mean, min, max manually
    value_mean = sum(values) / len(values)
    value_min = min(values)
    value_max = max(values)
    
    score_mean = sum(scores) / len(scores)
    
    # Äáº¿m categories
    category_counts = {}
    for sample in samples:
        cat = sample['category']
        category_counts[cat] = category_counts.get(cat, 0) + 1
    
    # Táº¡o processed data
    processed_data = {
        'original_count': len(samples),
        'statistics': {
            'value_stats': {
                'mean': round(value_mean, 2),
                'min': round(value_min, 2),
                'max': round(value_max, 2),
                'range': round(value_max - value_min, 2)
            },
            'score_stats': {
                'mean': round(score_mean, 2),
                'min': min(scores),
                'max': max(scores)
            },
            'category_distribution': category_counts
        },
        'processed_samples': samples  # Keep original samples
    }
    
    print(f"âœ… ÄÃ£ xá»­ lÃ½ {len(samples)} samples")
    print(f"ğŸ“Š Value mean: {value_mean:.2f}, Score mean: {score_mean:.2f}")
    
    return processed_data

@step
def create_features(processed_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    âš™ï¸ Táº¡o features má»›i
    """
    print("âš™ï¸ Äang táº¡o features...")
    
    samples = processed_data['processed_samples']
    
    # Táº¡o features má»›i cho má»—i sample
    enhanced_samples = []
    for sample in samples:
        enhanced = sample.copy()
        
        # Feature engineering
        enhanced['value_normalized'] = sample['value'] / 100.0  # Normalize to 0-1
        enhanced['score_squared'] = sample['score'] ** 2
        enhanced['value_score_ratio'] = sample['value'] / sample['score'] if sample['score'] > 0 else 0
        
        # Categorical features
        enhanced['is_high_value'] = sample['value'] > 50
        enhanced['is_high_score'] = sample['score'] > 5
        enhanced['category_numeric'] = {'A': 1, 'B': 2, 'C': 3}[sample['category']]
        
        enhanced_samples.append(enhanced)
    
    feature_data = {
        'enhanced_samples': enhanced_samples,
        'feature_info': {
            'original_features': ['value', 'category', 'score'],
            'new_features': ['value_normalized', 'score_squared', 'value_score_ratio', 
                           'is_high_value', 'is_high_score', 'category_numeric'],
            'total_features': 9,
            'sample_count': len(enhanced_samples)
        },
        'statistics': processed_data['statistics']
    }
    
    print(f"âœ… ÄÃ£ táº¡o {len(feature_data['feature_info']['new_features'])} features má»›i")
    return feature_data

@step
def simple_model(feature_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸ¤– Model Ä‘Æ¡n giáº£n - rule-based classifier
    """
    print("ğŸ¤– Äang táº¡o model Ä‘Æ¡n giáº£n...")
    
    samples = feature_data['enhanced_samples']
    
    # Simple rule-based model: predict based on value and score
    predictions = []
    rules_used = {'high_value_high_score': 0, 'medium': 0, 'low': 0}
    
    for sample in samples:
        # Simple classification rules
        if sample['value'] > 70 and sample['score'] > 7:
            prediction = 'excellent'
            rules_used['high_value_high_score'] += 1
        elif sample['value'] > 40 or sample['score'] > 5:
            prediction = 'good'
            rules_used['medium'] += 1
        else:
            prediction = 'needs_improvement'
            rules_used['low'] += 1
        
        predictions.append({
            'id': sample['id'],
            'prediction': prediction,
            'confidence': min(sample['value'] + sample['score'] * 10, 100) / 100.0
        })
    
    # Calculate prediction distribution
    pred_counts = {}
    confidences = []
    for pred in predictions:
        pred_counts[pred['prediction']] = pred_counts.get(pred['prediction'], 0) + 1
        confidences.append(pred['confidence'])
    
    avg_confidence = sum(confidences) / len(confidences)
    
    model_info = {
        'model_type': 'rule_based_classifier',
        'predictions': predictions,
        'model_stats': {
            'prediction_distribution': pred_counts,
            'average_confidence': round(avg_confidence, 3),
            'rules_usage': rules_used,
            'total_predictions': len(predictions)
        },
        'rules': {
            'excellent': 'value > 70 AND score > 7',
            'good': 'value > 40 OR score > 5',
            'needs_improvement': 'else'
        }
    }
    
    print(f"âœ… Model completed. Avg confidence: {avg_confidence:.3f}")
    print(f"ğŸ“Š Predictions: {pred_counts}")
    
    return model_info

@step
def evaluate_model(model_info: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸ“ˆ ÄÃ¡nh giÃ¡ model Ä‘Æ¡n giáº£n
    """
    print("ğŸ“ˆ Äang Ä‘Ã¡nh giÃ¡ model...")
    
    predictions = model_info['predictions']
    
    # Simple evaluation metrics
    high_confidence_count = sum(1 for p in predictions if p['confidence'] > 0.7)
    medium_confidence_count = sum(1 for p in predictions if 0.4 <= p['confidence'] <= 0.7)
    low_confidence_count = sum(1 for p in predictions if p['confidence'] < 0.4)
    
    confidence_scores = [p['confidence'] for p in predictions]
    avg_confidence = sum(confidence_scores) / len(confidence_scores)
    min_confidence = min(confidence_scores)
    max_confidence = max(confidence_scores)
    
    evaluation_results = {
        'confidence_analysis': {
            'average': round(avg_confidence, 3),
            'min': round(min_confidence, 3),
            'max': round(max_confidence, 3),
            'high_confidence_count': high_confidence_count,
            'medium_confidence_count': medium_confidence_count,
            'low_confidence_count': low_confidence_count
        },
        'prediction_quality': {
            'total_predictions': len(predictions),
            'confidence_distribution': {
                'high': high_confidence_count,
                'medium': medium_confidence_count,
                'low': low_confidence_count
            }
        },
        'model_performance': {
            'reliability_score': round(high_confidence_count / len(predictions), 3),
            'coverage': 1.0,  # Model predicts for all samples
            'model_type': model_info['model_type']
        }
    }
    
    print(f"âœ… Evaluation completed. Reliability: {evaluation_results['model_performance']['reliability_score']:.3f}")
    return evaluation_results

@step
def create_summary(
    original_data: Dict[str, Any],
    processed_data: Dict[str, Any], 
    feature_data: Dict[str, Any],
    model_info: Dict[str, Any],
    evaluation: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ğŸ“‹ Tá»•ng há»£p káº¿t quáº£ cuá»‘i cÃ¹ng
    """
    print("ğŸ“‹ Äang táº¡o summary...")
    
    summary = {
        'pipeline_info': {
            'name': 'simple_ml_pipeline',
            'execution_time': datetime.now().isoformat(),
            'status': 'completed_successfully'
        },
        'data_summary': {
            'original_samples': original_data['metadata']['count'],
            'processed_samples': processed_data['original_count'],
            'features_created': len(feature_data['feature_info']['new_features']),
            'total_features': feature_data['feature_info']['total_features']
        },
        'model_summary': {
            'type': model_info['model_type'],
            'predictions_made': model_info['model_stats']['total_predictions'],
            'average_confidence': model_info['model_stats']['average_confidence'],
            'prediction_distribution': model_info['model_stats']['prediction_distribution']
        },
        'evaluation_summary': {
            'reliability_score': evaluation['model_performance']['reliability_score'],
            'high_confidence_predictions': evaluation['confidence_analysis']['high_confidence_count'],
            'average_confidence': evaluation['confidence_analysis']['average'],
        },
        'insights': {
            'most_common_prediction': max(model_info['model_stats']['prediction_distribution'].items(), 
                                        key=lambda x: x[1])[0],
            'confidence_level': 'high' if evaluation['confidence_analysis']['average'] > 0.7 else 
                              'medium' if evaluation['confidence_analysis']['average'] > 0.4 else 'low',
            'data_quality': 'good'  # Simple assessment
        }
    }
    
    print("âœ… Pipeline hoÃ n thÃ nh thÃ nh cÃ´ng!")
    print(f"ğŸ“Š Processed {summary['data_summary']['original_samples']} samples")
    print(f"ğŸ¤– Model reliability: {summary['evaluation_summary']['reliability_score']:.3f}")
    print(f"ğŸ¯ Most common prediction: {summary['insights']['most_common_prediction']}")
    
    return summary

# ===============================
# PIPELINE: Káº¿t há»£p cÃ¡c steps
# ===============================

@pipeline
def simple_ml_pipeline() -> Dict[str, Any]:
    """
    ğŸš€ Simple ML Pipeline - Pure Python, khÃ´ng cáº§n pandas
    
    Luá»“ng xá»­ lÃ½:
    1. ğŸ“Š Táº¡o dá»¯ liá»‡u vá»›i pure Python
    2. ğŸ”„ Xá»­ lÃ½ vÃ  tÃ­nh thá»‘ng kÃª  
    3. âš™ï¸ Feature engineering
    4. ğŸ¤– Rule-based model
    5. ğŸ“ˆ Evaluation
    6. ğŸ“‹ Summary
    """
    # Pipeline steps
    raw_data = generate_simple_data()
    processed_data = process_data(raw_data)
    feature_data = create_features(processed_data)
    model_info = simple_model(feature_data)
    evaluation = evaluate_model(model_info)
    summary = create_summary(raw_data, processed_data, feature_data, model_info, evaluation)
    
    return summary

# ===============================
# MAIN: Cháº¡y pipeline  
# ===============================

if __name__ == "__main__":
    print("ğŸš€ Báº¯t Ä‘áº§u cháº¡y Simple ML Pipeline (Pure Python)...")
    print("ğŸ”§ Pipeline nÃ y khÃ´ng sá»­ dá»¥ng pandas Ä‘á»ƒ trÃ¡nh lá»—i _ctypes")
    print("=" * 70)
    
    try:
        # Cháº¡y pipeline
        result = simple_ml_pipeline()
        
        print("=" * 70)
        print("ğŸ‰ Pipeline hoÃ n thÃ nh thÃ nh cÃ´ng!")
        print("\nğŸ“Š Káº¿t quáº£ cuá»‘i cÃ¹ng:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        print("\nğŸ” Äá»ƒ xem chi tiáº¿t pipeline:")
        print("- Web UI: http://127.0.0.1:8237/")
        print("- CLI: poetry run zenml pipeline runs list")
        
        print("\nğŸ’¡ Tip: Pipeline nÃ y sá»­ dá»¥ng pure Python, khÃ´ng cáº§n pandas!")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi cháº¡y pipeline: {e}")
        print(f"ğŸ” Chi tiáº¿t lá»—i: {type(e).__name__}")
        print("\nğŸ› ï¸ Troubleshooting:")
        print("1. Kiá»ƒm tra ZenML server: poetry run zenml status")
        print("2. Kiá»ƒm tra connection: poetry run zenml login http://127.0.0.1:8237 --api-key --no-verify-ssl")
        print("3. Thá»­ cháº¡y láº¡i: poetry run python simple_example_pipeline.py")
        import traceback
        print(f"\nğŸš¨ Full traceback:\n{traceback.format_exc()}")
        raise 